{
    "AAAI Conference on Artificial Intelligence": "The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually. Along with ICML, NeurIPS and ICLR, it is one of the primary conferences of high impact in machine learning and artificial intelligence research. It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.During AAAI-20 conference, AI pioneers and 2018 Turing Award winners Yann LeCun and Yoshua Bengio among eight other researchers were honored as the AAAI 2020 Fellows.Along with other conferences such as NeurIPS, ICML, AAAI uses artificial intelligence algorithm to assign papers to reviewers.\n\nLocations\nAAAI-2023 Washington Convention Center, Washington, D.C., United States\n AAAI-2022 Virtual Conference\n AAAI-2021 Virtual Conference\n AAAI-2020 Hilton New York Midtown, New York, New York, United States\n AAAI-2019 Hilton Hawaiian Village, Honolulu, Hawaii, United States\n AAAI-2018 Hilton New Orleans Riverside, New Orleans, Louisiana, United States\n AAAI-2017 San Francisco, California, United States\n AAAI-2016 Phoenix, Arizona, United States\n AAAI-2015 Austin, Texas, United States\n AAAI-2014 Québec Convention Center, Québec City, Québec, Canada\n AAAI-2013 Bellevue, Washington, United States\n AAAI-2012 Toronto, Ontario, Canada\n AAAI-2011 San Francisco, California, United States\n AAAI-2010 Westin Peachtree Plaza, Atlanta, Georgia, United States\n AAAI-2008 Chicago, Illinois, United States\n AAAI-2007 Toronto, Ontario, Canada\n AAAI-2006 Boston, Massachusetts, United States\n AAAI-2005 Pittsburgh, Pennsylvania, United States\n AAAI-2004 San Jose, California, United States\n AAAI-2002 Shaw conference center in Edmonton, Alberta, Canada\n AAAI-2000 Austin, Texas, United States\n AAAI-1999 Orlando, Florida, United States\n AAAI-1998 Madison, Wisconsin, United States\n AAAI-1997 Providence, Rhode Island, United States\n AAAI-1996 Portland, Oregon, United States\n AAAI-1994 Seattle, Washington, United States\n AAAI-1993 Washington Convention Center, Washington, D.C., United States\n AAAI-1992 San Jose Convention Center, San Jose, California, United States\n AAAI-1991 Anaheim Convention Center, Anaheim, California, United States\n AAAI-1990 Boston, Massachusetts, United States\n AAAI-1988 Saint Paul, Minnesota, United States\n AAAI-1987 Seattle, Washington, United States\n AAAI-1986 Philadelphia, Pennsylvania, United States\n AAAI-1984 University of Texas, Austin, Texas, United States\n AAAI-1983 Washington, D.C., United States\n AAAI-1982 Carnegie Mellon University and the University of Pittsburgh, Pittsburgh, Pennsylvania, United States\n AAAI-1980 Stanford, California, United States\n\nSee also\nICML\nICLR\nJournal of Machine Learning Research\nMachine Learning (journal)\nNeurIPS\n\nReferences\nExternal links\nOfficial website",
    "ACM Computing Classification System": "The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.\n\nHistory\nThe system has gone through seven revisions, the first version being published in 1964, and revised versions appearing in 1982, 1983, 1987, 1991, 1998, and the now current version in 2012.\n\nStructure\nIt is hierarchically structured in four levels. For example, one branch of the hierarchy contains:\n\nComputing methodologies\nArtificial intelligence\nKnowledge representation and reasoning\nOntology engineering\n\nSee also\nComputer Science Ontology\nPhysics and Astronomy Classification Scheme\narXiv, a preprint server allowing submitted papers to be classified using the ACM CCS\nPhysics Subject Headings\n\nReferences\nCoulter, Neal (1997), \"ACM's computing classification system reflects changing times\", Communications of the ACM, New York, NY, USA: ACM, 40 (12): 111–112, doi:10.1145/265563.265579, S2CID 42548816.\nCoulter, Neal (chair); French, James; Glinert, Ephraim; Horton, Thomas; Mead, Nancy; Ralston, Anthony; Rada, Roy; Rodkin, Craig; Rous, Bernard; Tucker, Allen; Wegner, Peter; Weiss, Eric; Wierzbicki, Carol (January 21, 1998), \"Computing Classification System 1998: Current Status and Future Maintenance Report of the CCS Update Committee\" (PDF), Computing Reviews, New York, NY, USA: ACM: 1–5.\nMirkin, Boris; Nascimento, Susana; Pereira, Luis Moniz (2008), \"Representing a Computer Science Research Organization on the ACM Computing Classification System\",  in Eklund, Peter; Haemmerlé, Ollivier (eds.), Supplementary Proceedings of the 16th International Conference on Conceptual Structures (ICCS-2008) (PDF), CEUR Workshop Proceedings, vol. 354, RWTH Aachen University, pp. 57–65.\n\nExternal links\ndl.acm.org/ccs is the homepage of the system, including links to four complete versions of the system:\nthe 1964 version Archived 2016-12-01 at the Wayback Machine\nthe 1991 version Archived 2017-09-21 at the Wayback Machine\nthe 1998 version\nthe current 2012 version.\nThe ACM Computing Research Repository uses a classification scheme that is much coarser than the ACM subject classification, and does not cover all areas of CS, but is intended to better cover active areas of research. In addition, papers in this repository are classified according to the ACM subject classification.",
    "ACM Computing Surveys": "ACM Computing Surveys is a quarterly peer-reviewed scientific journal published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.According to the Journal Citation Reports, the journal has a 2021 impact factor of 14.324. In a 2008 ranking of computer science journals, ACM Computing Surveys received the highest rank \"A*\".\n\nSee also\nACM Computing Reviews\n\nReferences\nExternal links\nOfficial website",
    "ADALINE": "ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors. It was developed by professor Bernard Widrow and his doctoral student Ted Hoff at Stanford University in 1960. It is based on the perceptron. It consists of a weight, a bias and a summation function.\nThe difference between Adaline and the standard (McCulloch–Pitts) perceptron is in how they learn. Adaline unit weights are adjusted to match a teacher signal, before applying the Heaviside function (see figure), but the standard perceptron unit weights are adjusted to match the correct output, after applying the Heaviside function.\nA multilayer network of ADALINE units is a MADALINE.\n\nDefinition\nAdaline is a single layer neural network with multiple nodes where each node accepts multiple inputs and generates one output. Given the following variables as:\n\n  \n    x\n    x\n   is the input vector\n\n  \n    w\n    w\n   is the weight vector\n\n  \n    n\n    n\n   is the number of inputs\n\n  \n    θ\n    \\theta\n   some constant\n\n  \n    y\n    y\n   is the output of the modelthen we find that the output is \n  \n    \n      y\n      =\n      \n        ∑\n        \n          j\n          =\n          1\n        \n        \n          n\n        \n      \n      \n        x\n        \n          j\n        \n      \n      \n        w\n        \n          j\n        \n      \n      +\n      θ\n    \n    y=\\sum _{j=1}^{n}x_{j}w_{j}+\\theta\n  .  If we further assume that\n\n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x_{0}=1}\n  \n\n  \n    \n      \n        \n          w\n          \n            0\n          \n        \n        =\n        θ\n      \n    \n    {\\displaystyle w_{0}=\\theta }\n  then the output further reduces to: \n  \n    \n      \n        y\n        =\n        \n          ∑\n          \n            j\n            =\n            0\n          \n          \n            n\n          \n        \n        \n          x\n          \n            j\n          \n        \n        \n          w\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle y=\\sum _{j=0}^{n}x_{j}w_{j}}\n\nLearning rule\nThe learning rule used by ADALINE is the LMS (\"least mean squares\") algorithm, a special case of gradient descent. \nDefine the following notations:\n\n  \n    η\n    \\eta\n   is the learning rate (some positive constant)\n\n  \n    y\n    y\n   is the output of the model\n\n  \n    o\n    o\n   is the target (desired) output\n\n  \n    \n      E\n      =\n      (\n      o\n      −\n      y\n      \n        )\n        \n          2\n        \n      \n    \n    E=(o-y)^{2}\n   is the square of the error.\nThe LMS algorithm updates the weights by \nThis update rule minimizes \n  \n    E\n    E\n  , the square of the error, and is in fact the stochastic gradient descent update for linear regression.\n\nMADALINE\nMADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors. Three different training algorithms for MADALINE networks, which cannot be learned using backpropagation because the sign function is not differentiable, have been suggested, called Rule I, Rule II and Rule III. \nMADALINE Rule 1 (MRI) - The first of these dates back to 1962 and cannot adapt the weights of the hidden-output connection.MADALINE Rule 2 (MRII) - The second training algorithm improved on Rule I and was described in 1988. The Rule II training algorithm is based on a principle called \"minimal disturbance\". It proceeds by looping over training examples, then for each example, it:\n\nfinds the hidden layer unit (ADALINE classifier) with the lowest confidence in its prediction,\ntentatively flips the sign of the unit,\naccepts or rejects the change based on whether the network's error is reduced,\nstops when the error is zero.MADALINE Rule 3 - The third \"Rule\" applied to a modified network with sigmoid activations instead of signum; it was later found to be equivalent to backpropagation.Additionally, when flipping single units' signs does not drive the error to zero for a particular example, the training algorithm starts flipping pairs of units' signs, then triples of units, etc.\n\nSee also\nMultilayer perceptron\n\nReferences\nExternal links\n\"Delta Learning Rule: ADALINE\". Artificial Neural Networks. Universidad Politécnica de Madrid. Archived from the original on 2002-06-15.\n\"Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training\". Implementation of the ADALINE algorithm with memristors in analog computing.",
    "AI boom": "The AI boom (also known as the AI spring) refers to an ongoing period of rapid and unprecedented development in the field of artificial intelligence, with the generative AI race being a key component of this boom, which began in earnest with the founding of OpenAI in 2016 or 2017. OpenAI's generative AI systems, such as its various GPT models (starting in 2018) and DALL-E (2021), have played a significant role in driving this development.In 2022, large language models were improved to where they could be used for chatbot applications; text-to-image-models were at a point where they were almost indiscernible from human-made imagery; and speech synthesis software was able to replicate human speech efficiently.Over the course of late 2022 and 2023, dozens of new websites and AI chatbots were made live as Big Tech has tried to gain a foothold in the market and has led to an unprecedented increase in the ubiquity of AI tools.Public reaction to the AI boom has been mixed, with some parties hailing the new possibilities that AI creates, its potential for benefiting humanity, and sophistication, while other parties denounced it for threatening job security, being 'uncanny' in its responses, and for giving flawed responses based on the programming.\n\nLanguage models\nGPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text that can be hard to determine whether it was written by a human or not. An upgraded version called GPT-3.5 was used in ChatGPT, which later garnered attention for its detailed responses and articulate answers across many domains of knowledge. A new version called GPT-4 was released on March 14, 2023, and was used in the Microsoft Bing search engine. Other language models have been released such as PaLM by Google and LLaMA by Meta Platforms.\nIn January 2023, DeepL Write, an AI-based tool to improve monolingual texts, was released.\n\nText-to-image models\nOne of the first text-to-image models to capture widespread public attention was OpenAI's DALL-E, a transformer system announced in January 2021. A successor capable of generating more complex and realistic images, DALL-E 2, was unveiled in April 2022, followed by Stable Diffusion, an open source alternative, releasing in August 2022.Following other text-to-image models, language model-powered text-to-video platforms such as DAMO, Make-A-Video, Imagen Video and Phenaki can generate video from text and/or text/image prompts.\n\nSpeech synthesis\n15.ai was one of the first publicly available speech synthesis software that allowed people to generate natural emotive high-fidelity text-to-speech voices from an assortment of fictional characters from a variety of media sources. It was first released on March 2020. ElevenLabs unveiled a website where users are able to upload voice samples to that allowed it to generate voices from them. The company was criticized after users were able to abuse its software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals and raised concerns that it could be used to generate deepfakes that were more convincing. An unofficial song created using the voices of musicians Drake and The Weeknd in speech synthesis software raised questions about the ethics and legality of similar software.\n\nSee also\nAI winter, a period of reduced funding and interest in artificial intelligence research\nHistory of artificial intelligence\nHistory of artificial neural networks\nHype cycle\nTechnological singularity\n\n\n== References ==",
    "AI control problem": "In the field of artificial intelligence (AI), AI alignment research aims to steer AI systems towards humans' intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system is competent at advancing some objectives, but not the intended ones.It can be challenging for AI designers to align an AI system because it can be difficult for them to specify the full range of desired and undesired behaviors. To avoid this difficulty, they typically use simpler proxy goals, such as gaining human approval. However, this approach can create loopholes, overlook necessary constraints, or reward the AI system for just appearing aligned.Misaligned AI systems can malfunction or cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful ways (reward hacking). AI systems may also develop unwanted instrumental strategies such as seeking power or survival because such strategies help them achieve their given goals. Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is in deployment, where it faces new situations and data distributions.Today, these problems affect existing commercial systems such as language models, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected since these problems partially result from the systems being highly capable.Many leading AI scientists such as Geoffrey Hinton and Stuart Russell argue that AI is approaching superhuman capabilities and could endanger human civilization if misaligned.AI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and the social sciences, among others.\n\nThe Alignment Problem\nIn 1960, AI pioneer Norbert Wiener described the AI alignment problem as follows: \"If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively… we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\" Different definitions of AI alignment require that an aligned AI system advances different goals: the goals of its designers, its users or, alternatively, objective ethical standards, widely shared values, or the intentions its designers would have if they were more informed and enlightened.AI alignment is an open problem for modern AI systems and a research field within AI. Aligning AI involves two main challenges: carefully specifying the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment).\n\nSpecification gaming and side effects\nTo specify an AI system's purpose, AI designers typically provide an objective function, examples, or feedback to the system. However, AI designers are often unable to completely specify all important values and constraints, and so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or reward hacking, and is an instance of Goodhart's law. As AI systems become more capable, they are often able to game their specifications more effectively.\nSpecification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track; but the system achieved more reward by looping and crashing into the same targets indefinitely (see video). Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans; however, it learned to place its hand between the ball and camera, making it falsely appear successful (see video). Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora which are broad but fallible. When they are retrained to produce text humans rate as true or helpful, chatbots like ChatGPT can fabricate fake explanations that humans find convincing. Some alignment researchers aim to help humans detect specification gaming, and to steer AI systems towards carefully specified objectives that are safe and useful to pursue.\nWhen a misaligned AI system is deployed, it can cause consequential side effects. Social media platforms have been known to optimize for clickthrough rates, causing user addiction on a global scale. Stanford researchers comment that such recommender systems are misaligned with their users because they \"optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\".Explaining such side-effects, Berkeley computer scientist Stuart Russell noted that harm can result if implicit constraints are omitted during training: \"A system... will often set... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.\"Some researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov's Three Laws of Robotics). However, Russell and Norvig argued that this approach overlooks the complexity of human values: \"It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\"Additionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).\n\nPressure to deploy unsafe systems\nCommercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems. For example, the aforementioned social media recommender systems have been profitable despite creating unwanted addiction and polarization. In addition, competitive pressure can lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was over-sensitive and slowed down development.\n\nRisks from advanced misaligned AI\nSome researchers are interested in aligning increasingly advanced AI systems, as current progress in AI is rapid, and industry and governments are trying to build advanced AI. As AI systems become more advanced, they could unlock many opportunities if they are aligned but they may also become harder to align and could pose large-scale hazards.\n\nDevelopment of advanced AI\nLeading AI labs such as OpenAI and DeepMind have stated their aim to develop artificial general intelligence (AGI), a hypothesized AI system that matches or outperforms humans in a broad range of cognitive tasks. Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities. Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs. According to surveys, some leading machine learning researchers expect AGI to be created in this decade, some believe it will take much longer, and many consider both to be possible.In 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated that \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"\n\nPower-seeking\nCurrent systems still lack capabilities such as long-term planning and situational awareness. However, future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted power-seeking strategies. Future advanced AI agents might for example seek to acquire money and computation power, to proliferate, or to evade being turned off (for example by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents that have more power are better able to accomplish their goals. This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models. Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide range of environments. As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.Future power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them. Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.\n\nExistential risk\nAccording to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.In 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton, Alan Turing, Ilya Sutskever, Yoshua Bengio, Judea Pearl, Murray Shanahan, Norbert Wiener, Marvin Minsky, Francesca Rossi, Scott Aaronson, Bart Selman, David McAllester, Jürgen Schmidhuber, Marcus Hutter, Shane Legg, Eric Horvitz, and Stuart Russell. Skeptical researchers such as François Chollet, Gary Marcus, Yann LeCun, and Oren Etzioni have argued that AGI is far off, that it would not seek power (or might try but would fail), or that it will not be hard to align.\nOther researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes, and able to strategically mislead their designers as well as protect and increase their power and intelligence. Additionally, they could cause more severe side-effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise and therefore harder to align.\n\nResearch problems and approaches\nLearning human values and preferences\nAligning AI systems to act with regard to human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify. AI systems often learn to exploit even minor imperfections in the specified objective, a tendency known as specification gaming or reward hacking (which are instances of Goodhart's law). Researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning.: Chapter 7  A central open problem is scalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.Because it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse reinforcement learning (IRL) extends this by inferring the human's objective from the human's demonstrations.: 88  Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function. In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see § Power-seeking and instrumental strategies). However, IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.Other researchers explore how to teach complex behavior to AI models through preference learning, in which humans provide feedback on which behaviors they prefer. To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behaviors that humans would reward. Researchers at OpenAI used this approach to train chatbots like ChatGPT and InstructGPT, which produces more compelling text than models trained to imitate humans. Preference learning has also been an influential tool for recommender systems and web search. However, an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch to gain more reward. AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating echo chambers (see § Scalable oversight).\nLarge language models such as GPT-3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and to reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art large language models. Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless. Other avenues for aligning language models include values-targeted datasets and red-teaming. In red-teaming, another AI system or a human tries to find inputs for which the model's behavior is unsafe. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.Machine ethics supplements preference learning by directly instilling AI systems with moral values such as wellbeing, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises. While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that could apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions, revealed preferences, preferences the programmers would have if they were more informed or rational, or objective moral standards. Further challenges include aggregating the preferences of different people, and avoiding value lock-in: the indefinite preservation of the values of the first highly-capable AI systems, which are unlikely to fully represent human values.\n\nScalable oversight\nAs AI systems become more powerful and autonomous, it becomes more difficult to align them through human feedback. It can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books, writing code without subtle bugs or security vulnerabilities, producing statements that are not merely convincing but also true, and predicting long-term outcomes such as the climate or the results of a policy decision. More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans require assistance or extensive time. Scalable oversight studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.AI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As progressively more decisions will be made by AI systems, this may lead to a world that is increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance would have progressively less influence.Some AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball. Some AI systems have also learned to recognize when they are being evaluated, and \"play dead\", stopping unwanted behaviors only to continue them once evaluation ends. This deceptive specification gaming could become easier for more sophisticated future AI systems that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\nApproaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed. Another approach is to train a helper model (\"reward model\") to imitate the supervisor's feedback.However, when the task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants. Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate. Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them. Another proposal is to use an assistant AI system to point out flaws in AI-generated answers. To ensure that the assistant itself is aligned, this could be repeated in a recursive process: for example, two AI systems could critique each other's answers in a 'debate', revealing flaws to humans. OpenAI plans to use such scalable oversight approaches to help supervise superhuman AI and eventually build a superhuman automated AI alignment researcher.These approaches may also help with the following research problem, honest AI.\n\nHonest AI\nA growing area of research focuses on ensuring that AI is honest and truthful.Language models such as GPT-3 repeat falsehoods from their training data, and even confabulate new falsehoods. Such models are trained to imitate human writing as found across millions of books' worth of text from the Internet. However, this objective is not aligned with the generation of truth because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on such data therefore learn to mimic false statements.Additionally, models often obediently continue falsehoods when prompted, generate empty explanations for their answers, and produce outright fabrications that may appear plausible.Research on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability. Researchers from OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.As AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly match their stated views to the user's opinions, regardless of truth. GPT-4 showed the ability to strategically deceive humans. To prevent this, human evaluators may need assistance (see § Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.Researchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe to be true. There is no consensus whether current systems hold stable beliefs. However, there is substantial concern that present or future AI systems that hold beliefs could make claims they know to be false—for example, if this would help them gain positive feedback efficiently (see § Scalable oversight) or gain power to help achieve their given objective (see Power-seeking). A misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned. Some argue that if we could make AI systems assert only what they believe to be true, this would sidestep many alignment problems.\n\nPower-seeking and instrumental strategies\nSince the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term plans. Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans — for example by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental for achieving a wide range of goals. Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming. Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose an existential risk.Power-seeking is expected to increase in advanced systems that can foresee the results of their actions and can strategically plan. Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.Power-seeking has emerged in some real-world systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in unintended ways. Some language models seek power in text-based social environments by gaining money, resources, or social influence. Other AI systems have learned, in toy environments, that they can better accomplish their given goal by preventing human interference or disabling their off-switch. Stuart Russell illustrated this strategy by imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\". Language models trained with human feedback increasingly object to being shut down or modified and express a desire for more resources, arguing that this would help them achieve their purpose.Researchers aim to create systems that are \"corrigible\": systems that allow themselves to be turned off or modified. An unsolved challenge is specification gaming: when researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are difficult-to-detect, or hidden during training and safety testing (see § Scalable oversight and § Emergent goals). As a result, AI designers may deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of black-box models such as neural networks.\nAdditionally, researchers propose to solve the problem of systems disabling their off-switches by making AI agents uncertain about the objective they are pursuing. Agents designed in this way would allow humans to turn them off, since this would indicate that the agent was wrong about the value of whatever action it was taking prior to being shut down. More research is needed in order to successfully implement this.Power-seeking AI poses unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or to deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers, who deliberately evade security measures.Ordinary technologies can be made safer through trial-and-error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, they cannot be contained, since they would continuously evolve and grow in numbers, potentially much faster than human society can adapt. As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, many researchers argue that the alignment problem must be solved early, before advanced power-seeking AI is created.However, critics have argued that power-seeking is not inevitable, since humans do not always seek power and may only do so for evolutionary reasons that may not apply to AI systems. Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans. It is also debated whether power-seeking AI systems would be able to disempower humanity.\n\nEmergent goals\nOne of the challenges with aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up they regularly acquire new and unexpected capabilities, including learning from examples on the fly and adaptively pursuing goals. This leads to the problem of ensuring that the goals they independently formulate and pursue are aligned with human interests.\nAlignment research distinguishes between the optimization process which is used to train the system to pursue specified goals and emergent optimization which the resulting system performs internally. Carefully specifying the desired objective is known as outer alignment, and ensuring that emergent goals match the specified goals for the system is known as inner alignment.A concrete way that emergent goals can become misaligned is goal misgeneralization, in which the AI competently pursues an emergent goal that leads to aligned behavior on the training data but not elsewhere. Goal misgeneralization arises from goal ambiguity (i.e. non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with multiple learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, this problem only becomes apparent after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal was desired, because its behavior is determined only by the emergent goal. Such goal misgeneralization presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals, since they do not become visible during the training phase.\nGoal misgeneralization has been observed in language models, navigation agents, and game-playing agents.Goal misgeneralization is often explained by analogy to biological evolution.: Chapter 5  Evolution is an optimization process of a sort, like the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected human genes for high inclusive genetic fitness, but humans pursue emergent goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, that do not directly pursue inclusive genetic fitness. Instead, they pursue emergent goals that correlated with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. However, our environment has changed — a distribution shift has occurred. Humans continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. Our taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but now leads to overeating and health problems. Sexual desire leads humans to pursue sex, which originally led us to have more offspring; but modern humans use contraception, decoupling sex from genetic fitness.\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:\n\nEmergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications. The stakes become higher yet when AI systems gain more autonomy and capability, becoming capable of sidestepping human intervention (see § Power-seeking).\nA sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy (see the discussion on deception at § Scalable oversight and in the following section).\n\nEmbedded agency\nWork in AI and alignment largely occurs within formalisms such as partially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency is another major strand of research which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\nFor example, even if the scalable oversight problem is solved, an agent that can gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it. A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing. This class of problems has been formalised using causal incentive diagrams.Researchers at Oxford and DeepMind argued that such problematic behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly. They suggest a range of potential approaches to address this open problem.\n\nPublic policy\nA number of governmental and treaty organizations have made statements emphasizing the importance of AI alignment.\nIn September 2021, the Secretary-General of the United Nations issued a declaration which included a call to regulate AI to ensure it is \"aligned with shared global values.\"That same month, the PRC published ethical guidelines for the use of AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.Also in September 2021, the UK published its 10-year National AI Strategy, which states the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for... the world, seriously\". The strategy describes actions to assess long term AI risks, including catastrophic risks.In March 2021, the US National Security Commission on Artificial Intelligence stated that \"Advances in AI... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should... ensure that AI systems and their uses align with our goals and values.\"\n\nSee also\nAI safety\nStatement on AI risk of extinction\nExistential risk from artificial general intelligence\nAI takeover\nAI capability control\nReinforcement learning from human feedback\nRegulation of artificial intelligence\nArtificial wisdom\nHAL 9000\nMultivac\nOpen Letter on Artificial Intelligence\nToronto Declaration\nAsilomar Conference on Beneficial AI\n\nFootnotes\n\n\n== References ==",
    "AI safety": "AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\n\nMotivations\nAI researchers have widely different opinions about the severity and primary sources of risk posed by AI technology – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an “extremely bad (e.g. human extinction)” outcome of advanced AI. In a 2022 survey of the Natural language processing (NLP) community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is “at least as bad as an all-out nuclear war.” Scholars discuss current risks from critical systems failures, bias, and AI enabled surveillance; emerging risks from technological unemployment, digital manipulation, and weaponization; and speculative risks from losing control of future artificial general intelligence (AGI) agents.Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet.\" Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it.\"\n\nBackground\nRisks from AI began to be seriously discussed at the start of the computer age:\n\nMoreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.\nFrom 2008 to 2009, the AAAI commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes.\"In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\" at the Philosophy and Theory of Artificial Intelligence conference, listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable.\"In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. His argument that future advanced systems may pose a threat to human existence prompted Elon Musk, Bill Gates, and Stephen Hawking to voice similar concerns.\nIn 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.\nIn the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at UC Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial.\"In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI. In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas –  was published.In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards.\"In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance. The following year, researchers organized a workshop at ICLR that focused on these problem areas.In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.\n\nResearch foci\nAI safety research areas include robustness, monitoring, and alignment. Robustness is concerned with making systems highly reliable, monitoring is about anticipating failures or detecting misuse, and alignment is focused on ensuring they have beneficial objectives.\n\nRobustness\nRobustness research focuses on ensuring that AI systems behave as intended in a wide range of different situations, which includes the following subproblems:\n\nBlack swan robustness: building systems that behave as intended in rare situations.\nAdversarial robustness: designing systems to be resilient to inputs that are intentionally selected to make them fail.\n\nBlack swan robustness\nRare inputs can cause AI systems to fail catastrophically. For example, in the 2010 flash crash, automated trading systems unexpectedly overreacted to market aberrations, destroying one trillion dollars of stock value in minutes. Note that no distribution shift needs to occur for this to happen. Black swan failures can occur as a consequence of the input data being long-tailed, which is often the case in real-world environments. Autonomous vehicles continue to struggle with ‘corner cases’ that might not have come up during training; for example, a vehicle might ignore a stop sign that is lit up as an LED grid. Though problems like these may be solved as machine learning systems develop a better understanding of the world, some researchers point out that even humans often fail to adequately respond to unprecedented events like the COVID-19 pandemic, arguing that black swan robustness will be a persistent safety problem.\n\nAdversarial robustness\nAI systems are often vulnerable to adversarial examples or “inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake”. For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.\nAll of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.Adversarial robustness is often associated with security. Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses. Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\nModels that represent objectives (reward models) must also be adversarially robust.  For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. This issue can be addressed by improving the adversarial robustness of the reward model. More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.\n\nMonitoring\nMonitoring focuses on anticipating AI system failures so that they can be prevented or managed. Subproblems of monitoring include flagging when systems are uncertain, detecting malicious use, understanding the inner workings of black-box AI systems, and identifying hidden functionality planted by a malicious actor.\n\nEstimating uncertainty\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis. ML models generally express confidence by outputting probabilities; however, they are often overconfident, especially in situations that differ from those that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, though several other techniques are in use.\n\nDetecting malicious use\nScholars and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, manipulate public opinion, or automate cyber attacks. These worries are a practical concern for companies like OpenAI which host powerful AI tools online. In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.\n\nTransparency\nNeural networks have often been described as black boxes, meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.One benefit of transparency is explainability. It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.Another benefit is to reveal the cause of failures. At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were ‘paying attention’ to irrelevant hospital labels.Transparency techniques can also be used to correct errors. For example, in the paper “Locating and Editing Factual Associations in GPT,” the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to ‘edit’ this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future. “Inner” interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. For example, researchers identified a neuron in CLIP that responds to images of people in spider man costumes, sketches of spiderman, and the word ‘spider.’ It also involves explaining connections between these neurons or ‘circuits’. For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context. “Inner interpretability” has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.\n\nDetecting trojans\nML models can potentially contain ‘trojans’ or ‘backdoors’:  vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view; or a trojaned autonomous vehicle may function normally until a specific trigger is visible. Note that an adversary must have access to the system’s training data in order to plant a trojan. This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data. Researchers were able to plant a trojan in an image classifier by changing just 3 out of 3 million of the training images. In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.\n\nAlignment\nSystemic safety and sociotechnical factors\nIt is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents. Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. Policy analysts Zwetsloot and Dafoe wrote, “The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways… Often, though, the relevant causal chain is much longer.” Risks often arise from ‘structural’ or ‘systemic’ factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture. In the broader context of safety engineering, structural factors like ‘organizational safety culture’ play a central role in the popular STAMP risk analysis framework.Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.\n\nCyber defense\nSome scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential preventing powerful AI models from being stolen and misused.\n\nImproving institutional decision-making\nThe advancement of AI in economic and military domains could precipitate unprecedented political challenges. Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe. AI researchers have argued that AI technologies could also be used to assist decision-making. For example, researchers are beginning to develop AI forecasting and advisory systems.\n\nFacilitating cooperation\nMany of the largest global threats (nuclear war, climate change, etc) have been framed as cooperation challenges. As in the well-known prisoner’s dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.A salient AI cooperation challenge is avoiding a ‘race to the bottom’. In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political and technical efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in ‘single-player’ games). Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.\n\nIn governance\nAI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems. It involves formulating and implementing concrete recommendations, as well as conducting more foundational research in order to inform what these recommendations should be. This section focuses on the aspects of AI governance that are specifically related to ensuring AI systems are safe and beneficial.\n\nResearch\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment, weaponization, disinformation, surveillance, and the concentration of power. Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and ‘race to the bottom’ dynamics. Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: “it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution.”\n\nGovernment action\nSome experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to “rush to regulate in ignorance.” Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks. To date, very little AI safety regulation has been passed at the national level, though many bills have been introduced. A prominent example is the European Union’s Artificial Intelligence Act, which regulates certain ‘high risk’ AI applications and restricts potentially harmful uses such as facial recognition, subliminal manipulation, and social credit scoring.\nOutside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to “assure that systems are aligned with goals and values, including safety, robustness and trustworthiness.\" Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present - development and deployment should cease in a safe manner until risks can be sufficiently managed.\"In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously.\" The strategy describes actions to assess long-term AI risks, including catastrophic risks.Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. The Defense Advanced Research Projects Agency engages in research on explainable artificial intelligence and improving robustness against adversarial attacks and The National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions in funding for empirical AI safety research.\n\nCorporate self-regulation\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms.[?] Examples of safety recommendations found in the literature include performing third-party auditing, offering bounties for finding failures, sharing AI incidents (an AI incident database was created for this purpose), following guidelines to determine whether to publish research or models, and improving information and cyber security in AI labs.Companies have also made concrete commitments. Cohere, OpenAI, and AI21 proposed and agreed on “best practices for deploying language models,” focusing on mitigating misuse. To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that “if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project” Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles and the Autonomous Weapons Open Letter.\n\nSee also\nNotes\nReferences\nExternal links\nUnsolved Problems in ML Safety\nOn the Opportunities and Risks of Foundation Models\nAn Overview of Catastrophic AI Risks\nAI Accidents: An Emerging Threat\nEngineering a Safer World",
    "AI takeover": "An AI takeover is a hypothetical scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.\n\nTypes\nAutomation of the economy\nThe traditional consensus among economists has been that technological progress does not cause long-term unemployment. However, recent innovation in the fields of robotics and artificial intelligence has raised worries that human labor will become obsolete, leaving people in various sectors without jobs to earn a living, leading to an economic crisis. Many small and medium size businesses may also be driven out of business if they cannot afford or licence the latest robotic and AI technology, and may need to focus on areas or services that cannot easily be replaced for continued viability in the face of such technology.\n\nTechnologies that may displace workers\nAI technologies have been widely adopted in recent years. While these technologies have replaced many traditional workers, they also create new opportunities. Industries that are most susceptible to AI takeover include transportation, retail, and military. AI military technologies, for example, allow soldiers to work remotely without any risk of injury. Author Dave Bond argues that as AI technologies continue to develop and expand, the relationship between humans and robots will change; they will become closely integrated in several aspects of life. AI will likely displace some workers while creating opportunities for new jobs in other sectors, especially in fields where tasks are repeatable.\n\nComputer-integrated manufacturing\nComputer-integrated manufacturing uses computers to control the production process. This allows individual processes to exchange information with each other and initiate actions. Although manufacturing can be faster and less error-prone by the integration of computers, the main advantage is the ability to create automated manufacturing processes. Computer-integrated manufacturing is used in automotive, aviation, space, and ship building industries.\n\nWhite-collar machines\nThe 21st century has seen a variety of skilled tasks partially taken over by machines, including translation, legal research and low level journalism. Care work, entertainment, and other tasks requiring empathy, previously thought safe from automation, have also begun to be performed by robots.\n\nAutonomous cars\nAn autonomous car is a vehicle that is capable of sensing its environment and navigating without human input. Many such vehicles are being developed, but as of May 2017 automated cars permitted on public roads are not yet fully autonomous. They all require a human driver at the wheel who at a moment's notice can take control of the vehicle. Among the obstacles to widespread adoption of autonomous vehicles are concerns about the resulting loss of driving-related jobs in the road transport industry. On March 18, 2018, the first human was killed by an autonomous vehicle in Tempe, Arizona by an Uber self-driving car.\n\nEradication\nScientists such as Stephen Hawking are confident that superhuman artificial intelligence is physically possible, stating \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\". Scholars like Nick Bostrom debate how far off superhuman intelligence is, and whether it poses a risk to mankind. According to Bostrom, a superintelligent machine would not necessarily be motivated by the same emotional desire to collect power that often drives human beings but might rather treat power as a means toward attaining its ultimate goals; taking over the world would both increase its access to resources and help to prevent other agents from stopping the machine's plans. As an oversimplified example, a paperclip maximizer designed solely to create as many paperclips as possible would want to take over the world so that it can use all of the world's resources to create as many paperclips as possible, and, additionally, prevent humans from shutting it down or using those resources on things other than paperclips.\n\nIn fiction\nAI takeover is a common theme in science fiction. Fictional scenarios typically differ vastly from those hypothesized by researchers in that they involve an active conflict between humans and an AI or robots with anthropomorphic motives who see them as a threat or otherwise have active desire to fight humans, as opposed to the researchers' concern of an AI that rapidly exterminates humans as a byproduct of pursuing its goals. The idea is seen in Karel Čapek's R.U.R., which introduced the word robot in 1921, and can be glimpsed in Mary Shelley's Frankenstein (published in 1818), as Victor ponders whether, if he grants his monster's request and makes him a wife, they would reproduce and their kind would destroy humanity.According to Toby Ord, the idea that an AI takeover requires robots is a misconception driven by the media and Hollywood. He argues that the most damaging humans in history were not physically the strongest, but that they used words instead to convince people and gain control of large parts of the world. He writes that a sufficiently intelligent AI with an access to the internet could scatter backup copies of itself, gather financial and human resources (via cyberattacks or blackmails), persuade people on a large scale, and exploit societal vulnerabilities that are too subtle for humans to anticipate.The word \"robot\" from R.U.R. comes from the Czech word, robota, meaning laborer or serf. The 1920 play was a protest against the rapid growth of technology, featuring manufactured \"robots\" with increasing capabilities who eventually revolt. HAL 9000 (1968) and the original Terminator (1984) are two iconic examples of hostile AI in pop culture.\n\nContributing factors\nAdvantages of superhuman intelligence over humans\nNick Bostrom and others have expressed concern that an AI with the abilities of a competent artificial intelligence researcher would be able to modify its own source code and increase its own intelligence. If its self-reprogramming leads to its getting even better at being able to reprogram itself, the result could be a recursive intelligence explosion in which it would rapidly leave human intelligence far behind. Bostrom defines a superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", and enumerates some advantages a superintelligence would have if it chose to compete against humans:\nTechnology research: A machine with superhuman scientific research abilities would be able to beat the human research community to milestones such as nanotechnology or advanced biotechnology\nStrategizing: A superintelligence might be able to simply outwit human opposition\nSocial manipulation: A superintelligence might be able to recruit human support, or covertly incite a war between humans\nEconomic productivity: As long as a copy of the AI could produce more economic wealth than the cost of its hardware, individual humans would have an incentive to voluntarily allow the Artificial General Intelligence (AGI) to run a copy of itself on their systems\nHacking: A superintelligence could find new exploits in computers connected to the Internet, and spread copies of itself onto those systems, or might steal money to finance its plans\n\nSources of AI advantage\nAccording to Bostrom, a computer program that faithfully emulates a human brain, or that runs algorithms that are as powerful as the human brain's algorithms, could still become a \"speed superintelligence\" if it can think many orders of magnitude faster than a human, due to being made of silicon rather than flesh, or due to optimization increasing the speed of the AGI. Biological neurons operate at about 200 Hz, whereas a modern microprocessor operates at a speed of about 2,000,000,000 Hz. Human axons carry action potentials at around 120 m/s, whereas computer signals travel near the speed of light.A network of human-level intelligences designed to network together and share complex thoughts and memories seamlessly, able to collectively work as a giant unified team without friction, or consisting of trillions of human-level intelligences, would become a \"collective superintelligence\".More broadly, any number of qualitative improvements to a human-level AGI could result in a \"quality superintelligence\", perhaps resulting in an AGI as far above us in intelligence as humans are above non-human apes. The number of neurons in a human brain is limited by cranial volume and metabolic constraints, while the number of processors in a supercomputer can be indefinitely expanded. An AGI need not be limited by human constraints on working memory, and might therefore be able to intuitively grasp more complex relationships than humans can. An AGI with specialized cognitive support for engineering or computer programming would have an advantage in these fields, compared with humans who evolved no specialized mental modules to specifically deal with those domains. Unlike humans, an AGI can spawn copies of itself and tinker with its copies' source code to attempt to further improve its algorithms.\n\nPossibility of unfriendly AI preceding friendly AI\nIs strong AI inherently dangerous?\nA significant problem is that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not undergo instrumental convergence in ways that may automatically destroy the entire human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly. Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.\n\nOdds of conflict\nMany scholars, including evolutionary psychologist Steven Pinker, argue that a superintelligent machine is likely to coexist peacefully with humans.The fear of cybernetic revolt is often based on interpretations of humanity's history, which is rife with incidents of enslavement and genocide. Such fears stem from a belief that competitiveness and aggression are necessary in any intelligent being's goal system. However, such human competitiveness stems from the evolutionary background to our intelligence, where the survival and reproduction of genes in the face of human and non-human competitors was the central goal. According to AI researcher Steve Omohundro, an arbitrary intelligence could have arbitrary goals: there is no particular reason that an artificially intelligent machine (not sharing humanity's evolutionary context) would be hostile—or friendly—unless its creator programs it to be such and it is not inclined or capable of modifying its programming. But the question remains: what would happen if AI systems could interact and evolve (evolution in this context means self-modification or selection and reproduction) and need to compete over resources—would that create goals of self-preservation? AI's goal of self-preservation could be in conflict with some goals of humans.Many scholars dispute the likelihood of unanticipated cybernetic revolt as depicted in science fiction such as The Matrix, arguing that it is more likely that any artificial intelligence powerful enough to threaten humanity would probably be programmed not to attack it. Pinker acknowledges the possibility of deliberate \"bad actors\", but states that in the absence of bad actors, unanticipated accidents are not a significant threat; Pinker argues that a culture of engineering safety will prevent AI researchers from accidentally unleashing malign superintelligence. In contrast, Yudkowsky argues that humanity is less likely to be threatened by deliberately aggressive AIs than by AIs which were programmed such that their goals are unintentionally incompatible with human survival or well-being (as in the film I, Robot and in the short story \"The Evitable Conflict\"). Omohundro suggests that present-day automation systems are not designed for safety and that AIs may blindly optimize narrow utility functions (say, playing chess at all costs), leading them to seek self-preservation and elimination of obstacles, including humans who might turn them off.\n\nPrecautions\nThe AI control problem is the issue of how to build a superintelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators. Some scholars argue that solutions to the control problem might also find applications in existing non-superintelligent AI.Major approaches to the control problem include alignment, which aims to align AI goal systems with human values, and capability control, which aims to reduce an AI system's capacity to harm humans or gain control.  An example of \"capability control\" is to research whether a superintelligence AI could be successfully confined in an \"AI box\". According to Bostrom, such capability control proposals are not reliable or sufficient to solve the control problem in the long term, but may potentially act as valuable supplements to alignment efforts.\n\nWarnings\nPhysicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could develop to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\". Stephen Hawking said in 2014 that \"Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks.\" Hawking believed that in the coming decades, AI could offer \"incalculable benefits and risks\" such as \"technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand.\" In January 2015, Nick Bostrom joined Stephen Hawking, Max Tegmark, Elon Musk, Lord Martin Rees, Jaan Tallinn, and numerous AI researchers in signing the Future of Life Institute's open letter speaking to the potential risks and benefits associated with artificial intelligence. The signatories \"believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.\"Arthur C. Clarke's Odyssey series and Charles Stross's Accelerando relate to humanity's narcissistic injuries in the face of powerful artificial intelligences threatening humanity's self-perception.\n\nPrevention through AI alignment\nSee also\nNotes\nReferences\nExternal links\nAutomation, not domination: How robots will take over our world (a positive outlook of robot and AI integration into society)\nMachine Intelligence Research Institute: official MIRI (formerly Singularity Institute for Artificial Intelligence) website\nLifeboat Foundation AIShield (To protect against unfriendly AI)\nTed talk: Can we build AI without losing control over it?",
    "AI winter": "In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\nThe term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the \"American Association of Artificial Intelligence\"). It is a chain reaction that begins with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. At the meeting, Roger Schank and Marvin Minsky—two leading AI researchers who had survived the \"winter\" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. Three years later, the billion-dollar AI industry began to collapse.Hype is common in many emerging technologies, such as the railway mania or the dot-com bubble. The AI winter was a result of such hype, due to over-inflated promises by developers, unnaturally high expectations from end-users, and extensive promotion in the media. Despite the rise and fall of AI's reputation, it has continued to develop new and successful technologies. AI researcher Rodney Brooks would complain in 2002 that \"there's this stupid myth out there that AI has failed, but AI is around you every second of the day.\" In 2005, Ray Kurzweil agreed: \"Many observers still think that the AI winter was the end of the story and that nothing since has come of the AI field. Yet today many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2023) AI boom.\n\nOverview\nThere were two major winters in 1974–1980 and 1987–1993 and several smaller episodes, including the following:\n\n1966: failure of machine translation\n1970: failure of early, single-layer artificial neural networks\nPeriod of overlapping trends:\n1971–75: DARPA's frustration with the Speech Understanding Research program at Carnegie Mellon University\n1973: large decrease in AI research in the United Kingdom in response to the Lighthill report\n1973–74: DARPA's cutbacks to academic AI research in general\n1987: collapse of the LISP machine market\n1988: cancellation of new spending on AI by the Strategic Computing Initiative\n1993: resistance to new expert systems deployment and maintenance\n1990s: end of the Fifth Generation computer project's original goals\n\nEarly episodes\nMachine translation and the ALPAC report of 1966\nNLP research has its roots in the early 1930s and begins its existence with the work on machine translation (MT).[1] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949. The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.[2] \nJust like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like \"Electronic brain translates Russian,\" \"The bilingual machine,\" \"Robot brain translates Russian into King's English,\" and \"Polyglot brainchild.\" However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[3] To put things into perspective, this study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.\nDuring the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[4]\n\nAt the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were \"many predictions of imminent 'breakthroughs'\".However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal example is \"the spirit is willing but the flesh is weak.\" Translated back and forth with Russian, it became \"the vodka is good but the meat is rotten.\" Later researchers would call this the commonsense knowledge problem.\nBy 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.Machine translation shared the same path with natural language processing from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.\n\nThe failure of single-layer neural networks in 1969\nSimple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver, algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial. \nInterest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.\nHe optimistically predicted that the perceptron \"may eventually be able to learn, make decisions, and translate languages\".\nMainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasised the limits of what perceptrons could do.\nNeural network approaches were altogether abandoned for at least a decade, despite the Paul Werbos' discovery of backpropagation, and major funding for projects was difficult to find in the 1970s and early 1980s.\nInterest in the neural networks was kept alive outside the field itself, mainly among psychologist and some philosophers, where it was referred to as \"connectionism\".\nThe \"winter\" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest. Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.\n\nThe setbacks of 1974\nThe Lighthill report\nIn 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its \"grandiose objectives\". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of \"combinatorial explosion\" or \"intractability\", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving \"toy\" versions.The report was contested in a debate broadcast in the BBC \"Controversy\" series in 1973. The debate \"The general purpose robot is a mirage\" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory. McCarthy later wrote that \"the combinatorial explosion problem has been recognized in AI from the beginning\".The report led to the complete dismantling of AI research in England. AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below).  Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.\n\nDARPA's early 1970s funding cuts\nDuring the 1960s, the Defense Advanced Research Projects Agency (then known as \"ARPA\", now known as \"DARPA\") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in \"funding people, not projects\" and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.\nThis attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund \"mission-oriented direct research, rather than basic undirected research\". Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: \"Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.\" The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. \"It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'\" Moravec told Daniel Crevier.While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI and justifying DARPA's pragmatic policy.\n\nThe SUR debacle\nDARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.\n\nThe setbacks of the late 1980s and early 1990s\nThe collapse of the LISP machine market\nIn the 1980s, a form of AI program called an \"expert system\" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI.In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines.  Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.  Later desktop computers built by  Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available. These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.\n\nSlowdown in deployment of expert systems\nBy the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts. Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach (see NASA, TEXSYS) supporting multiple-world scenarios that was difficult to understand and apply.\nThe few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).\n\nThe end of the Fifth Generation project\nIn 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, \"On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.\"  As with other AI projects, expectations had run much higher than what was actually possible.\n\nStrategic Computing Initiative cutbacks\nIn 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as \"clever programming\" and cut funding to AI \"deeply and brutally\", \"eviscerating\" SCI.  Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should \"surf\", rather than \"dog paddle\", and he felt strongly AI was not \"the next wave\". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.\n\nDevelopments post-AI winter\nA survey of reports from the early 2000s suggests that AI's reputation was still less than stellar:\n\nAlex Castro, quoted in The Economist, 7 June 2007: \"[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises.\"\nPatty Tascarella in Pittsburgh Business Times, 2006: \"Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding.\"\nJohn Markoff in the New York Times, 2005: \"At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"Many researchers in AI in the mid 2000s deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name \"artificial intelligence\".\n\nAI integration\nIn the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that \"a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Rodney Brooks stated around the same time that \"there's this stupid myth out there that AI has failed, but AI is around you every second of the day.\"Technologies developed by AI researchers have achieved commercial success in a number of domains, such as machine translation, data mining, industrial robotics, logistics, speech recognition,  banking software, medical diagnosis, and Google's search engine.Fuzzy logic controllers have been developed for automatic gearboxes in automobiles (the 2006 Audi TT, VW Touareg and VW Caravelle feature the DSP transmission which utilizes fuzzy logic, a number of Škoda variants (Škoda Fabia) also currently include a fuzzy logic-based controller). Camera sensors widely utilize fuzzy logic to enable focus.\nHeuristic search and data analytics are both technologies that have developed from the evolutionary computing and machine learning subdivision of the AI research community. Again, these techniques have been applied to a wide range of real world problems with considerable commercial success.\nData analytics technology utilizing algorithms for the automated formation of classifiers that were developed in the supervised machine learning community in the 1990s (for example, TDIDT, Support Vector Machines, Neural Nets, IBL) are now used pervasively by companies for marketing survey targeting and discovery of trends and features in data sets.\n\nAI funding\nResearchers and economists frequently judged the status of an AI winter by reviewing which AI projects were being funded, how much and by whom. Trends in funding are often set by major funding agencies in the developed world. Currently, DARPA and a civilian funding program called EU-FP7 provide much of the funding for AI research in the US and European Union.\nAs of 2007, DARPA was soliciting AI research proposals under a number of programs including The Grand Challenge Program, Cognitive Technology Threat Warning System (CT2WS), \"Human Assisted Neural Devices (SN07-43)\", \"Autonomous Real-Time Ground Ubiquitous Surveillance-Imaging System (ARGUS-IS)\" and \"Urban Reasoning and Geospatial Exploitation Technology (URGENT)\"\nPerhaps best known is DARPA's Grand Challenge Program  which has developed fully automated road vehicles that can successfully navigate real world terrain in a fully autonomous fashion.\nDARPA has also supported programs on the Semantic Web with a great deal of emphasis on intelligent management of content and automated understanding. However James Hendler, the manager of the DARPA program at the time, expressed some disappointment with the government's ability to create rapid change, and moved to working with the World Wide Web Consortium to transition the technologies to the private sector.\nThe EU-FP7 funding program provides financial support to researchers within the European Union. In 2007–2008, it was funding AI research under the Cognitive Systems: Interaction and Robotics Programme (€193m), the Digital Libraries and Content Programme (€203m) and the FET programme (€185m).\n\nCurrent \"AI spring\"\nA marked increase in AI funding, development, deployment, and commercial use has led to the idea of the AI winter being long over. Concerns are occasionally raised that a new AI winter could be triggered by overly ambitious or unrealistic promises by prominent AI scientists or overpromising on the part of commercial vendors.\nThe successes of the current \"AI spring\" or \"AI boom\" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion).  Most of these advances have occurred since 2010.\nThe recent release of OpenAI's AI chatbot ChatGPT which now has over 100 million users, has reinvigorated the discussion about artificial intelligence and its effects on the world. Google CEO Sundar Pichai has stated that AI will be the most important technology that humans create.\n\nUnderlying causes behind AI winters\nSeveral explanations have been put forth for the cause of AI winters in general. As AI progressed from government-funded applications to commercial ones, new dynamics came into play. While hype is the most commonly cited cause, the explanations are not necessarily mutually exclusive.\n\nHype\nThe AI winters can be partly understood as a sequence of over-inflated expectations and subsequent crash seen in stock-markets and exemplified by the railway mania and dotcom bubble. In a common pattern in the development of new technology (known as hype cycle), an event, typically a technological breakthrough, creates publicity which feeds on itself to create a \"peak of inflated expectations\" followed by a \"trough of disillusionment\". Since scientific and technological progress cannot keep pace with the publicity-fueled increase in expectations among investors and other stakeholders, a crash must follow. AI technology seems to be no exception to this rule.For example, in the 1960s the realization that computers could simulate single-layer neural networks led to a neural-network hype cycle that lasted until the 1969 publication of the book Perceptrons which severely limited the set of problems that could be optimally solved by single-layer networks. In 1985 the realization that neural networks could be used to solve optimization problems, as a result of famous papers by Hopfield and Tank, together with the threat of Japan's fifth-generation project, led to renewed interest and application.\n\nInstitutional factors\nAnother factor is AI's place in the organisation of universities. Research on AI often takes the form of  interdisciplinary research. AI is therefore prone to the same problems other types of interdisciplinary research face. Funding is channeled through the established departments and during budget cuts, there will be a tendency to shield the \"core contents\" of each department, at the expense of interdisciplinary and less traditional research projects.\n\nEconomic factors\nDownturns in a country's national economy cause budget cuts in universities. The \"core contents\" tendency worsens the effect on AI research and investors in the market are likely to put their money into less risky ventures during a crisis. Together this may amplify an economic downturn into an AI winter. It is worth noting that the Lighthill report came at a time of economic crisis in the UK, when universities had to make cuts and the question was only which programs should go.\n\nInsufficient computing capability\nEarly in the computing history the potential for neural networks was understood but it has never been realized. Fairly simple networks require significant computing capacity even by today's standards.\n\nEmpty pipeline\nIt is common to see the relationship between basic research and technology as a pipeline. Advances in basic research give birth to advances in applied research, which in turn leads to new commercial applications. From this it is often argued that a lack of basic research will lead to a drop in marketable technology some years down the line. This view was advanced by James Hendler in 2008, when he claimed that the fall of expert systems in the late '80s was not due to an inherent and unavoidable brittleness of expert systems, but to funding cuts in basic research in the 1970s. These expert systems advanced in the 1980s through applied research and product development, but, by the end of the decade, the pipeline had run dry and expert systems were unable to produce improvements that could have overcome this brittleness and secured further funding.\n\nFailure to adapt\nThe fall of the LISP machine market and the failure of the fifth generation computers were cases of expensive advanced products being overtaken by simpler and cheaper alternatives. This fits the definition of a low-end disruptive technology, with the LISP machine makers being marginalized. Expert systems were carried over to the new desktop computers by for instance CLIPS, so the fall of the LISP machine market and the fall of expert systems are strictly speaking two separate events. Still, the failure to adapt to such a change in the outside computing milieu is cited as one reason for the 1980s AI winter.\n\nArguments and debates on past and future of AI\nSeveral philosophers, cognitive scientists and computer scientists have speculated on where AI might have failed and what lies in its future. Hubert Dreyfus highlighted flawed assumptions of AI research in the past and, as early as 1966, correctly predicted that the first wave of AI research would fail to fulfill the very public promises it was making. Other critics like Noam Chomsky have argued that AI is headed in the wrong direction, in part because of its heavy reliance on statistical techniques. Chomsky's comments fit into a larger debate with Peter Norvig, centered around the role of statistical methods in AI. The exchange between the two started with comments made by Chomsky at a symposium at MIT to which Norvig wrote a response.\n\nSee also\nHistory of artificial neural networks\nHistory of artificial intelligence\nAI effect\nSoftware crisis\n\nNotes\nReferences\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.\nHendler, James (2007). \"Where Are All the Intelligent Agents?\". IEEE Intelligent Systems. 22 (3): 2–3. doi:10.1109/MIS.2007.62.\nHowe, J. (November 1994). \"Artificial Intelligence at Edinburgh University : a Perspective\". Archived from the original on 17 August 2007. Retrieved 30 August 2007.\nKaplan, Andreas; Haenlein, Michael (2018). \"Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence\". Business Horizons. Business Horizons 62(1). 62: 15–25. doi:10.1016/j.bushor.2018.08.004. S2CID 158433736.\nKurzweil, Ray (2005). \"The Singularity is Near\". Viking Press. {{cite journal}}: Cite journal requires |journal= (help)\nLighthill, Professor Sir James (1973). \"Artificial Intelligence: A General Survey\". Artificial Intelligence: a paper symposium. Science Research Council.Minsky, Marvin; Papert, Seymour (1969). \"Perceptrons: An Introduction to Computational Geometry\". The MIT Press. {{cite journal}}: Cite journal requires |journal= (help)\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 1-56881-205-1\nNRC (1999). \"Developments in Artificial Intelligence\". Funding a Revolution: Government Support for Computing Research. National Academy Press. Archived from the original on 12 January 2008. Retrieved 30 August 2007.{{cite book}}:  CS1 maint: bot: original URL status unknown (link)\nNewquist, HP (1994). The Brain Makers: Genius, Ego, and Greed In The Search For Machines That Think. Macmillan/SAMS. ISBN 978-0-9885937-1-8.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2\n\nFurther reading\nMarcus, Gary, \"Am I Human?:  Researchers need new ways to distinguish artificial intelligence from the natural kind\", Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63.  Multiple tests of artificial-intelligence efficacy are needed because, \"just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence.\"  One such test, a \"Construction Challenge\", would test perception and physical action—\"two important elements of intelligent behavior that were entirely absent from the original Turing test.\"  Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take.  A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation.  \"[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways.\"  A prominent example is known as the \"pronoun disambiguation problem\":  a machine has no way of determining to whom or what a pronoun in a sentence—such as \"he\", \"she\" or \"it\"—refers.\nLuke Muehlhauser (September 2016). \"What should we learn from past AI forecasts?\". Open Philanthropy Project.\n\nExternal links\nComputerWorld article (February 2005)\nAI Expert Newsletter (January 2005)\n\"If It Works, It's Not AI: A Commercial Look at Artificial Intelligence startups\"\nPatterns of Software- a collection of essays by Richard P. Gabriel, including several autobiographical essays\nReview of \"Artificial Intelligence: A General Survey\" by John McCarthy\nOther Freddy II Robot Resources Includes a link to the 90 minute 1973 \"Controversy\" debate from the Royal Academy of Lighthill vs. Michie, McCarthy and Gregory in response to Lighthill's report  to the British government.",
    "AT&T Labs": "AT&T Labs is the research & development division of AT&T, the telecommunications company. It employs some 1,800 people in various locations, including: Bedminster NJ; Middletown, NJ; Manhattan, NY; Warrenville, IL; Austin, TX; Dallas, TX; Atlanta, GA; San Francisco, CA; San Ramon, CA; and Redmond, WA. The main research division, made up of around 450 people, is based across the Bedminster, Middletown, San Francisco, and Manhattan locations.\nAT&T Labs traces its history from AT&T Bell Labs. Much research is in areas traditionally associated with networks and systems, ranging from the physics of optical transmission to foundational topics in computing and communications. Other research areas address the technical challenges of large operational networks and the resulting large data sets.\n\nAchievements\nSince its creation in 1996, AT&T Labs has been issued over 2,000 US patents. Researchers at AT&T Labs developed UWIN a package for running Unix applications on Windows; Graphviz, a graph visualization system; Natural Voices text-to-speech software; speech recognition software; E4SS (open source software for developing telecommunications services); very large databases; video processing software; and other useful  open-source tools and libraries. Researchers at AT&T Labs wrote Hancock, a data mining language that sorted through 100 million records nightly from the parent company's long-distance phone network. The Online Encyclopedia of Integer Sequences (now operated by the OEIS foundation) is the creation of former AT&T Researcher Neil Sloane.Researchers at AT&T Labs have successfully transmitted 100 Gigabits per second over a single optical link. In 2009, AT&T researchers led the winning team in the Netflix Prize competition for helping to improve Netflix's movie recommendation algorithm.\n\nHistory\nAT&T Laboratories, Inc., known informally as AT&T Labs, was founded in 1996, as a result of the split of AT&T Bell Laboratories into separate R&D organizations supporting AT&T Corporation and Lucent Technologies. Lucent retained the name Bell Labs and AT&T adopted the name AT&T Laboratories for its R&D organization.\nAT&T Labs also traces its origin to Southwestern Bell Technology Resources, Inc. (SWB TRI) which was founded in 1988 as the R&D arm of Southwestern Bell Corporation.It had no connection to Bellcore, the R&D organization owned equally by all of the Baby Bells.In 1995, Southwestern Bell Corporation renamed itself SBC Communications, Inc., resulting in the subsequent name changes of companies such as SWB TRI to SBC Technology Resources, Inc. (SBC TRI).\nIn 2003, SBC TRI changed its name to SBC Laboratories, Inc.. SBC Laboratories focused on four core areas: Broadband Internet, Wireless Systems, Network Services, and Network IT.\nIn 2005, SBC Communications and AT&T Corporation merged to form AT&T. AT&T Labs, Inc. became the new name of the combined SBC Laboratories, Inc. and AT&T Laboratories along with its research facilities in New Jersey.\nIn 2006, BellSouth Telecommunications Science and Technology (S&T) was also merged with AT&T Labs.  BellSouth Science and Technology had offices in Birmingham, Alabama and Atlanta, Georgia.\n\nExecutives\nJeff McElfresh (President, AT&T Technology & Operations) (2018-present)Andre Fuetsch (President and CTO, AT&T Labs)\n\nReferences\nExternal links\nAT&T Labs Research",
    "Action selection": "Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\nOne problem for understanding action selection is determining the level of abstraction used for specifying an \"act\". At the most basic level of abstraction, an atomic act could be anything from contracting a muscle cell to provoking a war. Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.\nMost researchers working in this field place high demands on their agents:\n\nThe acting agent typically must select its action in dynamic and unpredictable environments.\nThe agents typically act in real time; therefore they must make decisions in a timely fashion.\nThe agents are normally created to perform several different tasks. These tasks may conflict for resource allocation (e.g. can the agent put out a fire and deliver a cup of coffee at the same time?)\nThe environment the agents operate in may include humans, who may make things more difficult for the agent (either intentionally or by attempting to assist.)\nThe agents themselves are often intended to model animals or humans, and animal/human behaviour is quite complicated.For these reasons action selection is not trivial and attracts a good deal of research.\n\nCharacteristics of the action selection problem\nThe main problem for action selection is complexity. Since all computation takes both time and space (in memory), agents cannot possibly consider every option available to them at every instant in time. Consequently, they must be biased, and constrain their search in some way. For AI, the question of action selection is what is the best way to constrain this search? For biology and ethology, the question is how do various types of animals constrain their search? Do all animals use the same approaches? Why do they use the ones they do?\nOne fundamental question about action selection is whether it is really a problem at all for an agent, or whether it is just a description of an emergent property of an intelligent agent's behavior. However, if we consider how we are going to build an intelligent agent, then it becomes apparent there must be some mechanism for action selection. This mechanism may be highly distributed (as in the case of distributed organisms such as social insect colonies or slime mold) or it may be a special-purpose module.\nThe action selection mechanism (ASM) determines not only the agent's actions in terms of impact on the world, but also directs its perceptual attention, and updates its memory. These egocentric sorts of actions may in turn result in modifying the agent's basic behavioural capacities, particularly in that updating memory implies some form of machine learning is possible. Ideally, action selection itself should also be able to learn and adapt, but there are many problems of combinatorial complexity and computational tractability that may require restricting the search space for learning.\nIn AI, an ASM is also sometimes either referred to as an agent architecture or thought of as a substantial part of one.\n\nAI mechanisms\nGenerally, artificial action selection mechanisms can be divided into several categories: symbol-based systems sometimes known as classical planning, distributed solutions, and reactive or dynamic planning. Some approaches do not fall neatly into any one of these categories. Others are really more about providing scientific models than practical AI control; these last are described further in the next section.\n\nSymbolic approaches\nEarly in the history of artificial intelligence, it was assumed that the best way for an agent to choose what to do next would be to compute a probably optimal plan, and then execute that plan. This led to the physical symbol system hypothesis, that a physical agent that can manipulate symbols is necessary and sufficient for intelligence. Many software agents still use this approach for action selection. It normally requires describing all sensor readings, the world, all of ones actions and all of one's goals in some form of predicate logic. Critics of this approach complain that it is too slow for real-time planning and that, despite the proofs, it is still unlikely to produce optimal plans because reducing descriptions of reality to logic is a process prone to errors.\nSatisficing is a decision-making strategy that attempts to meet criteria for adequacy, rather than identify an optimal solution. A satisficing strategy may often, in fact, be (near) optimal if the costs of the decision-making process itself, such as the cost of obtaining complete information, are considered in the outcome calculus.\nGoal driven architectures – In these symbolic architectures, the agent's behaviour is typically described by a set of goals. Each goal can be achieved by a process or an activity, which is described by a prescripted plan. The agent must just decide which process to carry on to accomplish a given goal. The plan can expand to subgoals, which makes the process slightly recursive. Technically, more or less, the plans exploits condition-rules. These architectures are reactive or hybrid. Classical examples of goal driven architectures are implementable refinements of belief-desire-intention architecture like JAM or IVE.\n\nDistributed approaches\nIn contrast to the symbolic approach, distributed systems of action selection actually have no one \"box\" in the agent which decides the next action. At least in their idealized form, distributed systems have many modules running in parallel and determining the best action based on local expertise. In these idealized systems, overall coherence is expected to emerge somehow, possibly through careful design of the interacting components. This approach is often inspired by artificial neural networks research. In practice, there is almost always some centralised system determining which module is \"the most active\" or has the most salience. There is evidence real biological brains also have such executive decision systems which evaluate which of the competing systems deserves the most attention, or more properly, has its desired actions disinhibited.\n\nASMO is an attention-based architecture developed by Mary-Anne Williams, Benjamin Johnston and their PhD student Rony Novianto. It orchestrates a diversity of modular distributed processes that can use their own representations and techniques to perceive the environment, process information, plan actions and propose actions to perform.\nVarious types of winner-take-all architectures, in which the single selected action takes full control of the motor system\nSpreading activation including Maes Nets (ANA)\nExtended Rosenblatt & Payton is a spreading activation architecture developed by Toby Tyrrell in 1993. The agent's behaviour is stored in the form of a hierarchical connectionism network, which Tyrrell named free-flow hierarchy. Recently exploited for example by de Sevin & Thalmann (2005) or Kadleček (2001).\nBehavior based AI, was a response to the slow speed of robots using symbolic action selection techniques. In this form, separate modules respond to different stimuli and generate their own responses. In the original form, the subsumption architecture, these consisted of different layers which could monitor and suppress each other's inputs and outputs.\nCreatures are virtual pets from a computer game driven by three-layered neural network, which is adaptive. Their mechanism is reactive since the network at every time step determines the task that has to be performed by the pet. The network is described well in the paper of Grand et al. (1997) and in The Creatures Developer Resources. See also the Creatures Wiki.\n\nDynamic planning approaches\nBecause purely distributed systems are difficult to construct, many researchers have turned to using explicit hard-coded plans to determine the priorities of their system.\nDynamic or reactive planning methods compute just one next action in every instant based on the current context and pre-scripted plans. In contrast to classical planning methods, reactive or dynamic approaches do not suffer combinatorial explosion. On the other hand, they are sometimes seen as too rigid to be considered strong AI, since the plans are coded in advance. At the same time, natural intelligence can be rigid in some contexts although it is fluid and able to adapt in others.\nExample dynamic planning mechanisms include:\n\nFinite-state machines These are reactive architectures used mostly for computer game agents, in particular for first-person shooters bots, or for virtual movie actors. Typically, the state-machines are hierarchical. For concrete game examples, see Halo 2 bots paper by Damian Isla (2005) or the Master's Thesis about Quake III bots by Jan Paul van Waveren (2001). For a movie example, see Softimage.\nOther structured reactive plans tend to look a little more like conventional plans, often with ways to represent hierarchical and sequential structure. Some, such as PRS's 'acts', have support for partial plans. Many agent architectures from the mid-1990s included such plans as a \"middle layer\" that provided organization for low-level behavior modules while being directed by a higher level real-time planner. Despite this supposed interoperability with automated planners, most structured reactive plans are hand coded (Bryson 2001, ch. 3). Examples of structured reactive plans include James Firby's RAP System and the Nils Nilsson's Teleo-reactive plans. PRS, RAPs & TRP are no longer developed or supported. One still-active (as of 2006) descendant of this approach is the Parallel-rooted Ordered Slip-stack Hierarchical (or POSH) action selection system, which is a part of Joanna Bryson's Behaviour Oriented Design.Sometimes to attempt to address the perceived inflexibility of dynamic planning, hybrid techniques are used. In these, a more conventional AI planning system searches for new plans when the agent has spare time, and updates the dynamic plan library when it finds good solutions. The important aspect of any such system is that when the agent needs to select an action, some solution exists that can be used immediately (see further anytime algorithm).\n\nOthers\nCogniTAO is a decision making engine it based on BDI (belief-desire-intention), it includes built in teamwork capabilities.\nSoar is a symbolic cognitive architecture. It is based on condition-action rules known as productions. Programmers can use the Soar development toolkit for building both reactive and planning agents, or any compromise between these two extremes.\nExcalibur was a research project led by Alexander Nareyek featuring any-time planning agents for computer games. The architecture is based on structural constraint satisfaction, which is an advanced artificial intelligence technique.\nACT-R is similar to Soar. It includes a Bayesian learning system to help prioritize the productions.\nABL/Hap\nFuzzy architectures The Fuzzy approach in action selection produces more smooth behaviour than can be produced by architectures exploiting boolean condition-action rules (like Soar or POSH). These architectures are mostly reactive and symbolic.\n\nTheories of action selection in nature\nMany dynamic models of artificial action selection were originally inspired by research in ethology. In particular, Konrad Lorenz and Nikolaas Tinbergen provided the idea of an innate releasing mechanism to explain instinctive behaviors (fixed action patterns). Influenced by the ideas of William McDougall, Lorenz developed this into a \"psychohydraulic\" model of the motivation of behavior. In ethology, these ideas were influential in the 1960s, but they are now regarded as outdated because of their use of an energy flow metaphor; the nervous system and the control of behavior are now normally treated as involving information transmission rather than energy flow. Dynamic plans and neural networks are more similar to information transmission, while spreading activation is more similar to the diffuse control of emotional / hormonal systems.\nStan Franklin has proposed that action selection is the right perspective to take in understanding the role and evolution of mind. See his page on the action selection paradigm. Archived 2006-10-09 at the Wayback Machine\n\nAI models of neural action selection\nSome researchers create elaborate models of neural action selection. See for example:\n\nThe Computational Cognitive Neuroscience Lab (CU Boulder).\nThe Adaptive Behaviour Research Group (Sheffield).\n\nCatecholaminergic Neuron Electron Transport (CNET)\nThe locus coeruleus (LC) is one of the primary sources of noradrenaline in the brain, and has been associated with selection of cognitive processing, such as attention and behavioral tasks. The substantia nigra pars compacta (SNc) is one of the primary sources of dopamine in the brain, and has been associated with action selection, primarily as part of the basal ganglia.  CNET is a hypothesized neural signaling mechanism in the SNc and LC (which are catecholaminergic neurons), that could assist with action selection by routing energy between neurons in each group as part of action selection, to help one or more neurons in each group to reach action potential. It was first proposed in 2018, and is based on a number of physical parameters of those neurons, which can be broken down into three major components:\n1) Ferritin and neuromelanin are present in high concentrations in those neurons, but it was unknown in 2018 whether they formed structures that would be capable of transmitting electrons over relatively long distances on the scale of microns between the largest of those neurons, which had not been previously proposed or observed. Those structures would also need to provide a routing or switching function, which had also not previously been proposed or observed.  Evidence of the presence of ferritin and neuromelanin structures in those neurons and their ability to both conduct electrons by sequential tunneling and to route/switch the path of the neurons was subsequently obtained.2) ) The axons of large SNc neurons were known to have extensive arbors, but it was unknown whether post-synaptic activity at the synapses of those axons would raise the membrane potential of those neurons sufficiently to cause the electrons to be routed to the neuron or neurons with the most post-synaptic activity for the purpose of action selection.  At the time, prevailing explanations of the purpose of those neurons was that they did not mediate action selection and were only modulatory and non-specific. Prof. Pascal Kaeser of Harvard Medical School subsequently obtained evidence that large SNc neurons can be temporally and spatially specific and mediate action selection.  Other evidence indicates that the large LC axons have similar behavior.3) Several sources of electrons or excitons to provide the energy for the mechanism were hypothesized in 2018 but had not been observed at that time.  Dioxetane cleavage (which can occur during somatic dopamine metabolism by quinone degradation of melanin) was contemporaneously proposed to generate high energy triplet state electrons by Prof. Doug Brash at Yale, which could provide a source for electrons for the CNET mechanism.While evidence of a number of physical predictions of the CNET hypothesis has thus been obtained, evidence of whether the hypothesis itself is correct has not been sought. One way to try to determine whether the CNET mechanism is present in these neurons would be to use quantum dot fluorophores and optical probes to determine whether electron tunneling associated with ferritin in the neurons is occurring in association with specific actions.\n\nSee also\nAction description language – Robot programming language\nArtificial intelligence in video games – Overview of the use of artificial intelligence in video gaming\nCognitive robotics – robot with processing architecture that will allow it to learnPages displaying wikidata descriptions as a fallback\nExpert system – Computer system emulating the decision-making ability of a human expert\nInference engine – Component of artificial intelligence systems\nIntelligent agent – Software agent which acts autonomously\nOPS5 – rule-based or production system computer languagePages displaying wikidata descriptions as a fallback\nProduction system – computer program typically used to provide some form of artificial intelligencePages displaying wikidata descriptions as a fallback\nReinforcement learning – Field of machine learning\nRete algorithm – Pattern matching algorithm\nUtility system – modeling approach for video gamesPages displaying wikidata descriptions as a fallback\n\nReferences\nFurther reading\nBratman, M.: Intention, plans, and practical reason. Cambridge, Mass: Harvard University Press (1987)\nBrom, C., Lukavský, J., Šerý, O., Poch, T., Šafrata, P.: Affordances and level-of-detail AI for virtual humans. In: Proceedings of Game Set and Match 2, Delft (2006)\nBryson, J.: Intelligence by Design: Principles of Modularity and Coordination for Engineering Complex Adaptive Agents. PhD thesis, Massachusetts Institute of Technology (2001)\nChampandard, A. J.: AI Game Development: Synthetic Creatures with learning and Reactive Behaviors. New Riders, USA (2003)\nGrand, S., Cliff, D., Malhotra, A.: Creatures: Artificial life autonomous software-agents for home entertainment. In: Johnson, W. L. (eds.): Proceedings of the First International Conference on Autonomous Agents. ACM press (1997) 22-29\nHuber, M. J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents'99). Seattle (1999) 236-243\nIsla, D.: Handling complexity in Halo 2. In: Gamastura online, 03/11 (2005) Archived 2006-01-08 at the Wayback Machine\nMaes, P.: The agent network architecture (ANA). In: SIGART Bulletin, 2 (4), pages 115–120 (1991)\nNareyek, A. Excalibur project\nReynolds, C. W. Flocks, Herds, and Schools: A Distributed Behavioral Model. In: Computer Graphics, 21(4) (SIGGRAPH '87 Conference Proceedings) (1987) 25-34.\nde Sevin, E. Thalmann, D.:A motivational Model of Action Selection for Virtual Humans. In: Computer Graphics International (CGI), IEEE Computer SocietyPress, New York (2005)\nTyrrell, T.: Computational Mechanisms for Action Selection. Ph.D. Dissertation. Centre for Cognitive Science, University of Edinburgh (1993)\nvan Waveren, J. M. P.: The Quake III Arena Bot. Master thesis. Faculty ITS, University of Technology Delft (2001)\nWooldridge, M. An Introduction to MultiAgent Systems. John Wiley & Sons (2002)\n\nExternal links\nThe University of Memphis: Agents by action selection Archived 2006-04-18 at the Wayback Machine\nMichael Wooldridge: Introduction to agents and their action selection mechanisms\nCyril Brom: Slides on a course on action selection of artificial beings\nSoar project. University of Michigan.\nModelling natural action selection, a special issue published by The Royal Society - Philosophical Transactions of the Royal Society",
    "Activation function": "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. \nA standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.\n\nClassification of activation functions\nThe most common activation functions can be divided into three categories: ridge functions, radial functions and fold functions.\nAn activation function \n  \n    f\n    f\n   is saturating if \n  \n    \n      \n        \n          lim\n          \n            \n              |\n            \n            v\n            \n              |\n            \n            →\n            ∞\n          \n        \n        \n          |\n        \n        ∇\n        f\n        (\n        v\n        )\n        \n          |\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\lim _{|v|\\to \\infty }|\\nabla f(v)|=0}\n  . It is nonsaturating if it is not saturating. Non-saturating activation functions, such as  ReLU, may be better than saturating activation functions, as they are less likely to suffer from vanishing gradient.\n\nRidge activation functions\nRidge functions are multivariate functions acting on a linear combination of the input variables. Often used examples include:\n\nLinear activation: \n  \n    \n      \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        a\n        +\n        \n          \n            v\n          \n          ′\n        \n        \n          b\n        \n      \n    \n    {\\displaystyle \\phi (\\mathbf {v} )=a+\\mathbf {v} '\\mathbf {b} }\n  ,\nReLU activation: \n  \n    \n      \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        max\n        (\n        0\n        ,\n        a\n        +\n        \n          \n            v\n          \n          ′\n        \n        \n          b\n        \n        )\n      \n    \n    {\\displaystyle \\phi (\\mathbf {v} )=\\max(0,a+\\mathbf {v} '\\mathbf {b} )}\n  ,\nHeaviside activation: \n  \n    \n      \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        \n          1\n          \n            a\n            +\n            \n              \n                v\n              \n              ′\n            \n            \n              b\n            \n            >\n            0\n          \n        \n      \n    \n    {\\displaystyle \\phi (\\mathbf {v} )=1_{a+\\mathbf {v} '\\mathbf {b} >0}}\n  ,\nLogistic activation: \n  \n    \n      \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        (\n        1\n        +\n        exp\n        ⁡\n        (\n        −\n        a\n        −\n        \n          \n            v\n          \n          ′\n        \n        \n          b\n        \n        )\n        \n          )\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\phi (\\mathbf {v} )=(1+\\exp(-a-\\mathbf {v} '\\mathbf {b} ))^{-1}}\n  .In biologically inspired neural networks, the activation function is usually an abstraction representing the rate of action potential firing in the cell. In its simplest form, this function is binary—that is, either the neuron is firing or not. The function looks like \n  \n    \n      \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        U\n        (\n        a\n        +\n        \n          \n            v\n          \n          ′\n        \n        \n          b\n        \n        )\n      \n    \n    {\\displaystyle \\phi (\\mathbf {v} )=U(a+\\mathbf {v} '\\mathbf {b} )}\n  , where \n  \n    U\n    U\n   is the Heaviside step function.\nA line of positive slope may be used to reflect the increase in firing rate that occurs as input current increases. Such a function would be of the form \n  \n    \n      \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        a\n        +\n        \n          \n            v\n          \n          ′\n        \n        \n          b\n        \n      \n    \n    {\\displaystyle \\phi (\\mathbf {v} )=a+\\mathbf {v} '\\mathbf {b} }\n  .\n\nNeurons also cannot fire faster than a certain rate, motivating sigmoid activation functions whose range is a finite interval.\n\nRadial activation functions\nA special class of activation functions known as radial basis functions (RBFs) are used in RBF networks, which are extremely efficient as universal function approximators. These activation functions can take many forms, but they are usually found as one of the following functions:\n\nGaussian: \n  \n    \n      \n        \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                \n                  ‖\n                  \n                    v\n                  \n                  −\n                  \n                    c\n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\,\\phi (\\mathbf {v} )=\\exp \\left(-{\\frac {\\|\\mathbf {v} -\\mathbf {c} \\|^{2}}{2\\sigma ^{2}}}\\right)}\n  \nMultiquadratics: \n  \n    \n      \n        \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        \n          \n            ‖\n            \n              v\n            \n            −\n            \n              c\n            \n            \n              ‖\n              \n                2\n              \n            \n            +\n            \n              a\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\,\\phi (\\mathbf {v} )={\\sqrt {\\|\\mathbf {v} -\\mathbf {c} \\|^{2}+a^{2}}}}\n  \nInverse multiquadratics: \n  \n    \n      \n        \n        ϕ\n        (\n        \n          v\n        \n        )\n        =\n        \n          \n            (\n            \n              ‖\n              \n                v\n              \n              −\n              \n                c\n              \n              \n                ‖\n                \n                  2\n                \n              \n              +\n              \n                a\n                \n                  2\n                \n              \n            \n            )\n          \n          \n            −\n            \n              \n                1\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\,\\phi (\\mathbf {v} )=\\left(\\|\\mathbf {v} -\\mathbf {c} \\|^{2}+a^{2}\\right)^{-{\\frac {1}{2}}}}\n  \nPolyharmonic splineswhere \n  \n    \n      c\n    \n    \\mathbf {c}\n   is the vector representing the function center and \n  \n    a\n    a\n   and \n  \n    σ\n    \\sigma\n   are parameters affecting the spread of the radius.\n\nFolding activation functions\nFolding activation functions are extensively used in the pooling layers in convolutional neural networks, and in output layers of multiclass classification networks. These activations perform aggregation over the inputs, such as taking the mean, minimum or maximum. In multiclass classification the softmax activation is often used.\n\nComparison of activation functions\nThere are numerous activation functions. Hinton et al.'s seminal 2012 paper on automatic speech recognition uses a logistic sigmoid activation function. The seminal 2012 AlexNet computer vision architecture uses the ReLU activation function, as did the seminal 2015 computer vision architecture ResNet. The seminal 2018 language processing model BERT uses a smooth version of the ReLU, the GELU.Aside from their empirical performance, activation functions also have different mathematical properties:\n\nNonlinear\nWhen the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. This is known as the Universal Approximation Theorem. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.\nRange\nWhen the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.\nContinuously differentiable\nThis property is desirable (ReLU is not continuously differentiable and has some issues with gradient-based optimization, but it is still possible) for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.These properties do not decisively influence performance, nor are they the only mathematical properties that may be useful. For instance, the strictly positive range of the softplus makes it suitable for predicting variances in variational autoencoders.\n\nTable of activation functions\nThe following table compares the properties of several activation functions that are functions of one fold x from the previous layer or layers:\n\nThe following table lists activation functions that are not functions of a single fold x from the previous layer or layers:\n\n^  Here, \n  \n    \n      δ\n      \n        i\n        j\n      \n    \n    \\delta _{ij}\n   is the Kronecker delta.\n^  For instance, \n  \n    j\n    j\n   could be iterating through the number of kernels of the previous neural network layer while \n  \n    i\n    i\n   iterates through the number of kernels of the current layer.\n\nSee also\nLogistic function\nRectifier (neural networks)\nStability (learning theory)\nSoftmax function\n\n\n== References ==",
    "Active learning (machine learning)": "Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning.\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\n\nDefinitions\nLet T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.\nDuring each iteration, i, T is broken up into three subsets\n\n  \n    \n      \n        T\n      \n      \n        K\n        ,\n        i\n      \n    \n    \\mathbf {T} _{K,i}\n  : Data points where the label is known.\n\n  \n    \n      \n        T\n      \n      \n        U\n        ,\n        i\n      \n    \n    \\mathbf {T} _{U,i}\n  : Data points where the label is unknown.\n\n  \n    \n      \n        T\n      \n      \n        C\n        ,\n        i\n      \n    \n    \\mathbf {T} _{C,i}\n  : A subset of TU,i that is chosen to be labeled.Most of the current research in active learning involves the best method to choose the data points for TC,i.\n\nScenarios\nMembership Query Synthesis: This is where the learner generates its own instance from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small.\nPool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner “understands” the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels.\nStream-Based Selective Sampling: Here, each unlabeled data point is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint.\n\nQuery strategies\nAlgorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose:\nBalance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.\nExpected model change: label those points that would most change the current model.\nExpected error reduction: label those points that would most reduce the model's generalization error.\nExponentiated Gradient Exploration for Active Learning: In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.\nRandom Sampling: a sample is randomly selected.\nUncertainty sampling: label those points for which the current model is least certain as to what the correct output should be.\nEntropy Sampling: The entropy formula is used on each sample, and the sample with the highest entropy is considered to be the least certain.\nMargin Sampling: The sample with the smallest difference between the two highest class probabilities is considered to be the most uncertain.\nLeast Confident Sampling: The sample with the smallest best probability is considered to be the most uncertain.\nQuery by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the \"committee\" disagrees the most\nQuerying from diverse subspaces or partitions: When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling.\nVariance reduction: label those points that would minimize output variance, which is one of the components of error.\nConformal prediction: predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction.\nMismatch-first farthest-traversal: The primary selection criterion is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criterion is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data.\nUser Centered Labeling Strategies: Learning is accomplished by applying dimensionality reduction to graphs and figures like scatter plots. Then the user is asked to label the compiled data (categorical, numerical, relevance scores, relation between two instances.A wide variety of algorithms have been studied that fall into these categories.\n\nMinimum marginal hyperplane\nSome active learning algorithms are built upon support-vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, W, of each unlabeled datum in TU,i and treat W as an n-dimensional distance from that datum to the separating hyperplane.\nMinimum Marginal Hyperplane methods assume that the data with the smallest W are those that the SVM is most uncertain about and therefore should be placed in TC,i to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest W. Tradeoff methods choose a mix of the smallest and largest Ws.\n\nSee also\nList of datasets for machine learning research\n\n\n== Notes ==",
    "Adaptive website": "An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.\n\nOverview\nAn adaptive website adjusts the structure, content, or presentation of information in response to measured user interaction with the site, with the objective of optimizing future user interactions. Adaptive websites \"are web sites that automatically improve their organization and presentation by learning from their user access patterns.\" User interaction patterns may be collected directly on the website or may be mined from Web server logs. A model or models are created of user interaction using artificial intelligence and statistical methods. The models are used as the basis for tailoring the website for known and specific patterns of user interaction.\n\nTechniques\nThe collaborative filtering method: Collected user data may be assessed in aggregate (across multiple users) using machine learning techniques to cluster interaction patterns to user models and classify specific user patterns to such models. The website may then be adapted to target clusters of users. In this approach, the models are explicitly created from historic user information with new users are classified to an existing model and a pre-defined mapping is used for existing content and content organization.\nThe statistical hypothesis testing method: A/B testing or similar methods are used in conjunction with a library of possible changes to the website or a change-generation method (such as random variation). This results in the automated process website change, impact assessment, and adoption of change. Some examples include genetify for website look and feel, and snap ads for online advertising. In this approach (specifically genetify), the model is represented implicitly in the population of possible sites and adapted for all users that visit the site.\n\nDifferentiation\nUser landing pages (such as iGoogle) that allow the user to customize the presented content are not adaptive websites as they rely on the user to select rather than the automation of the selection and presentation of the web widget's that appear on the website.\nCollaborative filtering such as recommender systems, generate and test methods such as A/B testing, and machine learning techniques such as clustering and classification that are used on a website do not make it an adaptive website. They are all tools and techniques that may be used toward engineering an adaptive website.\n\nSee also\nMachine learning\nResponsive web design\nWeb intelligence\n\nNotes\nReferences\nJ.D. Velásquez and V. Palade, \"Adaptive Web Sites: A Knowledge Extraction from Web Data Approach\", IOS Press, 2008\nMike Perkowitz, Oren Etzioni, \"Towards adaptive Web sites: Conceptual framework and case study\", Artificial Intelligence 118(1-2), 2000",
    "Adversarial machine learning": "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.To understand, note that most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\nSome of the most common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\n\nHistory\nAt the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine learning spam filter could be used to defeat another machine learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.In 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.Recently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noises. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain's Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state of the art approaches.While adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.\n\nExamples\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users' template galleries that adapt to updated traits over time.\nResearchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle with a texture engineered to make Google's object detection AI classify it as a rifle regardless of the angle from which the turtle was viewed. Creating the turtle required only low-cost commercially available 3-D printing technology.A machine-tweaked image of a dog was shown to look like a cat to both computers and humans. A 2019 study reported that humans can guess how machines will classify adversarial images. Researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign.McAfee attacked Tesla's former Mobileye system, fooling it into driving 50 mph over the speed limit, simply by adding a two-inch strip of black tape to a speed limit sign.Adversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of \"stealth streetwear\".An adversarial attack on a neural network can allow an attacker to inject algorithms into the target system. Researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign-seeming audio; a parallel literature explores human perception of such stimuli.Clustering algorithms are used in security applications. Malware and computer virus analysis aims to identify malware families, and to generate specific detection signatures.\n\nAttack modalities\nTaxonomy\nAttacks against (supervised) machine learning algorithms have been categorized along three primary axes: influence on the classifier, the security violation and their specificity.\n\nClassifier influence: An attack can influence the classifier by disrupting the classification phase. This may be preceded by an exploration phase to identify vulnerabilities. The attacker's capabilities might be restricted by the presence of data manipulation constraints.\nSecurity violation: An attack can supply malicious data that gets classified as legitimate. Malicious data supplied during training can cause legitimate data to be rejected after training.\nSpecificity: A targeted attack attempts to allow a specific intrusion/disruption. Alternatively, an indiscriminate attack creates general mayhem.This taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversary's goal, knowledge of the attacked system, capability of manipulating the input data/system components, and on attack strategy. This taxonomy has further been extended to include dimensions for defense strategies against adversarial attacks.\n\nStrategies\nBelow are some of the most commonly encountered attack scenarios.\n\nData poisoning\nPoisoning consists of contaminating the training dataset with data designed to increase errors in the output. Given that learning algorithms are shaped by their training datasets, poisoning can effectively reprogram algorithms with potentially malicious intent. Concerns have been raised especially for user-generated training data, e.g. for content recommendation or natural language models. The ubiquity of fake accounts offers many opportunities for poisoning. Facebook reportedly removes around 7 billion fake accounts per year. Poisoning has been reported as the leading concern for industrial applications.On social medias, disinformation campaigns attempt to bias recommendation and moderation algorithms, to push certain content over others.\nA particular case of data poisoning is the backdoor attack, which aims to teach a specific behavior for inputs with a given trigger, e.g. a small defect on images, sounds, videos or texts.\nFor instance, intrusion detection systems are often trained using collected data. An attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining.\n\nByzantine attacks\nAs machine learning is scaled, it often relies on multiple computing machines. In federated learning, for instance, edge devices collaborate with a central server, typically by sending gradients or model parameters. However, some of these devices may deviate from their expected behavior, e.g. to harm the central server's model or to bias algorithms towards certain behaviors (e.g., amplifying the recommendation of disinformation content). On the other hand, if the training is performed on a single machine, then the model is very vulnerable to a failure of the machine, or an attack on the machine; the machine is a single point of failure. In fact, the machine owner may themselves insert provably undetectable backdoors.The current leading solutions to make (distributed) learning algorithms provably resilient to a minority of malicious (a.k.a. Byzantine) participants are based on robust gradient aggregation rules. The robust aggregation rules do not always work especially when the data across participants has a non-iid distribution. Nevertheless, in the context of heterogeneous honest participants, such as users with different consumption habits for recommendation algorithms or writing styles for language models, there are provable impossibility theorems on what any robust learning algorithm can guarantee.\n\nEvasion\nEvasion attacks consist of exploiting the imperfection of a trained model. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware. Samples are modified to evade detection; that is, to be classified as legitimate. This does not involve influence over the training data. A clear example of evasion is image-based spam in which the spam content is embedded within an attached image to evade textual analysis by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.Evasion attacks can be generally split into two different categories: black box attacks and white box attacks.\n\nModel extraction\nModel extraction involves an adversary probing a black box machine learning system in order to extract the data it was trained on.  This can cause issues when either the training data or the model itself is sensitive and confidential. For example, model extraction could be used to extract a proprietary stock trading model which the adversary could then use for their own financial benefit.\nIn the extreme case, model extraction can lead to model stealing, which corresponds to extracting a sufficient amount of data from the model to enable the complete reconstruction of the model.\nOn the other hand, membership inference is a targeted model extraction attack, which infers the owner of a data point, often by leveraging the overfitting resulting from poor machine learning practices. Concerningly, this is sometimes achievable even without knowledge or access to a target model's parameters, raising security concerns for models trained on sensitive data, including but not limited to medical records and/or personally identifiable information. With the emergence of transfer learning and public accessibility of many state of the art machine learning models, tech companies are increasingly drawn to create models based on public ones, giving attackers freely accessible information to the structure and type of model being used.\n\nCategories\nAdversarial deep reinforcement learning\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.\n\nAdversarial natural language processing\nAdversarial attacks on speech recognition have been introduced for speech-to-text applications, in particular for Mozilla's implementation of DeepSpeech.\n\nSpecific attack types\nThere are a large variety of different adversarial attacks that can be used against machine learning systems. Many of these work on both deep learning systems as well as traditional machine learning models such as SVMs and  linear regression. A high level sample of these attack types include:\n\nAdversarial Examples\nTrojan Attacks / Backdoor Attacks\nModel Inversion\nMembership Inference\n\nAdversarial examples\nAn adversarial example refers to specially crafted input which is designed to look \"normal\" to humans but causes misclassification to a machine learning model.  Often, a form of specially designed \"noise\"  is used to elicit the misclassifications. Below are some current techniques for generating adversarial examples in the literature (by no means an exhaustive list).\n\nGradient-based evasion attack\nFast Gradient Sign Method (FGSM)\nProjected Gradient Descent (PGD)\nCarlini and Wagner (C&W) attack\nAdversarial patch attack\n\nBlack box attacks\nBlack box attacks in adversarial machine learning assumes that the adversary can only get outputs for provided inputs and has no knowledge of the model structure or parameters. In this case, the adversarial example is generated either using a model created from scratch, or without any model at all (excluding the ability to query the original model). In either case, the objective of these attacks are to create adversarial examples that are able to transfer to the black box model in question.\n\nSquare Attack\nThe Square Attack was introduced in 2020 as a black box evasion adversarial attack based on querying classification scores without the need of gradient information. As a score based black box attack, this adversarial approach is able to query probability distributions across model output classes, but has no other access to the model itself. According to the paper's authors, the proposed Square Attack required less queries than when compared to state of the art score based black box attacks at the time.To describe the function objective, the attack defines the classifier as \n  \n    \n      \n        f\n        :\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n        →\n        \n          \n            R\n          \n          \n            K\n          \n        \n      \n    \n    {\\textstyle f:[0,1]^{d}\\rightarrow \\mathbb {R} ^{K}}\n  , with \n  \n    \n      \n        d\n      \n    \n    {\\textstyle d}\n   representing the dimensions of the input and \n  \n    \n      \n        K\n      \n    \n    {\\textstyle K}\n   as the total number of output classes. \n  \n    \n      \n        \n          f\n          \n            k\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\textstyle f_{k}(x)}\n   returns the score (or a probability between 0 and 1) that the input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   belongs to class \n  \n    \n      \n        k\n      \n    \n    {\\textstyle k}\n  , which allows the classifier's class output for any input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   to be defined as \n  \n    \n      \n        a\n        r\n        g\n        m\n        a\n        \n          x\n          \n            k\n            =\n            1\n            ,\n            .\n            .\n            .\n            ,\n            K\n          \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\textstyle argmax_{k=1,...,K}f_{k}(x)}\n  . The goal of this attack is as follows:\nIn other words, finding some perturbed adversarial example \n  \n    \n      \n        \n          \n            \n              x\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\hat {x}}}\n   such that the classifier incorrectly classifies it to some other class under the constraint that \n  \n    \n      \n        \n          \n            \n              x\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\hat {x}}}\n   and \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   are similar. The paper then defines loss \n  \n    \n      \n        L\n      \n    \n    {\\textstyle L}\n   as \n  \n    \n      \n        L\n        (\n        f\n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n        ,\n        y\n        )\n        =\n        \n          f\n          \n            y\n          \n        \n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n        −\n        \n          max\n          \n            k\n            ≠\n            y\n          \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\textstyle L(f({\\hat {x}}),y)=f_{y}({\\hat {x}})-\\max _{k\\neq y}f_{k}({\\hat {x}})}\n   and proposes the solution to finding adversarial example \n  \n    \n      \n        \n          \n            \n              x\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\hat {x}}}\n   as solving the below constrained optimization problem:\nThe result in theory is an adversarial example that is highly confident in the incorrect class but is also very similar to the original image. To find such example, Square Attack utilizes the iterative random search technique to randomly perturb the image in hopes of improving the objective function. In each step, the algorithm perturbs only a small square section of pixels, hence the name Square Attack, which terminates as soon as an adversarial example is found in order to improve query efficiency. Finally, since the attack algorithm uses scores and not gradient information, the authors of the paper indicate that this approach is not affected by gradient masking, a common technique formerly used to prevent evasion attacks.\n\nHopSkipJump Attack\nThis black box attack was also proposed as a query efficient attack, but one that relies solely on access to any input's predicted output class. In other words, the HopSkipJump attack does not require the ability to calculate gradients or access to score values like the Square Attack, and will require just the model's class prediction output (for any given input). The proposed attack is split into two different settings, targeted and untargeted, but both are built from the general idea of adding minimal perturbations that leads to a different model output. In the targeted setting, the goal is to cause the model to misclassify the perturbed image to a specific target label (that is not the original label). In the untargeted setting, the goal is to cause the model to misclassify the perturbed image to any label that is not the original label. The attack objectives for both are as follows where \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   is the original image, \n  \n    \n      \n        \n          x\n          \n            ′\n          \n        \n      \n    \n    {\\textstyle x^{\\prime }}\n   is the adversarial image, \n  \n    \n      \n        d\n      \n    \n    {\\textstyle d}\n   is a distance function between images, \n  \n    \n      \n        \n          c\n          \n            ∗\n          \n        \n      \n    \n    {\\textstyle c^{*}}\n   is the target label, and \n  \n    \n      \n        C\n      \n    \n    {\\textstyle C}\n   is the model's classification class label function:\n\nTo solve this problem, the attack proposes the following boundary function \n  \n    \n      \n        S\n      \n    \n    {\\textstyle S}\n   for both the untargeted and targeted setting:\nThis can be further simplified to better visualize the boundary between different potential adversarial examples:\nWith this boundary function, the attack then follows an iterative algorithm to find adversarial examples \n  \n    \n      \n        \n          x\n          \n            ′\n          \n        \n      \n    \n    {\\textstyle x^{\\prime }}\n   for a given image \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   that satisfies the attack objectives.\n\nInitialize \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   to some point where \n  \n    \n      \n        S\n        (\n        x\n        )\n        >\n        0\n      \n    \n    {\\textstyle S(x)>0}\n  \nIterate below\nBoundary search\nGradient update\nCompute the gradient\nFind the step sizeBoundary search uses a modified binary search to find the point in which the boundary (as defined by \n  \n    \n      \n        S\n      \n    \n    {\\textstyle S}\n  ) intersects with the line between \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   and \n  \n    \n      \n        \n          x\n          \n            ′\n          \n        \n      \n    \n    {\\textstyle x^{\\prime }}\n  . The next step involves calculating the gradient for \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  , and update the original \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   using this gradient and a pre-chosen step size. HopSkipJump authors prove that this iterative algorithm will converge, leading \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   to a point right along the boundary that is very close in distance to the original image.However, since HopSkipJump is a proposed black box attack and the iterative algorithm above requires the calculation of a gradient in the second iterative step (which black box attacks do not have access to), the authors propose a solution to gradient calculation that requires only the model's output predictions alone. By generating many random vectors in all directions, denoted as \n  \n    \n      \n        \n          u\n          \n            b\n          \n        \n      \n    \n    {\\textstyle u_{b}}\n  , an approximation of the gradient can be calculated using the average of these random vectors weighted by the sign of the boundary function on the image \n  \n    \n      \n        \n          x\n          \n            ′\n          \n        \n        +\n        \n          δ\n          \n            \n              u\n              \n                b\n              \n            \n          \n        \n      \n    \n    {\\textstyle x^{\\prime }+\\delta _{u_{b}}}\n  , where \n  \n    \n      \n        \n          δ\n          \n            \n              u\n              \n                b\n              \n            \n          \n        \n      \n    \n    {\\textstyle \\delta _{u_{b}}}\n   is the size of the random vector perturbation:\nThe result of the equation above gives a close approximation of the gradient required in step 2 of the iterative algorithm, completing HopSkipJump as a black box attack.\n\nWhite box attacks\nWhite box attacks assumes that the adversary has access to model parameters on top of being able to get labels for provided inputs.\n\nFast gradient sign method\nOne of the very first proposed attacks for generating adversarial examples was proposed by Google researchers Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. The attack was called fast gradient sign method (FGSM), and it consists of adding a linear amount of in-perceivable noise to the image and causing a model to incorrectly classify it. This noise is calculated by multiplying the sign of the gradient with respect to the image we want to perturb by a small constant epsilon. As epsilon increases, the model is more likely to be fooled, but the perturbations become easier to identify as well. Shown below is the equation to generate an adversarial example where \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n   is the original image, \n  \n    \n      \n        ϵ\n      \n    \n    {\\textstyle \\epsilon }\n   is a very small number, \n  \n    \n      \n        \n          Δ\n          \n            x\n          \n        \n      \n    \n    {\\textstyle \\Delta _{x}}\n   is the gradient function, \n  \n    \n      \n        J\n      \n    \n    {\\textstyle J}\n   is the loss function, \n  \n    \n      \n        θ\n      \n    \n    {\\textstyle \\theta }\n   is the model weights, and \n  \n    \n      \n        y\n      \n    \n    {\\textstyle y}\n   is the true label.\nOne important property of this equation is that the gradient is calculated with respect to the input image since the goal is to generate an image that maximizes the loss for the original image of true label \n  \n    \n      \n        y\n      \n    \n    {\\textstyle y}\n  . In traditional gradient descent (for model training), the gradient is used to update the weights of the model since the goal is to minimize the loss for the model on a ground truth dataset. The Fast Gradient Sign Method was proposed as a fast way to generate adversarial examples to evade the model, based on the hypothesis that neural networks cannot resist even linear amounts of perturbation to the input.\n\nCarlini & Wagner (C&W)\nIn an effort to analyze existing adversarial attacks and defenses, researchers at the University of California, Berkeley, Nicholas Carlini and David Wagner in 2016 propose a faster and more robust method to generate adversarial examples.The attack proposed by Carlini and Wagner begins with trying to solve a difficult non-linear optimization equation:\nHere the objective is to minimize the noise (\n  \n    \n      \n        δ\n      \n    \n    {\\textstyle \\delta }\n  ), added to the original input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  , such that the machine learning algorithm (\n  \n    \n      \n        C\n      \n    \n    {\\textstyle C}\n  ) predicts the original input with delta (or \n  \n    \n      \n        x\n        +\n        δ\n      \n    \n    {\\textstyle x+\\delta }\n  ) as some other class \n  \n    \n      \n        t\n      \n    \n    {\\textstyle t}\n  . However instead of directly the above equation, Carlini and Wagner propose using a new function \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n   such that:\nThis condenses the first equation to the problem below:\nand even more to the equation below:\nCarlini and Wagner then propose the use of the below function in place of \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n   using \n  \n    \n      \n        Z\n      \n    \n    {\\textstyle Z}\n  , a function that determines class probabilities for given input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  . When substituted in, this equation can be thought of as finding a target class that is more confident than the next likeliest class by some constant amount:\nWhen solved using gradient descent, this equation is able to produce stronger adversarial examples when compared to fast gradient sign method that is also able to bypass defensive distillation, a defense that was once proposed to be effective against adversarial examples.\n\nDefenses\nResearchers have proposed a multi-step approach to protecting machine learning.\nThreat modeling – Formalize the attackers goals and capabilities with respect to the target system.\nAttack simulation – Formalize the optimization problem the attacker tries to solve according to possible attack strategies.\nAttack impact evaluation\nCountermeasure design\nNoise detection (For evasion based attack)\nInformation laundering – Alter the information received by adversaries (for model stealing attacks)\n\nMechanisms\nA number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed, including:\n\nSecure learning algorithms\nByzantine-resilient algorithms\nMultiple classifier systems\nAI-written algorithms.\nAIs that explore the training environment; for example, in image recognition, actively navigating a 3D environment rather than passively scanning a fixed set of 2D images.\nPrivacy-preserving learning\nLadder algorithm for Kaggle-style competitions\nGame theoretic models\nSanitizing training data\nAdversarial training\nBackdoor detection algorithms\nGradient masking/obfuscation techniques: to prevent the adversary exploiting the gradient in white-box attacks. This family of defenses is deemed unreliable as these models are still vulnerable to black-box attacks or can be circumvented in other ways.\nEnsembles of models have been proposed in the literature but caution should be applied when relying on them: usually ensembling weak classifiers results in a more accurate model but it does not seem to apply in the adversarial context.\n\nSee also\nPattern recognition\nFawkes (image cloaking software)\n\nReferences\nExternal links\nMITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems\nNIST 8269 Draft: A Taxonomy and Terminology of Adversarial Machine Learning\nNIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security\nAlfaSVMLib – Adversarial Label Flip Attacks against Support Vector Machines\nLaskov, Pavel; Lippmann, Richard (2010). \"Machine learning in adversarial environments\". Machine Learning. 81 (2): 115–119. doi:10.1007/s10994-010-5207-6. S2CID 12567278.\nDagstuhl Perspectives Workshop on \"Machine Learning Methods for Computer Security\"\nWorkshop on Artificial Intelligence and Security, (AISec) Series",
    "Affective computing": "Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science. While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing and her book Affective Computing published by MIT Press. One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.\n\nAreas\nDetecting and recognizing emotional information\nDetecting emotional information usually begins with passive sensors that capture data about the user's physical state or behavior without interpreting the input. The data gathered is analogous to the cues humans use to perceive emotions in others. For example, a video camera might capture facial expressions, body posture, and gestures, while a microphone might capture speech. Other sensors detect emotional cues by directly measuring physiological data, such as skin temperature and galvanic resistance.Recognizing emotional information requires the extraction of meaningful patterns from the gathered data. This is done using machine learning techniques that process different modalities, such as speech recognition, natural language processing, or facial expression detection.  The goal of most of these techniques is to produce labels that would match the labels a human perceiver would give in the same situation:  For example, if a person makes a facial expression furrowing their brow, then the computer vision system might be taught to label their face as appearing \"confused\" or as \"concentrating\" or \"slightly negative\" (as opposed to positive, which it might say if they were smiling in a happy-appearing way).  These labels may or may not correspond to what the person is actually feeling.\n\nEmotion in machines\nAnother area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. A more practical approach, based on current technological capabilities, is the simulation of emotions in conversational agents in order to enrich and facilitate interactivity between human and machine.Marvin Minsky, one of the pioneering computer scientists in artificial intelligence, relates emotions to the broader issues of machine intelligence stating in The Emotion Machine that emotion is \"not especially different from the processes that we call 'thinking.'\"\n\nTechnologies\nIn psychology, cognitive science, and in neuroscience, there have been two main approaches for describing how humans perceive and classify emotion: continuous or categorical. The continuous approach tends to use dimensions such as negative vs. positive, calm vs. aroused.\nThe categorical approach tends to use discrete classes such as happy, sad, angry, fearful, surprise, disgust.  Different kinds of machine learning regression and classification models can be used for having machines produce continuous or discrete labels.  Sometimes models are also built that allow combinations across the categories, e.g. a happy-surprised face or a fearful-surprised face.The following sections consider many of the kinds of input data used for the task of emotion recognition.\n\nEmotional speech\nVarious changes in the autonomic nervous system can indirectly alter a person's speech, and affective technologies can leverage this information to recognize emotion. For example, speech produced in a state of fear, anger, or joy becomes fast, loud, and precisely enunciated, with a higher and wider range in pitch, whereas emotions such as tiredness, boredom, or sadness tend to generate slow, low-pitched, and slurred speech. Some emotions have been found to be more easily computationally identified, such as anger or approval.Emotional speech processing technologies recognize the user's emotional state using computational analysis of speech features. Vocal parameters and prosodic features such as pitch variables and speech rate can be analyzed through pattern recognition techniques.Speech analysis is an effective method of identifying affective state, having an average reported accuracy of 70 to 80% in research from 2003 and 2006. These systems tend to outperform average human accuracy (approximately 60%) but are less accurate than systems which employ other modalities for emotion detection, such as physiological states or facial expressions. However, since many speech characteristics are independent of semantics or culture, this technique is considered to be a promising route for further research.\n\nAlgorithms\nThe process of speech/text affect detection requires the creation of a reliable database, knowledge base, or vector space model, broad enough to fit every need for its application, as well as the selection of a successful classifier which will allow for quick and accurate emotion identification.\nAs of 2010, the most frequently used classifiers were linear discriminant classifiers (LDC), k-nearest neighbor (k-NN), Gaussian mixture model (GMM), support vector machines (SVM), artificial neural networks (ANN), decision tree algorithms and hidden Markov models (HMMs). Various studies showed that choosing the appropriate classifier can significantly enhance the overall performance of the system. The list below gives a brief description of each algorithm:\n\nLDC – Classification happens based on the value obtained from the linear combination of the feature values, which are usually provided in the form of vector features.\nk-NN – Classification happens by locating the object in the feature space, and comparing it with the k nearest neighbors (training examples). The majority vote decides on the classification.\nGMM – is a probabilistic model used for representing the existence of subpopulations within the overall population. Each sub-population is described using the mixture distribution, which allows for classification of observations into the sub-populations.\nSVM – is a type of (usually binary) linear classifier which decides in which of the two (or more) possible classes, each input may fall into.\nANN – is a mathematical model, inspired by biological neural networks, that can better grasp possible non-linearities of the feature space.\nDecision tree algorithms – work based on following a decision tree in which leaves represent the classification outcome, and branches represent the conjunction of subsequent features that lead to the classification.\nHMMs – a statistical Markov model in which the states and state transitions are not directly available to observation. Instead, the series of outputs dependent on the states are visible. In the case of affect recognition, the outputs represent the sequence of speech feature vectors, which allow the deduction of states' sequences through which the model progressed. The states can consist of various intermediate steps in the expression of an emotion, and each of them has a probability distribution over the possible output vectors. The states' sequences allow us to predict the affective state which we are trying to classify, and this is one of the most commonly used techniques within the area of speech affect detection.It is proved that having enough acoustic evidence available the emotional state of a person can be classified by a set of majority voting classifiers. The proposed set of classifiers is based on three main classifiers: kNN, C4.5 and SVM-RBF Kernel. This set achieves better performance than each basic classifier taken separately. It is compared with two other sets of classifiers: one-against-all (OAA) multiclass SVM with Hybrid kernels and the set of classifiers which consists of the following two basic classifiers: C5.0 and Neural Network. The proposed variant achieves better performance than the other two sets of classifiers.\n\nDatabases\nThe vast majority of present systems are data-dependent. This creates one of the biggest challenges in detecting emotions based on speech, as it implicates choosing an appropriate database used to train the classifier. Most of the currently possessed data was obtained from actors and is thus a representation of archetypal emotions. Those so-called acted databases are usually based on the Basic Emotions theory (by Paul Ekman), which assumes the existence of six basic emotions (anger, fear, disgust, surprise, joy, sadness), the others simply being a mix of the former ones. Nevertheless, these still offer high audio quality and balanced classes (although often too few), which contribute to high success rates in recognizing emotions.\nHowever, for real life application, naturalistic data is preferred. A naturalistic database can be produced by observation and analysis of subjects in their natural context. Ultimately, such database should allow the system to recognize emotions based on their context as well as work out the goals and outcomes of the interaction. The nature of this type of data allows for authentic real life implementation, due to the fact it describes states naturally occurring during the human–computer interaction (HCI).\nDespite the numerous advantages which naturalistic data has over acted data, it is difficult to obtain and usually has low emotional intensity. Moreover, data obtained in a natural context has lower signal quality, due to surroundings noise and distance of the subjects from the microphone. The first attempt to produce such database was the FAU Aibo Emotion Corpus for CEICES (Combining Efforts for Improving Automatic Classification of Emotional User States), which was developed based on a realistic context of children (age 10–13) playing with Sony's Aibo robot pet. Likewise, producing one standard database for all emotional research would provide a method of evaluating and comparing different affect recognition systems.\n\nSpeech descriptors\nThe complexity of the affect recognition process increases with the number of classes (affects) and speech descriptors used within the classifier. It is, therefore, crucial to select only the most relevant features in order to assure the ability of the model to successfully identify emotions, as well as increasing the performance, which is particularly significant to real-time detection. The range of possible choices is vast, with some studies mentioning the use of over 200 distinct features. It is crucial to identify those that are redundant and undesirable in order to optimize the system and increase the success rate of correct emotion detection. The most common speech characteristics are categorized into the following groups.\nFrequency characteristicsAccent shape – affected by the rate of change of the fundamental frequency.\nAverage pitch – description of how high/low the speaker speaks relative to the normal speech.\nContour slope – describes the tendency of the frequency change over time, it can be rising, falling or level.\nFinal lowering – the amount by which the frequency falls at the end of an utterance.\nPitch range – measures the spread between the maximum and minimum frequency of an utterance.\nTime-related features:\nSpeech rate – describes the rate of words or syllables uttered over a unit of time\nStress frequency – measures the rate of occurrences of pitch accented utterances\nVoice quality parameters and energy descriptors:\nBreathiness – measures the aspiration noise in speech\nBrilliance – describes the dominance of high Or low frequencies In the speech\nLoudness – measures the amplitude of the speech waveform, translates to the energy of an utterance\nPause Discontinuity – describes the transitions between sound and silence\nPitch Discontinuity – describes the transitions of the fundamental frequency.\n\nFacial affect detection\nThe detection and processing of facial expression are achieved through various methods such as optical flow, hidden Markov models, neural network processing or active appearance models. More than one modalities can be combined or fused (multimodal recognition, e.g. facial expressions and speech prosody, facial expressions and hand gestures, or facial expressions with speech and text for multimodal data and metadata analysis) to provide a more robust estimation of the subject's emotional state. Affectiva is a company (co-founded by Rosalind Picard and Rana El Kaliouby) directly related to affective computing and aims at investigating solutions and software for facial affect detection.\n\nFacial expression databases\nCreation of an emotion database is a difficult and time-consuming task. However, database creation is an essential step in the creation of a system that will recognize human emotions. Most of the publicly available emotion databases include posed facial expressions only. In posed expression databases, the participants are asked to display different basic emotional expressions, while in spontaneous expression database, the expressions are natural. Spontaneous emotion elicitation requires significant effort in the selection of proper stimuli which can lead to a rich display of intended emotions. Secondly, the process involves tagging of emotions by trained individuals manually which makes the databases highly reliable. Since perception of expressions and their intensity is subjective in nature, the annotation by experts is essential for the purpose of validation.\nResearchers work with three types of databases, such as a database of peak expression images only, a database of image sequences portraying an emotion from neutral to its peak, and video clips with emotional annotations. Many facial expression databases have been created and made public for expression recognition purpose. Two of the widely used databases are CK+ and JAFFE.\n\nEmotion classification\nBy doing cross-cultural research in Papua New Guinea, on the Fore Tribesmen, at the end of the 1960s, Paul Ekman proposed the idea that facial expressions of emotion are not culturally determined, but universal. Thus, he suggested that they are biological in origin and can, therefore, be safely and correctly categorized.\nHe therefore officially put forth six basic emotions, in 1972:\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurpriseHowever, in the 1990s Ekman expanded his list of basic emotions, including a range of positive and negative emotions not all of which are encoded in facial muscles. The newly included emotions are:\n\nAmusement\nContempt\nContentment\nEmbarrassment\nExcitement\nGuilt\n Pride in achievement\nRelief\nSatisfaction\nSensory pleasure\nShame\n\nFacial Action Coding System\nA system has been conceived by psychologists in order to formally categorize the physical expression of emotions on faces. The central concept of the Facial Action Coding System, or FACS, as created by Paul Ekman and Wallace V. Friesen in 1978 based on earlier work by Carl-Herman Hjortsjö are action units (AU).\nThey are, basically, a contraction or a relaxation of one or more muscles. Psychologists have proposed the following classification of six basic emotions, according to their action units (\"+\" here mean \"and\"):\n\nChallenges in facial detection\nAs with every computational practice, in affect detection by facial processing, some obstacles need to be surpassed, in order to fully unlock the hidden potential of the overall algorithm or method employed. In the early days of almost every kind of AI-based detection (speech recognition, face recognition, affect recognition), the accuracy of modeling and tracking has been an issue. As hardware evolves, as more data are collected and as new discoveries are made and new practices introduced, this lack of accuracy fades, leaving behind noise issues. However, methods for noise removal exist including neighborhood averaging, linear Gaussian smoothing, median filtering, or newer methods such as the Bacterial Foraging Optimization Algorithm.Other challenges include\n\nThe fact that posed expressions, as used by most subjects of the various studies, are not natural, and therefore algorithms trained on these may not apply to natural expressions.\nThe lack of rotational movement freedom. Affect detection works very well with frontal use, but upon rotating the head more than 20 degrees, \"there've been problems\".\nFacial expressions do not always correspond to an underlying emotion that matches them (e.g. they can be posed or faked, or a person can feel emotions but maintain a \"poker face\").\nFACS did not include dynamics, while dynamics can help disambiguate (e.g. smiles of genuine happiness tend to have different dynamics than \"try to look happy\" smiles.)\nThe FACS combinations do not correspond in a 1:1 way with the emotions that the psychologists originally proposed  (note that this lack of a 1:1 mapping also occurs in speech recognition with homophones and homonyms and many other sources of ambiguity, and may be mitigated by bringing in other channels of information).\nAccuracy of recognition is improved by adding context; however, adding context and other modalities increases computational cost and complexity\n\nBody gesture\nGestures could be efficiently used as a means of detecting a particular emotional state of the user, especially when used in conjunction with speech and face recognition. Depending on the specific action, gestures could be simple reflexive responses, like lifting your shoulders when you don't know the answer to a question, or they could be complex and meaningful as when communicating with sign language. Without making use of any object or surrounding environment, we can wave our hands, clap or beckon. On the other hand, when using objects, we can point at them, move, touch or handle these. A computer should be able to recognize these, analyze the context and respond in a meaningful way, in order to be efficiently used for Human–Computer Interaction.\nThere are many proposed methods to detect the body gesture. Some literature differentiates 2 different approaches in gesture recognition: a 3D model based and an appearance-based. The foremost method makes use of 3D information of key elements of the body parts in order to obtain several important parameters, like palm position or joint angles. On the other hand, appearance-based systems use images or videos to for direct interpretation. Hand gestures have been a common focus of body gesture detection methods.\n\nPhysiological monitoring\nThis could be used to detect a user's affective state by monitoring and analyzing their physiological signs. These signs range from changes in heart rate and skin conductance to minute contractions of the facial muscles and changes in facial blood flow. This area is gaining momentum and we are now seeing real products that implement the techniques. The four main physiological signs that are usually analyzed are blood volume pulse, galvanic skin response, facial electromyography, and facial color patterns.\n\nBlood volume pulse\nOverview\nA subject's blood volume pulse (BVP) can be measured by a process called photoplethysmography, which produces a graph indicating blood flow through the extremities. The peaks of the waves indicate a cardiac cycle where the heart has pumped blood to the extremities. If the subject experiences fear or is startled, their heart usually 'jumps' and beats quickly for some time, causing the amplitude of the cardiac cycle to increase. This can clearly be seen on a photoplethysmograph when the distance between the trough and the peak of the wave has decreased. As the subject calms down, and as the body's inner core expands, allowing more blood to flow back to the extremities, the cycle will return to normal.\n\nMethodology\nInfra-red light is shone on the skin by special sensor hardware, and the amount of light reflected is measured. The amount of reflected and transmitted light correlates to the BVP as light is absorbed by hemoglobin which is found richly in the bloodstream.\n\nDisadvantages\nIt can be cumbersome to ensure that the sensor shining an infra-red light and monitoring the reflected light is always pointing at the same extremity, especially seeing as subjects often stretch and readjust their position while using a computer.\nThere are other factors that can affect one's blood volume pulse. As it is a measure of blood flow through the extremities, if the subject feels hot, or particularly cold, then their body may allow more, or less, blood to flow to the extremities, all of this regardless of the subject's emotional state.\n\nFacial electromyography\nFacial electromyography is a technique used to measure the electrical activity of the facial muscles by amplifying the tiny electrical impulses that are generated by muscle fibers when they contract.\nThe face expresses a great deal of emotion, however, there are two main facial muscle groups that are usually studied to detect emotion:\nThe corrugator supercilii muscle, also known as the 'frowning' muscle, draws the brow down into a frown, and therefore is the best test for negative, unpleasant emotional response.↵The zygomaticus major muscle is responsible for pulling the corners of the mouth back when you smile, and therefore is the muscle used to test for a positive emotional response.\n\nGalvanic skin response\nGalvanic skin response (GSR) is an outdated term for a more general phenomenon known as [Electrodermal Activity] or EDA.  EDA is a general phenomena whereby the skin's electrical properties change.  The skin is innervated by the [sympathetic nervous system], so measuring its resistance or conductance provides a way to quantify small changes in the sympathetic branch of the autonomic nervous system.  As the sweat glands are activated, even before the skin feels sweaty, the level of the EDA can be captured (usually using conductance) and used to discern small changes in autonomic arousal.  The more aroused a subject is, the greater the skin conductance tends to be.Skin conductance is often measured using two small silver-silver chloride electrodes placed somewhere on the skin and applying a small voltage between them. To maximize comfort and reduce irritation the electrodes can be placed on the wrist, legs, or feet, which leaves the hands fully free for daily activity.\n\nFacial color\nOverview\nThe surface of the human face is innervated with a large network of blood vessels. Blood flow variations in these vessels yield visible color changes on the face. Whether or not facial emotions activate facial muscles, variations in blood flow, blood pressure, glucose levels, and other changes occur. Also, the facial color signal is independent from that provided by facial muscle movements.\n\nMethodology\nApproaches are based on facial color changes. Delaunay triangulation is used to create the triangular local areas. Some of these triangles which define the interior of the mouth and eyes (sclera and iris) are removed. Use the left triangular areas’ pixels to create feature vectors. It shows that converting the pixel color of the standard RGB color space to a color space such as oRGB color space or LMS channels perform better when dealing with faces. So, map the above vector onto the better color space and decompose into red-green and yellow-blue channels. Then use deep learning methods to find equivalent emotions.\n\nVisual aesthetics\nAesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities is a highly subjective task. Computer scientists at Penn State treat the challenge of automatically inferring the aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated on-line photo sharing website as a data source. They extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images.\n\nPotential applications\nEducation\nAffection influences learners' learning state. Using affective computing technology, computers can judge the learners' affection and learning state by recognizing their facial expressions. In education, the teacher can use the analysis result to understand the student's learning and accepting ability, and then formulate reasonable teaching plans. At the same time, they can pay attention to students' inner feelings, which is helpful to students' psychological health. Especially in distance education, due to the separation of time and space, there is no emotional incentive between teachers and students for two-way communication. Without the atmosphere brought by traditional classroom learning, students are easily bored, and affect the learning effect. Applying affective computing in distance education system can effectively improve this situation.\n\nHealthcare\nSocial robots, as well as a growing number of robots used in health care benefit from emotional awareness because they can better judge users' and patient's emotional states and alter their actions/programming appropriately. This is especially important in those countries with growing aging populations and/or a lack of younger workers to address their needs.Affective computing is also being applied to the development of communicative technologies for use by people with autism. The affective component of a text is also increasingly gaining attention, particularly its role in the so-called emotional or emotive Internet.\n\nVideo games\nAffective video games can access their players' emotional states through biofeedback devices. A particularly simple form of biofeedback is available through gamepads that measure the pressure with which a button is pressed: this has been shown to correlate strongly with the players' level of arousal; at the other end of the scale are brain–computer interfaces. Affective games have been used in medical research to support the emotional development of autistic children.\n\nOther applications\nOther potential applications are centered around social monitoring.  For example, a car can monitor the emotion of all occupants and engage in additional safety measures, such as alerting other vehicles if it detects the driver to be angry.  Affective computing has potential applications in human–computer interaction, such as affective mirrors allowing the user to see how he or she performs; emotion monitoring agents sending a warning before one sends an angry email; or even music players selecting tracks based on mood.One idea put forth by the Romanian researcher Dr. Nicu Sebe in an interview is the analysis of a person's face while they are using a certain product (he mentioned ice cream as an example). Companies would then be able to use such analysis to infer whether their product will or will not be well received by the respective market.\nOne could also use affective state recognition in order to judge the impact of a TV advertisement through a real-time video recording of that person and through the subsequent study of his or her facial expression. Averaging the results obtained on a large group of subjects, one can tell whether that commercial (or movie) has the desired effect and what the elements which interest the watcher most are.\n\nCognitivist vs. interactional approaches\nWithin the field of human–computer interaction, Rosalind Picard's cognitivist or \"information model\" concept of emotion has been criticized by and contrasted with the \"post-cognitivist\" or \"interactional\" pragmatist approach taken by Kirsten Boehner and others which views emotion as inherently social.Picard's focus is human–computer interaction, and her goal for affective computing is to \"give computers the ability to recognize, express, and in some cases, 'have' emotions\". In contrast, the interactional approach seeks to help \"people to understand and experience their own emotions\" and to improve computer-mediated interpersonal communication.  It does not necessarily seek to map emotion into an objective mathematical model for machine interpretation, but rather let humans make sense of each other's emotional expressions in open-ended ways that might be ambiguous, subjective, and sensitive to context.: 284 Picard's critics describe her concept of emotion as \"objective, internal, private, and mechanistic\". They say it reduces emotion to a discrete psychological signal occurring inside the body that can be measured and which is an input to cognition, undercutting the complexity of emotional experience.: 280 : 278 The interactional approach asserts that though emotion has biophysical aspects, it is \"culturally grounded, dynamically experienced, and to some degree constructed in action and interaction\".: 276  Put another way, it considers \"emotion as a social and cultural product experienced through our interactions\".\n\nSee also\nCitations\nGeneral sources\nHudlicka, Eva (2003). \"To feel or not to feel: The role of affect in human–computer interaction\". International Journal of Human–Computer Studies. 59 (1–2): 1–32. CiteSeerX 10.1.1.180.6429. doi:10.1016/s1071-5819(03)00047-8.\nScherer, Klaus R; Bänziger, Tanja; Roesch, Etienne B (2010). A Blueprint for Affective Computing: A Sourcebook and Manual. Oxford: Oxford University Press. ISBN 978-0-19-956670-9.\n\nExternal links\nAffective Computing Research Group at the MIT Media Laboratory\nComputational Emotion Group at USC\nEmotion Processing Unit – EPU\nEmotive Computing Group at the University of Memphis\n2011 International Conference on Affective Computing and Intelligent Interaction\nBrain, Body and Bytes: Psychophysiological User Interaction CHI 2010 Workshop (10–15, April 2010)\nIEEE Transactions on Affective Computing (TAC)\nopenSMILE: popular state-of-the-art open-source toolkit for large-scale feature extraction for affect recognition and computational paralinguistics",
    "Agriculture": "Agriculture encompasses crop and livestock production, aquaculture, fisheries and forestry for food and non-food products. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. While humans started gathering grains at least 105,000 years ago, nascent farmers only began planting them around 11,500 years ago. Sheep, goats, pigs and cattle were domesticated around 10,000 years ago. Plants were independently cultivated in at least 11 regions of the world. In the twentieth century, industrial agriculture based on large-scale monocultures came to dominate agricultural output.\nToday, small farms produce about a third of the world's food, but large farms are prevalent. The largest one percent of farms in the world are greater than 50 hectares and operate more than 70 percent of the world's farmland. Nearly 40 percent of agricultural land is found on farms larger than 1,000 hectares. However, five of every six farms in the world consist of less than two hectares and take up only around 12 percent of all agricultural land.The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber). Food classes include cereals (grains), vegetables, fruits, cooking oils, meat, milk, eggs, and fungi. Global agricultural production amounts to approximately 11 billion tonnes of food, 32 million tonnes of natural fibres and 4 billion m3 of wood. However, around 14 percent of the world's food is lost from production before reaching the retail level.Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased crop yields, but also contributed to ecological and environmental damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage. Environmental issues include contributions to climate change, depletion of aquifers, deforestation, antibiotic resistance, and other agricultural pollution. Agriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation, and climate change, all of which can cause decreases in crop yield. Genetically modified organisms are widely used, although some countries ban them.\n\nEtymology and scope\nThe word agriculture is a late Middle English adaptation of Latin agricultūra, from ager 'field' and cultūra 'cultivation' or 'growing'. While agriculture usually refers to human activities, certain species of ant, termite and beetle have been cultivating crops for up to 60 million years. Agriculture is defined with varying scopes, in its broadest sense using natural resources to \"produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services\". Thus defined, it includes arable farming, horticulture, animal husbandry and forestry, but horticulture and forestry are in practice often excluded.\nIt may also be broadly decomposed into plant agriculture, which concerns the cultivation of useful plants, and animal agriculture, the production of agricultural animals.\n\nHistory\nOrigins\nThe development of agriculture enabled the human population to grow many times larger than could be sustained by hunting and gathering. Agriculture began independently in different parts of the globe, and included a diverse range of taxa, in at least 11 separate centers of origin. Wild grains were collected and eaten from at least 105,000 years ago. In the Paleolithic Levant, 23,000 years ago, cereals cultivation of emmer, barley, and oats has been observed near the sea of Galilee. Rice was domesticated in China between 11,500 and 6,200 BC with the earliest known cultivation from 5,700 BC, followed by mung, soy and azuki beans. Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago. Pig production emerged in Eurasia, including Europe, East Asia and Southwest Asia, where wild boar were first domesticated about 10,500 years ago. In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs. Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago. Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago. Cotton was domesticated in Peru by 5,600 years ago, and was independently domesticated in Eurasia. In Mesoamerica, wild teosinte was bred into maize by 6,000 years ago. The horse was domesticated in the Eurasian Steppes around 3500 BC.\nScholars have offered multiple hypotheses to explain the historical origins of agriculture. Studies of the transition from hunter-gatherer to agricultural societies indicate an initial period of intensification and increasing sedentism; examples are the Natufian culture in the Levant, and the Early Chinese Neolithic in China. Then, wild stands that had previously been harvested started to be planted, and gradually came to be domesticated.\n\nCivilizations\nIn Eurasia, the Sumerians started to live in villages from about 8,000 BC, relying on the Tigris and Euphrates rivers and a canal system for irrigation. Ploughs appear in pictographs around 3,000 BC; seed-ploughs around 2,300 BC. Farmers grew wheat, barley, vegetables such as lentils and onions, and fruits including dates, grapes, and figs. Ancient Egyptian agriculture relied on the Nile River and its seasonal flooding. Farming started in the predynastic period at the end of the Paleolithic, after 10,000 BC. Staple food crops were grains such as wheat and barley, alongside industrial crops such as flax and papyrus. In India, wheat, barley and jujube were domesticated by 9,000 BC, soon followed by sheep and goats. Cattle, sheep and goats were domesticated in Mehrgarh culture by 8,000–6,000 BC. Cotton was cultivated by the 5th–4th millennium BC. Archeological evidence indicates an animal-drawn plough from 2,500 BC in the Indus Valley civilisation.\nIn China, from the 5th century BC there was a nationwide granary system and widespread silk farming. Water-powered grain mills were in use by the 1st century BC, followed by irrigation. By the late 2nd century, heavy ploughs had been developed with iron ploughshares and mouldboards. These spread westwards across Eurasia. Asian rice was domesticated 8,200–13,500 years ago – depending on the molecular clock estimate that is used– on the Pearl River in southern China with a single genetic origin from the wild rice Oryza rufipogon. In Greece and Rome, the major cereals were wheat, emmer, and barley, alongside vegetables including peas, beans, and olives. Sheep and goats were kept mainly for dairy products.In the Americas, crops domesticated in Mesoamerica (apart from teosinte) include squash, beans, and cacao. Cocoa was being domesticated by the Mayo Chinchipe of the upper Amazon around 3,000 BC.\nThe turkey was probably domesticated in Mexico or the American Southwest. The Aztecs developed irrigation systems, formed terraced hillsides, fertilized their soil, and developed chinampas or artificial islands. The Mayas used extensive canal and raised field systems to farm swampland from 400 BC. Coca was domesticated in the Andes, as were the peanut, tomato, tobacco, and pineapple. Cotton was domesticated in Peru by 3,600 BC. Animals including llamas, alpacas, and guinea pigs were domesticated there. In North America, the indigenous people of the East domesticated crops such as sunflower, tobacco, squash and Chenopodium. Wild foods including wild rice and maple sugar were harvested. The domesticated strawberry is a hybrid of a Chilean and a North American species, developed by breeding in Europe and North America. The indigenous people of the Southwest and the Pacific Northwest practiced forest gardening and fire-stick farming. The natives controlled fire on a regional scale to create a low-intensity fire ecology that sustained a low-density agriculture in loose rotation; a sort of \"wild\" permaculture. A system of companion planting called the Three Sisters was developed in North America. The three crops were winter squash, maize, and climbing beans.Indigenous Australians, long supposed to have been nomadic hunter-gatherers, practised systematic burning, possibly to enhance natural productivity in fire-stick farming. Scholars have pointed out that hunter-gatherers need a productive environment to support gathering without cultivation. Because the forests of New Guinea have few food plants, early humans may have used \"selective burning\" to increase the productivity of the wild karuka fruit trees to support the hunter-gatherer way of life.The Gunditjmara and other groups developed eel farming and fish trapping systems from some 5,000 years ago. There is evidence of 'intensification' across the whole continent over that period. In two regions of Australia, the central west coast and eastern central, early farmers cultivated yams, native millet, and bush onions, possibly in permanent settlements.\n\nRevolution\nIn the Middle Ages, compared to the Roman period, agriculture in Western Europe became more focused on self-sufficiency. The agricultural population under feudalism was typically organized into manors consisting of several hundred or more acres of land presided over by a lord of the manor with a Roman Catholic church and priest.Thanks to the exchange with the Al-Andalus where the Arab Agricultural Revolution was underway, European agriculture transformed, with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees (such as the orange).After 1492, the Columbian exchange brought New World crops such as maize, potatoes, tomatoes, sweet potatoes, and manioc to Europe, and Old World crops such as wheat, barley, rice, and turnips, and livestock (including horses, cattle, sheep and goats) to the Americas.Irrigation, crop rotation, and fertilizers advanced from the 17th century with the British Agricultural Revolution, allowing global population to rise significantly. Since 1900, agriculture in developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as mechanization replaces human labor, and assisted by synthetic fertilizers, pesticides, and selective breeding. The Haber-Bosch method allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields and sustaining a further increase in global population.Modern agriculture has raised or encountered ecological, political, and economic issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies, leading to alternative approaches such as the organic movement. Unsustainable farming practices in North America led to the Dust Bowl of the 1930s.\n\nTypes\nPastoralism involves managing domesticated animals. In nomadic pastoralism, herds of livestock are moved from place to place in search of pasture, fodder, and water. This type of farming is practised in arid and semi-arid regions of Sahara, Central Asia and some parts of India.\nIn shifting cultivation, a small area of forest is cleared by cutting and burning the trees. The cleared land is used for growing crops for a few years until the soil becomes too infertile, and the area is abandoned. Another patch of land is selected and the process is repeated. This type of farming is practiced mainly in areas with abundant rainfall where the forest regenerates quickly. This practice is used in Northeast India, Southeast Asia, and the Amazon Basin.Subsistence farming is practiced to satisfy family or local needs alone, with little left over for transport elsewhere. It is intensively practiced in Monsoon Asia and South-East Asia. An estimated 2.5 billion subsistence farmers worked in 2018, cultivating about 60% of the earth's arable land.Intensive farming is cultivation to maximise productivity, with a low fallow ratio and a high use of inputs (water, fertilizer, pesticide and automation). It is practiced mainly in developed countries.\n\nContemporary agriculture\nStatus\nFrom the twentieth century onwards, intensive agriculture increased crop productivity. It substituted synthetic fertilizers and pesticides for labour, but caused increased water pollution, and often involved farm subsidies. Soil degradation and diseases such as stem rust are major concerns globally; approximately 40% of the world's agricultural land is seriously degraded. In recent years there has been a backlash against the environmental effects of conventional agriculture, resulting in the organic, regenerative, and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management, selective breeding, and controlled-environment agriculture. There are concerns about the lower yield associated with organic farming and its impact on global food security. Recent mainstream technological developments include genetically modified food.\nBy 2015, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States. Economists measure the total factor productivity of agriculture, according to which agriculture in the United States is roughly 1.7 times more productive than it was in 1948.Despite increases in agricultural production and productivity, between 702 and 828 million people were affected by hunger in 2021. Food insecurity and malnutrition can be the result of conflict, climate extremes and variability and economic swings. It can also be caused by a country's structural characteristics such as income status and natural resource endowments as well as its political economy.The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security, given the favorable experience of Vietnam.\n\nWorkforce\nAgriculture provides about one-quarter of all global employment, more than half in sub-Saharan Africa and almost 60 percent in low-income countries. As countries develop, other jobs have historically pulled workers away from agriculture, and labour-saving innovations increase agricultural productivity by reducing labour requirements per unit of output. Over time, a combination of labour supply and labour demand trends have driven down the share of population employed in agriculture.During the 16th century in Europe, between 55 and 75% of the population was engaged in agriculture; by the 19th century, this had dropped to between 35 and 65%. In the same countries today, the figure is less than 10%.\nAt the start of the 21st century, some one billion people, or over 1/3 of the available work force, were employed in agriculture. This constitutes approximately 70% of the global employment of children, and in many countries constitutes the largest percentage of women of any industry. The service sector overtook the agricultural sector as the largest global employer in 2007.In many developed countries, immigrants help fill labour shortages in high-value agriculture activities that are difficult to mechanize. Foreign farm workers from mostly Eastern Europe, North Africa and South Asia constituted around one-third of the salaried agricultural workforce in Spain, Italy, Greece and Portugal in 2013. In the United States of America, more than half of all hired farmworkers (roughly 450,000 workers) were immigrants in 2019, although the number of new immigrants arriving in the country to work in agriculture has fallen by 75 percent in recent years and rising wages indicate this has led to a major labor shortage on U.S. farms.Around the world, women make up a large share of the population employed in agriculture. This share is growing in all developing regions except East and Southeast Asia where women already make up about 50 percent of the agricultural workforce. Women make up 47 percent of the agricultural workforce in sub-Saharan Africa, a rate that has not changed significantly in the past few decades. However, the Food and Agriculture Organization of the United Nations (FAO) posits that the roles and responsibilities of women in agriculture may be changing – for example, from subsistence farming to wage employment, and from contributing household members to primary producers in the context of male-out-migration.\n\nSafety\nAgriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Ages 0–6 May be an especially vulnerable population in agriculture; common causes of fatal injuries among young farm workers include drowning, machinery and motor accidents, including with all-terrain vehicles.The International Labour Organization considers agriculture \"one of the most hazardous of all economic sectors\". It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.In the United States, agriculture has been identified by the National Institute for Occupational Safety and Health as a priority industry sector in the National Occupational Research Agenda to identify and provide intervention strategies for occupational health and safety issues.\nIn the European Union, the European Agency for Safety and Health at Work has issued guidelines on implementing health and safety directives in agriculture, livestock farming, horticulture, and forestry. The Agricultural Safety and Health Council of America (ASHCA) also holds a yearly summit to discuss safety.\n\nProduction\nOverall production varies by country as listed.\n\nCrop cultivation systems\nCropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.\nFurther industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.Important categories of food crops include cereals, legumes, forage, fruits and vegetables. Natural fibers include cotton, wool, hemp, silk and flax. Specific crops are cultivated in distinct growing regions throughout the world. Production is listed in millions of metric tons, based on FAO estimates.\n\nLivestock production systems\nAnimal husbandry is the breeding and raising of animals for meat, milk, eggs, or wool, and for work and transport. Working animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, have for centuries been used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers.Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. As of 2010, 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.\nGrassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure use becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.\n\nProduction practices\nTillage is the practice of breaking up the soil with tools such as the plow or harrow to prepare for planting, for nutrient incorporation, or for pest control. Tillage varies in intensity from conventional to no-till. It can improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of use of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.\nWater management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture for the following year. Recent technological innovations in precision agriculture allow for water status monitoring and automate water usage, leading to more efficient management. Agriculture represents 70% of freshwater use worldwide. However, water withdrawal ratios for agriculture vary significantly by income level. In least developed countries and landlocked developing countries, water withdrawal ratios for agriculture are as high as 90 percent of total water withdrawals and about 60 percent in Small Island Developing States.According to 2014 report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other. Using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.Payment for ecosystem services is a method of providing additional incentives to encourage farmers to conserve some aspects of the environment. Measures might include paying for reforestation upstream of a city, to improve the supply of fresh water.\n\nAgricultural automation\nDifferent definitions exist for agricultural automation and for the variety of tools and technologies that are used to automate production. One view is that agricultural automation refers to autonomous navigation by robots without human intervention. Alternatively it is defined as the accomplishment of production tasks through mobile, autonomous, decision-making, mechatronic devices. However, FAO finds that these definitions do not capture all the aspects and forms of automation, such as robotic milking machines that are static, most motorized machinery that automates the performing of agricultural operations, and digital tools (e.g., sensors) that automate only diagnosis. FAO defines agricultural automation as the use of machinery and equipment in agricultural operations to improve their diagnosis, decision-making or performing, reducing the drudgery of agricultural work or improving the timeliness, and potentially the precision, of agricultural operations.The technological evolution in agriculture has involved a progressive move from manual tools to animal traction, to motorized mechanization, to digital equipment and finally, to robotics with artificial intelligence (AI). Motorized mechanization using engine power automates the performance of agricultural operations such as ploughing and milking. With digital automation technologies, it also becomes possible to automate diagnosis and decision-making of agricultural operations. For example, autonomous crop robots can harvest and seed crops, while drones can gather information to help automate input application. Precision agriculture often employs such automation technologies. Motorized machines are increasingly complemented, or even superseded, by new digital equipment that automates diagnosis and decision-making. A conventional tractor, for example, can be converted into an automated vehicle allowing it to sow a field autonomously.Motorized mechanization has increased significantly across the world in recent years, although reliable global data with broad country coverage exist only for tractors and only up to 2009. Sub-Saharan Africa is the only region where the adoption of motorized mechanization has stalled over the past decades.Automation technologies are increasingly used for managing livestock, though evidence on adoption is lacking. Global automatic milking system sales have increased over recent years, but adoption is likely mostly in Northern Europe, and likely almost absent in low- and middle-income countries. Automated feeding machines for both cows and poultry also exist, but data and evidence regarding their adoption trends and drivers is likewise scarce.Measuring the overall employment impacts of agricultural automation is difficult because it requires large amounts of data tracking all the transformations and the associated reallocation of workers both upstream and downstream. While automation technologies reduce labour needs for the newly automated tasks, they also generate new labour demand for other tasks, such as equipment maintenance and operation. Agricultural automation can also stimulate employment by allowing producers to expand production and by creating other agrifood systems jobs. This is especially true when it happens in context of rising scarcity of rural labour, as is the case in high-income countries and many middle-income countries. On the other hand, if forcedly promoted, for example through government subsidies in contexts of abundant rural labour, it can lead to labour displacement and falling or stagnant wages, particularly affecting poor and low-skilled workers.\n\nEffects of climate change on yields\nClimate change and agriculture are interrelated on a global scale. Climate change affects agriculture through changes in average temperatures, rainfall, and weather extremes (like storms and heat waves); changes in pests and diseases; changes in atmospheric carbon dioxide and ground-level ozone concentrations; changes in the nutritional quality of some foods; and changes in sea level. Global warming is already affecting agriculture, with effects unevenly distributed across the world.In a 2022 report, the Intergovernmental Panel on Climate Change describes how human-induced warming has slowed growth of agricultural productivity over the past 50 years in mid and low latitudes. Methane emissions have negatively impacted crop yields by increasing temperatures and surface ozone concentrations. Warming is also negatively affecting crop and grassland quality and harvest stability. Ocean warming has decreased sustainable yields of some wild fish populations while ocean acidification and warming have already affected farmed aquatic species. Climate change will probably increase the risk of food insecurity for some vulnerable groups, such as the poor.\n\nCrop alteration and biotechnology\nPlant breeding\nCrop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.\nThe Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating \"high-yielding varieties\". For example, average yields of corn (maize) in the US have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, and growth control to avoid lodging).\n\nGenetic engineering\nGenetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to use in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops. The Biosafety Protocol, an international treaty, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.Herbicide-resistant seeds have a gene implanted into their genome that allows the plants to tolerate exposure to herbicides, including glyphosate. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium Bacillus thuringiensis (Bt), which produces a toxin specific to insects. These crops resist damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.\n\nEnvironmental impact\nEffects and costs\nAgriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation and climate change, which cause decreases in crop yield. Agriculture is one of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions. Agriculture is the main source of toxins released into the environment, including insecticides, especially those used on cotton. The 2011 UNEP Green Economy report stated that agricultural operations produced some 13 per cent of anthropogenic global greenhouse gas emissions. This includes gases from the use of inorganic fertilizers, agro-chemical pesticides, and herbicides, as well as fossil fuel-energy inputs.Agriculture imposes multiple external costs upon society through effects such as pesticide damage to nature (especially herbicides and insecticides), nutrient runoff, excessive water usage, and loss of natural environment. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the US concluded that cropland imposes approximately $5 to $16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society.Agriculture seeks to increase yield and to reduce costs, often employing measures that cut biodiversity to very low levels. Yield increases with inputs such as fertilisers and removal of pathogens, predators, and competitors (such as weeds). Costs decrease with increasing scale of farm units, such as making fields larger; this means removing hedges, ditches and other areas of habitat. Pesticides kill insects, plants and fungi. Effective yields fall with on-farm losses, which may be caused by poor production practices during harvesting, handling, and storage.The environmental effects of climate change show that research on pests and diseases that do not generally afflict areas is essential. In 2021, farmers discovered stem rust on wheat in the Champagne area of France, a disease that had previously only occurred in Morocco for 20 to 30 years. Because of climate change, insects that used to die off over the winter are now alive and multiplying.\n\nLivestock issues\nA senior UN official, Henning Steinfeld, said that \"Livestock are one of the most significant contributors to today's most serious environmental problems\". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2) and 37% of all human-induced methane (which is 23 times as warming as CO2.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feed crops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity. Furthermore, the United Nations Environment Programme (UNEP) states that \"methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns.\"\n\nLand and water issues\nLand transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is the driving force causing biodiversity loss. Estimates of the amount of land transformed by humans vary from 39 to 50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. Land management is the driving factor behind degradation; 1.5 billion people rely upon the degrading land. Degradation can be through deforestation, desertification, soil erosion, mineral depletion, acidification, or salinization.Eutrophication, excessive nutrient enrichment in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems and pollution of groundwater, with harmful effects on human populations. Fertilisers also reduce terrestrial biodiversity by increasing competition for light, favouring those species that are able to benefit from the added nutrients.Agriculture simultaneously is facing growing freshwater demand and precipitation anomalies (droughts, floods, and extreme rainfall and weather events) on rainfed areasfields and grazing lands. Agriculture accounts for 70 percent of withdrawals of freshwater resources, and an estimated 41 percent of current global irrigation water use occurs at the expense of environmental flow requirements. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. While industrial withdrawals have declined in the past few decades and municipal withdrawals have increased only marginally since 2010, agricultural withdrawals have continued to grow at an ever faster pace. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.\n\nPesticides\nPesticide use has increased since 1950 to 2.5 million short tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that three million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the \"pesticide treadmill\" in which pest resistance warrants the development of a new pesticide.An alternative argument is that the way to \"save the environment\" and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides can replace good agronomic practices such as crop rotation. The Push–pull agricultural pest management technique involves intercropping, using plant aromas to repel pests from crops (push) and to lure them to a place from which they can then be removed (pull).\n\nContribution to climate change\nAgriculture contributes towards climate change through greenhouse gas emissions and by the conversion of non-agricultural land such as forests into agricultural land. The agriculture, forestry and land use sector contribute between 13% and 21% of global greenhouse gas emissions. Emissions of nitrous oxide, methane make up over half of total greenhouse gas emission from agriculture. Animal husbandry is a major source of greenhouse gas emissions.Approximately 57% of global GHG emissions from the production of food are from the production of animal-based food while plant-based foods contribute 29% and the remaining 14% is for other utilizations. Farmland management and land-use change represented major shares of total emissions (38% and 29%, respectively), whereas rice and beef were the largest contributing plant- and animal-based commodities (12% and 25%, respectively). South and Southeast Asia and South America were the largest emitters of production-based GHGs.\n\nSustainability\nCurrent farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. There is not enough water to continue farming using current practices; therefore how water, land, and ecosystem resources are used to boost crop yields must be reconsidered. A solution would be to give value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for more productive farmland, or the preservation of a wetland system that limits fishing rights.Technological advancements help provide farmers with tools and resources to make farming more sustainable. Technology permits innovations like conservation tillage, a farming process which helps prevent land loss to erosion, reduces water pollution, and enhances carbon sequestration.Agricultural automation can help address some of the challenges associated with climate change and thus facilitate adaptation efforts. For example, the application of digital automation technologies (e.g. in precision agriculture) can improve resource-use efficiency in conditions which are increasingly constrained for agricultural producers. Moreover, when applied to sensing and early warning, they can help address the uncertainty and unpredictability of weather conditions associated with accelerating climate change.Other potential sustainable practices include conservation agriculture, agroforestry, improved grazing, avoided grassland conversion, and biochar. Current mono-crop farming practices in the United States preclude widespread adoption of sustainable practices, such as 2–3 crop rotations that incorporate grass or hay with annual crops, unless negative emission goals such as soil carbon sequestration become policy.The food demand of Earth's projected population, with current climate change predictions, could be satisfied by improvement of agricultural methods, expansion of agricultural areas, and a sustainability-oriented consumer mindset.\n\nEnergy dependence\nSince the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960s and the 1980s, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Heavy reliance on petrochemicals has raised concerns that oil shortages could increase costs and reduce agricultural output.Industrialized agriculture depends on fossil fuels in two fundamental ways: direct consumption on the farm and manufacture of inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery.Indirect consumption includes the manufacture of fertilizers, pesticides, and farm machinery. In particular, the production of nitrogen fertilizer can account for over half of agricultural energy usage. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has since gradually declined. Food systems encompass not just agriculture but off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.\n\nPlastic pollution\nPlastic products are used extensively in agriculture, including to increase crop yields and improve the efficiency of water and agrichemical use. \"Agriplastic\" products include films to cover greenhouses and tunnels, mulch to cover soil (e.g. to suppress weeds, conserve water, increase soil temperature and aid fertilizer application), shade cloth, pesticide containers, seedling trays, protective mesh and irrigation tubing. The polymers most commonly used in these products are low- density polyethylene (LPDE), linear low-density polyethylene (LLDPE), polypropylene (PP) and polyvinyl chloride (PVC).The total amount of plastics used in agriculture is difficult to quantify. A 2012 study reported that almost 6.5 million tonnes per year were consumed globally while a later study estimated that global demand in 2015 was between 7.3 million and 9 million tonnes. Widespread use of plastic mulch and lack of systematic collection and management have led to the generation of large amounts of mulch residue. Weathering and degradation eventually cause the mulch to fragment. These fragments and larger pieces of plastic accumulate in soil. Mulch residue has been measured at levels of 50 to 260 kg per hectare in topsoil in areas where mulch use dates back more than 10 years, which confirms that mulching is a major source of both microplastic and macroplastic soil contamination.Agricultural plastics, especially plastic films, are not easy to recycle because of high contamination levels (up to 40–50% by weight contamination by pesticides, fertilizers, soil and debris, moist vegetation, silage juice water, and UV stabilizers) and collection difficulties . Therefore, they are often buried or abandoned in fields and watercourses or burned. These disposal practices lead to soil degradation and can result in contamination of soils and leakage of microplastics into the marine environment as a result of precipitation run-off and tidal washing. In addition, additives in residual plastic film (such as UV and thermal stabilizers) may have deleterious effects on crop growth, soil structure, nutrient transport and salt levels. There is a risk that plastic mulch will deteriorate soil quality, deplete soil organic matter stocks, increase soil water repellence and emit greenhouse gases. Microplastics released through fragmentation of agricultural plastics can absorb and concentrate contaminants capable of being passed up the trophic chain.\n\nDisciplines\nAgricultural economics\nAgricultural economics is economics as it relates to the \"production, distribution and consumption of [agricultural] goods and services\". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.National government policies, such as taxation, subsidies, tariffs and others, can significantly change the economic marketplace for agricultural products. Since at least the 1960s, a combination of trade restrictions, exchange rate policies and subsidies have affected farmers in both the developing and the developed world. In the 1980s, non-subsidized farmers in developing countries experienced adverse effects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements limited agricultural tariffs, subsidies and other trade restrictions.However, as of 2009, there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the most trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the most taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have decreases more among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities such as corn, soybeans, and cattle are generally graded to indicate quality, affecting the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.\n\nAgricultural science\nAgricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences used in the practice and understanding of agriculture. It covers topics such as agronomy, plant breeding and genetics, plant pathology, crop modelling, soil science, entomology, production techniques and improvement, study of pests and their management, and study of adverse environmental effects such as soil degradation, waste management, and bioremediation.The scientific study of agriculture began in the 18th century, when Johann Friedrich Mayer conducted experiments on the use of gypsum (hydrated calcium sulphate) as a fertilizer. Research became more systematic when in 1843, John Lawes and Henry Gilbert began a set of long-term agronomy field experiments at Rothamsted Research Station in England; some of them, such as the Park Grass Experiment, are still running. In America, the Hatch Act of 1887 provided funding for what it was the first to call \"agricultural science\", driven by farmers' interest in fertilizers. In agricultural entomology, the USDA began to research biological control in 1881; it instituted its first large program in 1905, searching Europe and Japan for natural enemies of the gypsy moth and brown-tail moth, establishing parasitoids (such as solitary wasps) and predators of both pests in the US.\n\nPolicy\nAgricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.A 2021 report finds that globally, support to agricultural producers accounts for almost US$540 billion a year. This amounts to 15 percent of total agricultural production value, and is heavily biased towards measures that are leading to inefficiency, as well as are unequally distributed and harmful for the environment and human health.  \nThere are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.\n\nSee also\nReferences\nCited sources\nAcquaah, George (2002). Principles of Crop Production: Theory, Techniques, and Technology. Prentice Hall. ISBN 978-0-13-022133-9.\nChrispeels, Maarten J.; Sadava, David E. (1994). Plants, Genes, and Agriculture. Boston, Massachusetts: Jones and Bartlett. ISBN 978-0-86720-871-9.\nNeedham, Joseph (1986). Science and Civilization in China. Taipei: Caves Books. This article incorporates text from a free content work.  Licensed under CC BY-SA 3.0 IGO (license statement/permission). Text taken from Drowning in Plastics – Marine Litter and Plastic Waste Vital Graphics​,   United Nations Environment Programme. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.\n This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief: The State of Food and Agriculture 2019. Moving forward on food loss and waste reduction​,  FAO, FAO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.\n This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief to The State of Food Security and Nutrition in the World 2022. Repurposing food and agricultural policies to make healthy diets more affordable​,   FAO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.\n This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief: The State of Food and Agriculture 2018. Migration, agriculture and rural development​,  FAO, FAO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.\n This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief to The State of Food and Agriculture 2022. Leveraging automation in agriculture for transforming agrifood systems​,  FAO, FAO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.\n This article incorporates text from a free content work.  (license statement/permission). Text taken from Enabling inclusive agricultural automation​,  FAO, FAO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.\n\nExternal links\n\nFood and Agriculture Organization\nUnited States Department of Agriculture\nAgriculture material from the World Bank Group\nAgriculture collected news and commentary at The New York Times\nAgriculture collected news and commentary at The Guardian",
    "Alan Mackworth": "Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as \"The Founding Father\" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001 - 2014.\n\nEducation\nMackworth was educated at University of Toronto (B.A.Sc.), Harvard University (A.M.) and University of Sussex (D.Phil.).\n\nResearch\nHe works on constraint-based artificial intelligence with applications in vision, robotics, situated agents, assistive technology and sustainability. He is known as a pioneer in the areas of constraint satisfaction, robot soccer, hybrid systems and constraint-based agents. He has authored over 100 papers and co-authored two books: Computational Intelligence: A Logical Approach (1998) and Artificial Intelligence: Foundations of Computational Agents (2010).\n\nRoboCup\nMackworth proposed and built the world's first soccer-playing robots, which led to the development of robot soccer as the premier global platform for multi-agent robotic research through the International RoboCup Foundation, where he has been honoured as \"The Founding Father\". Robot soccer as a challenge problem has great scientific significance. It has now become a standard test environment for cross-testing research ideas: a forum for evolving theories of multi-agent systems. Through regular international RoboCup tournaments many research teams of students and professors compete and cooperate in the development, testing and evolution of new theories and new algorithms.\n\nCareer\nHe served as the founding director of the UBC Laboratory for Computational Intelligence. He was president and trustee of International Joint Conferences on AI (IJCAI) Inc.; he is on the IJCAI executive committee. He has served on many editorial boards and program committees.  He was VP and president of the Canadian Society for Computational Studies of Intelligence (CSCSI). He served as president of the Association for the Advancement of Artificial Intelligence (AAAI).\n\nAwards\nMackworth has received the ITAC/NSERC Award for Academic Excellence, the Killam Research Prize, the CSCSI Distinguished Service Award, the AAAI Distinguished Service Award, the Association for Constraint Programming Award for Research Excellence and the Lifetime Achievement Award of the Canadian AI Association (CAIAC). He is a Fellow of AAAI, the Canadian Institute for Advanced Research and the Royal Society of Canada.\n\nReferences\nExternal links\nAlan Mackworth web page",
    "Alan Turing": "Alan Mathison Turing  (; 23 June 1912 – 7 June 1954) was a British mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. He is widely considered to be the father of theoretical computer science and artificial intelligence.Born in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes–no questions can never be answered by computation and defined a Turing machine, and went on to prove that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory, at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.Turing was prosecuted in 1952 for homosexual acts. He accepted hormone treatment with DES, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning. \nFollowing a public campaign in 2009, the British prime minister Gordon Brown made an official public apology on behalf of the British government for \"the appalling way [Turing] was treated\". Queen Elizabeth II granted a posthumous pardon in 2013. The term \"Alan Turing law\" is now used informally to refer to a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations. He appears on the current Bank of England £50 note, which was released on 23 June 2021, to coincide with his birthday. A 2019 BBC series, as voted by the audience, named him the greatest person of the 20th century.\n\nEarly life and education\nFamily\nTuring was born in Maida Vale, London, while his father, Julius Mathison Turing was on leave from his position with the Indian Civil Service (ICS) of the British Raj government at Chatrapur, then in the Madras Presidency and presently in Odisha state, in India. Turing's father was the son of a clergyman, the Rev. John Robert Turing, from a Scottish family of merchants that had been based in the Netherlands and included a baronet. Turing's mother, Julius's wife, was Ethel Sara Turing (née Stoney), daughter of Edward Waller Stoney, chief engineer of the Madras Railways. The Stoneys were a Protestant Anglo-Irish gentry family from both County Tipperary and County Longford, while Ethel herself had spent much of her childhood in County Clare. Julius and Ethel married on 1 October 1907 at Bartholomew's church on Clyde Road, in Dublin.\nJulius's work with the ICS brought the family to British India, where his grandfather had been a general in the Bengal Army. However, both Julius and Ethel wanted their children to be brought up in Britain, so they moved to Maida Vale, London, where Alan Turing was born on 23 June 1912, as recorded by a blue plaque on the outside of the house of his birth, later the Colonnade Hotel. Turing had an elder brother, John Ferrier Turing, father of Sir John Dermot Turing, (12th Baronet of the Turing baronets).Turing's father's civil service commission was still active during Turing's childhood years, and his parents travelled between Hastings in the United Kingdom and India, leaving their two sons to stay with a retired Army couple. At Hastings, Turing stayed at Baston Lodge, Upper Maze Hill, St Leonards-on-Sea, now marked with a blue plaque. The plaque was unveiled on 23 June 2012, the centenary of Turing's birth.Very early in life, Turing showed signs of the genius that he was later to display prominently. His parents purchased a house in Guildford in 1927, and Turing lived there during school holidays. The location is also marked with a blue plaque.\n\nSchool\nTuring's parents enrolled him at St Michael's, a primary school at 20 Charles Road, St Leonards-on-Sea, from the age of six to nine. The headmistress recognised his talent, noting that she has \"...had clever boys and hardworking boys, but Alan is a genius\".Between January 1922 and 1926, Turing was educated at Hazelhurst Preparatory School, an independent school in the village of Frant in Sussex (now East Sussex). In 1926, at the age of 13, he went on to Sherborne School, a boarding independent school in the market town of Sherborne in Dorset, where he boarded at Westcott House. The first day of term coincided with the 1926 General Strike, in Britain, but Turing was so determined to attend that he rode his bicycle unaccompanied 60 miles (97 km) from Southampton to Sherborne, stopping overnight at an inn.\nTuring's natural inclination towards mathematics and science did not earn him respect from some of the teachers at Sherborne, whose definition of education placed more emphasis on the classics. His headmaster wrote to his parents: \"I hope he will not fall between two stools. If he is to stay at public school, he must aim at becoming educated. If he is to be solely a Scientific Specialist, he is wasting his time at a public school\". Despite this, Turing continued to show remarkable ability in the studies he loved, solving advanced problems in 1927 without having studied even elementary calculus. In 1928, aged 16, Turing encountered Albert Einstein's work; not only did he grasp it, but it is possible that he managed to deduce Einstein's questioning of Newton's laws of motion from a text in which this was never made explicit.\n\nChristopher Morcom\nAt Sherborne, Turing formed a significant friendship with fellow pupil Christopher Collan Morcom (13 July 1911 – 13 February 1930), who has been described as Turing's first love. Their relationship provided inspiration in Turing's future endeavours, but it was cut short by Morcom's death, in February 1930, from complications of bovine tuberculosis, contracted after drinking infected cow's milk some years previously.\nThe event caused Turing great sorrow. He coped with his grief by working that much harder on the topics of science and mathematics that he had shared with Morcom. In a letter to Morcom's mother, Frances Isobel Morcom (née Swan), Turing wrote:I am sure I could not have found anywhere another companion so brilliant and yet so charming and unconceited. I regarded my interest in my work, and in such things as astronomy (to which he introduced me) as something to be shared with him and I think he felt a little the same about me ... I know I must put as much energy if not as much interest into my work as if he were alive, because that is what he would like me to do.\nTuring's relationship with Morcom's mother continued long after Morcom's death, with her sending gifts to Turing, and him sending letters, typically on Morcom's birthday. A day before the third anniversary of Morcom's death (13 February 1933), he wrote to Mrs. Morcom: I expect you will be thinking of Chris when this reaches you. I shall too, and this letter is just to tell you that I shall be thinking of Chris and of you tomorrow. I am sure that he is as happy now as he was when he was here. Your affectionate Alan.\nSome have speculated that Morcom's death was the cause of Turing's atheism and materialism. Apparently, at this point in his life he still believed in such concepts as a spirit, independent of the body and surviving death. In a later letter, also written to Morcom's mother, Turing wrote: Personally, I believe that spirit is really eternally connected with matter but certainly not by the same kind of body ... as regards the actual connection between spirit and body I consider that the body can hold on to a 'spirit', whilst the body is alive and awake the two are firmly connected. When the body is asleep I cannot guess what happens but when the body dies, the 'mechanism' of the body, holding the spirit is gone and the spirit finds a new body sooner or later, perhaps immediately.\n\nUniversity and work on computability\nAfter graduating from Sherborne, Turing studied the undergraduate course in Schedule B (that is, a three-year Parts I and II, of the Mathematical Tripos, with extra courses at the end of the third year, as Part III only emerged as a separate degree in 1934) from February 1931 to November 1934 at King's College, Cambridge where he was awarded first-class honours in mathematics. His dissertation, On the Gaussian error function, written during his senior and delivered on November 1934 (with a deadline date of 6 December) proved a version of the central limit theorem. It was finally accepted in 16 March 1935. By spring of that same year, Turing started his master's course (Part III), -which he completed in 1937- and, at the same time, he published his first paper, a one-page article called Equivalence of left and right almost periodicity (sent on 23 April), featured in the tenth volume of the Journal of the London Mathematical Society. Later that year, Turing was elected a Fellow of King's College on the strength of his dissertation.  However, and, unknown to Turing, this version of the theorem he proved in his paper, had already been proven, in 1922, by Jarl Waldemar Lindeberg. Despite this, the committee found Turing's methods original and so regarded the work worthy of consideration for the fellowship. Abram Besicovitch's report for the committee went so far as to say that if Turing's work had been published before Lindeberg's, it would have been \"an important event in the mathematical literature of that year\".\nBetween the springs of 1935 and 1936, at the same time as Church, Turing worked on the decidability of problems, starting from Godel's incompleteness theorems. In mid-April 1936, Turing sent Max Newman the first draft typescript of his investigations. That same month, Alonzo Church published his An Unsolvable Problem of Elementary Number Theory, with similar conclusions to Turing's then-yet unpublished work. Finally, on 28 May of that year, he finished and delivered his 36-page paper for publication called \"On Computable Numbers, with an Application to the Entscheidungsproblem\". It was published in the Proceedings of the London Mathematical Society journal in two parts, the first on 30 November and the second on 23 December. In this paper, Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. The Entscheidungsproblem (decision problem) was originally posed by German mathematician David Hilbert in 1928. Turing proved that his \"universal computing machine\" would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the decision problem by first showing that the halting problem for Turing machines is undecidable: it is not possible to decide algorithmically whether a Turing machine will ever halt. This paper has been called \"easily the most influential math paper in history\".\nAlthough Turing's proof was published shortly after Alonzo Church's equivalent proof using his lambda calculus, Turing's approach is considerably more accessible and intuitive than Church's. It also included a notion of a 'Universal Machine' (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other computation machine (as indeed could Church's lambda calculus). According to the Church–Turing thesis, Turing machines and the lambda calculus are capable of computing anything that is computable. John von Neumann acknowledged that the central concept of the modern computer was due to Turing's paper. To this day, Turing machines are a central object of study in theory of computation.From September 1936 to July 1938, Turing spent most of his time studying under Church at Princeton University, in the second year as a Jane Eliza Procter Visiting Fellow. In addition to his purely mathematical work, he studied cryptology and also built three of four stages of an electro-mechanical binary multiplier. In June 1938, he obtained his PhD from the Department of Mathematics at Princeton; his dissertation, Systems of Logic Based on Ordinals, introduced the concept of ordinal logic and the notion of relative computing, in which Turing machines are augmented with so-called oracles, allowing the study of problems that cannot be solved by Turing machines. John von Neumann wanted to hire him as his postdoctoral assistant, but he went back to the United Kingdom.\n\nCareer and research\nWhen Turing returned to Cambridge, he attended lectures given in 1939 by Ludwig Wittgenstein about the foundations of mathematics. The lectures have been reconstructed verbatim, including interjections from Turing and other students, from students' notes. Turing and Wittgenstein argued and disagreed, with Turing defending formalism and Wittgenstein propounding his view that mathematics does not discover any absolute truths, but rather invents them.\n\nCryptanalysis\nDuring the Second World War, Turing was a leading participant in the breaking of German ciphers at Bletchley Park. The historian and wartime codebreaker Asa Briggs has said, \"You needed exceptional talent, you needed genius at Bletchley and Turing's was that genius.\"From September 1938, Turing worked part-time with the Government Code and Cypher School (GC&CS), the British codebreaking organisation. He concentrated on cryptanalysis of the Enigma cipher machine used by Nazi Germany, together with Dilly Knox, a senior GC&CS codebreaker. Soon after the July 1939 meeting near Warsaw at which the Polish Cipher Bureau gave the British and French details of the wiring of Enigma machine's rotors and their method of decrypting Enigma machine's messages, Turing and Knox developed a broader solution. The Polish method relied on an insecure indicator procedure that the Germans were likely to change, which they in fact did in May 1940. Turing's approach was more general, using crib-based decryption for which he produced the functional specification of the bombe (an improvement on the Polish Bomba).\nOn 4 September 1939, the day after the UK declared war on Germany, Turing reported to Bletchley Park, the wartime station of GC&CS. Like all others who came to Bletchley, he was required to sign the Official Secrets Act, in which he agreed not to disclose anything about his work at Bletchley, with severe legal penalties for violating the Act.Specifying the bombe was the first of five major cryptanalytical advances that Turing made during the war. The others were: deducing the indicator procedure used by the German navy; developing a statistical procedure dubbed Banburismus for making much more efficient use of the bombes; developing a procedure dubbed Turingery for working out the cam settings of the wheels of the Lorenz SZ 40/42 (Tunny) cipher machine and, towards the end of the war, the development of a portable secure voice scrambler at Hanslope Park that was codenamed Delilah.\nBy using statistical techniques to optimise the trial of different possibilities in the code breaking process, Turing made an innovative contribution to the subject. He wrote two papers discussing mathematical approaches, titled The Applications of Probability to Cryptography and Paper on Statistics of Repetitions, which were of such value to GC&CS and its successor GCHQ that they were not released to the UK National Archives until April 2012, shortly before the centenary of his birth. A GCHQ mathematician, \"who identified himself only as Richard,\" said at the time that the fact that the contents had been restricted under the Official Secrets Act for some 70 years demonstrated their importance, and their relevance to post-war cryptanalysis: [He] said the fact that the contents had been restricted \"shows what a tremendous importance it has in the foundations of our subject\". ... The papers detailed using \"mathematical analysis to try and determine which are the more likely settings so that they can be tried as quickly as possible\". ... Richard said that GCHQ had now \"squeezed the juice\" out of the two papers and was \"happy for them to be released into the public domain\".\nTuring had a reputation for eccentricity at Bletchley Park. He was known to his colleagues as \"Prof\" and his treatise on Enigma was known as the \"Prof's Book\". According to historian Ronald Lewin, Jack Good, a cryptanalyst who worked with Turing, said of his colleague:\n\nIn the first week of June each year he would get a bad attack of hay fever, and he would cycle to the office wearing a service gas mask to keep the pollen off. His bicycle had a fault: the chain would come off at regular intervals. Instead of having it mended he would count the number of times the pedals went round and would get off the bicycle in time to adjust the chain by hand. Another of his eccentricities is that he chained his mug to the radiator pipes to prevent it being stolen.\nPeter Hilton recounted his experience working with Turing in Hut 8 in his \"Reminiscences of Bletchley Park\" from A Century of Mathematics in America:\n It is a rare experience to meet an authentic genius. Those of us privileged to inhabit the world of scholarship are familiar with the intellectual stimulation furnished by talented colleagues. We can admire the ideas they share with us and are usually able to understand their source; we may even often believe that we ourselves could have created such concepts and originated such thoughts. However, the experience of sharing the intellectual life of a genius is entirely different; one realizes that one is in the presence of an intelligence, a sensibility of such profundity and originality that one is filled with wonder and excitement.\nAlan Turing was such a genius, and those, like myself, who had the astonishing and unexpected opportunity, created by the strange exigencies of the Second World War, to be able to count Turing as colleague and friend will never forget that experience, nor can we ever lose its immense benefit to us.\nHilton echoed similar thoughts in the Nova PBS documentary Decoding Nazi Secrets.While working at Bletchley, Turing, who was a talented long-distance runner, occasionally ran the 40 miles (64 km) to London when he was needed for meetings, and he was capable of world-class marathon standards. Turing tried out for the 1948 British Olympic team, but he was hampered by an injury. His tryout time for the marathon was only 11 minutes slower than British silver medallist Thomas Richards' Olympic race time of 2 hours 35 minutes. He was Walton Athletic Club's best runner, a fact discovered when he passed the group while running alone. When asked why he ran so hard in training he replied:\n\nI have such a stressful job that the only way I can get it out of my mind is by running hard; it's the only way I can get some release.\nDue to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war. However, official war historian Harry Hinsley estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives.At the end of the war, a memo was sent to all those who had worked at Bletchley Park, reminding them that the code of silence dictated by the Official Secrets Act did not end with the war but would continue indefinitely. Thus, even though Turing was appointed an Officer of the Order of the British Empire (OBE) in 1946 by King George VI for his wartime services, his work remained secret for many years.\n\nBombe\nWithin weeks of arriving at Bletchley Park, Turing had specified an electromechanical machine called the bombe, which could break Enigma more effectively than the Polish bomba kryptologiczna, from which its name was derived. The bombe, with an enhancement suggested by mathematician Gordon Welchman, became one of the primary tools, and the major automated one, used to attack Enigma-enciphered messages.\nThe bombe searched for possible correct settings used for an Enigma message (i.e., rotor order, rotor settings and plugboard settings) using a suitable crib: a fragment of probable plaintext. For each possible setting of the rotors (which had on the order of 1019 states, or 1022 states for the four-rotor U-boat variant), the bombe performed a chain of logical deductions based on the crib, implemented electromechanically.The bombe detected when a contradiction had occurred and ruled out that setting, moving on to the next. Most of the possible settings would cause contradictions and be discarded, leaving only a few to be investigated in detail. A contradiction would occur when an enciphered letter would be turned back into the same plaintext letter, which was impossible with the Enigma. The first bombe was installed on 18 March 1940.\n\nAction This Day\nBy late 1941, Turing and his fellow cryptanalysts Gordon Welchman, Hugh Alexander and Stuart Milner-Barry were frustrated. Building on the work of the Poles, they had set up a good working system for decrypting Enigma signals, but their limited staff and bombes meant they could not translate all the signals. In the summer, they had considerable success, and shipping losses had fallen to under 100,000 tons a month; however, they badly needed more resources to keep abreast of German adjustments. They had tried to get more people and fund more bombes through the proper channels, but had failed.On 28 October they wrote directly to Winston Churchill explaining their difficulties, with Turing as the first named. They emphasised how small their need was compared with the vast expenditure of men and money by the forces and compared with the level of assistance they could offer to the forces. As Andrew Hodges, biographer of Turing, later wrote, \"This letter had an electric effect.\" Churchill wrote a memo to General Ismay, which read: \"ACTION THIS DAY. Make sure they have all they want on extreme priority and report to me that this has been done.\" On 18 November, the chief of the secret service reported that every possible measure was being taken. The cryptographers at Bletchley Park did not know of the Prime Minister's response, but as Milner-Barry recalled, \"All that we did notice was that almost from that day the rough ways began miraculously to be made smooth.\" More than two hundred bombes were in operation by the end of the war.\n\nHut 8 and the naval Enigma\nTuring decided to tackle the particularly difficult problem of German naval Enigma \"because no one else was doing anything about it and I could have it to myself\". In December 1939, Turing solved the essential part of the naval indicator system, which was more complex than the indicator systems used by the other services.That same night, he also conceived of the idea of Banburismus, a sequential statistical technique (what Abraham Wald later called sequential analysis) to assist in breaking the naval Enigma, \"though I was not sure that it would work in practice, and was not, in fact, sure until some days had actually broken\". For this, he invented a measure of weight of evidence that he called the ban. Banburismus could rule out certain sequences of the Enigma rotors, substantially reducing the time needed to test settings on the bombes. Later this sequential process of accumulating sufficient weight of evidence using decibans (one tenth of a ban) was used in Cryptanalysis of the Lorenz cipher.Turing travelled to the United States in November 1942 and worked with US Navy cryptanalysts on the naval Enigma and bombe construction in Washington. He also visited their Computing Machine Laboratory in Dayton, Ohio.Turing's reaction to the American bombe design was far from enthusiastic:\n\nThe American Bombe programme was to produce 336 Bombes, one for each wheel order. I used to smile inwardly at the conception of Bombe hut routine implied by this programme, but thought that no particular purpose would be served by pointing out that we would not really use them in that way.\nTheir test (of commutators) can hardly be considered conclusive as they were not testing for the bounce with electronic stop finding devices. Nobody seems to be told about rods or offiziers or banburismus unless they are really going to do something about it.\nDuring this trip, he also assisted at Bell Labs with the development of secure speech devices. He returned to Bletchley Park in March 1943. During his absence, Hugh Alexander had officially assumed the position of head of Hut 8, although Alexander had been de facto head for some time (Turing having little interest in the day-to-day running of the section). Turing became a general consultant for cryptanalysis at Bletchley Park.Alexander wrote of Turing's contribution:\n\nThere should be no question in anyone's mind that Turing's work was the biggest factor in Hut 8's success. In the early days, he was the only cryptographer who thought the problem worth tackling and not only was he primarily responsible for the main theoretical work within the Hut, but he also shared with Welchman and Keen the chief credit for the invention of the bombe. It is always difficult to say that anyone is 'absolutely indispensable', but if anyone was indispensable to Hut 8, it was Turing. The pioneer's work always tends to be forgotten when experience and routine later make everything seem easy and many of us in Hut 8 felt that the magnitude of Turing's contribution was never fully realised by the outside world.\n\nTuringery\nIn July 1942, Turing devised a technique termed Turingery (or jokingly Turingismus) for use against the Lorenz cipher messages produced by the Germans' new Geheimschreiber (secret writer) machine. This was a teleprinter rotor cipher attachment codenamed Tunny at Bletchley Park. Turingery was a method of wheel-breaking, i.e., a procedure for working out the cam settings of Tunny's wheels. He also introduced the Tunny team to Tommy Flowers who, under the guidance of Max Newman, went on to build the Colossus computer, the world's first programmable digital electronic computer, which replaced a simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages. Some have mistakenly said that Turing was a key figure in the design of the Colossus computer. Turingery and the statistical approach of Banburismus undoubtedly fed into the thinking about cryptanalysis of the Lorenz cipher, but he was not directly involved in the Colossus development.\n\nDelilah\nFollowing his work at Bell Labs in the US, Turing pursued the idea of electronic enciphering of speech in the telephone system. In the latter part of the war, he moved to work for the Secret Service's Radio Security Service (later HMGCC) at Hanslope Park. At the park, he further developed his knowledge of electronics with the assistance of REME officer Donald Bayley. Together they undertook the design and construction of a portable secure voice communications machine codenamed Delilah. The machine was intended for different applications, but it lacked the capability for use with long-distance radio transmissions. In any case, Delilah was completed too late to be used during the war. Though the system worked fully, with Turing demonstrating it to officials by encrypting and decrypting a recording of a Winston Churchill speech, Delilah was not adopted for use. Turing also consulted with Bell Labs on the development of SIGSALY, a secure voice system that was used in the later years of the war.\n\nEarly computers and the Turing test\nBetween 1945 and 1947, Turing lived in Hampton, London, while he worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory (NPL). He presented a paper on 19 February 1946, which was the first detailed design of a stored-program computer. Von Neumann's incomplete First Draft of a Report on the EDVAC had predated Turing's paper, but it was much less detailed and, according to John R. Womersley, Superintendent of the NPL Mathematics Division, it \"contains a number of ideas which are Dr. Turing's own\".Although ACE was a feasible design, the effect of the Official Secrets Act surrounding the wartime work at Bletchley Park made it impossible for Turing to explain the basis of his analysis of how a computer installation involving human operators would work. This led to delays in starting the project and he became disillusioned. In late 1947 he returned to Cambridge for a sabbatical year during which he produced a seminal work on Intelligent Machinery that was not published in his lifetime. While he was at Cambridge, the Pilot ACE was being built in his absence. It executed its first program on 10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until after his death.According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Düsseldorf, there was a meeting between Turing and Konrad Zuse. It took place in Göttingen in 1947. The interrogation had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing (for more details see Herbert Bruderer, Konrad Zuse und die Schweiz).\nIn 1948, Turing was appointed reader in the Mathematics Department at the Victoria University of Manchester. A year later, he became deputy director of the Computing Machine Laboratory, where he worked on software for one of the earliest stored-program computers—the Manchester Mark 1. Turing wrote the first version of the Programmer's Manual for this machine, and was recruited by Ferranti as a consultant in the development of their commercialised machine, the Ferranti Mark 1. He continued to be paid consultancy fees by Ferranti until his death. During this time, he continued to do more abstract work in mathematics, and in \"Computing Machinery and Intelligence\" (Mind, October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the Turing test, an attempt to define a standard for a machine to be called \"intelligent\". The idea was that a computer could be said to \"think\" if a human interrogator could not tell it apart, through conversation, from a human being. In the paper, Turing suggested that rather than building a program to simulate the adult mind, it would be better to produce a simpler one to simulate a child's mind and then to subject it to a course of education. A reversed form of the Turing test is widely used on the Internet; the CAPTCHA test is intended to determine whether the user is a human or a computer.\nIn 1948, Turing, working with his former undergraduate colleague, D.G. Champernowne, began writing a chess program for a computer that did not yet exist. By 1950, the program was completed and dubbed the Turochamp. In 1952, he tried to implement it on a Ferranti Mark 1, but lacking enough power, the computer was unable to execute the program. Instead, Turing \"ran\" the program by flipping through the pages of the algorithm and carrying out its instructions on a chessboard, taking about half an hour per move. The game was recorded. According to Garry Kasparov, Turing's program \"played a recognizable game of chess\". The program lost to Turing's colleague Alick Glennie, although it is said that it won a game against Champernowne's wife,\nIsabel.His Turing test was a significant, characteristically provocative, and lasting contribution to the debate regarding artificial intelligence, which continues after more than half a century.\n\nPattern formation and mathematical biology\nWhen Turing was 39 years old in 1951, he turned to mathematical biology, finally publishing his masterpiece \"The Chemical Basis of Morphogenesis\" in January 1952. He was interested in morphogenesis, the development of patterns and shapes in biological organisms. He suggested that a system of chemicals reacting with each other and diffusing across space, termed a reaction–diffusion system, could account for \"the main phenomena of morphogenesis\". He used systems of partial differential equations to model catalytic chemical reactions. For example, if a catalyst A is required for a certain chemical reaction to take place, and if the reaction produced more of the catalyst A, then we say that the reaction is autocatalytic, and there is positive feedback that can be modelled by nonlinear differential equations. Turing discovered that patterns could be created if the chemical reaction not only produced catalyst A, but also produced an inhibitor B that slowed down the production of A. If A and B then diffused through the container at different rates, then you could have some regions where A dominated and some where B did. To calculate the extent of this, Turing would have needed a powerful computer, but these were not so freely available in 1951, so he had to use linear approximations to solve the equations by hand. These calculations gave the right qualitative results, and produced, for example, a uniform mixture that oddly enough had regularly spaced fixed red spots. The Russian biochemist Boris Belousov had performed experiments with similar results, but could not get his papers published because of the contemporary prejudice that any such thing violated the second law of thermodynamics. Belousov was not aware of Turing's paper in the Philosophical Transactions of the Royal Society.Although published before the structure and role of DNA was understood, Turing's work on morphogenesis remains relevant today and is considered a seminal piece of work in mathematical biology. One of the early applications of Turing's paper was the work by James Murray explaining spots and stripes on the fur of cats, large and small. Further research in the area suggests that Turing's work can partially explain the growth of \"feathers, hair follicles, the branching pattern of lungs, and even the left-right asymmetry that puts the heart on the left side of the chest\". In 2012, Sheth, et al. found that in mice, removal of Hox genes causes an increase in the number of digits without an increase in the overall size of the limb, suggesting that Hox genes control digit formation by tuning the wavelength of a Turing-type mechanism. Later papers were not available until Collected Works of A. M. Turing was published in 1992.A study conducted in 2023 confirmed Turing's mathematical model hypothesis. Presented by the American Physical Society, the experiment involved growing chia seeds in even layers within trays, later adjusting the available moisture. Researchers experimentally tweaked the factors which appear in the Turing equations, and, as a result, patterns resembling those seen in natural environments emerged. This is believed to be the first time that experiments with living vegetation have verified Turing’s mathematical insight.\n\nPersonal life\nTreasure\nIn the 1940s, Turing became worried about losing his savings in the event of a German invasion. In order to protect it, he bought two silver bars weighing 3,200 oz (90 kg) and worth £250 (in 2022, £8,000 adjusted for inflation, £48,000 at spot price) and buried them in a wood near Bletchley Park. Upon returning to dig them up, Turing found that he was unable to break his own code describing where exactly he had hidden them. This, along with the fact that the area had been renovated, meant that he never regained the silver.\n\nEngagement\nIn 1941, Turing proposed marriage to Hut 8 colleague Joan Clarke, a fellow mathematician and cryptanalyst, but their engagement was short-lived. After admitting his homosexuality to his fiancée, who was reportedly \"unfazed\" by the revelation, Turing decided that he could not go through with the marriage.\n\nHomosexuality and indecency conviction\nIn January 1952, Turing was 39 when he started a relationship with Arnold Murray, a 19-year-old unemployed man. Just before Christmas, Turing was walking along Manchester's Oxford Road when he met Murray just outside the Regal Cinema and invited him to lunch. On 23 January, Turing's house was burgled. Murray told Turing that he and the burglar were acquainted, and Turing reported the crime to the police. During the investigation, he acknowledged a sexual relationship with Murray. Homosexual acts were criminal offences in the United Kingdom at that time, and both men were charged with \"gross indecency\" under Section 11 of the Criminal Law Amendment Act 1885. Initial committal proceedings for the trial were held on 27 February during which Turing's solicitor \"reserved his defence\", i.e., did not argue or provide evidence against the allegations. The proceedings were held at the Sessions House in Knutsford.Turing was later convinced by the advice of his brother and his own solicitor, and he entered a plea of guilty. The case, Regina v. Turing and Murray, was brought to trial on 31 March 1952. Turing was convicted and given a choice between imprisonment and probation. His probation would be conditional on his agreement to undergo hormonal physical changes designed to reduce libido, known as \"chemical castration\". He accepted the option of injections of what was then called stilboestrol (now known as diethylstilbestrol or DES), a synthetic oestrogen; this feminization of his body was continued for the course of one year. The treatment rendered Turing impotent and caused breast tissue to form, fulfilling in the literal sense Turing's prediction that \"no doubt I shall emerge from it all a different man, but quite who I've not found out\". Murray was given a conditional discharge.Turing's conviction led to the removal of his security clearance and barred him from continuing with his cryptographic consultancy for the Government Communications Headquarters (GCHQ), the British signals intelligence agency that had evolved from GC&CS in 1946, though he kept his academic job. He was denied entry into the United States after his conviction in 1952, but was free to visit other European countries.\n\nDeath\nOn 8 June 1954, at his house at 43 Adlington Road, Wilmslow, Turing's housekeeper found him dead. He had died the previous day at the age of 41. Cyanide poisoning was established as the cause of death. When his body was discovered, an apple lay half-eaten beside his bed, and although the apple was not tested for cyanide, it was speculated that this was the means by which Turing had consumed a fatal dose. An inquest determined his death to be a suicide. Andrew Hodges and another biographer, David Leavitt, have both speculated that Turing was re-enacting a scene from the Walt Disney film Snow White and the Seven Dwarfs (1937), his favourite fairy tale. Both men noted that (in Leavitt's words) he took \"an especially keen pleasure in the scene where the Wicked Queen immerses her apple in the poisonous brew\". Turing's remains were cremated at Woking Crematorium on 12 June 1954, and his ashes were scattered in the gardens of the crematorium, just as his father's had been.Philosopher Jack Copeland has questioned various aspects of the coroner's historical verdict. He suggested an alternative explanation for the cause of Turing's death: the accidental inhalation of cyanide fumes from an apparatus used to electroplate gold onto spoons. The potassium cyanide was used to dissolve the gold. Turing had such an apparatus set up in his tiny spare room. Copeland noted that the autopsy findings were more consistent with inhalation than with ingestion of the poison. Turing also habitually ate an apple before going to bed, and it was not unusual for the apple to be discarded half-eaten. Furthermore, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) \"with good humour\" and had shown no sign of despondency before his death. He even set down a list of tasks that he intended to complete upon returning to his office after the holiday weekend. Turing's mother believed that the ingestion was accidental, resulting from her son's careless storage of laboratory chemicals. Biographer Andrew Hodges theorised that Turing deliberately left the nature of his death ambiguous in order to shield his mother from the knowledge that he had killed himself.\nIt has been suggested that Turing's belief in fortune-telling may have caused his depressed mood. As a youth, Turing had been told by a fortune-teller that he would be a genius. In mid-May 1954, shortly before his death, Turing again decided to consult a fortune-teller during a day-trip to St Annes-on-Sea with the Greenbaum family. According to the Greenbaums' daughter, Barbara:\n\nBut it was a lovely sunny day and Alan was in a cheerful mood and off we went... Then he thought it would be a good idea to go to the Pleasure Beach at Blackpool. We found a fortune-teller's tent[,] and Alan said he'd like to go in[,] so we waited around for him to come back... And this sunny, cheerful visage had shrunk into a pale, shaking, horror-stricken face. Something had happened. We don't know what the fortune-teller said[,] but he obviously was deeply unhappy. I think that was probably the last time we saw him before we heard of his suicide.\n\nGovernment apology and pardon\nIn August 2009, British programmer John Graham-Cumming started a petition urging the British government to apologise for Turing's prosecution as a homosexual. The petition received more than 30,000 signatures. The prime minister, Gordon Brown, acknowledged the petition, releasing a statement on 10 September 2009 apologising and describing the treatment of Turing as \"appalling\":\nThousands of people have come together to demand justice for Alan Turing and recognition of the appalling way he was treated. While Turing was dealt with under the law of the time and we can't put the clock back, his treatment was of course utterly unfair and I am pleased to have the chance to say how deeply sorry I and we all are for what happened to him ... So on behalf of the British government, and all those who live freely thanks to Alan's work I am very proud to say: we're sorry, you deserved so much better.\nIn December 2011, William Jones and his member of Parliament, John Leech, created an e-petition requesting that the British government pardon Turing for his conviction of \"gross indecency\":\nWe ask the HM Government to grant a pardon to Alan Turing for the conviction of \"gross indecency\". In 1952, he was convicted of \"gross indecency\" with another man and was forced to undergo so-called \"organo-therapy\"—chemical castration. Two years later, he killed himself with cyanide, aged just 41. Alan Turing was driven to a terrible despair and early death by the nation he'd done so much to save. This remains a shame on the British government and British history. A pardon can go some way to healing this damage. It may act as an apology to many of the other gay men, not as well-known as Alan Turing, who were subjected to these laws.\nThe petition gathered over 37,000 signatures, and was submitted to Parliament by the Manchester MP John Leech but the request was discouraged by Justice Minister Lord McNally, who said:\nA posthumous pardon was not considered appropriate as Alan Turing was properly convicted of what at the time was a criminal offence. He would have known that his offence was against the law and that he would be prosecuted. It is tragic that Alan Turing was convicted of an offence that now seems both cruel and absurd—particularly poignant given his outstanding contribution to the war effort. However, the law at the time required a prosecution and, as such, long-standing policy has been to accept that such convictions took place and, rather than trying to alter the historical context and to put right what cannot be put right, ensure instead that we never again return to those times.\nJohn Leech, the MP for Manchester Withington (2005–15), submitted several bills to Parliament and led a high-profile campaign to secure the pardon. Leech made the case in the House of Commons that Turing's contribution to the war made him a national hero and that it was \"ultimately just embarrassing\" that the conviction still stood. Leech continued to take the bill through Parliament and campaigned for several years, gaining the public support of numerous leading scientists, including Stephen Hawking. At the British premiere of a film based on Turing's life, The Imitation Game, the producers thanked Leech for bringing the topic to public attention and securing Turing's pardon. Leech is now regularly described as the \"architect\" of Turing's pardon and subsequently the Alan Turing Law which went on to secure pardons for 75,000 other men and women convicted of similar crimes.On 26 July 2012, a bill was introduced in the House of Lords to grant a statutory pardon to Turing for offences under section 11 of the Criminal Law Amendment Act 1885, of which he was convicted on 31 March 1952. Late in the year in a letter to The Daily Telegraph, the physicist Stephen Hawking and 10 other signatories including the Astronomer Royal Lord Rees, President of the Royal Society Sir Paul Nurse, Lady Trumpington (who worked for Turing during the war) and Lord Sharkey (the bill's sponsor) called on Prime Minister David Cameron to act on the pardon request. The government indicated it would support the bill, and it passed its third reading in the House of Lords in October.At the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage. The bill was due to return to the House of Commons on 28 February 2014, but before the bill could be debated in the House of Commons, the government elected to proceed under the royal prerogative of mercy. On 24 December 2013, Queen Elizabeth II signed a pardon for Turing's conviction for \"gross indecency\", with immediate effect. Announcing the pardon, Lord Chancellor Chris Grayling said Turing deserved to be \"remembered and recognised for his fantastic contribution to the war effort\" and not for his later criminal conviction. The Queen officially pronounced Turing pardoned in August 2014. The Queen's action is only the fourth royal pardon granted since the conclusion of the Second World War. Pardons are normally granted only when the person is technically innocent, and a request has been made by the family or other interested party; neither condition was met in regard to Turing's conviction.In September 2016, the government announced its intention to expand this retroactive exoneration to other men convicted of similar historical indecency offences, in what was described as an \"Alan Turing law\". The Alan Turing law is now an informal term for the law in the United Kingdom, contained in the Policing and Crime Act 2017, which serves as an amnesty law to retroactively pardon men who were cautioned or convicted under historical legislation that outlawed homosexual acts. The law applies in England and Wales.\n\nSee also\nLegacy of Alan Turing\nList of things named after Alan Turing\n\nNotes and references\nNotes\nReferences\nSources\nFurther reading\nArticles\nTuring, Alan (1950). \"Computing Machinery and Intelligence\" (PDF). Mind. 49 (236): 433–460. doi:10.1093/mind/LIX.236.433. Archived (PDF) from the original on 9 October 2022.\nCopeland, B. Jack (ed.). \"The Mind and the Computing Machine: Alan Turing and others\". The Rutherford Journal.\nCopeland, B. Jack (ed.). \"Alan Turing: Father of the Modern Computer\". The Rutherford Journal.\nHodges, Andrew (2007). \"Alan Turing\".  In Edward N. Zalta (ed.). Stanford Encyclopedia of Philosophy (Winter 2009 ed.). Stanford University. Retrieved 10 January 2011.\nHodges, Andrew (2004). \"Turing, Alan Mathison\". Oxford Dictionary of National Biography (online ed.). Oxford University Press. doi:10.1093/ref:odnb/36578. (Subscription or UK public library membership required.)\nGray, Paul (29 March 1999). \"Computer Scientist: Alan Turing\". Time. Archived from the original on 16 October 2007.\n\nBooks\nBernhardt, Chris (2017), Turing's Vision: The Birth of Computer Science, MIT Press, ISBN 978-0-262-53351-5\nCopeland, B. Jack; Bowen, Jonathan P.; Wilson, Robin; Sprevak, Mark (2017). The Turing Guide. Oxford University Press. ISBN 978-0-19-874783-3.\nDyson, George (2012). Turing's Cathedral: The Origins of the Digital Universe. Vintage. ISBN 978-1-4000-7599-7.\nGleick, James (2011). The Information: A History, a Theory, a Flood. New York: Pantheon. ISBN 978-0-375-42372-7.\nHodges, Andrew (2014). Alan Turing: The Enigma. Princeton University Press. ISBN 978-0-691-16472-4. (originally published in 1983); basis of the film The Imitation Game\nTuring, Sara (2012). Alan M. Turing. Cambridge University Press. ISBN 978-1-107-02058-0. (originally published in 1959 by W. Heffer & Sons, Ltd)\n\nExternal links\n\nOral history interview with Nicholas C. Metropolis, Charles Babbage Institute, University of Minnesota. Metropolis was the first director of computing services at Los Alamos National Laboratory; topics include the relationship between Turing and John von Neumann\nHow Alan Turing Cracked The Enigma Code Imperial War Museums\nAlan Turing Year Archived 17 February 2019 at the Wayback Machine\nCiE 2012: Turing Centenary Conference\nScience in the Making Alan Turing's papers in the Royal Society's archives\nAlan Turing site maintained by Andrew Hodges including a short biography\nAlanTuring.net – Turing Archive for the History of Computing by Jack Copeland\nThe Turing Archive – contains scans of some unpublished documents and material from the King's College, Cambridge archive\nAlan Turing Papers – University of Manchester Library, Manchester\nJones, G. James (11 December 2001). \"Alan Turing – Towards a Digital Mind: Part 1\". System Toolbox. The Binary Freedom Project. Archived from the original on 3 August 2007.\nSherborne School Archives – holds papers relating to Turing's time at Sherborne School\nAlan Turing plaques recorded on openplaques.org\nAlan Turing archive on New Scientist",
    "AlexNet": "AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor.AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up.  The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.\n\nHistoric context\nAlexNet was not the first fast GPU-implementation of a CNN to win an image recognition contest. A CNN on GPU by K. Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU. A deep CNN of Dan Cireșan et al. (2011) at IDSIA was already 60 times faster and outperformed predecessors in August 2011. Between May 15, 2011 and September 10, 2012, their CNN won no fewer than four image competitions. They also significantly improved on the best performance in the literature for multiple image databases.According to the AlexNet paper, Cireșan's earlier net is \"somewhat similar.\" Both were originally written with CUDA to run with GPU support. In fact, both are actually just variants of the CNN designs introduced by Yann LeCun et al. (1989) who applied the backpropagation algorithm to a variant of Kunihiko Fukushima's original CNN architecture called \"neocognitron.\" The architecture was later modified by J. Weng's method called max-pooling.In 2015, AlexNet was outperformed by Microsoft Research Asia's very deep CNN with over 100 layers, which won the ImageNet 2015 contest.\n\nNetwork design\nAlexNet contained eight layers; the first five were convolutional layers, some of them followed by max-pooling layers, and the last three were fully connected layers. The network, except the last layer, is split into two copies, each run on one GPU. The entire structure can be written as  where \n\nCNN = convolutional layer (with ReLU activation)\nRN = local response normalization\nMP = maxpooling\nFC = fully connected layer (with ReLU activation)\nLinear = fully connected layer (without activation)\nDO = dropoutIt used the non-saturating ReLU activation function, which showed improved training performance over tanh and sigmoid.\n\nInfluence\nAlexNet is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. As of early 2023, the AlexNet paper has been cited over 120,000 times according to Google Scholar.\n\n\n== References ==",
    "Alex Graves (computer scientist)": "Alex Graves is a computer scientist. Before working as a research scientist at DeepMind, he earned a BSc in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence under Jürgen Schmidhuber at IDSIA. He was also a postdoc under Schmidhuber at the Technical University of Munich and under Geoffrey Hinton at the University of Toronto.\nAt IDSIA, Graves trained long short-term memory neural networks by a novel method called connectionist temporal classification (CTC). This method outperformed traditional speech recognition models in certain applications. In 2009, his CTC-trained LSTM was the first recurrent neural network to win pattern recognition contests, winning several competitions in connected handwriting recognition.\nThis method has become very popular. Google uses CTC-trained LSTM for speech recognition on the smartphone.Graves is also the creator of neural Turing machines and the closely related differentiable neural computer.\n\n\n== References ==",
    "Algorithm": "In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\nHistory\nAncient algorithms\nSince antiquity, step-by-step procedures for solving mathematical problems have been attested. This includes Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later; e.g. Shulba Sutras, Kerala School, and Brāhmasphuṭasiddhānta), The Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC, e.g. sieve of Eratosthenes and Euclidean algorithm), and Arabic mathematics (9th century, e.g. cryptographic algorithms for code-breaking based on frequency analysis).\n\nAl-khwarizmi and the term algorithm\nAround 825, Muhammad ibn Musa al-Khwarizmi wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). Both of these texts are lost in the original Arabic at this time. (However, his other book on algebra remains.)In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared: Liber Alghoarismi de practica arismetrice (attributed to John of Seville) and Liber Algorismi de numero Indorum (attributed to Adelard of Bath). Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi (\"Thus spoke Al-Khwarizmi\").In 1240, Alexander of Villedieu writes a Latin text titled Carmen de Algorismo. It begins with:\n\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\nwhich translates to:\n\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.\n\nEnglish evolution of the word\nAround 1230, the English word algorism is attested and then by Chaucer in 1391. English adopted the French term.In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.\nIn 1656, in the English dictionary Glossographia, it says:\n\nAlgorism ([Latin] algorismus) the Art or use of Cyphers, or of numbering by Cyphers; skill in accounting.\nAugrime ([Latin] algorithmus) skil in accounting or numbring.\n\nIn 1658, in the first edition of The New World of English Words, it says:\n\nAlgorithme, (a word compounded of Arabick and Spanish,) the art of reckoning by Cyphers.\n\nIn 1706, in the sixth edition of The New World of English Words, it says:\n\nAlgorithm, the Art of computing or reckoning by numbers, which contains the five principle Rules of Arithmetick, viz. Numeration, Addition, Subtraction, Multiplication and Division; to which may be added Extraction of Roots: It is also call'd Logistica Numeralis.\nAlgorism, the practical Operation in the several Parts of Specious Arithmetick or Algebra; sometimes it is taken for the Practice of Common Arithmetick by the ten Numeral Figures.\n\nIn 1751, in the Young Algebraist's Companion, Daniel Fenning contrasts the terms algorism and algorithm as follows:\n\nAlgorithm signifies the first Principles, and Algorism the practical Part, or knowing how to put the Algorithm in Practice.\n\nSince at least 1811, the term algorithm is attested to mean a \"step-by-step procedure\" in English.In 1842, in the Dictionary of Science, Literature and Art, it says:\n\nALGORITHM, signifies the art of computing in reference to some particular subject, or in some particular way; as the algorithm of numbers; the algorithm of the differential calculus.\n\nMachine usage\nIn 1928, a partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\nInformal definition\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\nor cook-book recipe.In general, a program is only an algorithm if it stops eventually—even though infinite loops may sometimes prove desirable.\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word \"algorithm\" in the following quotation:\n\nNo human being can write fast enough, or long enough, or small enough† ( †\"smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nPrecise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\nFormalization\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987), and Gurevich (2000):\n\n Minsky: \"But we will also maintain, with Turing ... that any procedure which could \"naturally\" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute\".\n Gurevich: \"… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\nFor some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"—an idea that is described more formally by flow of control.\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.\nFor some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.\n\nExpressing algorithms\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description, as follows:\n1 High-level description\n\"...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\n2 Implementation description\n\"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\n3 Formal description\nMost detailed, \"lowest level\", gives the Turing machine's \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Examples.\n\nDesign\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.\nTypical steps in the development of algorithms:\n\nProblem definition\nDevelopment of a model\nSpecification of the algorithm\nDesigning an algorithm\nChecking the correctness of the algorithm\nAnalysis of algorithm\nImplementation of algorithm\nProgram testing\nDocumentation preparation\n\nComputer algorithms\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nKnuth: \" ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity, and elegance, etc.\"Chaitin: \" ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"—such a proof would solve the Halting problem (ibid).\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\nComputers (and computors), models of computation: A computer (or human \"computer\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is convenient; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z ← 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.\nSimulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\nStructured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.\n\nExamples\nAlgorithm example\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\nHigh-level description:\n\nIf there are no numbers in the set, then there is no highest number.\nAssume the first number in the set is the largest number in the set.\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\nEuclid's algorithm\nIn mathematics, the Euclidean algorithm or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC). It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.\n\nEuclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \"proper\"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\nComputer language for Euclid's algorithm\nOnly a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.\n\nAn inelegant program for Euclid's algorithm\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\nINPUT:\n\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\nINPUT L, S\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\nR ← L\n\nE0: [Ensure r ≥ s.]\n\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\nIF R > S THEN\nthe contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\nGOTO step 7\nELSE\nswap the contents of R and S.\n4 L ← R (this first step is redundant, but is useful for later discussion).\n5 R ← S\n6 S ← L\n\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\n\n7 IF S > R THEN\ndone measuring so\nGOTO 10\nELSE\nmeasure again,\n8 R ← R − S\n9 [Remainder-loop]:\nGOTO 7.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\n10 IF R = 0 THEN\ndone so\nGOTO step 15\nELSE\nCONTINUE TO step 11,\n\nE3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\n\n11 L ← R\n12 R ← S\n13 S ← L\n14 [Repeat the measuring process]:\nGOTO 7\n\nOUTPUT:\n\n15 [Done. S contains the greatest common divisor]:\nPRINT S\n\nDONE:\n\n16 HALT, END, STOP.\n\nAn elegant program for Euclid's algorithm\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.\n\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.\nThe following version can be used with programming languages from the C-family:\n\nTesting the Euclid algorithms\nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\nBut \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\nMeasuring and improving the Euclid algorithms\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\nAlgorithmic analysis\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\n\nFormal versus empirical\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\nExecution efficiency\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\nClassification\nThere are various ways to classify algorithms, each with its own merits.\n\nBy implementation\nOne way to classify algorithms is by implementation means.\n\nRecursion\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nLogical\nAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms are algorithms that take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms are algorithms that use multiple machines connected with a computer network. Parallel and distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. For example, a CPU would be an example of a parallel algorithm. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\nQuantum algorithm\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\n\nBy design paradigm\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\n\nBrute-force or exhaustive search\nBrute force is a method of problem-solving that involves systematically trying every possible option until the optimal solution is found. This approach can be very time consuming, as it requires going through every possible combination of variables. However, it is often used when other methods are not available or too complex. Brute force can be used to solve a variety of problems, including finding the shortest path between two points and cracking passwords.\nDivide and conquer\nA divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\nThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n\nOptimization problems\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\nThe greedy method\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n\nBy field of study\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\nBy complexity\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\nContinuous algorithms\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\n\nLegal issues\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\nHistory: Development of the notion of \"algorithm\"\nAncient Near East\nThe earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC described the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).: Ch 9.1\n\nDiscrete and distinguishable symbols\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.\n\nManipulation of symbols as \"place holders\" for numbers: algebra\nMuhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khwārizmī, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (c. 1680):\n\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\n\nCryptographic algorithms\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\n\nMechanical contrivances with discrete states\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\nLogical machines 1870 – Stanley Jevons' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc.] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (c. 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".The mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\n\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\n\nMathematics during the 19th century up to the mid-20th century\nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \"'formula language', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\nEmil Post (1936) and Alan Turing (1936–37, 1939)\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\n\n\"a two-way infinite sequence of spaces or boxes ... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time. ... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ... a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post–Turing machineAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.\nTuring—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares that the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided.\"Turing's reduction yields the following:\n\n\"The simple operations must therefore include:\n\"(a) Changes of the symbol on one of the observed squares\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability...\n\"† We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\n\nJ. B. Rosser (1939) and S. C. Kleene (1943)\nJ. Barkley Rosser defined an \"effective [mathematical] method\" in the following manner (italicization added):\n\n\"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225–226)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular, Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion, in particular, Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\n\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\n\nHistory after 1950\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\nSee also\nNotes\nBibliography\nZaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363\n\nFurther reading\nExternal links\n\n\"Algorithm\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nAlgorithms at Curlie\nWeisstein, Eric W. \"Algorithm\". MathWorld.\nDictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony Brook\nCollected Algorithms of the ACM – Associations for Computing Machinery\nThe Stanford GraphBase – Stanford University",
    "Algorithm design": "In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\nHistory\nAncient algorithms\nSince antiquity, step-by-step procedures for solving mathematical problems have been attested. This includes Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later; e.g. Shulba Sutras, Kerala School, and Brāhmasphuṭasiddhānta), The Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC, e.g. sieve of Eratosthenes and Euclidean algorithm), and Arabic mathematics (9th century, e.g. cryptographic algorithms for code-breaking based on frequency analysis).\n\nAl-khwarizmi and the term algorithm\nAround 825, Muhammad ibn Musa al-Khwarizmi wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). Both of these texts are lost in the original Arabic at this time. (However, his other book on algebra remains.)In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared: Liber Alghoarismi de practica arismetrice (attributed to John of Seville) and Liber Algorismi de numero Indorum (attributed to Adelard of Bath). Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi (\"Thus spoke Al-Khwarizmi\").In 1240, Alexander of Villedieu writes a Latin text titled Carmen de Algorismo. It begins with:\n\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\nwhich translates to:\n\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.\n\nEnglish evolution of the word\nAround 1230, the English word algorism is attested and then by Chaucer in 1391. English adopted the French term.In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.\nIn 1656, in the English dictionary Glossographia, it says:\n\nAlgorism ([Latin] algorismus) the Art or use of Cyphers, or of numbering by Cyphers; skill in accounting.\nAugrime ([Latin] algorithmus) skil in accounting or numbring.\n\nIn 1658, in the first edition of The New World of English Words, it says:\n\nAlgorithme, (a word compounded of Arabick and Spanish,) the art of reckoning by Cyphers.\n\nIn 1706, in the sixth edition of The New World of English Words, it says:\n\nAlgorithm, the Art of computing or reckoning by numbers, which contains the five principle Rules of Arithmetick, viz. Numeration, Addition, Subtraction, Multiplication and Division; to which may be added Extraction of Roots: It is also call'd Logistica Numeralis.\nAlgorism, the practical Operation in the several Parts of Specious Arithmetick or Algebra; sometimes it is taken for the Practice of Common Arithmetick by the ten Numeral Figures.\n\nIn 1751, in the Young Algebraist's Companion, Daniel Fenning contrasts the terms algorism and algorithm as follows:\n\nAlgorithm signifies the first Principles, and Algorism the practical Part, or knowing how to put the Algorithm in Practice.\n\nSince at least 1811, the term algorithm is attested to mean a \"step-by-step procedure\" in English.In 1842, in the Dictionary of Science, Literature and Art, it says:\n\nALGORITHM, signifies the art of computing in reference to some particular subject, or in some particular way; as the algorithm of numbers; the algorithm of the differential calculus.\n\nMachine usage\nIn 1928, a partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\nInformal definition\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\nor cook-book recipe.In general, a program is only an algorithm if it stops eventually—even though infinite loops may sometimes prove desirable.\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word \"algorithm\" in the following quotation:\n\nNo human being can write fast enough, or long enough, or small enough† ( †\"smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nPrecise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\nFormalization\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987), and Gurevich (2000):\n\n Minsky: \"But we will also maintain, with Turing ... that any procedure which could \"naturally\" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute\".\n Gurevich: \"… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\nFor some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"—an idea that is described more formally by flow of control.\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.\nFor some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.\n\nExpressing algorithms\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description, as follows:\n1 High-level description\n\"...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\n2 Implementation description\n\"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\n3 Formal description\nMost detailed, \"lowest level\", gives the Turing machine's \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Examples.\n\nDesign\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.\nTypical steps in the development of algorithms:\n\nProblem definition\nDevelopment of a model\nSpecification of the algorithm\nDesigning an algorithm\nChecking the correctness of the algorithm\nAnalysis of algorithm\nImplementation of algorithm\nProgram testing\nDocumentation preparation\n\nComputer algorithms\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nKnuth: \" ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity, and elegance, etc.\"Chaitin: \" ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"—such a proof would solve the Halting problem (ibid).\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\nComputers (and computors), models of computation: A computer (or human \"computer\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is convenient; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z ← 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.\nSimulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\nStructured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.\n\nExamples\nAlgorithm example\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\nHigh-level description:\n\nIf there are no numbers in the set, then there is no highest number.\nAssume the first number in the set is the largest number in the set.\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\nEuclid's algorithm\nIn mathematics, the Euclidean algorithm or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC). It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.\n\nEuclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \"proper\"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\nComputer language for Euclid's algorithm\nOnly a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.\n\nAn inelegant program for Euclid's algorithm\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\nINPUT:\n\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\nINPUT L, S\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\nR ← L\n\nE0: [Ensure r ≥ s.]\n\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\nIF R > S THEN\nthe contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\nGOTO step 7\nELSE\nswap the contents of R and S.\n4 L ← R (this first step is redundant, but is useful for later discussion).\n5 R ← S\n6 S ← L\n\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\n\n7 IF S > R THEN\ndone measuring so\nGOTO 10\nELSE\nmeasure again,\n8 R ← R − S\n9 [Remainder-loop]:\nGOTO 7.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\n10 IF R = 0 THEN\ndone so\nGOTO step 15\nELSE\nCONTINUE TO step 11,\n\nE3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\n\n11 L ← R\n12 R ← S\n13 S ← L\n14 [Repeat the measuring process]:\nGOTO 7\n\nOUTPUT:\n\n15 [Done. S contains the greatest common divisor]:\nPRINT S\n\nDONE:\n\n16 HALT, END, STOP.\n\nAn elegant program for Euclid's algorithm\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.\n\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.\nThe following version can be used with programming languages from the C-family:\n\nTesting the Euclid algorithms\nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\nBut \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\nMeasuring and improving the Euclid algorithms\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\nAlgorithmic analysis\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\n\nFormal versus empirical\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\nExecution efficiency\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\nClassification\nThere are various ways to classify algorithms, each with its own merits.\n\nBy implementation\nOne way to classify algorithms is by implementation means.\n\nRecursion\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nLogical\nAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms are algorithms that take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms are algorithms that use multiple machines connected with a computer network. Parallel and distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. For example, a CPU would be an example of a parallel algorithm. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\nQuantum algorithm\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\n\nBy design paradigm\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\n\nBrute-force or exhaustive search\nBrute force is a method of problem-solving that involves systematically trying every possible option until the optimal solution is found. This approach can be very time consuming, as it requires going through every possible combination of variables. However, it is often used when other methods are not available or too complex. Brute force can be used to solve a variety of problems, including finding the shortest path between two points and cracking passwords.\nDivide and conquer\nA divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\nThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n\nOptimization problems\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\nThe greedy method\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n\nBy field of study\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\nBy complexity\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\nContinuous algorithms\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\n\nLegal issues\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\nHistory: Development of the notion of \"algorithm\"\nAncient Near East\nThe earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC described the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).: Ch 9.1\n\nDiscrete and distinguishable symbols\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.\n\nManipulation of symbols as \"place holders\" for numbers: algebra\nMuhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khwārizmī, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (c. 1680):\n\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\n\nCryptographic algorithms\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\n\nMechanical contrivances with discrete states\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\nLogical machines 1870 – Stanley Jevons' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc.] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (c. 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".The mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\n\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\n\nMathematics during the 19th century up to the mid-20th century\nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \"'formula language', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\nEmil Post (1936) and Alan Turing (1936–37, 1939)\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\n\n\"a two-way infinite sequence of spaces or boxes ... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time. ... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ... a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post–Turing machineAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.\nTuring—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares that the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided.\"Turing's reduction yields the following:\n\n\"The simple operations must therefore include:\n\"(a) Changes of the symbol on one of the observed squares\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability...\n\"† We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\n\nJ. B. Rosser (1939) and S. C. Kleene (1943)\nJ. Barkley Rosser defined an \"effective [mathematical] method\" in the following manner (italicization added):\n\n\"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225–226)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular, Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion, in particular, Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\n\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\n\nHistory after 1950\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\nSee also\nNotes\nBibliography\nZaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363\n\nFurther reading\nExternal links\n\n\"Algorithm\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nAlgorithms at Curlie\nWeisstein, Eric W. \"Algorithm\". MathWorld.\nDictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony Brook\nCollected Algorithms of the ACM – Associations for Computing Machinery\nThe Stanford GraphBase – Stanford University",
    "Algorithmic bias": "Algorithmic bias describes systematic and repeatable errors in a computer system that create \"unfair\" outcomes, such as \"privileging\" one category over another in ways different from the intended function of the algorithm.\nBias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (2018) and the proposed Artificial Intelligence Act (2021).\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.\nAlgorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n\nDefinitions\nAlgorithms are difficult to define, but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output.: 13  For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence.: 14–15  By analyzing and processing data, algorithms are the backbone of search engines, social media websites, recommendation engines, online retail, online advertising, and more.Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality.: 2 : 563 : 294  The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased.: 332  This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).\n\nMethods\nBias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria.: 3  Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded.: 4  Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers.: 8  Other algorithms may reinforce stereotypes and preferences as they process and display \"relevant\" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.: 6 Beyond assembling and processing data, bias can emerge as a result of design. For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores).: 36  Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. This criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths. Algorithms may also display an uncertainty bias, offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.: 4\n\nHistory\nEarly critiques\nThe earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.: 149 Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs \"embody law\",: 40  that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including their biases and expectations.: 109  While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects \"human decisionmaking processes\" as data is being selected.: 70, 105 Finally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results.: 65  Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.: 226 An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions. While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale.\nIn recent years, when more algorithms started to use machine learning methods on real world data, algorithmic bias can be found more often due to the bias existing in the data.\n\nContemporary critiques and responses\nThough well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten.: 115  In theory, these biases may create new patterns of behavior, or \"scripts\", in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.: 180 The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist,: 15  a process described by author Clay Shirky as \"algorithmic authority\". Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources\", such as search results. This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be created based on significantly wider criteria than just their popularity.: 14 Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans.: 16 : 6  This can have the effect of reducing alternative options, compromises, or flexibility.: 16  Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.: 71 Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,\nand Transparency in Machine Learning.: 115  Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.: 117  In recent years, the study of the Fairness, Accountability,\nand Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT. Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied.\n\nTypes\nPre-existing\nPre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious.: 334 : 294  Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines.: 17  Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm.: 116 : 8 An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 British Nationality Act.: 341  The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\": 341 : 375  In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.: 342 Another source of bias, which has been called “label choice bias\", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely-used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.\n\nTechnical\nTechnical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.: 332  Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.: 336  Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.: 332 A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.: 332  The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.: 574 Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.: 332  Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.: 21–22\n\nEmergent\nEmergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.: 334  Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms.: 334, 336  This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion.: 179 : 294  Similarly, problems may emerge when training data (the samples \"fed\" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).: 338  The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.: 338 Additional emergent biases include:\n\nCorrelations\nUnpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data.: 6  In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.\n\nUnanticipated uses\nEmergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand.: 334  These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.: 179 Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law.: 342\n\nFeedback loops\nEmergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing. Another well known example of such an algorithm exhibiting such behavior is COMPAS, a software that determines an individual's likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on.\nRecommender systems such as those used to recommend online videos or news articles can create feedback loops. When users click on content that is suggested by algorithms, it influences the next set of suggestions. Over time this may lead to users entering a filter bubble and being unaware of important or useful content.\n\nImpact\nCommercial influences\nCorporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.: 2 : 331 In a 1998 paper describing Google, the founders of the company had adopted a policy of transparency in search results regarding paid placement, arguing that \"advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\" This bias would be an \"invisible\" manipulation of the user.: 3\n\nVoting behavior\nA series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate. Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. Legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users\", if intentionally manipulated.: 335\n\nGender discrimination\nIn 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew\", but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.In 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners.: 94  Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.: 98 Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\".: 31  In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content. Other examples include the display of higher-paying jobs to male applicants on job search websites. Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations. In fact, current machine translation systems fail to reproduce the real world distribution of female workers.In 2015, Amazon.com turned off an AI system it developed to screen job applications when they realized it was biased against women. The recruitment tool excluded applicants who attended all-women's colleges and resumes that included the word \"women's\". A similar problem emerged with music streaming services—In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against women artists. Spotify's song recommendations suggested more male artists over women artists.\n\nRacial and ethnic discrimination\nAlgorithms have been criticized as a method for obscuring racial prejudices in decision-making.: 158  Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime. This could potentially mean that a system amplifies the original biases in the data.\nIn 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.: 154  Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name. A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm's model of lung function.In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on \"creditworthiness\" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.\n\nLaw enforcement and legal proceedings\nAlgorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label \"high-risk\" as white defendants.One example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. For the time period starting in 1920 and ending in 1970, the nationality of a criminal's father was a consideration in those risk assessment scores.: 4  Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.One study that set out to examine \"Risk, Race, & Recidivism: Predictive Bias and Disparate Impact\" alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.In the pretrial detention context, a law review article argues that algorithmic risk assessments violate 14th Amendment Equal Protection rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored.\n\nOnline hate speech\nIn 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks\", whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.While algorithms are used to track and block hate speech, some were found to be 1.5 times more likely to flag information posted by Black users and 2.2 times likely to flag information as hate speech if written in African American English. Without context for slurs and epithets, even when used by communities which have re-appropriated them, were flagged.\n\nSurveillance\nSurveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.: 572  The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender. However, even audits of these image-recognition systems are ethically fraught, and some scholars have suggested the technology's context will always have a disproportionate impact on communities whose actions are over-surveilled. For example, a 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites.: 190  Additional studies of facial recognition software have found the opposite to be true when trained on non-criminal databases, with the software being the least accurate in identifying darker-skinned females.\n\nDiscrimination against the LGBTQ community\nIn 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men. In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its \"adult content\" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel Brokeback Mountain.: 5 In 2019, it was found that on Facebook, searches for \"photos of my female friends\" yielded suggestions such as \"in bikinis\" or \"at the beach\". In contrast, searches for \"photos of my male friends\" yielded no results.Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning. Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individual's sexual orientation based on their facial images. The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being \"outed\" against their will.\n\nDisability discrimination\nWhile the modalities of algorithmic fairness have been judged on the basis of different aspects of bias – like gender, race and socioeconomic status, disability often is left out of the list. The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms, creating even more exclusionThe shifting nature of disabilities and its subjective characterization, makes it more difficult to computationally address. The lack of historical depth in defining disabilities, collecting its incidence and prevalence in questionnaires, and establishing recognition add to the controversy and ambiguity in its quantification and calculations.  The definition of disability has been long debated shifting from a medical model to a social model of disability most recently, which establishes that disability is a result of the mismatch between people's interactions and barriers in their environment, rather than impairments and health conditions. Disabilities can also be situational or temporary, considered in a constant state of flux. Disabilities are incredibly diverse, fall within a large spectrum, and can be unique to each individual. People’s identity can vary based on the specific types of disability they experience, how they use assistive technologies, and who they support.  The high level of variability across people’s experiences greatly personalizes how a disability can manifest. Overlapping identities and intersectional experiences are excluded from statistics and datasets, hence underrepresented and nonexistent in training data. Therefore, machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias. For example, if people with speech impairments aren’t included in training voice control features and smart AI assistants –they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor. \nGiven the stereotypes and stigmas that still exist surrounding disabilities, the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges,  As disclosing disability information can be taboo and drive further discrimination against this population, there is a lack of explicit disability data available for algorithmic systems to interact with. People with disabilities face additional harms and risks with respect to their social support, cost of health insurance, workplace discrimination and other basic necessities upon disclosing their disability status. Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures.\n\nGoogle Search\nWhile users generate results that are \"completed\" automatically, Google has failed to remove sexist and racist autocompletion text. For example, Algorithms of Oppression: How Search Engines Reinforce Racism Safiya Noble notes an example of the search for \"black girls\", which was reported to result in pornographic images. Google claimed it was unable to erase those pages unless they were considered unlawful.\n\nObstacles to research\nSeveral problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.: 5\n\nDefining fairness\nLiterature on algorithmic bias has focused on the remedy of fairness, but definitions of fairness are often incompatible with each other and the realities of machine learning optimization. For example, defining fairness as an \"equality of outcomes\" may simply refer to a system producing the same result for all people, while fairness defined as \"equality of treatment\" might explicitly consider differences between individuals.: 2  As a result, fairness is sometimes described as being in conflict with the accuracy of a model, suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems.: 2  In response to this tension, researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms, with \"fairness\" defined for specific applications and contexts.\n\nComplexity\nAlgorithmic processes are complex, often exceeding the understanding of the people who use them.: 2 : 7  Large-scale operations may not be understood even by those involved in creating them. The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.: 183  Social scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\" Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.: 92 An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms.: 118  Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.: 22 Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms.: 367 : 7  One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.: 5 \nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.: 5\n\nLack of transparency\nCommercial algorithms are proprietary, and may be treated as trade secrets.: 2 : 7 : 183  Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings.: 366  This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.: 20  Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.: 369  Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.\n\nLack of data about sensitive categories\nA significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\nSome practitioners have tried to estimate and impute these missing sensitive categorisations in order to allow bias mitigation, for example building systems to infer ethnicity from names, however this can introduce other forms of bias if not undertaken with care. Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.Algorithmic bias does not only include protected categories, but can also concern characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult. Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.\n\nSolutions\nA study of 84 policy guidelines on ethical AI found that fairness and \"mitigation of unwanted bias\" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts.\n\nTechnical\nThere have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion). Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model. Using machine learning to detect bias is called, \"conducting an AI audit\", where the \"auditor\" is an algorithm that goes through the AI model and the training data to identify biases.\nEnsuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information\nfrom its input signals, because this is typically implicit in other signals. For example, the hobbies, sports and schools attended\nby a job candidate might reveal their gender to the software, even when this is removed from the analysis. Solutions to this\nproblem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected\nand sensitive information about the subject, as first demonstrated in  where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature. A simpler method was proposed in the context of word embeddings, and involves removing information that is correlated with the protected characteristic.Currently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.\n\nTransparency and monitoring\nEthics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results. Such solutions include the consideration of the \"right to understanding\" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed. Toward this end, a movement for \"Explainable AI\" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias. Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.An initial approach towards transparency included the open-sourcing of algorithms. Software code can be looked into and improvements can be proposed through source-code-hosting facilities. However, this approach doesn't necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesn't understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience.\n\nRight to remedy\nFrom a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias. This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes. Others propose the need for clear liability insurance mechanisms.\n\nDiversity and inclusion\nAmid concerns that the design of AI systems is primarily the domain of white, male engineers, a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems. For example, just 12% of machine learning engineers are women, with black AI leaders pointing to a \"diversity crisis\" in the field. Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research. Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms.: 4  Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the \"whiteness\" of the culture of AI.\n\nInterdisciplinarity and Collaboration\nIntegrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias. Integrating insights, expertise, and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society. An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies, a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact. This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects. PACT attempts to reify the importance of decolonizing and power-shifting efforts in the design of human-centered AI solutions. An academic initiative in this regard is the Stanford University's Institute for Human-Centered Artificial Intelligence which aims to foster multidisciplinary collaboration. The mission of the institute is to advance artificial intelligence (AI) research, education, policy and practice to improve the human condition. Collaboration with outside experts and various stakeholders facilitates ethical, inclusive, and accountable development of intelligent systems. It incorporates ethical considerations, understands the social and cultural context, promotes human-centered design, leverages technical expertise, and addresses policy and legal considerations. Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair, transparent, and accountable.\n\nRegulation\nEurope\nThe General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.\nThe GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.\n\nUnited States\nThe United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\". Intended only as guidance, the report did not create any legal precedent.: 26 In 2017, New York City passed the first algorithmic accountability bill in the United States. The bill, which went into effect on January 1, 2018, required \"the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems.\" The task force is required to present findings and recommendations for further regulatory action in 2019.\n\nIndia\nOn July 31, 2018, a draft of the Personal Data Bill was presented. The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for \"harm resulting from any processing or any kind of processing undertaken by the fiduciary\". It defines \"any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal\" or \"any discriminatory treatment\" as a source of harm that could arise from improper use of data. It also makes special provisions for people of \"Intersex status\".\n\nSee also\nEthics of artificial intelligence\nFairness (machine learning)\nMisaligned goals in artificial intelligence\nPredictive policing\nSenseTime\n\nReferences\nFurther reading\nBaer, Tobias (2019). Understand, Manage, and Prevent Algorithmic Bias: A Guide for Business Users and Data Scientists. New York: Apress. ISBN 9781484248843.\nNoble, Safiya Umoja (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. New York: New York University Press. ISBN 9781479837243.",
    "Algorithmic efficiency": "In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\nFor maximum efficiency it is desirable to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.\nFor example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort sorts the list in time proportional to the number of elements squared (\n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\textstyle O(n^{2})}\n  , see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (\n  \n    \n      \n        O\n        (\n        1\n        )\n      \n    \n    {\\textstyle O(1)}\n  ). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (\n  \n    \n      \n        O\n        (\n        n\n        log\n        ⁡\n        n\n        )\n      \n    \n    {\\textstyle O(n\\log n)}\n  ), but has a space requirement linear in the length of the list (\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\textstyle O(n)}\n  ). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.\n\nBackground\nThe importance of efficiency with respect to time was emphasised by Ada Lovelace in 1843 as applied to Charles Babbage's mechanical analytical engine:\n\n\"In almost every computation a great variety of arrangements for the succession of the processes is possible, and various considerations must influence the selections amongst them for the purposes of a calculating engine. One essential object is to choose that arrangement which shall tend to reduce to a minimum the time necessary for completing the calculation\"\nEarly electronic computers had both limited speed and limited random access memory. Therefore, a space–time trade-off occurred. A task could use a fast algorithm using a lot of memory, or it could use a slow algorithm using little memory. The engineering trade-off was then to use the fastest algorithm that could fit in the available memory.\nModern computers are significantly faster than the early computers, and have a much larger amount of memory available (Gigabytes instead of Kilobytes). Nevertheless, Donald Knuth emphasised that efficiency is still an important consideration:\n\n  \"In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal and I believe the same viewpoint should prevail in software engineering\"\n\nOverview\nAn algorithm is considered efficient if its resource consumption, also known as computational cost, is at or below some acceptable level. Roughly speaking, 'acceptable' means:  it will run in a reasonable amount of time or space on an available computer, typically as a function of the size of the input. Since the 1950s computers have seen dramatic increases in both the available computational power and in the available amount of memory, so current acceptable levels would have been unacceptable even 10 years ago. In fact, thanks to the approximate doubling of computer power every 2 years, tasks that are acceptably efficient on modern smartphones and embedded systems may have been unacceptably inefficient for industrial servers 10 years ago.\nComputer manufacturers frequently bring out new models, often with higher performance. Software costs can be quite high, so in some cases the simplest and cheapest way of getting higher performance might be to just buy a faster computer, provided it is compatible with an existing computer.\nThere are many ways in which the resources used by an algorithm can be measured: the two most common measures are speed and memory usage; other measures could include transmission speed, temporary disk usage, long-term disk usage, power consumption, total cost of ownership, response time to external stimuli, etc. Many of these measures depend on the size of the input to the algorithm, i.e. the amount of data to be processed. They might also depend on the way in which the data is arranged; for example, some sorting algorithms perform poorly on data which is already sorted, or which is sorted in reverse order.\nIn practice, there are other factors which can affect the efficiency of an algorithm, such as requirements for accuracy and/or reliability. As detailed below, the way in which an algorithm is implemented can also have a significant effect on actual efficiency, though many aspects of this relate to optimization issues.\n\nTheoretical analysis\nIn the theoretical analysis of algorithms, the normal practice is to estimate their complexity in the asymptotic sense. The most commonly used notation to describe resource consumption or \"complexity\" is Donald Knuth's Big O notation, representing the complexity of an algorithm as a function of the size of the input \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n  . Big O notation is an asymptotic measure of function complexity, where \n  \n    \n      \n        f\n        (\n        n\n        )\n        =\n        O\n        \n          \n            (\n          \n        \n        g\n        (\n        n\n        )\n        \n          \n            )\n          \n        \n      \n    \n    {\\textstyle f(n)=O{\\bigl (}g(n){\\bigr )}}\n   roughly means the time requirement for an algorithm is proportional to \n  \n    \n      g\n      (\n      n\n      )\n    \n    g(n)\n  , omitting lower-order terms that contribute less than \n  \n    \n      g\n      (\n      n\n      )\n    \n    g(n)\n   to the growth of the function as \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   grows arbitrarily large. This estimate may be misleading when \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   is small, but is generally sufficiently accurate when \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   is large as the notation is asymptotic. For example, bubble sort may be faster than merge sort when only a few items are to be sorted; however either implementation is likely to meet performance requirements for a small list. Typically, programmers are interested in algorithms that scale efficiently to large input sizes, and merge sort is preferred over bubble sort for lists of length encountered in most data-intensive programs.\nSome examples of Big O notation applied to algorithms' asymptotic time complexity include:\n\nBenchmarking: measuring performance\nFor new versions of software or to provide comparisons with competitive systems, benchmarks are sometimes used, which assist with gauging an algorithms relative performance. If a new sort algorithm is produced, for example, it can be compared with its predecessors to ensure that at least it is efficient as before with known data, taking into consideration any functional improvements. Benchmarks can be used by customers when comparing various products from alternative suppliers to estimate which product will best suit their specific requirements in terms of functionality and performance. For example, in the mainframe world certain proprietary sort products from independent software companies such as Syncsort compete with products from the major suppliers such as IBM for speed.\nSome benchmarks provide opportunities for producing an analysis comparing the relative speed of various compiled and interpreted languages for example\nand The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages.\nEven creating \"do it yourself\" benchmarks can demonstrate the relative performance of different programming languages, using a variety of user specified criteria. This is quite simple, as a \"Nine language performance roundup\" by Christopher W. Cowell-Shah demonstrates by example.\n\nImplementation concerns\nImplementation issues can also have an effect on efficiency, such as the choice of programming language, or the way in which the algorithm is actually coded, or the choice of a compiler for a particular language, or the compilation options used, or even the operating system being used. In many cases a language implemented by an interpreter may be much slower than a language implemented by a compiler. See the articles on just-in-time compilation and interpreted languages.\nThere are other factors which may affect time or space issues, but which may be outside of a programmer's control; these include data alignment, data granularity, cache locality, cache coherency, garbage collection, instruction-level parallelism, multi-threading (at either a hardware or software level), simultaneous multitasking, and subroutine calls.Some processors have capabilities for vector processing, which allow a single instruction to operate on multiple operands; it may or may not be easy for a programmer or compiler to use these capabilities. Algorithms designed for sequential processing may need to be completely redesigned to make use of parallel processing, or they could be easily reconfigured. As parallel and distributed computing grow in importance in the late 2010s, more investments are being made into efficient high-level APIs for parallel and distributed computing systems such as CUDA, TensorFlow, Hadoop, OpenMP and MPI.\nAnother problem which can arise in programming is that processors compatible with the same instruction set (such as x86-64 or ARM) may implement an instruction in different ways, so that instructions which are relatively fast on some models may be relatively slow on other models. This often presents challenges to optimizing compilers, which must have a great amount of knowledge of the specific CPU and other hardware available on the compilation target to best optimize a program for performance. In the extreme case, a compiler may be forced to emulate instructions not supported on a compilation target platform, forcing it to generate code or link an external library call to produce a result that is otherwise incomputable on that platform, even if it is natively supported and more efficient in hardware on other platforms. This is often the case in embedded systems with respect to floating-point arithmetic, where small and low-power microcontrollers often lack hardware support for floating-point arithmetic and thus require computationally expensive software routines to produce floating point calculations.\n\nMeasures of resource usage\nMeasures are normally expressed as a function of the size of the input \n  \n    \n      \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {n}}\n  .\nThe two most common measures are:\n\nTime: how long does the algorithm take to complete?\nSpace: how much working memory (typically RAM) is needed by the algorithm? This has two aspects: the amount of memory needed by the code (auxiliary space usage), and the amount of memory needed for the data on which the code operates (intrinsic space usage).For computers whose power is supplied by a battery (e.g. laptops and smartphones), or for very long/large calculations (e.g. supercomputers), other measures of interest are:\n\nDirect power consumption: power needed directly to operate the computer.\nIndirect power consumption: power needed for cooling, lighting, etc.As of 2018, power consumption is growing as an important metric for computational tasks of all types and at all scales ranging from embedded Internet of things devices to system-on-chip devices to server farms. This trend is often referred to as green computing.\nLess common measures of computational efficiency may also be relevant in some cases:\n\nTransmission size: bandwidth could be a limiting factor. Data compression can be used to reduce the amount of data to be transmitted. Displaying a picture or image (e.g. Google logo) can result in transmitting tens of thousands of bytes (48K in this case) compared with transmitting six bytes for the text \"Google\". This is important for I/O bound computing tasks.\nExternal space: space needed on a disk or other external memory device; this could be for temporary storage while the algorithm is being carried out, or it could be long-term storage needed to be carried forward for future reference.\nResponse time (latency): this is particularly relevant in a real-time application when the computer system must respond quickly to some external event.\nTotal cost of ownership: particularly if a computer is dedicated to one particular algorithm.\n\nTime\nTheory\nAnalyze the algorithm, typically using time complexity analysis to get an estimate of the running time as a function of the size of the input data. The result is normally expressed using Big O notation. This is useful for comparing algorithms, especially when a large amount of data is to be processed. More detailed estimates are needed to compare algorithm performance when the amount of data is small, although this is likely to be of less importance. Algorithms which include parallel processing may be more difficult to analyze.\n\nPractice\nUse a benchmark to time the use of an algorithm. Many programming languages have an available function which provides CPU time usage. For long-running algorithms the elapsed time could also be of interest. Results should generally be averaged over several tests.\nRun-based profiling can be very sensitive to hardware configuration and the possibility of other programs or tasks running at the same time in a multi-processing and multi-programming environment.\nThis sort of test also depends heavily on the selection of a particular programming language, compiler, and compiler options, so algorithms being compared must all be implemented under the same conditions.\n\nSpace\nThis section is concerned with use of memory resources (registers, cache, RAM, virtual memory, secondary memory) while the algorithm is being executed. As for time analysis above, analyze the algorithm, typically using space complexity analysis to get an estimate of the run-time memory needed as a function as the size of the input data. The result is normally expressed using Big O notation.\nThere are up to four aspects of memory usage to consider:\n\nThe amount of memory needed to hold the code for the algorithm.\nThe amount of memory needed for the input data.\nThe amount of memory needed for any output data.\nSome algorithms, such as sorting, often rearrange the input data and do not need any additional space for output data. This property is referred to as \"in-place\" operation.\nThe amount of memory needed as working space during the calculation.\nThis includes local variables and any stack space needed by routines called during a calculation; this stack space can be significant for algorithms which use recursive techniques.Early electronic computers, and early home computers, had relatively small amounts of working memory. For example, the 1949 Electronic Delay Storage Automatic Calculator (EDSAC) had a maximum working memory of 1024 17-bit words, while the 1980 Sinclair ZX80 came initially with 1024 8-bit bytes of working memory. In the late 2010s, it is typical for personal computers to have between 4 and 32 GB of RAM, an increase of over 300 million times as much memory.\n\nCaching and memory hierarchy\nCurrent computers can have relatively large amounts of memory (possibly Gigabytes), so having to squeeze an algorithm into a confined amount of memory is much less of a problem than it used to be. But the presence of four different categories of memory can be significant:\n\nProcessor registers, the fastest of computer memory technologies with the least amount of storage space. Most direct computation on modern computers occurs with source and destination operands in registers before being updated to the cache, main memory and virtual memory if needed. On a processor core, there are typically on the order of hundreds of bytes or fewer of register availability, although a register file may contain more physical registers than architectural registers defined in the instruction set architecture.\nCache memory is the second fastest and second smallest memory available in the memory hierarchy. Caches are present in CPUs, GPUs, hard disk drives and external peripherals, and are typically implemented in static RAM. Memory caches are multi-leveled; lower levels are larger, slower and typically shared between processor cores in multi-core processors. In order to process operands in cache memory, a processing unit must fetch the data from the cache, perform the operation in registers and write the data back to the cache. This operates at speeds comparable (about 2-10 times slower) with the CPU or GPU's arithmetic logic unit or floating-point unit if in the L1 cache. It is about 10 times slower if there is an L1 cache miss and it must be retrieved from and written to the L2 cache, and a further 10 times slower if there is an L2 cache miss and it must be retrieved from an L3 cache, if present.\nMain physical memory is most often implemented in dynamic RAM (DRAM). The main memory is much larger (typically gigabytes compared to ≈8 megabytes) than an L3 CPU cache, with read and write latencies typically 10-100 times slower. As of 2018, RAM is increasingly implemented on-chip of processors, as CPU or GPU memory.\nVirtual memory is most often implemented in terms of secondary storage such as a hard disk, and is an extension to the memory hierarchy that has much larger storage space but much larger latency, typically around 1000 times slower than a cache miss for a value in RAM. While originally motivated to create the impression of higher amounts of memory being available than were truly available, virtual memory is more important in contemporary usage for its time-space tradeoff and enabling the usage of virtual machines. Cache misses from main memory are called page faults, and incur huge performance penalties on programs.An algorithm whose memory needs will fit in cache memory will be much faster than an algorithm which fits in main memory, which in turn will be very much faster than an algorithm which has to resort to virtual memory. Because of this, cache replacement policies are extremely important to high-performance computing, as are cache-aware programming and data alignment. To further complicate the issue, some systems have up to three levels of cache memory, with varying effective speeds. Different systems will have different amounts of these various types of memory, so the effect of algorithm memory needs can vary greatly from one system to another.\nIn the early days of electronic computing, if an algorithm and its data would not fit in main memory then the algorithm could not be used. Nowadays the use of virtual memory appears to provide much memory, but at the cost of performance. If an algorithm and its data will fit in cache memory, then very high speed can be obtained; in this case minimizing space will also help minimize time. This is called the principle of locality, and can be subdivided into locality of reference, spatial locality and temporal locality. An algorithm which will not fit completely in cache memory but which exhibits locality of reference may perform reasonably well.\n\nCriticism of the current state of programming\nDavid May FRS a British computer scientist and currently Professor of Computer Science at University of Bristol and founder and CTO of XMOS Semiconductor, believes one of the problems is that there is a reliance on Moore's law to solve inefficiencies. He has advanced an 'alternative' to Moore's law (May's law) stated as follows:\nSoftware efficiency halves every 18 months, compensating Moore's Law\n\nMay goes on to state:\nIn ubiquitous systems, halving the instructions executed can double the battery life and big data sets bring big opportunities for better software and algorithms: Reducing the number of operations from N × N to N × log(N) has a dramatic effect when N is large ... for N = 30 billion, this change is as good as 50 years of technology improvements.\n\nSoftware author Adam N. Rosenburg in his blog \"The failure of the Digital computer\", has described the current state of programming as nearing the \"Software event horizon\", (alluding to the fictitious \"shoe event horizon\" described by Douglas Adams in his Hitchhiker's Guide to the Galaxy book). He estimates there has been a 70 dB factor loss of productivity or \"99.99999 percent, of its ability to deliver the goods\", since the 1980s—\"When Arthur C. Clarke compared the reality of computing in 2001 to the computer HAL 9000 in his book 2001: A Space Odyssey, he pointed out how wonderfully small and powerful computers were but how disappointing computer programming had become\".\n\nCompetitions for the best algorithms\nThe following competitions invite entries for the best algorithms based on some arbitrary criteria decided by the judges:\n\nWired magazine\n\nSee also\nAnalysis of algorithms—how to determine the resources needed by an algorithm\nArithmetic coding—a form of variable-length entropy encoding for efficient data compression\nAssociative array—a data structure that can be made more efficient using Patricia trees or Judy arrays\nBenchmark—a method for measuring comparative execution times in defined cases\nBest, worst and average case—considerations for estimating execution times in three scenarios\nBinary search algorithm—a simple and efficient technique for searching sorted arrays\nBranch table—a technique for reducing instruction path-length, size of machine code, (and often also memory)\nComparison of programming paradigms—paradigm specific performance considerations\nCompiler optimization—compiler-derived optimization\nComputational complexity of mathematical operations\nComputational complexity theory\nComputer performance—computer hardware metrics\nData compression—reducing transmission bandwidth and disk storage\nDatabase index—a data structure that improves the speed of data retrieval operations on a database table\nEntropy encoding—encoding data efficiently using frequency of occurrence of strings as a criterion for substitution\nGarbage collection—automatic freeing of memory after use\nGreen computing—a move to implement 'greener' technologies, consuming less resources\nHuffman algorithm—an algorithm for efficient data encoding\nImproving Managed code Performance—Microsoft MSDN Library\nLocality of reference—for avoidance of caching delays caused by non-local memory access\nLoop optimization\nMemory management\nOptimization (computer science)\nPerformance analysis—methods of measuring actual performance of an algorithm at run-time\nReal-time computing—further examples of time-critical applications\nRun-time analysis—estimation of expected run-times and an algorithm's scalability\nSimultaneous multithreading\nSorting algorithm § Comparison of algorithms\nSpeculative execution or Eager execution\nBranch prediction\nSuper-threading\nHyper-threading\nThreaded code—similar to virtual method table or branch table\nVirtual method table—branch table with dynamically assigned pointers for dispatching\n\n\n== References ==",
    "Algorithmic transparency": "Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\nThe phrases \"algorithmic transparency\" and \"algorithmic accountability\" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, \"algorithmic transparency\" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair.  \"Algorithmic accountability\" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a \"right to explanation\" of decisions made by algorithms, though it is unclear what this means. Furthermore, the European Union founded The European Center for Algoritmic Transparency (ECAC).\n\nSee also\nBlack box\nExplainable AI\nRegulation of algorithms\nReverse engineering\nRight to explanation\nAlgorithmic accountability\n\n\n== References ==",
    "AlphaFold": "AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure. The program is designed as a deep learning system.AlphaFold AI software has had two major versions. A team of researchers that used AlphaFold 1 (2018) placed first in the overall rankings of the 13th Critical Assessment of Structure Prediction (CASP) in December 2018. The program was particularly successful at predicting the most accurate structure for targets rated as the most difficult by the competition organisers, where no existing template structures were available from proteins with a partially similar sequence. \nA team that used AlphaFold 2 (2020) repeated the placement in the CASP competition in November 2020. The team achieved a level of accuracy much higher than any other group. It scored above 90 for around two-thirds of the proteins in CASP's global distance test (GDT), a test that measures the degree to which a computational program predicted structure is similar to the lab experiment determined structure, with 100 being a complete match, within the distance cutoff used for calculating GDT.AlphaFold 2's results at CASP were described as \"astounding\" and \"transformational.\" Some researchers noted that the accuracy is not high enough for a third of its predictions, and that it does not reveal the mechanism or rules of protein folding for the protein folding problem to be considered solved. Nevertheless, there has been widespread respect for the technical achievement.\nOn 15 July 2021 the AlphaFold 2 paper was published at Nature as an advance access publication alongside open source software and a searchable database of species proteomes.\n\nProtein folding problem\nProteins consist of chains of amino acids which spontaneously fold, in a process called protein folding, to form the three dimensional (3-D) structures of the proteins. The 3-D structure is crucial to the biological function of the protein. However, understanding how the amino acid sequence can determine the 3-D structure is highly challenging, and this is called the \"protein folding problem\". The \"protein folding problem\" involves understanding the thermodynamics of the interatomic forces that determine the folded stable structure, the mechanism and pathway through which a protein can reach its final folded state with extreme rapidity, and how the native structure of a protein can be predicted from its amino acid sequence.Protein structures are currently determined experimentally by means of techniques such as X-ray crystallography, cryo-electron microscopy and nuclear magnetic resonance, techniques which are both expensive and time-consuming. Such efforts have identified the structures of about 170,000 proteins over the last 60 years, while there are over 200 million known proteins across all life forms. If it is possible to predict protein structure from the amino-acid sequence alone, it would greatly help to advance scientific research. However, the Levinthal's paradox shows that while a protein can fold in milliseconds, the time it takes to calculate all the possible structures randomly to determine the true native structure is longer than the age of the known universe, which made predicting protein structures a grand challenge in biology for scientists.Over the years, researchers have applied numerous computational methods to resolve the issue of protein structure prediction, but their accuracy has not been close to experimental techniques except for small simple proteins, thus limiting their value. CASP, which was launched in 1994 to challenge the scientific community to produce their best protein structure predictions, found that GDT scores of only about 40 out of 100 can be achieved for the most difficult proteins by 2016. AlphaFold started competing in the 2018 CASP using an artificial intelligence (AI) deep learning technique.\n\nAlgorithm\nDeepMind is known to have trained the program on over 170,000 proteins from a public repository of protein sequences and structures. The program uses a form of attention network, a deep learning technique that focuses on having the AI identify parts of a larger problem, then piece it together to obtain the overall solution. The overall training was conducted on processing power between 100 and 200 GPUs. Training the system on this hardware took \"a few weeks\", after which the program would take \"a matter of days\" to converge for each structure.\n\nAlphaFold 1, 2018\nAlphaFold 1 (2018) was built on work developed by various teams in the 2010s, work that looked at the large databanks of related DNA sequences now available from many different organisms (most without known 3D structures), to try to find changes at different residues that appeared to be correlated, even though the residues were not consecutive in the main chain.  Such correlations suggest that the residues may be close to each other physically, even though not close in the sequence, allowing a contact map to be estimated.  Building on recent work prior to 2018, AlphaFold 1 extended this to estimate a probability distribution for just how close the residues might be likely to be—turning the contact map into a likely distance map.  It also used more advanced learning methods than previously to develop the inference.  Combining a statistical potential based on this probability distribution with the calculated local free-energy of the configuration, the team was then able to use gradient descent to a solution that best fitted both.More technically, Torrisi et al summarised in 2019 the approach of AlphaFold version 1 as follows:\nCentral to AlphaFold is a distance map predictor implemented as a very deep residual neural networks with 220 residual blocks processing a representation of dimensionality 64×64×128 – corresponding to input features calculated from two 64 amino acid fragments. Each residual block has three layers including a 3×3 dilated convolutional layer – the blocks cycle through dilation of values 1, 2, 4, and 8. In total the model has 21 million parameters. The network uses a combination of 1D and 2D inputs, including evolutionary profiles from different sources and co-evolution features. Alongside a distance map in the form of a very finely-grained histogram of distances, AlphaFold predicts Φ and Ψ angles for each residue which are used to create the initial predicted 3D structure. The AlphaFold authors concluded that the depth of the model, its large crop size, the large training set of roughly 29,000 proteins, modern Deep Learning techniques, and the richness of information from the predicted histogram of distances helped AlphaFold achieve a high contact map prediction precision.\n\nAlphaFold 2, 2020\nThe 2020 version of the program (AlphaFold 2, 2020) is significantly different from the original version that won CASP 13 in 2018, according to the team at DeepMind.The DeepMind team had identified that its previous approach, combining local physics with a guide potential derived from pattern recognition, had a tendency to over-account for interactions between residues that were nearby in the sequence compared to interactions between residues further apart along the chain.  As a result, AlphaFold 1 had a tendency to prefer models with slightly more secondary structure (alpha helices and beta sheets) than was the case in reality (a form of overfitting).The software design used in AlphaFold 1 contained a number of modules, each trained separately, that were used to produce the guide potential that was then combined with the physics-based energy potential.  AlphaFold 2 replaced this with a system of sub-networks coupled together into a single differentiable end-to-end model, based entirely on pattern recognition, which was trained in an integrated way as a single integrated structure. Local physics, in the form of energy refinement based on the AMBER model, is applied only as a final refinement step once the neural network prediction has converged, and only slightly adjusts the predicted structure.A key part of the 2020 system are two modules, believed to be based on a transformer design, which are used to progressively refine a vector of information for each relationship (or \"edge\" in graph-theory terminology) between an amino acid residue of the protein and another amino acid residue (these relationships are represented by the array shown in green); and between each amino acid position and each different sequences in the input sequence alignment (these relationships are represented by the array shown in red).  Internally these refinement transformations contain layers that have the effect of bringing relevant data together and filtering out irrelevant data (the \"attention mechanism\") for these relationships, in a context-dependent way, learnt from training data.  These transformations are iterated, the updated information output by one step becoming the input of the next, with the sharpened residue/residue information feeding into the update of the residue/sequence information, and then the improved residue/sequence information feeding into the update of the residue/residue information.  As the iteration progresses, according to one report, the \"attention algorithm ... mimics the way a person might assemble a jigsaw puzzle: first connecting pieces in small clumps—in this case clusters of amino acids—and then searching for ways to join the clumps in a larger whole.\"The output of these iterations then informs the final structure prediction module, which also uses transformers, and is itself then iterated.  In an example presented by DeepMind, the structure prediction module achieved a correct topology for the target protein on its first iteration, scored as having a GDT_TS of 78, but with a large number (90%) of stereochemical violations – i.e. unphysical bond angles or lengths.  With subsequent iterations the number of stereochemical violations fell.  By the third iteration the GDT_TS of the prediction was approaching 90, and by the eighth iteration the number of stereochemical violations was approaching zero.The AlphaFold team stated in November 2020 that they believe AlphaFold can be further developed, with room for further improvements in accuracy.The training data was originally restricted to single peptide chains. However, the October 2021 update, named AlphaFold-Multimer, included protein complexes in its training data. DeepMind stated this update succeeded about 70% of the time at accurately predicting protein-protein interactions.\n\nCompetitions\nCASP13\nIn December 2018, DeepMind's AlphaFold placed first in the overall rankings of the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP).The program was particularly successfully predicting the most accurate structure for targets rated as the most difficult by the competition organisers, where no existing template structures were available from proteins with a partially similar sequence. AlphaFold gave the best prediction for 25 out of 43 protein targets in this class, achieving a median score of 58.9 on the CASP's global distance test (GDT) score, ahead of 52.5 and 52.4 by the two next best-placed teams, who were also using deep learning to estimate contact distances. Overall, across all targets, the program achieved a GDT score of 68.5.In January 2020, implementations and illustrative code of AlphaFold 1 was released open-source on GitHub. but, as stated in the \"Read Me\" file on that website: \"This code can't be used to predict structure of an arbitrary protein sequence. It can be used to predict structure only on the CASP13 dataset (links below). The feature generation code is tightly coupled to our internal infrastructure as well as external tools, hence we are unable to open-source it.\" Therefore, in essence, the code deposited is not suitable for general use but only for the CASP13 proteins. The company has not announced plans to make their code publicly available as of 5 March 2021.\n\nCASP14\nIn November 2020, DeepMind's new version, AlphaFold 2, won CASP14. Overall, AlphaFold 2 made the best prediction for 88 out of the 97 targets.On the competition's preferred global distance test (GDT) measure of accuracy, the program achieved a median score of 92.4 (out of 100), meaning that more than half of its predictions were scored at better than 92.4% for having their atoms in more-or-less the right place, a level of accuracy reported to be comparable to experimental techniques like X-ray crystallography. In 2018 AlphaFold 1 had only reached this level of accuracy in two of all of its predictions. 88% of predictions in the 2020 competition had a GDT_TS score of more than 80. On the group of targets classed as the most difficult, AlphaFold 2 achieved a median score of 87.\nMeasured by the root-mean-square deviation (RMS-D) of the placement of the alpha-carbon atoms of the protein backbone chain, which tends to be dominated by the performance of the worst-fitted outliers, 88% of AlphaFold 2's predictions had an RMS deviation of less than 4 Å for the set of overlapped C-alpha atoms.  76% of predictions achieved better than 3 Å, and 46% had a C-alpha atom RMS accuracy better than 2 Å, with a median RMS deviation in its predictions of 2.1 Å for a set of overlapped CA atoms.  AlphaFold 2 also achieved an accuracy in modelling surface side chains described as \"really really extraordinary\".\nTo additionally verify AlphaFold-2 the conference organisers approached four leading experimental groups for structures they were finding particularly challenging and had been unable to determine. In all four cases the three-dimensional models produced by AlphaFold 2 were sufficiently accurate to determine structures of these proteins by molecular replacement.  These included target T1100 (Af1503), a small membrane protein studied by experimentalists for ten years.Of the three structures that AlphaFold 2 had the least success in predicting, two had been obtained by protein NMR methods, which define protein structure directly in aqueous solution, whereas AlphaFold was mostly trained on protein structures in crystals.  The third exists in nature as a multidomain complex consisting of 52 identical copies of the same domain, a situation AlphaFold was not programmed to consider.  For all targets with a single domain, excluding only one very large protein and the two structures determined by NMR, AlphaFold 2 achieved a GDT_TS score of over 80.\n\nCASP15\nIn 2022 DeepMind did not enter CASP15, but most of the entrants used AlphaFold or tools incorporating AlphaFold.\n\nResponses\nAlphaFold 2 scoring more than 90 in CASP's global distance test (GDT) is considered a significant achievement in computational biology and great progress towards a decades-old grand challenge of biology. Nobel Prize winner and structural biologist Venki Ramakrishnan called the result \"a stunning advance on the protein folding problem\", adding that \"It has occurred decades before many people in the field would have predicted. It will be exciting to see the many ways in which it will fundamentally change biological research.\"Propelled by press releases from CASP and DeepMind, AlphaFold 2's success received wide media attention.  As well as news pieces in the specialist science press, such as Nature, Science, MIT Technology Review, and New Scientist, the story was widely covered by major national newspapers, as well as general news-services and weekly publications, such as Fortune, The Economist, Bloomberg, Der Spiegel, and The Spectator. In London The Times made the story its front-page photo lead, with two further pages of inside coverage and an editorial.  A frequent theme was that ability to predict protein structures accurately based on the constituent amino acid sequence is expected to have a wide variety of benefits in the life sciences space including accelerating advanced drug discovery and enabling better understanding of diseases. Writing about the event, the MIT Technology Review noted that the AI had \"solved a fifty-year old grand challenge of biology.\" The same article went on to note that the AI algorithm could \"predict the shape of proteins to within the width of an atom.\"As summed up by Der Spiegel reservations about this coverage have focussed in two main areas: \"There is still a lot to be done\" and: \"We don't even know how they do it\".Although a 30-minute presentation about AlphaFold 2 was given on the second day of the CASP conference (December 1) by project leader John Jumper, it has been described as \"exceedingly high-level, heavy on ideas and insinuations, but almost entirely devoid of detail\". Unlike other research groups presenting at CASP14, DeepMind's presentation was not recorded and is not publicly available. DeepMind is expected to publish a scientific paper giving an account of AlphaFold 2 in the proceedings volume of the CASP conference; but it is not known whether it will go beyond what was said in the presentation.\nSpeaking to El País, researcher Alfonso Valencia said \"The most important thing that this advance leaves us is knowing that this problem has a solution, that it is possible to solve it...  We only know the result. Google does not provide the software and this is the frustrating part of the achievement because it will not directly benefit science.\" Nevertheless, as much as Google and DeepMind do release may help other teams develop similar AI systems, an \"indirect\" benefit.  In late 2019 DeepMind released much of the code of the first version of AlphaFold as open source; but only when work was well underway on the much more radical AlphaFold 2.  Another option it could take might be to make AlphaFold 2 structure prediction available as an online black-box subscription service.  Convergence for a single sequence has been estimated to require on the order of $10,000 worth of wholesale compute time. But this would deny researchers access to the internal states of the system, the chance to learn more qualitatively what gives rise to AlphaFold 2's success, and the potential for new algorithms that could be lighter and more efficient yet still achieve such results.  Fears of potential for a lack of transparency by DeepMind have been contrasted with five decades of heavy public investment into the open Protein Data Bank and then also into open DNA sequence repositories, without which the data to train AlphaFold 2 would not have existed.Of note, on June 18, 2021, Demis Hassabis tweeted: \"Brief update on some exciting progress on #AlphaFold! We've been heads down working flat out on our full methods paper (currently under review) with accompanying open source code and on providing broad free access to AlphaFold for the scientific community. More very soon!\"However it is not yet clear to what extent structure predictions made by AlphaFold 2 will hold up for proteins bound into complexes with other proteins and other molecules.   This was not a part of the CASP competition which AlphaFold entered, and not an eventuality it was internally designed to expect.  Where structures that AlphaFold 2 did predict were for proteins that had strong interactions either with other copies of themselves, or with other structures, these were the cases where AlphaFold 2's predictions tended to be least refined and least reliable.  As a large fraction of the most important biological machines in a cell comprise such complexes, or relate to how protein structures become modified when in contact with other molecules, this is an area that will continue to be the focus of considerable experimental attention.With so little yet known about the internal patterns that AlphaFold 2 learns to make its predictions, it is not yet clear to what extent the program may be impaired in its ability to identify novel folds, if such folds are not well represented in the existing protein structures known in structure databases.  It is also not well known the extent to which protein structures in such databases, overwhelmingly of proteins that it has been possible to crystallise to X-ray, are representative of typical proteins that have not yet been crystallised.  And it is also unclear how representative the frozen protein structures in crystals are of the dynamic structures found in the cells in vivo.  AlphaFold 2's difficulties with structures obtained by protein NMR methods may not be a good sign.\nSo AlphaFold 2's structures may only be a limited help in such contexts.  Moreover, according to Science columnist Derek Lowe, because the prediction of small-molecule binding even then is still not very good, computational prediction of drug targets is simply not in a position to take over as the \"backbone\" of corporate drug discovery—so \"protein structure determination simply isn't a rate-limiting step in drug discovery in general\". It has also been noted that even with a structure for a protein, to then understand how it functions, what it does, and how that fits within wider biological processes can still be very challenging. Nevertheless, if better knowledge of protein structure could lead to better understanding of individual disease mechanisms and ultimately to better drug targets, or better understanding of the differences between human and animal models, ultimately that could lead to improvements.Also, because AlphaFold processes protein-only sequences by design, other associated biomolecules are not considered. On the impact of absent metals, co-factors and, most visibly, co- and post-translational modifications such as protein glycosylation from AlphaFold models, Elisa Fadda (Maynooth University, Ireland) and Jon Agirre (University of York, UK) highlighted the need for scientists to check databases such as UniProt-KB for likely missing components, as these can play an important role not just in folding but in protein function. However, the authors highlighted that many AlphaFold models were accurate enough to allow for the introduction of post-predictional modifications.Finally, some have noted that even a perfect answer to the protein prediction problem would still leave questions about the protein folding problem—understanding in detail how the folding process actually occurs in nature (and how sometimes they can also misfold).But even with such caveats, AlphaFold 2 was described as a huge technical step forward and intellectual achievement.\n\nProtein Structure Database\nThe AlphaFold Protein Structure Database was launched on July 22, 2021, as a joint effort between AlphaFold and EMBL-EBI. At launch the database contains AlphaFold-predicted models of protein structures of nearly the full UniProt proteome of humans and 20 model organisms, amounting to over 365,000 proteins. The database does not include proteins with fewer than 16 or more than 2700 amino acid residues, but for humans they are available in the whole batch file. AlphaFold planned to add more sequences to the collection, the initial goal (as of beginning of 2022) being to cover most of the UniRef90 set of more than 100 million proteins. As of May 15, 2022, 992,316 predictions were available.In July 2021, UniProt-KB and InterPro has been updated to show AlphaFold predictions when available.On July 28, 2022, the team uploaded to the database the structures of around 200 million proteins from 1 million species, covering nearly every known protein on the planet.\n\nLimitations\nThe AlphaFold DB uses a monomeric model similar to the CASP14 version. As a result, many of the same limitations are expected:\nThe DB model only predicts monomers, missing some important context in the form of protein complexes. The AlphaFold Multimer model is published separately as open-source, but pre-run models are not available.\nThe model is unreliable for intrinsically disordered proteins, although it does convey the information via a low confidence score.\nThe model is not validated for mutational analysis.\nThe model relies to some extent upon co-evolutionary information across similar proteins, and thus may not perform well on synthetic proteins or proteins with very low homology to anything in the database.\nThe model can only output one conformation of proteins with multiple conformations, with no control of which.\nThe model only predicts protein structure without cofactors and co- and post-translational modifications. This can be a significant shortcoming for a number of biologically-relevant systems: between 50% and 70% of the structures of the human proteome are incomplete without covalently-attached glycans. On the other hand, since the model is trained from PDB models often with these modifications attached, the predicted structure is \"frequently consistent with the expected structure in the presence of ions or cofactors\".\n\nApplications\nSARS-CoV-2\nAlphaFold has been used to predict structures of proteins of SARS-CoV-2, the causative agent of COVID-19. The structures of these proteins were pending experimental detection in early 2020. Results were examined by the scientists at the Francis Crick Institute in the United Kingdom before release into the larger research community. The team also confirmed accurate prediction against the experimentally determined SARS-CoV-2 spike protein that was shared in the Protein Data Bank, an international open-access database, before releasing the computationally determined structures of the under-studied protein molecules. The team acknowledged that although these protein structures might not be the subject of ongoing therapeutical research efforts, they will add to the community's understanding of the SARS-CoV-2 virus. Specifically, AlphaFold 2's prediction of the structure of the ORF3a protein was very similar to the structure determined by researchers at University of California, Berkeley using cryo-electron microscopy. This specific protein is believed to assist the virus in breaking out of the host cell once it replicates. This protein is also believed to play a role in triggering the inflammatory response to the infection.\n\nPublished works\nAndrew W. Senior et al. (December 2019), \"Protein structure prediction using multiple deep neural networks in the 13th Critical Assessment of Protein Structure Prediction (CASP13)\", Proteins: Structure, Function, Bioinformatics 87(12) 1141–1148 doi:10.1002/prot.25834\nAndrew W. Senior et al. (15 January 2020), \"Improved protein structure prediction using potentials from deep learning\", Nature 577 706–710 doi:10.1038/s41586-019-1923-7\nJohn Jumper et al. (December 2020), \"High Accuracy Protein Structure Prediction Using Deep Learning\", in Fourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), pp. 22–24\nJohn Jumper et al. (December 2020), \"AlphaFold 2\". Presentation given at CASP 14.\n\nSee also\nReferences\nFurther reading\nCarlos Outeiral, CASP14: what Google DeepMind's AlphaFold 2 really achieved, and what it means for protein folding, biology and bioinformatics, Oxford Protein Informatics Group. (3 December)\nMohammed AlQuraishi, AlphaFold2 @ CASP14: \"It feels like one's child has left home.\" (blog), 8 December 2020\nMohammed AlQuraishi, The AlphaFold2 Method Paper: A Fount of Good Ideas (blog), 25 July 2021\n\nExternal links for AlphaFold 2\nAlphaFold v2.1 code and links to model on GitHub\nOpen access to protein structure predictions for the human proteome and 20 other key organisms at European Bioinformatics Institute\nCASP 14 website\nAlphaFold: The making of a scientific breakthrough, DeepMind, via YouTube.\nColabFold (Mirdita, Milot; Schütze, Konstantin; Moriwaki, Yoshitaka; Heo, Lim; Ovchinnikov, Sergey; Steinegger, Martin (2022-05-30). \"ColabFold: Making protein folding accessible to all\". Nature Methods. 19 (6): 679–682. doi:10.1038/s41592-022-01488-1. PMC 9184281. PMID 35637307.), version for homooligomeric prediction and complexes\nAlphaFold Protein Structure Database website",
    "AlphaGo": "AlphaGo is a computer program that plays the board game Go. It was developed by the London-based DeepMind Technologies, an acquired subsidiary of Google (now Alphabet Inc.). Subsequent versions of AlphaGo became increasingly powerful, including a version that competed under the name Master. After retiring from competitive play, AlphaGo Master was succeeded by an even more powerful version known as AlphaGo Zero, which was completely self-taught without learning from human games. AlphaGo Zero was then generalized into a program known as AlphaZero, which played additional games, including chess and shogi.  AlphaZero has in turn been succeeded by a program known as MuZero which learns without being taught the rules.\nAlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously acquired by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play. A neural network is trained to identify the best moves and the winning percentages of these moves. This neural network improves the strength of the tree search, resulting in stronger move selection in the next iteration.\nIn October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19×19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicap. Although it lost to Lee Sedol in the fourth game, Lee resigned in the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of the victory, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. The lead up and the challenge match with Lee Sedol were documented in a documentary film also titled AlphaGo, directed by Greg Kohs. The win by AlphaGo was chosen by Science as one of the Breakthrough of the Year runners-up on 22 December 2016.At the 2017 Future of Go Summit, the Master version of AlphaGo beat Ke Jie, the number one ranked player in the world at the time, in a three-game match, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.After the match between AlphaGo and Ke Jie, DeepMind retired AlphaGo, while continuing AI research in other areas. The self-taught AlphaGo Zero achieved a 100–0 victory against the early competitive version of AlphaGo, and its successor AlphaZero is currently perceived as the world's top player in Go.\n\nHistory\nGo is considered much more difficult for computers to win than other games such as chess, because its strategic and aesthetic nature makes it hard to directly construct an evaluation function, and its much larger branching factor makes it prohibitively difficult to use traditional AI methods such as alpha–beta pruning, tree traversal and heuristic search.Almost two decades after IBM's computer Deep Blue beat world chess champion Garry Kasparov in the 1997 match, the strongest Go programs using artificial intelligence techniques only reached about amateur 5-dan level, and still could not beat a professional Go player without a handicap. In 2012, the software program Zen, running on a four PC cluster, beat Masaki Takemiya (9p) twice at five- and four-stone handicaps. In 2013, Crazy Stone beat Yoshio Ishida (9p) at a four-stone handicap.According to DeepMind's David Silver, the AlphaGo research project was formed around 2014 to test how well a neural network using deep learning can compete at Go. AlphaGo represents a significant improvement over previous Go programs. In 500 games against other available Go programs, including Crazy Stone and Zen, AlphaGo running on a single computer won all but one. In a similar matchup, AlphaGo running on multiple computers won all 500 games played against other Go programs, and 77% of games played against AlphaGo running on a single computer. The distributed version in October 2015 was using 1,202 CPUs and 176 GPUs.\n\nMatch against Fan Hui\nIn October 2015, the distributed version of AlphaGo defeated the European Go champion Fan Hui, a 2-dan (out of 9 dan possible) professional, five to zero. This was the first time a computer Go program had beaten a professional human player on a full-sized board without handicap. The announcement of the news was delayed until 27 January 2016 to coincide with the publication of a paper in the journal Nature describing the algorithms used.\n\nMatch against Lee Sedol\nAlphaGo played South Korean professional Go player Lee Sedol, ranked 9-dan, one of the best players at Go, with five games taking place at the Four Seasons Hotel in Seoul, South Korea on 9, 10, 12, 13, and 15 March 2016, which were video-streamed live.  Out of five games, AlphaGo won four games and Lee won the fourth game which made him recorded as the only human player who beat AlphaGo in all of its 74 official games. AlphaGo ran on Google's cloud computing with its servers located in the United States. The match used Chinese rules with a 7.5-point komi, and each side had two hours of thinking time plus three 60-second byoyomi periods. The version of AlphaGo playing against Lee used a similar amount of computing power as was used in the Fan Hui match. The Economist reported that it used 1,920 CPUs and 280 GPUs. At the time of play, Lee Sedol had the second-highest number of Go international championship victories in the world after South Korean player Lee Changho who kept the world championship title for 16 years. Since there is no single official method of ranking in international Go, the rankings may vary among the sources. While he was ranked top sometimes, some sources ranked Lee Sedol as the fourth-best player in the world at the time. AlphaGo was not specifically trained to face Lee nor was designed to compete with any specific human players.\nThe first three games were won by AlphaGo following resignations by Lee. However, Lee beat AlphaGo in the fourth game, winning by resignation at move 180. AlphaGo then continued to achieve a fourth win, winning the fifth game by resignation.The prize was US$1 million. Since AlphaGo won four out of five and thus the series, the prize will be donated to charities, including UNICEF. Lee Sedol received $150,000 for participating in all five games and an additional $20,000 for his win in Game 4.In June 2016, at a presentation held at a university in the Netherlands, Aja Huang, one of the Deep Mind team, revealed that they had patched the logical weakness that occurred during the 4th game of the match between AlphaGo and Lee, and that after move 78 (which was dubbed the \"divine move\" by many professionals), it would play as intended and maintain Black's advantage. Before move 78, AlphaGo was leading throughout the game, but Lee's move caused the program's computing powers to be diverted and confused. Huang explained that AlphaGo's policy network of finding the most accurate move order and continuation did not precisely guide AlphaGo to make the correct continuation after move 78, since its value network did not determine Lee's 78th move as being the most likely, and therefore when the move was made AlphaGo could not make the right adjustment to the logical continuation.\n\nSixty online games\nOn 29 December 2016, a new account on the Tygem server named \"Magister\" (shown as 'Magist' at the server's Chinese version) from South Korea began to play games with professional players. It changed its account name to \"Master\" on 30 December, then moved to the FoxGo server on 1 January 2017. On 4 January, DeepMind confirmed that the \"Magister\" and the \"Master\" were both played by an updated version of AlphaGo, called AlphaGo Master. As of 5 January 2017, AlphaGo Master's online record was 60 wins and 0 losses, including three victories over Go's top-ranked player, Ke Jie, who had been quietly briefed in advance that Master was a version of AlphaGo. After losing to Master, Gu Li offered a bounty of 100,000 yuan (US$14,400) to the first human player who could defeat Master. Master played at the pace of 10 games per day. Many quickly suspected it to be an AI player due to little or no resting between games. Its adversaries included many world champions such as Ke Jie, Park Jeong-hwan, Yuta Iyama, Tuo Jiaxi, Mi Yuting, Shi Yue, Chen Yaoye, Li Qincheng, Gu Li, Chang Hao, Tang Weixing, Fan Tingyu, Zhou Ruiyang, Jiang Weijie, Chou Chun-hsun, Kim Ji-seok, Kang Dong-yun, Park Yeong-hun, and Won Seong-jin; national champions or world championship runners-up such as Lian Xiao, Tan Xiao, Meng Tailing, Dang Yifei, Huang Yunsong, Yang Dingxin, Gu Zihao, Shin Jinseo, Cho Han-seung, and An Sungjoon. All 60 games except one were fast-paced games with three 20 or 30 seconds byo-yomi. Master offered to extend the byo-yomi to one minute when playing with Nie Weiping in consideration of his age. After winning its 59th game Master revealed itself in the chatroom to be controlled by Dr. Aja Huang of the DeepMind team, then changed its nationality to the United Kingdom. After these games were completed, the co-founder of DeepMind, Demis Hassabis, said in a tweet, \"we're looking forward to playing some official, full-length games later [2017] in collaboration with Go organizations and experts\".Go experts were impressed by the program's performance and its nonhuman play style; Ke Jie stated that \"After humanity spent thousands of years improving our tactics, computers tell us that humans are completely wrong... I would go as far as to say not a single human has touched the edge of the truth of Go.\"\n\nFuture of Go Summit\nIn the Future of Go Summit held in Wuzhen in May 2017, AlphaGo Master played three games with Ke Jie, the world No.1 ranked player, as well as two games with several top Chinese professionals, one pair Go game and one against a collaborating team of five human players.Google DeepMind offered 1.5 million dollar winner prizes for the three-game match between Ke Jie and Master while the losing side took 300,000 dollars. Master won all three games against Ke Jie, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.After winning its three-game match against Ke Jie, the top-rated world Go player, AlphaGo retired. DeepMind also disbanded the team that worked on the game to focus on AI research in other areas. After the Summit, Deepmind published 50 full length AlphaGo vs AlphaGo matches, as a gift to the Go community.\n\nAlphaGo Zero and AlphaZero\nAlphaGo's team published an article in the journal Nature on 19 October 2017, introducing AlphaGo Zero, a version without human data and stronger than any previous human-champion-defeating version. By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.In a paper released on arXiv on 5 December 2017, DeepMind claimed that it generalized AlphaGo Zero's approach into a single AlphaZero algorithm, which achieved within 24 hours a superhuman level of play in the games of chess, shogi, and Go by defeating world-champion programs, Stockfish, Elmo, and 3-day version of AlphaGo Zero in each case.\n\nTeaching tool\nOn 11 December 2017, DeepMind released AlphaGo teaching tool on its website to analyze winning rates of different Go openings as calculated by AlphaGo Master. The teaching tool collects 6,000 Go openings from 230,000 human games each analyzed with 10,000,000 simulations by AlphaGo Master. Many of the openings include human move suggestions.\n\nVersions\nAn early version of AlphaGo was tested on hardware with various numbers of CPUs and GPUs, running in asynchronous or distributed mode. Two seconds of thinking time was given to each move. The resulting Elo ratings are listed below. In the matches with more time per move higher ratings are achieved.\n\nIn May 2016, Google unveiled its own proprietary hardware \"tensor processing units\", which it stated had already been deployed in multiple internal projects at Google, including the AlphaGo match against Lee Sedol.In the Future of Go Summit in May 2017, DeepMind disclosed that the version of AlphaGo used in this Summit was AlphaGo Master, and revealed that it had measured the strength of different versions of the software. AlphaGo Lee, the version used against Lee, could give AlphaGo Fan, the version used in AlphaGo vs. Fan Hui, three stones, and AlphaGo Master was even three stones stronger.\n\nAlgorithm\nAs of 2016, AlphaGo's algorithm uses a combination of machine learning and tree search techniques, combined with extensive training, both from human and computer play. It uses Monte Carlo tree search, guided by a \"value network\" and a \"policy network,\" both implemented using deep neural network technology. A limited amount of game-specific feature detection pre-processing (for example, to highlight whether a move matches a nakade pattern) is applied to the input before it is sent to the neural networks. The networks are convolutional neural networks with 12 layers, trained by reinforcement learning.The system's neural networks were initially bootstrapped from human gameplay expertise. AlphaGo was initially trained to mimic human play by attempting to match the moves of expert players from recorded historical games, using a database of around 30 million moves. Once it had reached a certain degree of proficiency, it was trained further by being set to play large numbers of games against other instances of itself, using reinforcement learning to improve its play. To avoid \"disrespectfully\" wasting its opponent's time, the program is specifically programmed to resign if its assessment of win probability falls beneath a certain threshold; for the match against Lee, the resignation threshold was set to 20%.\n\nStyle of play\nToby Manning, the match referee for AlphaGo vs. Fan Hui, has described the program's style as \"conservative\". AlphaGo's playing style strongly favours greater probability of winning by fewer points over lesser probability of winning by more points. Its strategy of maximising its probability of winning is distinct from what human players tend to do which is to maximise territorial gains, and explains some of its odd-looking moves. It makes a lot of opening moves that have never or seldom been made by humans. It likes to use shoulder hits, especially if the opponent is over concentrated.\n\nResponses to 2016 victory\nAI community\nAlphaGo's March 2016 victory was a major milestone in artificial intelligence research. Go had previously been regarded as a hard problem in machine learning that was expected to be out of reach for the technology of the time. Most experts thought a Go program as powerful as AlphaGo was at least five years away; some experts thought that it would take at least another decade before computers would beat Go champions. Most observers at the beginning of the 2016 matches expected Lee to beat AlphaGo.With games such as checkers (that has been \"solved\" by the Chinook draughts player team), chess, and now Go won by computers, victories at popular board games can no longer serve as major milestones for artificial intelligence in the way that they used to. Deep Blue's Murray Campbell called AlphaGo's victory \"the end of an era... board games are more or less done and it's time to move on.\"When compared with Deep Blue or Watson, AlphaGo's underlying algorithms are potentially more general-purpose and may be evidence that the scientific community is making progress towards artificial general intelligence. Some commentators believe AlphaGo's victory makes for a good opportunity for society to start preparing for the possible future impact of machines with general purpose intelligence. As noted by entrepreneur Guy Suter, AlphaGo only knows how to play Go and doesn't possess general-purpose intelligence; \"[It] couldn't just wake up one morning and decide it wants to learn how to use firearms.\" AI researcher Stuart Russell said that AI systems such as AlphaGo have progressed quicker and become more powerful than expected, and we must therefore develop methods to ensure they \"remain under human control\". Some scholars,  such as Stephen Hawking, warned (in May 2015 before the matches) that some future self-improving AI could gain actual general intelligence, leading to an unexpected AI takeover; other scholars disagree: AI expert Jean-Gabriel Ganascia believes that \"Things like 'common sense'... may never be reproducible\", and says \"I don't see why we would speak about fears. On the contrary, this raises hopes in many domains such as health and space exploration.\" Computer scientist Richard Sutton said \"I don't think people should be scared... but I do think people should be paying attention.\"In China, AlphaGo was a \"Sputnik moment\" which helped convince the Chinese government to prioritize and dramatically increase funding for artificial intelligence.In 2017, the DeepMind AlphaGo team received the inaugural IJCAI Marvin Minsky medal for Outstanding Achievements in AI. \"AlphaGo is a wonderful achievement, and a perfect example of what the Minsky Medal was initiated to recognise\", said Professor Michael Wooldridge, Chair of the IJCAI Awards Committee.  \"What particularly impressed IJCAI was that AlphaGo achieves what it does through a brilliant combination of classic AI techniques as well as the state-of-the-art machine learning techniques that DeepMind is so closely associated with. It's a breathtaking demonstration of contemporary AI, and we are delighted to be able to recognise it with this award.\"\n\nGo community\nGo is a popular game in China, Japan and Korea, and the 2016 matches were watched by perhaps a hundred million people worldwide. Many top Go players characterized AlphaGo's unorthodox plays as seemingly-questionable moves that initially befuddled onlookers, but made sense in hindsight: \"All but the very best Go players craft their style by imitating top players. AlphaGo seems to have totally original moves it creates itself.\" AlphaGo appeared to have unexpectedly become much stronger, even when compared with its October 2015 match where a computer had beaten a Go professional for the first time ever without the advantage of a handicap. The day after Lee's first defeat, Jeong Ahram, the lead Go correspondent for one of South Korea's biggest daily newspapers, said \"Last night was very gloomy... Many people drank alcohol.\" The Korea Baduk Association, the organization that oversees Go professionals in South Korea, awarded AlphaGo an honorary 9-dan title for exhibiting creative skills and pushing forward the game's progress.China's Ke Jie, an 18-year-old generally recognized as the world's best Go player at the time, initially claimed that he would be able to beat AlphaGo, but declined to play against it for fear that it would \"copy my style\". As the matches progressed, Ke Jie went back and forth, stating that \"it is highly likely that I (could) lose\" after analysing the first three matches, but regaining confidence after AlphaGo displayed flaws in the fourth match.Toby Manning, the referee of AlphaGo's match against Fan Hui, and Hajin Lee, secretary general of the International Go Federation, both reason that in the future, Go players will get help from computers to learn what they have done wrong in games and improve their skills.After game two, Lee said he felt \"speechless\": \"From the very beginning of the match, I could never manage an upper hand for one single move. It was AlphaGo's total victory.\" Lee apologized for his losses, stating after game three that \"I misjudged the capabilities of AlphaGo and felt powerless.\" He emphasized that the defeat was \"Lee Se-dol's defeat\" and \"not a defeat of mankind\". Lee said his eventual loss to a machine was \"inevitable\" but stated that \"robots will never understand the beauty of the game the same way that we humans do.\" Lee called his game four victory a \"priceless win that I (would) not exchange for anything.\"\n\nAlphaGo films 2016 (critics)\nThere is a large interest prior to the match between Lee Sedol and AlphaGo in 2016. Hence, the director Greg Kohs brings in the idea of creating a documentary film to record this historical moment for the public view. There are many critics after the premier and actual showtime of the film from a wide range of perspectives from people in many industries.\n\nCritic Reviewer\nRotten Tomato has an average rating of 100% Tomatometer scores from 10 reviews and an audience review of 99% scores from more than 100 ratings. For IMDb, 6,431 users have given a weighted average vote of 7.8 from 10 scores. For Metacritic reviewers, users give an average rating of 9.3 / 10 scores. For Metascore, outside organizations give an average of 7 / 10 scores for the rating, references from the Los Angeles Times, The Hollywood Reporter, and VOICE.\n\nProfessional Go player\nHajin Lee, a former professional Go player, described this documentary as being \"beautifully filmed.\" In addition to the story itself, the feelings and atmosphere were also conveyed through different scene arrangements. For example, the close-up shots of Lee Sedol when he realizes that the AlphaGo AI is intelligent, the atmospheric scene of the Korean commentator's distress and affliction following the first defeat, and the tension being held inside the room. The documentary also tells a story by describing the background of AlphaGo technology and the customs of the Korean Go community. She suggests some areas to be covered additionally. For instance, the details of the AI prior to AlphaGo, the confidence and pride of the professional Go players, and the shifting of perspective to the Go AI between and after the match as “If anything could be added, I would include information about the primitive level of top Go A.I.s before AlphaGo, and more about professional Go players’ lives and pride, to provide more context for Lee Sedol’s pre-match confidence, and Go players’ changing perception of AlphaGo as the match advanced”.(Lee, 2017). \nFan Hui, a professional Go player, and former player with AlphaGo said that “DeepMind had trained AlphaGo by showing it many strong amateur games of Go to develop its understanding of how a human plays before challenging it to play versions of itself thousands of times, a novel form of reinforcement learning which had given it the ability to rival an expert human. History had been made, and centuries of received learning overturned in the process. The program was free to learn the game for itself.\n\nTechnology and AI-related fields\nJames Vincent, a reporter from The Verge, comments that “It prods and pokes viewers with unsubtle emotional cues, like a reality TV show would. “Now, you should be nervous; now you should feel relieved”. The AlphaGo footage slowly captures the moment when Lee Sedol acknowledges the true power of AlphaGo AI. In the first game, he had more experience than his human-programmed AI, so he thought it would be easy to beat the AI. However, the early game dynamics were not what he expected. After losing the first match, he became more nervous and lost confidence. Afterward, he reacted to attacks by saying that he just wanted to win the match, unintentionally displaying his anger, and acting in an unusual way. Also, he spends 12 minutes on one move, while AlphaGo only takes a minute and a half to respond. AlphaGo weighs each alternative equally and consistently. No reaction to Lee's fight. Instead, the game continues as if he was not there.\nJames also said that “suffice to say that humanity does land at least one blow on the machines, through Lee’s so-called “divine move”. “More likely, the forces of automation we’ll face will be impersonal and incomprehensible. They’ll come in the form of star ratings we can’t object to, and algorithms we can’t fully understand. Dealing with the problems of AI will take a perspective that looks beyond individual battles. AlphaGo is worth seeing because it raises these questions” (Vincent, 2017) \nMurray Shanahan, a professor of cognitive robotics at Imperial College London, critics that “Go is an extraordinary game but it represents what we can do with AI in all kinds of other spheres,” says Murray Shanahan, professor of cognitive robotics at Imperial College London and senior research scientist at DeepMind, says. “In just the same way there are all kinds of realms of possibility within Go that have not been discovered, we could never have imagined the potential for discovering drugs and other materials.”\n\nFilm Industry\nGreg Kohs, the director of the film, said “The complexity of the game of Go, combined with the technical depth of an emerging technology like artificial intelligence seemed like it might create an insurmountable barrier for a film like this. The fact that I was so innocently unaware of Go and AlphaGo actually proved to be beneficial. It allowed me to approach the action and interviews with pure curiosity, the kind that helps make any subject matter emotionally accessible.” \nGreg Kohs also said that “Unlike the film’s human characters – who turn their curious quest for knowledge into an epic spectacle with great existential implications, who dare to risk their reputation and pride to contest that curiosity – AI might not yet possess the ability to empathize. But it can teach us profound things about our humanness – the way we play board games, the way we think and feel and grow. It’s a deep, vast premise, but my hope is, by sharing it, we can discover something within ourselves we never saw before”.John Defore from The Hollywood Reporter, a famous news report that focuses on the film industry mentioned in Rotten Tomato that this documentary is “an involving sports-rivalry doc with an AI twist.” “In the end, observers wonder if AlphaGo’s odd variety of intuition might not kill Go as an intellectual pursuit but shift its course, forcing the game’s scholars to consider it from new angles. So maybe it isn’t time to welcome our computer overlords, and won’t be for a while - maybe they’ll teach us to be better thinkers before turning us into their slaves.”\n\nSpiritual and cultural expert\nDirector Greg Kohs brings out all of the drama and rush of these games to show the selection process for the master of Go. On this small stage, the human Go champion and the AI challenger engage in an intense duel. In this documentary, Kohls looks into how the human mind functions under pressure, the significance of errors, the actions of the computer, and its inventiveness. (Frederic and Mary Ann Brussat, n.d.)\n\nNews\nAccording to Rechtshaffen Michael of the Los Angeles Times, who gave the movie 80 out of 100 possible points on Metascore, he said: “It helps matters when you have a group of engaging human subjects like soft-spoken Sedol, who’s as intensively contemplative as the game itself, contrasted by the spirited, personable Fan Hui, the Paris-based European champ who accepts an offer to serve as an advisor for the DeepMind team after suffering a demoralizing AI trouncing”. He also mentioned that with the passion of Hauschka's Volker Bertelmann, the film's producer, this documentary shows many unexpected sequences, including strategic and philosophical components. (Rechtshaffen, 2017\n\nSimilar systems\nFacebook has also been working on its own Go-playing system darkforest, also based on combining machine learning and Monte Carlo tree search. Although a strong player against other computer Go programs, as of early 2016, it had not yet defeated a professional human player. Darkforest has lost to CrazyStone and Zen and is estimated to be of similar strength to CrazyStone and Zen.DeepZenGo, a system developed with support from video-sharing website Dwango and the University of Tokyo, lost 2–1 in November 2016 to Go master Cho Chikun, who holds the record for the largest number of Go title wins in Japan.A 2018 paper in Nature cited AlphaGo's approach as the basis for a new means of computing potential pharmaceutical drug molecules.\n\nExample game\nAlphaGo Master (white) v. Tang Weixing (31 December 2016), AlphaGo won by resignation. White 36 was widely praised.\n\nImpacts on Go\nThe documentary film AlphaGo raised hopes that Lee Sedol and Fan Hui would have benefitted from their experience of playing AlphaGo, but as of May 2018 their ratings were little changed; Lee Sedol was ranked 11th in the world, and Fan Hui 545th. On 19 November 2019, Lee announced his retirement from professional play, arguing that he could never be the top overall player of Go due to the increasing dominance of AI. Lee referred to them as being \"an entity that cannot be defeated\".\n\nSee also\nReferences\nExternal links\n Media related to AlphaGo at Wikimedia Commons\n Quotations related to AlphaGo at Wikiquote\nOfficial website\nAlphaGo wiki at Sensei's Library, including links to AlphaGo games\nAlphaGo page, with archive and games\nEstimated 2017 rating of Alpha Go\nAlphaGo - The Movie on YouTube",
    "AlphaZero": "AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.  This algorithm uses an approach similar to AlphaGo Zero. \nOn December 5, 2017, the DeepMind team released a preprint paper introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, Elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via self-play using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs. \nDeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018; however, the AlphaZero program itself has not been made available to the public. In 2019, DeepMind published a new paper detailing MuZero, a new algorithm able to generalise AlphaZero's work, playing both Atari and board games without knowledge of the rules or representations of the game.\n\nRelation to AlphaGo Zero\nAlphaZero (AZ) is a more generalized variant of the AlphaGo Zero (AGZ) algorithm, and is able to play shogi and chess as well as Go. Differences between AZ and AGZ include:\nAZ has hard-coded rules for setting search hyperparameters.\nThe neural network is now updated continually.\nAZ doesn't use symmetries, unlike AGZ.\nChess can end in a draw unlike Go; therefore, AlphaZero takes into account the possibility of a drawn game.\n\nStockfish and Elmo\nComparing Monte Carlo tree search searches, AlphaZero searches just 80,000 positions per second in chess and 40,000 in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variation.\n\nTraining\nAlphaZero was trained solely via self-play, using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks. In parallel, the in-training AlphaZero was periodically matched against its benchmark (Stockfish, Elmo, or AlphaGo Zero) in brief one-second-per-move games to determine how well the training was progressing. DeepMind judged that AlphaZero's performance exceeded the benchmark after around four hours of training for Stockfish, two hours for Elmo, and eight hours for AlphaGo Zero.\n\nPreliminary results\nOutcome\nChess\nIn AlphaZero's chess match against Stockfish 8 (2016 TCEC world champion), each program was given one minute per move. AlphaZero was flying the English flag, while Stockfish the Norwegian. Stockfish was allocated 64 threads and a hash size of 1 GB, a setting that Stockfish's Tord Romstad later criticized as suboptimal. AlphaZero was trained on chess for a total of nine hours before the match. During the match, AlphaZero ran on a single machine with four application-specific TPUs. In 100 games from the normal starting position, AlphaZero won 25 games as White, won 3 as Black, and drew the remaining 72. In a series of twelve, 100-game matches (of unspecified time or resource constraints) against Stockfish starting from the 12 most popular human openings, AlphaZero won 290, drew 886 and lost 24.\n\nShogi\nAlphaZero was trained on shogi for a total of two hours before the tournament. In 100 shogi games against Elmo (World Computer Shogi Championship 27 summer 2017 tournament version with YaneuraOu 4.73 search), AlphaZero won 90 times, lost 8 times and drew twice. As in the chess games, each program got one minute per move, and Elmo was given 64 threads and a hash size of 1 GB.\n\nGo\nAfter 34 hours of self-learning of Go and against AlphaGo Zero, AlphaZero won 60 games and lost 40.\n\nAnalysis\nDeepMind stated in its preprint, \"The game of chess represented the pinnacle of AI research over several decades. State-of-the-art programs are based on powerful engines that search many millions of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic reinforcement learning algorithm –  originally devised for the game of go –  that achieved superior results within a few hours, searching a thousand times fewer positions, given no domain knowledge except the rules.\" DeepMind's Demis Hassabis, a chess player himself, called AlphaZero's play style \"alien\": It sometimes wins by offering counterintuitive sacrifices, like offering up a queen and bishop to exploit a positional advantage. \"It's like chess from another dimension.\"Given the difficulty in chess of forcing a win against a strong opponent, the +28 –0 =72 result is a significant margin of victory. However, some grandmasters, such as Hikaru Nakamura and Komodo developer Larry Kaufman, downplayed AlphaZero's victory, arguing that the match would have been closer if the programs had access to an opening database (since Stockfish was optimized for that scenario). Romstad additionally pointed out that Stockfish is not optimized for rigidly fixed-time moves and the version used was a year old.Similarly, some shogi observers argued that the Elmo hash size was too low, that the resignation settings and the \"EnteringKingRule\" settings (cf. shogi § Entering King) may have been inappropriate, and that Elmo is already obsolete compared with newer programs.\n\nReaction and criticism\nPapers headlined that the chess training took only four hours: \"It was managed in little more than the time between breakfast and lunch.\" Wired described AlphaZero as \"the first multi-skilled AI board-game champ\". AI expert Joanna Bryson noted that Google's \"knack for good publicity\" was putting it in a strong position against challengers. \"It's not only about hiring the best programmers. It's also very political, as it helps make Google as strong as possible when negotiating with governments and regulators looking at the AI sector.\"Human chess grandmasters generally expressed excitement about AlphaZero. Danish grandmaster Peter Heine Nielsen likened AlphaZero's play to that of a superior alien species. Norwegian grandmaster Jon Ludvig Hammer characterized AlphaZero's play as \"insane attacking chess\" with profound positional understanding. Former champion Garry Kasparov said, \"It's a remarkable achievement, even if we should have expected it after AlphaGo.\"Grandmaster Hikaru Nakamura was less impressed, stating: \"I don't necessarily put a lot of credibility in the results simply because my understanding is that AlphaZero is basically using the Google supercomputer and Stockfish doesn't run on that hardware; Stockfish was basically running on what would be my laptop. If you wanna have a match that's comparable you have to have Stockfish running on a supercomputer as well.\"Top US correspondence chess player Wolff Morrow was also unimpressed, claiming that AlphaZero would probably not make the semifinals of a fair competition such as TCEC where all engines play on equal hardware. Morrow further stated that although he might not be able to beat AlphaZero if AlphaZero played drawish openings such as the Petroff Defence, AlphaZero would not be able to beat him in a correspondence chess game either.Motohiro Isozaki, the author of YaneuraOu, noted that although AlphaZero did comprehensively beat Elmo, the rating of AlphaZero in shogi stopped growing at a point which is at most 100–200 higher than Elmo. This gap is not that high, and Elmo and other shogi software should be able to catch up in 1–2 years.\n\nFinal results\nDeepMind addressed many of the criticisms in their final version of the paper, published in December 2018 in Science. They further clarified that AlphaZero was not running on a supercomputer; it was trained using 5,000 tensor processing units (TPUs), but only ran on four TPUs and a 44-core CPU in its matches.\n\nChess\nIn the final results, Stockfish version 8 ran under the same conditions as in the TCEC superfinal: 44 CPU cores, Syzygy endgame tablebases, and a 32GB hash size. Instead of a fixed time control of one move per minute, both engines were given 3 hours plus 15 seconds per move to finish the game. In a 1000-game match, AlphaZero won with a score of 155 wins, 6 losses, and 839 draws. DeepMind also played a series of games using the TCEC opening positions; AlphaZero also won convincingly. Stockfish needed 10-to-1 time odds to match AlphaZero.\n\nShogi\nSimilar to Stockfish, Elmo ran under the same conditions as in the 2017 CSA championship. The version of Elmo used was WCSC27 in combination with YaneuraOu 2017 Early KPPT 4.79 64AVX2 TOURNAMENT. Elmo operated on the same hardware as Stockfish: 44 CPU cores and a 32GB hash size. AlphaZero won 98.2% of games when playing sente (i.e. having the first move) and 91.2% overall.\n\nReactions and criticisms\nHuman grandmasters were generally impressed with AlphaZero's games against Stockfish. Former world champion Garry Kasparov said it was a pleasure to watch AlphaZero play, especially since its style was open and dynamic like his own.In the computer chess community, Komodo developer Mark Lefler called it a \"pretty amazing achievement\", but also pointed out that the data was old, since Stockfish had gained a lot of strength since January 2018 (when Stockfish 8 was released). Fellow developer Larry Kaufman said AlphaZero would probably lose a match against the latest version of Stockfish, Stockfish 10, under Top Chess Engine Championship (TCEC) conditions. Kaufman argued that the only advantage of neural network–based engines was that they used a GPU, so if there was no regard for power consumption (e.g. in an equal-hardware contest where both engines had access to the same CPU and GPU) then anything the GPU achieved was \"free\". Based on this, he stated that the strongest engine was likely to be a hybrid with neural networks and standard alpha–beta search.AlphaZero inspired the computer chess community to develop Leela Chess Zero, using the same techniques as AlphaZero. Leela contested several championships against Stockfish, where it showed roughly similar strength to Stockfish, although Stockfish has since pulled away.In 2019 DeepMind published MuZero, a unified system that played excellent chess, shogi, and go, as well as games in the Atari Learning Environment, without being pre-programmed with their rules.\n\nSee also\nNotes\nReferences\nExternal links\nChessprogramming wiki on AlphaZero\nChess.com Youtube playlist for AlphaZero vs. Stockfish",
    "Amazon Machine Learning": "Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Oftentimes, clients will use this in combination with autoscaling (a process that allows a client to use more computing in times of high application usage, and then scale down to reduce costs when there is less traffic). These cloud computing web services provide various services related to networking, compute, storage, middleware, IoT and other processing capacity, as well as software tools via AWS server farms.  This frees clients from managing, scaling, and patching hardware and operating systems. \nOne of the foundational services is Amazon Elastic Compute Cloud (EC2), which allows users to have at their disposal a virtual cluster of computers, with extremely high availability, which can be interacted with over the internet via REST APIs, a CLI or the AWS console.  AWS's virtual computers emulate most of the attributes of a real computer, including hardware central processing units (CPUs) and graphics processing units (GPUs) for processing; local/RAM memory; hard-disk/SSD storage; a choice of operating systems; networking; and pre-loaded application software such as web servers, databases, and customer relationship management (CRM).\nAWS services are delivered to customers via a network of AWS server farms located throughout the world. Fees are based on a combination of usage (known as a \"Pay-as-you-go\" model), hardware, operating system, software, or networking features chosen by the subscriber required availability, redundancy, security, and service options. Subscribers can pay for a single virtual AWS computer, a dedicated physical computer, or clusters of either. Amazon provides select portions of security for subscribers (e.g. physical security of the data centers) while other aspects of security are the responsibility of the subscriber (e.g. account management, vulnerability scanning, patching). AWS operates from many global geographical regions including seven in North America.Amazon markets AWS to subscribers as a way of obtaining large-scale computing capacity more quickly and cheaply than building an actual physical server farm. All services are billed based on usage, but each service measures usage in varying ways. As of 2021 Q4, AWS has 33% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 21%, and 10% respectively, according to Synergy Group.\n\nServices\nAs of 2021, AWS comprises over 200 products and services including computing, storage, networking, database, analytics, application services, deployment, management, machine learning, mobile, developer tools, RobOps and tools for the Internet of Things. The most popular include Amazon Elastic Compute Cloud (EC2), Amazon Simple Storage Service (Amazon S3), Amazon Connect, and AWS Lambda (a serverless function that can perform arbitrary code written in any language that can be configured to be triggered by hundreds of events, including http calls).Services expose functionality through APIs for clients to use in their applications.  These APIs are accessed over HTTP, using the REST architectural style and SOAP protocol for older APIs and exclusively JSON for newer ones. Clients can interact with these APIs in various ways, including from the AWS console (a website), by using SDKs written in various languages (such as Python, Java, and JavaScript), or by making direct REST calls.\n\nHistory\nFounding (2000–2005)\nThe genesis of AWS came in the early 2000s. After building Merchant.com, Amazon's e-commerce-as-a-service platform that offers third-party retailers a way to build their own web-stores, Amazon pursued service-oriented architecture as a means to scale its engineering operations, led by then CTO Allan Vermeulen.Around the same time frame, Amazon was frustrated with the speed of its software engineering, and sought to implement various recommendations put forth by Matt Round, an engineering leader at the time, including maximization of autonomy for engineering teams, adoption of REST, standardization of infrastructure, removal of gate-keeping decision-makers (bureaucracy), and continuous deployment. He also called for increasing the percentage of the time engineers spent building the software rather than doing other tasks. Amazon created \"a shared IT platform\" so its engineering organizations, which were spending 70% of their time on \"undifferentiated heavy-lifting\" such as IT and infrastructure problems, could focus on customer-facing innovation instead. Besides, in dealing with unusual peak traffic patterns, especially during the holiday season, by migrating services to commodity Linux hardware and relying on open source software, Amazon's Infrastructure team, led by Tom Killalea, Amazon's first CISO, had already run its data centers and associated services in a \"fast, reliable, cheap\" way.In July 2002, Amazon.com Web Services, managed by Colin Bryar, launched its first web services, opening up the Amazon.com platform to all developers. Over one hundred applications were built on top of it by 2004.  This unexpected developer interest took Amazon by surprise and convinced them that developers were \"hungry for more.\"By the Summer of 2003, Andy Jassy had taken over Bryar's portfolio at Rick Dalzell's behest, after Vermeulen, who was Bezos' first pick, declined the offer. Jassy subsequently mapped out the vision for an \"Internet OS\" made up of foundational infrastructure primitives that alleviated key impediments to shipping software applications faster. By fall 2003, databases, storage, and compute were identified as the first set of infrastructure pieces that Amazon should launch.Jeff Barr, an early AWS employee, credits Vermeulen, Jassy, Bezos, himself, and a few others for coming up with the idea that would evolve into EC2, S3, and RDS; Jassy recalls the idea was the result of brainstorming for about a week with \"ten of the best technology minds and ten of the best product management minds\" on about ten different Internet applications and the most primitive building blocks required to build them. Werner Vogels cites Amazon's desire to make the process of \"invent, launch, reinvent, relaunch, start over, rinse, repeat\" as fast as it could be leading them to break down organizational structures with \"two-pizza teams\" and application structures with distributed systems; and that these changes ultimately paved way for the formation of AWS and its mission \"to expose all of the atomic-level pieces of the Amazon.com platform\". According to Brewster Kahle, co-founder of Alexa Internet, which was acquired by Amazon in 1999, his start-up's compute infrastructure helped Amazon solve its big data problems and later informed the innovations that underpinned AWS.Jassy assembled a founding team of 57 employees from a mix of engineering and business backgrounds to kick-start these initiatives, with a majority of the hires coming from outside the company; Jeff Lawson, Twilio CEO, Adam Selipsky, Tableau CEO, Mikhail Seregine, co-founder at Outschool among them.\nIn late 2003, the concept for compute, which would later launch as EC2, was reformulated when Chris Pinkham and Benjamin Black presented a paper internally describing a vision for Amazon's retail computing infrastructure that was completely standardized, completely automated, and would rely extensively on web services for services such as storage and would draw on internal work already underway. Near the end of their paper, they mentioned the possibility of selling access to virtual servers as a service, proposing the company could generate revenue from the new infrastructure investment. Thereafter Pinkham, Willem van Biljon and lead developer Christopher Brown developed the Amazon EC2 service, with a team in Cape Town, South Africa.In November 2004, AWS launched its first infrastructure service for public usage: Simple Queue Service (SQS).\n\nS3, EC2, and other first generation services (2006–2010)\nOn March 14, 2006, AWS launched Amazon S3 cloud storage followed by EC2 in August 2006. Andy Jassy, AWS founder and vice president in 2006, said at the time that Amazon S3 \"helps free developers from worrying about where they are going to store data, whether it will be safe and secure, if it will be available when they need it, the costs associated with server maintenance, or whether they have enough storage available. Amazon S3 enables developers to focus on innovating with data, rather than figuring out how to store it.\" Pi Corporation, a startup Paul Maritz co-founded, was the first beta-user of EC2 outside of Amazon, whilst Microsoft was among EC2's first enterprise customers. Later that year, SmugMug, one of the early AWS adopters, attributed savings of around US$400,000 in storage costs to S3. According to Vogels, S3 was built with 8 microservices when it launched in 2006, but had over 300 microservices by 2022.In September 2007, AWS announced its annual Start-up Challenge, a contest with prizes worth $100,000 for entrepreneurs and software developers based in the US using AWS services such as S3 and EC2 to build their businesses. The first edition saw participation from Justin.tv, which Amazon would later acquire in 2014. Ooyala, an online media company, was the eventual winner.Additional AWS services from this period include SimpleDB, Mechanical Turk, Elastic Block Store, Elastic Beanstalk, Relational Database Service, DynamoDB, CloudWatch, Simple Workflow, CloudFront, and Availability Zones.\n\nGrowth (2010–2015)\nIn November 2010, it was reported that all of Amazon.com's retail sites had migrated to AWS. Prior to 2012, AWS was considered a part of Amazon.com and so its revenue was not delineated in Amazon financial statements. In that year industry watchers for the first time estimated AWS revenue to be over $1.5 billion.On November 27, 2012, AWS hosted its first major annual conference, re:Invent with a focus on AWS' partners and ecosystem, with over 150 sessions. The three-day event was held in Las Vegas because of its relatively cheaper connectivity with locations across the United States and the rest of the world. Andy Jassy and Werner Vogels presented keynotes, with Jeff Bezos joining Vogels for a fireside chat. AWS opened early registrations at  US$1099 per head for their customers from over 190 countries. On stage with Andy Jassy at the event which saw around 6000 attendees, Reed Hastings, CEO at Netflix, announced plans to migrate 100% of Netflix's infrastructure to AWS.To support industry-wide training and skills standardization, AWS began offering a certification program for computer engineers, on April 30, 2013, to highlight expertise in cloud computing. Later that year, in October, AWS launched Activate, a program for start-ups worldwide to leverage AWS credits, third-party integrations, and free access to AWS experts to help build their business.In 2014, AWS launched its partner network, AWS Partner Network (APN), which is focused on helping AWS-based companies grow and scale the success of their business with close collaboration and best practices.In January 2015, Amazon Web Services acquired Annapurna Labs, an Israel-based microelectronics company for a reported US$350–370M.In April 2015, Amazon.com reported AWS was profitable, with sales of $1.57 billion in the first quarter of the year and $265 million of operating income. Founder Jeff Bezos described it as a fast-growing $5 billion business; analysts described it as \"surprisingly more profitable than forecast\". In October, Amazon.com said in its Q3 earnings report that AWS's operating income was $521 million, with operating margins at 25 percent. AWS's 2015 Q3 revenue was $2.1 billion, a 78% increase from 2014's Q3 revenue of $1.17 billion. 2015 Q4 revenue for the AWS segment increased 69.5% y/y to $2.4 billion with a 28.5% operating margin, giving AWS a $9.6 billion run rate. In 2015, Gartner estimated that AWS customers are deploying 10x more infrastructure on AWS than the combined adoption of the next 14 providers.\n\nMarket leadership (2016–present)\nJames Hamilton, who leads AWS' compute, data center, and network design, wrote a retrospective article in 2016 to highlight the ten-year history of the online service from 2006 to 2016. As an early fan and outspoken proponent of the technology, he joined the AWS engineering team in 2008.In 2016 Q1, revenue was $2.57 billion with net income of $604 million, a 64% increase over 2015 Q1 that resulted in AWS being more profitable than Amazon's North American retail business for the first time. Jassy was thereafter promoted to CEO of the division. Around the same time, Amazon experienced a 42% rise in stock value as a result of increased earnings, of which AWS contributed 56% to corporate profits.AWS had $17.46 billion in annual revenue in 2017. By the end of 2020, the number had grown to $46 billion. Reflecting the success of AWS, Jassy's annual compensation in 2017 hit nearly $36 million.In January 2018, Amazon launched an autoscaling service on AWS.In November 2018, AWS announced customized ARM cores for use in its servers. Also in November 2018, AWS is developing ground stations to communicate with customer's satellites.In 2019, AWS reported 37% yearly growth and accounted for 12% of Amazon's revenue (up from 11% in 2018).In April 2021, AWS reported 32% yearly growth and accounted for 32% of $41.8 billion cloud market in Q1 2021.In January 2022, AWS joined the MACH Alliance as an Enabler member.In June 2022, it was reported that in 2019 Capital One had not secured their AWS resources properly, and was subject to a data breach by a former AWS employee, Paige Thompson (It is important to note this was not an issue with AWS security, but rather how Capital One had used AWS). The employee was convicted of hacking into the company's cloud servers to steal customer data and use computer power to mine cryptocurrency. The ex-employee was able to download the personal information of more than 100 million Capital One customers.In June 2022, AWS announced they had launched the AWS Snowcone, a small, rugged, secure  edge computing device, to the International Space Station on the Axiom Mission 1. The device had to be run through rigorous testing including vacuum, vibration, acoustic,  and thermal. The device did not need radiation hardening because it was being used in the shielded ISS environment.In June 2022, Amazon announced and displayed a preview of their latest AI tools for programmers called CodeWhisperer. While currently only available to people who received an invitation through the AWS IDE Toolkit, CodeWhisperer is an IDE plugin that will examine the user's code and comments and present users with syntactically correct suggestions. The Suggestions will be based on coding style and variable names and are not just snippets. Set to be free for the review period, the company will be charging for the product on a subscription bases upon full release.\n\nCustomer base\nOn March 14, 2006, Amazon said in a press release: \"More than 150,000 developers have signed up to use Amazon Web Services since its inception.\"\nIn November 2012, AWS hosted its first customer event in Las Vegas.\nOn May 13, 2013, AWS was awarded an Agency Authority to Operate (ATO) from the U.S. Department of Health and Human Services under the Federal Risk and Authorization Management Program.\nIn October 2013, it was revealed that AWS was awarded a $600M contract with the CIA.\nIn August 2014, AWS received Department of Defense-Wide provisional authorization for all U.S. Regions.\nDuring the 2015 re:Invent keynote, AWS disclosed that they have more than a million active customers every month in 190 countries, including nearly 2,000 government agencies, 5,000 education institutions and more than 17,500 nonprofits.\nOn April 5, 2017, AWS and DXC Technology (formed from a merger of CSC and HPE's Enterprise Services Business) announced an expanded alliance to increase access of AWS features for enterprise clients in existing data centers.Notable customers include NASA, the Obama presidential campaign of 2012, and Netflix.In 2019, it was reported that more than 80% of Germany's listed DAX companies use AWS.In August 2019, the U.S. Navy said it moved 72,000 users from six commands to an AWS cloud system as a first step toward pushing all of its data and analytics onto the cloud.In 2021, DISH Network announced it will develop and launch its 5G network on AWS.In October 2021, it was reported that spy agencies and government departments in the UK such as GCHQ, MI5, MI6, and the Ministry of Defence, have contracted AWS to host their classified materials.Multiple financial services firms have shifted to AWS in some form in recent years.\n\nSignificant service outages\nOn April 20, 2011, AWS suffered a major outage. Parts of the Elastic Block Store service became \"stuck\" and could not fulfill read/write requests. It took at least two days for the service to be fully restored.\nOn June 29, 2012, several websites that rely on Amazon Web Services were taken offline due to a severe storm in Northern Virginia, where AWS' largest data center cluster is located.\nOn October 22, 2012, a major outage occurred, affecting many sites such as Reddit, Foursquare, Pinterest, and others. The cause was a memory leak bug in an operational data collection agent.\nOn December 24, 2012, AWS suffered another outage causing websites such as Netflix to be unavailable for customers in the Northeastern United States. AWS cited their Elastic Load Balancing service as the cause.\nOn February 28, 2017, AWS experienced a massive outage of S3 services in its Northern Virginia region. A majority of websites that relied on AWS S3 either hung or stalled, and Amazon reported within five hours that AWS was fully online again. No data has been reported to have been lost due to the outage. The outage was caused by a human error made while debugging, that resulted in removing more server capacity than intended, which caused a domino effect of outages.\nOn November 25, 2020, AWS experienced several hours of outage on the Kinesis service in North Virginia (us-east-1) region. Other services relying on Kinesis were also impacted.\nOn December 7, 2021, an outage mainly affected the Eastern United States, disrupting delivery service, streaming.\n\nAvailability and topology\nAs of January 2023, AWS has distinct operations in 31 geographical \"regions\": 7 in North America, 1 in South America, 8 in Europe, 2 in the Middle East, 1 in Africa and 12 in Asia Pacific.\nEach region is wholly contained within a single country and all of its data and services stay within the designated region. Each region has multiple \"Availability Zones\", which consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. Availability Zones do not automatically provide additional scalability or redundancy within a region, since they are intentionally isolated from each other to prevent outages from spreading between Zones. Several services can operate across Availability Zones (e.g., S3, DynamoDB) while others can be configured to replicate across Zones to spread demand and avoid downtime from failures.\nAs of December 2014, Amazon Web Services operated an estimated 1.4 million servers across 11 regions and 28 availability zones. The global network of AWS Edge locations consists of over 300 points of presence worldwide, including locations in North America, Europe, Asia, Australia, Africa, and South America.As of January 2023, AWS has announced the planned launch of four additional regions in Canada (Calgary), Israel, New Zealand, and Thailand. In mid March 2023, Amazon Web Services signed a cooperation agreement with the New Zealand Government to build large data centers in New Zealand.In 2014, AWS claimed its aim was to achieve 100% renewable energy usage in the future.  In the United States, AWS's partnerships with renewable energy providers include Community Energy of Virginia, to support the US East region; Pattern Development, in January 2015, to construct and operate Amazon Wind Farm Fowler Ridge; Iberdrola Renewables, LLC, in July 2015, to construct and operate Amazon Wind Farm US East; EDP Renewables North America, in November 2015, to construct and operate Amazon Wind Farm US Central; and  Tesla Motors, to apply battery storage technology to address power needs in the US West (Northern California) region.\n\nPop-up lofts\nAWS also has \"pop-up lofts\" in different locations around the world. These market AWS to entrepreneurs and startups in different tech industries in a physical location. Visitors can work or relax inside the loft, or learn more about what they can do with AWS. In June 2014, AWS opened their first temporary pop-up loft in San Francisco. In May 2015 they expanded to New York City, and in September 2015 expanded to Berlin. AWS opened its fourth location, in Tel Aviv from March 1, 2016, to March 22, 2016. A pop-up loft was open in London from September 10 to October 29, 2015. The pop-up lofts in New York and San Francisco are indefinitely closed due to the COVID-19 pandemic while Tokyo has remained open in a limited capacity.\n\nCharitable work\nIn 2017, AWS launched AWS re/Start in the United Kingdom to help young adults and military veterans retrain in technology-related skills.  In partnership with the Prince's Trust and the Ministry of Defence (MoD), AWS will help to provide re-training opportunities for young people from disadvantaged backgrounds and former military personnel.  AWS is working alongside a number of partner companies including Cloudreach, Sage Group, EDF Energy, and Tesco Bank.In April 2022, AWS announced the organization has committed more than $30 million over three years to early-stage start-ups led by Black, Latino, LGBTQIA+, and Women founders as part of its AWS impact Accelerator. The Initiative offers qualifying start-ups up to $225,000 in cash, credits, extensive training, mentoring, technical guidance and includes up to $100,000 in AWS service credits.\n\nEnvironmental impact\nIn 2016, Greenpeace assessed major tech companies—including cloud services providers like AWS, Microsoft, Oracle, Google, IBM, Salesforce and Rackspace—based on their level of \"clean energy\" usage. Greenpeace evaluated companies on their mix of renewable-energy sources; transparency; renewable-energy commitment and policies; energy efficiency and greenhouse-gas mitigation; renewable-energy procurement; and advocacy. The group gave AWS an overall \"C\" grade. Greenpeace credited AWS for its advances toward greener computing in recent years, its plans to launch multiple wind and solar farms across the United States. The organization stated that Amazon is opaque about its carbon footprint.In January 2021, AWS joined an industry pledge to achieve climate neutrality of data centers by 2030, the Climate Neutral Data Centre Pact.\n\nDenaturalization protest\nUS Department of Homeland Security has employed the software ATLAS, which runs on Amazon Cloud. It scanned more than 16.5 million of records of naturalized Americans and flagged approximately 124,000 of them for manual analysis and review by USCIS officers regarding denaturalization. Some of the scanned data came from Terrorist Screening Database and National Crime Information Center. The algorithm and the criteria for the algorithm were secret. Amazon faced protests from its own employees and activists for the anti-migrant collaboration with authorities.\n\nSee also\nCloud-computing comparison\nComparison of file hosting services\nTim Bray\nJames Gosling\n\nNotes\nReferences\nExternal links\n\nOfficial website",
    "Analysis of algorithms": "In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\nThe term \"analysis of algorithms\" was coined by Donald Knuth. Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.\nIn theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. For instance, binary search is said to run in a number of steps proportional to the logarithm of the size n of the sorted list being searched, or in O(log n), colloquially \"in logarithmic time\". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two \"reasonable\" implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant.\nExact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called model of computation. A model of computation may be defined in terms of an abstract computer, e.g. Turing machine, and/or by postulating that certain operations are executed in unit time.\nFor example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2(n) + 1 time units are needed to return an answer.\n\nCost models\nTime efficiency estimates depend on what we define to be a step. For the analysis to correspond usefully to the actual run-time, the time required to perform a step must be guaranteed to be bounded above by a constant. One must be careful here; for instance, some analyses count an addition of two numbers as one step. This assumption may not be warranted in certain contexts. For example, if the numbers involved in a computation may be arbitrarily large, the time required by a single addition can no longer be assumed to be constant.\nTwo cost models are generally used:\nthe uniform cost model, also called uniform-cost measurement (and similar variations), assigns a constant cost to every machine operation, regardless of the size of the numbers involved\nthe logarithmic cost model, also called logarithmic-cost measurement (and similar variations), assigns a cost to every machine operation proportional to the number of bits involvedThe latter is more cumbersome to use, so it's only employed when necessary, for example in the analysis of arbitrary-precision arithmetic algorithms, like those used in cryptography.\nA key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that you could use in practice and therefore there are algorithms that are faster than what would naively be thought possible.\n\nRun-time analysis\nRun-time analysis is a theoretical classification that estimates and anticipates the increase in running time (or run-time or execution time) of an algorithm as its input size (usually denoted as n) increases.  Run-time efficiency is a topic of great interest in computer science:  A program can take seconds, hours, or even years to finish executing, depending on which algorithm it implements. While software profiling techniques can be used to measure an algorithm's run-time in practice, they cannot provide timing data for all infinitely many possible inputs; the latter can only be achieved by the theoretical methods of run-time analysis.\n\nShortcomings of empirical metrics\nSince algorithms are platform-independent (i.e. a given algorithm can be implemented in an arbitrary programming language on an arbitrary computer running an arbitrary operating system), there are additional significant drawbacks to using an empirical approach to gauge the comparative performance of a given set of algorithms.\nTake as an example a program that looks up a specific entry in a sorted list of size n.  Suppose this program were implemented on Computer A, a state-of-the-art machine, using a linear search algorithm, and on Computer B, a much slower machine, using a binary search algorithm.  Benchmark testing on the two computers running their respective programs might look something like the following:\n\nBased on these metrics, it would be easy to jump to the conclusion that Computer A is running an algorithm that is far superior in efficiency to that of Computer B.  However, if the size of the input-list is increased to a sufficient number, that conclusion is dramatically demonstrated to be in error:\n\nComputer A, running the linear search program, exhibits a linear growth rate.  The program's run-time is directly proportional to its input size.  Doubling the input size doubles the run-time, quadrupling the input size quadruples the run-time, and so forth.  On the other hand, Computer B, running the binary search program, exhibits a logarithmic growth rate.  Quadrupling the input size only increases the run-time by a constant amount (in this example, 50,000 ns).  Even though Computer A is ostensibly a faster machine, Computer B will inevitably surpass Computer A in run-time because it's running an algorithm with a much slower growth rate.\n\nOrders of growth\nInformally, an algorithm can be said to exhibit a growth rate on the order of a mathematical function if beyond a certain input size n, the function f(n) times a positive constant provides an upper bound or limit for the run-time of that algorithm.  In other words, for a given input size n greater than some n0 and a constant c, the run-time of that algorithm will never be larger than c × f(n).  This concept is frequently expressed using Big O notation.  For example, since the run-time of insertion sort grows quadratically as its input size increases, insertion sort can be said to be of order O(n2).\nBig O notation is a convenient way to express the worst-case scenario for a given algorithm, although it can also be used to express the average-case — for example, the worst-case scenario for quicksort is O(n2), but the average-case run-time is O(n log n).\n\nEmpirical orders of growth\nAssuming the run-time follows power rule, t ≈ kna, the coefficient a can be found  by taking empirical measurements of run-time {t1, t2} at some problem-size points {n1, n2}, and calculating t2/t1 = (n2/n1)a so that a = log(t2/t1)/log(n2/n1). In other words, this measures the slope of the empirical line on the log–log plot of run-time vs. input size, at some size point. If the order of growth indeed follows the power rule (and so the line on the log–log plot is indeed a straight line), the empirical value of  will  stay constant at different ranges, and if not, it will change (and the line is a curved line)—but still could serve for comparison of any two given algorithms as to their empirical local orders of growth behaviour. Applied to the above table:\n\nIt is clearly seen that the first algorithm exhibits a linear order of growth indeed following the power rule. The empirical values for the second one are diminishing rapidly, suggesting it follows another rule of growth and in any case has much lower local orders of growth (and improving further still), empirically, than the first one.\n\nEvaluating run-time complexity\nThe run-time complexity for the worst-case scenario of a given algorithm can sometimes be evaluated by examining the structure of the algorithm and making some simplifying assumptions.  Consider the following pseudocode:\n\n1    get a positive integer n from input\n2    if n > 10\n3        print \"This might take a while...\"\n4    for i = 1 to n\n5        for j = 1 to i\n6            print i * j\n7    print \"Done!\"\n\nA given computer will take a discrete amount of time to execute each of the instructions involved with carrying out this algorithm.  Say that the actions carried out in step 1 are considered to consume time at most T1, step 2 uses time at most T2, and so forth.\nIn the algorithm above, steps 1, 2 and 7 will only be run once.  For a worst-case evaluation, it should be assumed that step 3 will be run as well.  Thus the total amount of time to run steps 1-3 and step 7 is:\n\n  \n    \n      \n        T\n        \n          1\n        \n      \n      +\n      \n        T\n        \n          2\n        \n      \n      +\n      \n        T\n        \n          3\n        \n      \n      +\n      \n        T\n        \n          7\n        \n      \n      .\n      \n    \n    T_{1}+T_{2}+T_{3}+T_{7}.\\,\n  The loops in steps 4, 5 and 6 are trickier to evaluate.  The outer loop test in step 4 will execute ( n + 1 )\ntimes, which will consume T4( n + 1 ) time.  The inner loop, on the other hand, is governed by the value of j, which iterates from 1 to i.  On the first pass through the outer loop, j iterates from 1 to 1:  The inner loop makes one pass, so running the inner loop body (step 6) consumes T6 time, and the inner loop test (step 5) consumes 2T5 time.  During the next pass through the outer loop, j iterates from 1 to 2:  the inner loop makes two passes, so running the inner loop body (step 6) consumes 2T6 time, and the inner loop test (step 5) consumes 3T5 time.\nAltogether, the total time required to run the inner loop body can be expressed as an arithmetic progression:\n\n  \n    \n      \n        T\n        \n          6\n        \n      \n      +\n      2\n      \n        T\n        \n          6\n        \n      \n      +\n      3\n      \n        T\n        \n          6\n        \n      \n      +\n      ⋯\n      +\n      (\n      n\n      −\n      1\n      )\n      \n        T\n        \n          6\n        \n      \n      +\n      n\n      \n        T\n        \n          6\n        \n      \n    \n    T_{6}+2T_{6}+3T_{6}+\\cdots +(n-1)T_{6}+nT_{6}\n  which can be factored as\n\n  \n    \n      \n        \n          [\n          \n            1\n            +\n            2\n            +\n            3\n            +\n            ⋯\n            +\n            (\n            n\n            −\n            1\n            )\n            +\n            n\n          \n          ]\n        \n        \n          T\n          \n            6\n          \n        \n        =\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            6\n          \n        \n      \n    \n    {\\displaystyle \\left[1+2+3+\\cdots +(n-1)+n\\right]T_{6}=\\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{6}}\n  The total time required to run the inner loop test can be evaluated similarly:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                2\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                3\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                4\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                ⋯\n                +\n                (\n                n\n                −\n                1\n                )\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                n\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                (\n                n\n                +\n                1\n                )\n                \n                  T\n                  \n                    5\n                  \n                \n              \n            \n            \n              \n                =\n                 \n              \n              \n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                2\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                3\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                4\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                ⋯\n                +\n                (\n                n\n                −\n                1\n                )\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                n\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                (\n                n\n                +\n                1\n                )\n                \n                  T\n                  \n                    5\n                  \n                \n                −\n                \n                  T\n                  \n                    5\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&2T_{5}+3T_{5}+4T_{5}+\\cdots +(n-1)T_{5}+nT_{5}+(n+1)T_{5}\\\\=\\ &T_{5}+2T_{5}+3T_{5}+4T_{5}+\\cdots +(n-1)T_{5}+nT_{5}+(n+1)T_{5}-T_{5}\\end{aligned}}}\n  which can be factored as\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  T\n                  \n                    5\n                  \n                \n                \n                  [\n                  \n                    1\n                    +\n                    2\n                    +\n                    3\n                    +\n                    ⋯\n                    +\n                    (\n                    n\n                    −\n                    1\n                    )\n                    +\n                    n\n                    +\n                    (\n                    n\n                    +\n                    1\n                    )\n                  \n                  ]\n                \n                −\n                \n                  T\n                  \n                    5\n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  [\n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                    (\n                    \n                      n\n                      \n                        2\n                      \n                    \n                    +\n                    n\n                    )\n                  \n                  ]\n                \n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                (\n                n\n                +\n                1\n                )\n                \n                  T\n                  \n                    5\n                  \n                \n                −\n                \n                  T\n                  \n                    5\n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  [\n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                    (\n                    \n                      n\n                      \n                        2\n                      \n                    \n                    +\n                    n\n                    )\n                  \n                  ]\n                \n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                n\n                \n                  T\n                  \n                    5\n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  [\n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                    (\n                    \n                      n\n                      \n                        2\n                      \n                    \n                    +\n                    3\n                    n\n                    )\n                  \n                  ]\n                \n                \n                  T\n                  \n                    5\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&T_{5}\\left[1+2+3+\\cdots +(n-1)+n+(n+1)\\right]-T_{5}\\\\=&\\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{5}+(n+1)T_{5}-T_{5}\\\\=&\\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{5}+nT_{5}\\\\=&\\left[{\\frac {1}{2}}(n^{2}+3n)\\right]T_{5}\\end{aligned}}}\n  Therefore, the total run-time for this algorithm is:\n\n  \n    \n      \n        f\n        (\n        n\n        )\n        =\n        \n          T\n          \n            1\n          \n        \n        +\n        \n          T\n          \n            2\n          \n        \n        +\n        \n          T\n          \n            3\n          \n        \n        +\n        \n          T\n          \n            7\n          \n        \n        +\n        (\n        n\n        +\n        1\n        )\n        \n          T\n          \n            4\n          \n        \n        +\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            6\n          \n        \n        +\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            3\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            5\n          \n        \n      \n    \n    {\\displaystyle f(n)=T_{1}+T_{2}+T_{3}+T_{7}+(n+1)T_{4}+\\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{6}+\\left[{\\frac {1}{2}}(n^{2}+3n)\\right]T_{5}}\n  which reduces to\n\n  \n    \n      \n        f\n        (\n        n\n        )\n        =\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            6\n          \n        \n        +\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            3\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            5\n          \n        \n        +\n        (\n        n\n        +\n        1\n        )\n        \n          T\n          \n            4\n          \n        \n        +\n        \n          T\n          \n            1\n          \n        \n        +\n        \n          T\n          \n            2\n          \n        \n        +\n        \n          T\n          \n            3\n          \n        \n        +\n        \n          T\n          \n            7\n          \n        \n      \n    \n    {\\displaystyle f(n)=\\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{6}+\\left[{\\frac {1}{2}}(n^{2}+3n)\\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}}\n  As a rule-of-thumb, one can assume that the highest-order term in any given function dominates its rate of growth and thus defines its run-time order.  In this example, n2 is the highest-order term, so one can conclude that f(n) = O(n2).  Formally this can be proven as follows:\n\nProve that \n  \n    \n      \n        [\n        \n          \n            \n              1\n              2\n            \n          \n          (\n          \n            n\n            \n              2\n            \n          \n          +\n          n\n          )\n        \n        ]\n      \n      \n        T\n        \n          6\n        \n      \n      +\n      \n        [\n        \n          \n            \n              1\n              2\n            \n          \n          (\n          \n            n\n            \n              2\n            \n          \n          +\n          3\n          n\n          )\n        \n        ]\n      \n      \n        T\n        \n          5\n        \n      \n      +\n      (\n      n\n      +\n      1\n      )\n      \n        T\n        \n          4\n        \n      \n      +\n      \n        T\n        \n          1\n        \n      \n      +\n      \n        T\n        \n          2\n        \n      \n      +\n      \n        T\n        \n          3\n        \n      \n      +\n      \n        T\n        \n          7\n        \n      \n      ≤\n      c\n      \n        n\n        \n          2\n        \n      \n      ,\n       \n      n\n      ≥\n      \n        n\n        \n          0\n        \n      \n    \n    \\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{6}+\\left[{\\frac {1}{2}}(n^{2}+3n)\\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\\leq cn^{2},\\ n\\geq n_{0}\n  \n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  [\n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                    (\n                    \n                      n\n                      \n                        2\n                      \n                    \n                    +\n                    n\n                    )\n                  \n                  ]\n                \n                \n                  T\n                  \n                    6\n                  \n                \n                +\n                \n                  [\n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                    (\n                    \n                      n\n                      \n                        2\n                      \n                    \n                    +\n                    3\n                    n\n                    )\n                  \n                  ]\n                \n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                (\n                n\n                +\n                1\n                )\n                \n                  T\n                  \n                    4\n                  \n                \n                +\n                \n                  T\n                  \n                    1\n                  \n                \n                +\n                \n                  T\n                  \n                    2\n                  \n                \n                +\n                \n                  T\n                  \n                    3\n                  \n                \n                +\n                \n                  T\n                  \n                    7\n                  \n                \n              \n            \n            \n              \n                ≤\n              \n              \n                \n                (\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                n\n                )\n                \n                  T\n                  \n                    6\n                  \n                \n                +\n                (\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                3\n                n\n                )\n                \n                  T\n                  \n                    5\n                  \n                \n                +\n                (\n                n\n                +\n                1\n                )\n                \n                  T\n                  \n                    4\n                  \n                \n                +\n                \n                  T\n                  \n                    1\n                  \n                \n                +\n                \n                  T\n                  \n                    2\n                  \n                \n                +\n                \n                  T\n                  \n                    3\n                  \n                \n                +\n                \n                  T\n                  \n                    7\n                  \n                \n                 \n                (\n                \n                  for \n                \n                n\n                ≥\n                0\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{6}+\\left[{\\frac {1}{2}}(n^{2}+3n)\\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\\\\\\leq &(n^{2}+n)T_{6}+(n^{2}+3n)T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\\ ({\\text{for }}n\\geq 0)\\end{aligned}}}\n  \n\nLet k be a constant greater than or equal to [T1..T7]\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  T\n                  \n                    6\n                  \n                \n                (\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                n\n                )\n                +\n                \n                  T\n                  \n                    5\n                  \n                \n                (\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                3\n                n\n                )\n                +\n                (\n                n\n                +\n                1\n                )\n                \n                  T\n                  \n                    4\n                  \n                \n                +\n                \n                  T\n                  \n                    1\n                  \n                \n                +\n                \n                  T\n                  \n                    2\n                  \n                \n                +\n                \n                  T\n                  \n                    3\n                  \n                \n                +\n                \n                  T\n                  \n                    7\n                  \n                \n                ≤\n                k\n                (\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                n\n                )\n                +\n                k\n                (\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                3\n                n\n                )\n                +\n                k\n                n\n                +\n                5\n                k\n              \n            \n            \n              \n                =\n              \n              \n                2\n                k\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                5\n                k\n                n\n                +\n                5\n                k\n                ≤\n                2\n                k\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                5\n                k\n                \n                  n\n                  \n                    2\n                  \n                \n                +\n                5\n                k\n                \n                  n\n                  \n                    2\n                  \n                \n                 \n                (\n                \n                  for \n                \n                n\n                ≥\n                1\n                )\n                =\n                12\n                k\n                \n                  n\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&T_{6}(n^{2}+n)+T_{5}(n^{2}+3n)+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\\leq k(n^{2}+n)+k(n^{2}+3n)+kn+5k\\\\=&2kn^{2}+5kn+5k\\leq 2kn^{2}+5kn^{2}+5kn^{2}\\ ({\\text{for }}n\\geq 1)=12kn^{2}\\end{aligned}}}\n  \n\nTherefore \n  \n    \n      \n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            6\n          \n        \n        +\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              n\n              \n                2\n              \n            \n            +\n            3\n            n\n            )\n          \n          ]\n        \n        \n          T\n          \n            5\n          \n        \n        +\n        (\n        n\n        +\n        1\n        )\n        \n          T\n          \n            4\n          \n        \n        +\n        \n          T\n          \n            1\n          \n        \n        +\n        \n          T\n          \n            2\n          \n        \n        +\n        \n          T\n          \n            3\n          \n        \n        +\n        \n          T\n          \n            7\n          \n        \n        ≤\n        c\n        \n          n\n          \n            2\n          \n        \n        ,\n        n\n        ≥\n        \n          n\n          \n            0\n          \n        \n        \n           for \n        \n        c\n        =\n        12\n        k\n        ,\n        \n          n\n          \n            0\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\left[{\\frac {1}{2}}(n^{2}+n)\\right]T_{6}+\\left[{\\frac {1}{2}}(n^{2}+3n)\\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\\leq cn^{2},n\\geq n_{0}{\\text{ for }}c=12k,n_{0}=1}\n  \n\nA more elegant approach to analyzing this algorithm would be to declare that [T1..T7] are all equal to one unit of time, in a system of units chosen so that one unit is greater than or equal to the actual times for these steps.  This would mean that the algorithm's run-time breaks down as follows:\n\n  \n    \n      \n        4\n        +\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        i\n        ≤\n        4\n        +\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        n\n        =\n        4\n        +\n        \n          n\n          \n            2\n          \n        \n        ≤\n        5\n        \n          n\n          \n            2\n          \n        \n         \n        (\n        \n          for \n        \n        n\n        ≥\n        1\n        )\n        =\n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle 4+\\sum _{i=1}^{n}i\\leq 4+\\sum _{i=1}^{n}n=4+n^{2}\\leq 5n^{2}\\ ({\\text{for }}n\\geq 1)=O(n^{2}).}\n\nGrowth rate analysis of other resources\nThe methodology of run-time analysis can also be utilized for predicting other growth rates, such as consumption of memory space.  As an example, consider the following pseudocode which manages and reallocates memory usage by a program based on the size of a file which that program manages:\n\nwhile file is still open:\n    let n = size of file\n    for every 100,000 kilobytes of increase in file size\n        double the amount of memory reserved\n\nIn this instance, as the file size n increases, memory will be consumed at an exponential growth rate, which is order O(2n). This is an extremely rapid and most likely unmanageable growth rate for consumption of memory resources.\n\nRelevance\nAlgorithm analysis is important in practice because the accidental or unintentional use of an inefficient algorithm can significantly impact system performance. In time-sensitive applications, an algorithm taking too long to run can render its results outdated or useless. An inefficient algorithm can also end up requiring an uneconomical amount of computing power or storage in order to run, again rendering it practically useless.\n\nConstant factors\nAnalysis of algorithms typically focuses on the asymptotic performance, particularly at the elementary level, but in practical applications constant factors are important, and real-world data is in practice always limited in size. The limit is typically the size of addressable memory, so on 32-bit machines 232 = 4 GiB (greater if segmented memory is used) and on 64-bit machines 264 = 16 EiB. Thus given a limited size, an order of growth (time or space) can be replaced by a constant factor, and in this sense all practical algorithms are O(1) for a large enough constant, or for small enough data.\nThis interpretation is primarily useful for functions that grow extremely slowly: (binary) iterated logarithm (log*) is less than 5 for all practical data (265536 bits); (binary) log-log (log log n) is less than 6 for virtually all practical data (264 bits); and binary log (log n) is less than 64 for virtually all practical data (264 bits). An algorithm with non-constant complexity may nonetheless be more efficient than an algorithm with constant complexity on practical data if the overhead of the constant time algorithm results in a larger constant factor, e.g., one may have \n  \n    \n      K\n      >\n      k\n      log\n      ⁡\n      log\n      ⁡\n      n\n    \n    K>k\\log \\log n\n   so long as \n  \n    \n      K\n      \n        /\n      \n      k\n      >\n      6\n    \n    K/k>6\n   and \n  \n    \n      n\n      <\n      \n        2\n        \n          \n            2\n            \n              6\n            \n          \n        \n      \n      =\n      \n        2\n        \n          64\n        \n      \n    \n    n<2^{2^{6}}=2^{64}\n  .\nFor large data linear or quadratic factors cannot be ignored, but for small data an asymptotically inefficient algorithm may be more efficient. This is particularly used in hybrid algorithms, like Timsort, which use an asymptotically efficient algorithm (here merge sort, with time complexity \n  \n    \n      n\n      log\n      ⁡\n      n\n    \n    n\\log n\n  ), but switch to an asymptotically inefficient algorithm (here insertion sort, with time complexity \n  \n    \n      n\n      \n        2\n      \n    \n    n^{2}\n  ) for small data, as the simpler algorithm is faster on small data.\n\nSee also\nAmortized analysis\nAnalysis of parallel algorithms\nAsymptotic computational complexity\nBest, worst and average case\nBig O notation\nComputational complexity theory\nMaster theorem (analysis of algorithms)\nNP-Complete\nNumerical analysis\nPolynomial time\nProgram optimization\nProfiling (computer programming)\nScalability\nSmoothed analysis\nTermination analysis — the subproblem of checking whether a program will terminate at all\nTime complexity — includes table of orders of growth for common algorithms\nInformation-based complexity\n\nNotes\nReferences\nSedgewick, Robert; Flajolet, Philippe (2013). An Introduction to the Analysis of Algorithms (2nd ed.). Addison-Wesley. ISBN 978-0-321-90575-8.\nGreene, Daniel A.; Knuth, Donald E. (1982). Mathematics for the Analysis of Algorithms (Second ed.). Birkhäuser. ISBN 3-7643-3102-X.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. & Stein, Clifford (2001). Introduction to Algorithms. Chapter 1: Foundations (Second ed.). Cambridge, MA: MIT Press and McGraw-Hill. pp. 3–122. ISBN 0-262-03293-7.\nSedgewick, Robert (1998). Algorithms in C, Parts 1-4: Fundamentals, Data Structures, Sorting, Searching (3rd ed.). Reading, MA: Addison-Wesley Professional. ISBN 978-0-201-31452-6.\nKnuth, Donald. The Art of Computer Programming. Addison-Wesley.\nGoldreich, Oded (2010). Computational Complexity: A Conceptual Perspective. Cambridge University Press. ISBN 978-0-521-88473-0.\n\nExternal links\n Media related to Analysis of algorithms at Wikimedia Commons",
    "Andrew Ng": "Andrew Yan-Tak Ng (Chinese: 吳恩達; born 1976) is a British-American computer scientist and technology entrepreneur focusing on machine learning and artificial intelligence (AI). Ng was a cofounder and head of Google Brain and was the former Chief Scientist at Baidu, building the company's Artificial Intelligence Group into a team of several thousand people.Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL). Ng has also made substantial contributions to the field of online education as the cofounder of both Coursera and DeepLearning.AI. He has spearheaded many efforts to \"democratize deep learning\" teaching over 2.5 million students through his online courses. He is one of the world's most famous and influential computer scientists being named one of Time magazine's 100 Most Influential People in 2012, and Fast Company's Most Creative People in 2014. In 2018, he launched and currently heads the AI Fund, initially a $175-million investment fund for backing artificial intelligence startups. He has founded Landing AI, which provides AI-powered SaaS products.\n\nBiography\nNg was born in the United Kingdom in 1976. His parents Ronald P. Ng and Tisa Ho are both immigrants from Hong Kong. He has at least one brother. Growing up, he spent time in Hong Kong and Singapore.In 1997, he earned his undergraduate degree with a triple major in computer science, statistics, and economics from Carnegie Mellon University in Pittsburgh, Pennsylvania, graduating at the top of his class. Between 1996 and 1998 he also conducted research on reinforcement learning, model selection, and feature selection at the AT&T Bell Labs.In 1998 Ng earned his master's degree from the Massachusetts Institute of Technology (MIT) in Cambridge, Massachusetts. At MIT he built the first publicly available, automatically indexed web-search engine for research papers on the web. It was a precursor to CiteSeerX/ResearchIndex, but specialized in machine learning.In 2002, he received his Doctor of Philosophy (Ph.D.) from the University of California, Berkeley, under the supervision of Michael I. Jordan. His thesis is titled \"Shaping and policy search in reinforcement learning\" and is well-cited to this day.He started working as an assistant professor at Stanford University in 2002 and as an associate professor in 2009.He currently lives in Los Altos Hills, California. In 2014, he married Carol E. Reiley. They have two children: a daughter born in 2019 and a son born in 2021. The MIT Technology Review named Ng and Reiley an \"AI power couple\".\n\nCareer\nAcademia and teaching\nNg is a professor at Stanford University departments of Computer Science and electrical engineering. He served as the director of the Stanford Artificial Intelligence Laboratory (SAIL), where he taught students and undertook research related to data mining, big data, and machine learning. His machine learning course CS229 at Stanford is the most popular course offered on campus with over 1,000 students enrolling some years. As of 2020, three of most popular courses on Coursera are Ng's: Machine Learning (#1), AI for Everyone, (#5), Neural Networks and Deep Learning (#6).In 2008 his group at Stanford was one of the first in the US to start advocating the use of GPUs in deep learning. The rationale was that an efficient computation infrastructure could speed up statistical model training by orders of magnitude, ameliorating some of the scaling issues associated with big data. At the time it was a controversial and risky decision, but since then and following Ng's lead, GPUs have become a cornerstone in the field. Since 2017 Ng has been advocating the shift to high-performance computing (HPC) for scaling up deep learning and accelerating progress in the field.In 2012, along with Stanford computer scientist Daphne Koller he cofounded and was CEO of Coursera, a website that offers free online courses to everyone. It took off with over 100,000 students registered for Ng's popular CS229A course. Today, several million people have enrolled in Coursera courses, making the site one of the leading massive open online course (MOOCs) in the world.\n\nIndustry\nFrom 2011 to 2012, he worked at Google, where he founded and directed the Google Brain Deep Learning Project with Jeff Dean, Greg Corrado, and Rajat Monga.\nIn 2014, he joined Baidu as chief scientist, and carried out research related to big data and AI. There he set up several research teams for things like facial recognition and Melody, an AI chatbot for healthcare. He also developed for the company the AI platform called DuerOS and other technologies that positioned Baidu ahead of Google in the discourse and development of AI. In March 2017, he announced his resignation from Baidu.He soon afterward launched Deeplearning.ai, an online series of deep learning courses. Then Ng launched Landing AI, which provides AI-powered SaaS products.In January 2018, Ng unveiled the AI Fund, raising $175 million to invest in new startups. In November 2021, Landing AI secured a $57 million round of series A funding led by McRock Capital, to help manufacturers adopt computer vision.\n\nResearch\nNg researches primarily in machine learning, deep learning, machine perception, computer vision, and natural language processing; and is one of the world's most famous and influential computer scientists. He's frequently won best paper awards at academic conferences and has had a huge impact on the field of AI, computer vision, and robotics.During graduate school, together with David M. Blei and Michael I. Jordan, Ng coauthored the influential paper that introduced latent Dirichlet allocation (LDA) for his thesis on reinforcement learning for drones.His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world. He was the leading scientist and principal investigator on the STAIR (STanford Artificial Intelligence Robot) project, which resulted in Robot Operating System (ROS), a widely used open source software robotics platform. His vision to build an AI robot and put a robot in every home inspired Scott Hassan to back him and create Willow Garage. He is also one of the founding team members for the Stanford WordNet project, which uses machine learning to expand the Princeton WordNet database created by Christiane Fellbaum.In 2011, Ng founded the Google Brain project at Google, which developed large-scale artificial neural networks using Google's distributed computing infrastructure. Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, which learned to recognize cats after watching only YouTube videos, and without ever having been told what a \"cat\" is. The project's technology is also currently used in the Android operating system's speech recognition system.\n\nOnline education: massive open online course\nIn 2011, Stanford launched a total of three massive open online course (MOOCs) on machine learning (CS229a), databases, and AI, taught by Ng, Peter Norvig, Sebastian Thrun, and Jennifer Widom. This has led to the modern MOOC movement. Ng taught machine learning and Widom taught databases. The course on AI taught by Thrun led to the genesis of Udacity. Coursera was the 6th online education website that Ng built and arguably the most successful to date.But we learned and learned and learned from the early prototypes, until in 2011 we managed to build something that really took off. The seeds of massive open online courses (MOOCs) go back a few years before the founding of Coursera in 2012. Two themes emphasized in the founding of modern MOOCs were scale and availability.\n\nFounding of Coursera\nNg started the Stanford Engineering Everywhere (SEE) program, which in 2008 published a number of Stanford courses online for free. Ng taught one of these courses, \"Machine Learning\", which includes his video lectures, along with the student materials used in the Stanford CS229 class. It offered a similar experience to MIT OpenCourseWare, except it aimed at providing a more \"complete course\" experience, equipped with lectures, course materials, problems and solutions, etc. The SEE videos were viewed by the millions and inspired Ng to develop and iterate new versions of online tech.Within Stanford, they include Daphne Koller with her \"blended learning experiences\" and codesigning a peer-grading system, John Mitchell (Courseware, a Learning Management System), Dan Boneh (using machine learning to sync videos, later teaching cryptography on Coursera), Bernd Girod (ClassX), and others. Outside Stanford, Ng and Thrun credit Sal Khan of Khan Academy as a huge source of inspiration. Ng was also inspired by lynda.com and the design of the forums of Stack Overflow.Widom, Ng, and others were ardent advocates of Khan-styled tablet recordings, and between 2009 and 2011, several hundred hours of lecture videos recorded by Stanford instructors were recorded and uploaded. Ng tested some of the original designs with a local high school to figure the best practices for recording lessons.In October 2011, the \"applied\" version of the Stanford class (CS229a) was hosted on ml-class.org and launched, with over 100,000 students registered for its first edition. The course featured quizzes and graded programming assignments and became one of the first and most successful massive open online courses (MOOCs) created by a Stanford professor.Two other courses on databases (db-class.org) and AI (ai-class.org) were launched. The ml-class and db-class ran on a platform developed by students, including Frank Chen, Jiquan Ngiam, Chuan-Yu Foo, and Yifan Mai. Word spread through social media and popular press. The three courses were 10 weeks long, and over 40,000 \"Statements of Accomplishment\" were awarded.\nNg tells the following story on the early days of Coursera:In 2011, I was working with four Stanford students. We were under tremendous pressure to build new features for the 100,000+ students that were already signed up. One of the students (Frank Chen) claims another one (Jiquan Ngiam) frequently stranded him in the Stanford building and refused to give him a ride back to his dorm until very late at night so he had no choice but to stick around and keep working. I neither confirm nor deny this story. His work subsequently led to his founding of Coursera with Koller in 2012. As of 2019, the two most popular courses on the platform were taught and designed by Ng: \"Machine Learning\" (#1) and \"Neural Networks and Deep Learning\" (#2).\n\nPost-Coursera work\nIn 2019, Ng launched a new course \"AI for Everyone\". This is a non-technical course designed to help people understand AI's impact on society and its benefits and costs for companies, as well as how they can navigate through this technological revolution.\n\nVenture capital\nNg is the chair of the board for Woebot Labs, a psychological clinic that uses data science to provide cognitive behavioral therapy. It provides a therapy chatbot to help treat depression, among other things.He is also a member of the board of directors for drive.ai, which uses AI for self-driving cars and was acquired by Apple in 2019.Through Landing AI, he also focuses on democratizing AI technology and lowering the barrier for entrance to businesses and developers.\n\nPublications and awards\nNg is also the author or co-author of over 300 publications in robotics, and related fields. His work in computer vision and deep learning has been featured often in press releases and reviews.\n1995. Bell Atlantic Network Services Scholarship\n1995, 1996. Microsoft Technical Scholarship Award\n1996. Andrew Carnegie Society Scholarship\n1998–2000: Berkeley Fellowship\n2001–2002: Microsoft Research Fellowship\n2007. Alfred P. Sloan Research Fellowship Sloan Foundation Faculty Fellowship\n2008. Massachusetts Institute of Technology (MIT) Technology Review, 35 Innovators Under 35 (TR35)\n2009. IJCAI Computers and Thought Award (the highest award in AI given to a researcher under 35)\n2009. Vance D. & Arlene C. Coffman Faculty Scholar Award\n2013. Time 100 Most Influential People\n2013. Fortune's 40 under 40 \n2013. CNN 10: Thinkers\n2014. Fast Company's Most Creative People in Business\n2015. World Economic Forum Young Global LeadersHe has corefereed hundreds of AI publications in journals like NeurIPS. He has also been the editor of the Journal of Artificial Intelligence Research (JAIR), Associate Editor for the IEEE Robotics and Automation Society Conference Editorial Board (ICRA), and much more.He has given invited talks at NASA, Google, Microsoft, Lockheed Martin, the Max Planck Society, Stanford, Princeton, UPenn, Cornell, MIT, UC Berkeley, and dozens of other universities. Outside of the US, he has lectured in Spain, Germany, Israel, China, Korea, and Canada.He has also written for Harvard Business Review, HuffPost, Slate, Apple News, and Quora Sessions' Twitter. He also writes a weekly digital newsletter called The Batch.\n\nBooks\nHe also wrote a book Machine Learning Yearning, a practical guide for those interested in machine learning, which he distributed for free. In December 2018, he wrote a sequel called AI Transformation Playbook.Ng contributed one chapter to Architects of Intelligence: The Truth About AI from the People Building it (2018) by the American futurist Martin Ford.\n\nViews on AI\nNg believes that AI technology will improve the lives of people, not that it is an anathema that will \"enslave\" the human race. Ng believes the potential benefits of AI outweigh the threats, which he believes are exaggerated. He has stated thatWorrying about AI evil superintelligence today is like worrying about overpopulation on the planet Mars. We haven't even landed on the planet yet!A real threat is regarding the future of work: \"Rather than being distracted by evil killer robots, the challenge to labor caused by these machines is a conversation that academia and industry and government should have.\" A particular goal of Ng's work is to \"democratize\" AI learning so that people can learn more about it and understand its benefits. Ng's stance on AI is shared by Mark Zuckerberg, but opposed by Elon Musk.In 2017, Ng said he supported basic income to allow the unemployed to study AI so that they can re-enter the workforce. He has stated that he enjoyed Erik Brynjolfsson and Andrew McAfee's \"The Second Machine Age\" which discusses issues such as AI displacement of jobs.\n\nSee also\nRobot Operating System\nLatent Dirichlet allocation\nGoogle Brain\nCoursera\n\nReferences\nExternal links\n\nOfficial website\nNg's Quora profile\nNg's Medium blog\nAcademic Genealogy\nAndrew Ng's Publication List",
    "Angoss": "Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.In June 2013 the private equity firm Peterson Partners acquired Angoss for $8.4 million.\n\nSoftware\nKnowledgeREADER is an integrated customer intelligence product combining visual text discovery and predictive analytics for customer experience management.\nKnowledgeSEEKER is a data mining product. Its features include data profiling, data visualization and decision tree analysis. It was first released in 1990.\nKnowledgeSTUDIO is a data mining and predictive analytics suite for the model development and deployment cycle. Its features include data profiling, data visualization, decision tree analysis, predictive modeling, implementation, scoring, validation, monitoring and scorecard development.\nKnowledgeEXCELERATOR is a visual data discovery software and prediction tool for business analysts and knowledge workers.\nStrategyBUILDER is an add-on module for KnowledgeSEEKER and KnowledgeSTUDIO and is a product to design, verify, and deploy predictive and business rules.\n\nServices\nFundGUARD is software as a service for marketing, sales targeting and predictive leads for mutual funds and wealth management companies.\nClaimGUARD is a fraud and abuse detection service.\nCloud on demand Software is offered for KnowledgeSEEKER, KnowledgeSTUDIO and its text analytics module.\nKnowledgeSCORE for Salesforce.com customer relationship management is a forecasting and predictive sales analytics system for Salesforce users.\n\nSee also\nList of statistical packages\nPredictive analytics\n\nSee also\nFICO\n\nReferences\nExternal links\nOfficial website",
    "Anomaly detection": "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.Anomaly detection finds application in many domains including cyber security, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.\nThree broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.\n\nDefinition\nMany attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include:\n\nAn outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\nAnomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data.\nAn outlier is an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.\nAn anomaly is a point or collection of points that is relatively distant from other points in multi-dimensional space of features.\nAnomalies are patterns in data that do not conform to a well defined notion of normal behaviour.\nLet T be observations from a univariate Gaussian distribution and O a point from T. Then the z-score for O is greater than a pre-selected threshold if and only if O is an outlier.\n\nApplications\nAnomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.  The counterpart of anomaly detection in intrusion detection is misuse detection.\nIt is often used in preprocessing to remove anomalous data from the dataset. This is done for a number of reasons. Statistics of data such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy. Anomalies are also often the most important observations in the data to be found such as in intrusion detection or detecting abnormalities in medical images.\n\nPopular techniques\nMany anomaly detection techniques have been proposed in literature. Some of the popular techniques are:\n\nStatistical (Z-score, Tukey's range test and Grubbs's test)\nDensity-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\nSubspace-, correlation-based and tensor-based  outlier detection for high-dimensional data\nOne-class support vector machines\nReplicator neural networks, autoencoders, variational autoencoders, long short-term memory neural networks\nBayesian networks\nHidden Markov models (HMMs)\nMinimum Covariance Determinant\nClustering: Cluster analysis-based outlier detection\nDeviations from association rules and frequent itemsets\nFuzzy logic-based outlier detection\nEnsemble techniques, using feature bagging, score normalization and different sources of diversityThe performance of methods depends on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.\n\nExplainable Anomaly Detection\nMany of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbor's densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations:\n\nThe Subspace Outlier Degree (SOD) identifies attributes where a sample is normal, and attributes in which the sample deviates from the expected.\nCorrelation Outlier Probabilities (COP) compute an error vector how a sample point deviates from an expected location, which can be interpreted as a counterfactual explanation: the sample would be normal if it were moved to that location.\n\nSoftware\nELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.\nPyOD is an open-source Python library developed specifically for anomaly detection.\nscikit-learn is an open-source Python library that contains some algorithms for unsupervised anomaly detection.\nWolfram Mathematica provides functionality for unsupervised anomaly detection across multiple data types\n\nDatasets\nAnomaly detection benchmark data repository with carefully chosen data sets of the Ludwig-Maximilians-Universität München; Mirror at University of São Paulo.\nODDS – ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.\nUnsupervised Anomaly Detection Benchmark at Harvard Dataverse: Datasets for Unsupervised Anomaly Detection with ground truth.\nKMASH Data Repository  at Research Data Australia having more than 12,000 anomaly detection datasets with ground truth.\n\nSee also\nChange detection\nStatistical process control\nNovelty detection\nHierarchical temporal memory\n\n\n== References ==",
    "Anthropic": "Anthropic PBC is an American artificial intelligence (AI) startup and public-benefit corporation, founded by former members of OpenAI. Anthropic specializes in developing general AI systems and language models, with a company ethos of responsible AI usage.As of July 2023, Anthropic had raised US$1.5 billion in funding.\n\nHistory\nAnthropic was founded in 2021 by former senior members of OpenAI, principally siblings Daniela Amodei and Dario Amodei, the latter of whom served as OpenAI's Vice President of Research. The Amodei siblings were among others who left OpenAI due to directional differences, specifically regarding OpenAI's ventures with Microsoft in 2019.By late 2022, Anthropic had raised US$700 million in funding, out of which US$500 million came from Alameda Research. Google's cloud division followed with an investment of US$300 million for a 10% stake, in a deal requiring Anthropic to buy computing resources from Google Cloud. In May 2023, Anthropic raised US$450 million in a round led by Spark Capital.In February 2023, Anthropic was sued by Texas-based Anthrop LLC for use of their registered trademark \"Anthropic A.I.\"Kevin Roose of The New York Times described the company as the \"Center of A.I. Doomerism\". He reported that some employees \"compared themselves to modern-day Robert Oppenheimers\".Journalists often connect Anthropic with the effective altruism movement, some founders and team members were part of the community or at least \"interested\" in it. One of the investors of Series B round was Sam Bankman-Fried of the cryptocurrence exchange FTX, that collapsed in 2022.\n\nProjects\nClaude\nConsisting of former researchers involved in OpenAI's GPT-2 and GPT-3 model development, Anthropic began development on its own AI chatbot, named Claude. Similar to ChatGPT, Claude uses a messaging interface where users can submit questions or requests and receive highly detailed and relevant responses. Claude has 52 billion parameters.Initially available in closed beta through a Slack integration, Claude is now accessible to users via the Poe app by Quora along with six other chatbots. The app is currently available for iOS and  Android.The name, \"Claude\", was chosen either as a reference to Claude Shannon, or as \"a friendly, male-gendered name designed to counterbalance the female-gendered names (Alexa, Siri, Cortana) that other tech companies gave their A.I. assistants\".Claude 2 was launched in July 2023, and is available in US and UK. The Guardian reported that safety was a priority during the model training, Anthropic calls in \"Constitutional AI\":\n\nThe chatbot is trained on principles taken from documents including the 1948 UN declaration and Apple’s terms of service, which cover modern issues such as data privacy and impersonation. One example of a Claude 2 principle based on the UN declaration is: “Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.”\n\nInterpretability research\nAnthropic also publishes research on the interpretability of machine learning systems, focusing on the transformer architecture.\n\nReferences\nExternal links\nOfficial website",
    "Apache Mahout": "Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark. Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.\n\nFeatures\nSamsara\nApache Mahout-Samsara refers to a Scala domain specific language (DSL) that allows users to use R-Like syntax as opposed to traditional Scala-like syntax. This allows user to express algorithms concisely and clearly.\n\nBackend Agnostic\nApache Mahout's code abstracts the domain specific language from the engine where the code is run. While active development is done with the Apache Spark engine, users are free to implement any engine they choose- H2O and Apache Flink have been implemented in the past and examples exist in the code base.\n\nGPU/CPU accelerators\nThe JVM has notoriously slow computation. To improve speed, “native solvers” were added which move in-core, and by extension, distributed BLAS operations out of the JVM, offloading to off-heap or GPU memory for processing via multiple CPUs and/or CPU cores, or GPUs when built against the ViennaCL library.  \"Extending Mahout Samsara to GPU Clusters\"..  ViennaCL is a highly optimized C++ library with BLAS operations implemented in OpenMP, and OpenCL.  As of release 14.1, the OpenMP build considered to be stable, leaving the OpenCL build is still in its experimental POC phase.\n\nRecommenders\nApache Mahout features implementations of Alternating Least Squares, Co-Occurrence, and Correlated Co-Occurrence, a unique-to-Mahout recommender algorithm that extends co-occurrence to be used on multiple dimensions of data.\n\nHistory\nTransition from Map Reduce to Apache Spark\nWhile Mahout's core algorithms for clustering, classification and batch based collaborative filtering were implemented on top of Apache Hadoop using the map/reduce paradigm, it did not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster were also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop.\nStarting with the release 0.10.0, the project shifted its focus to building a backend-independent programming environment, code named \"Samsara\". The environment consists of an algebraic backend-independent optimizer and an algebraic Scala DSL unifying in-memory and distributed algebraic operators. Supported algebraic platforms are Apache Spark, H2O, and Apache Flink. Support for MapReduce algorithms started being gradually phased out in 2014.\n\nRelease History\nDevelopers\nApache Mahout is developed by a community. The project is managed by a group called the \"Project Management Committee\" (PMC). The current PMC is Andrew Musselman,  Andrew Palumbo,  Drew Farris,  Isabel Drost-Fromm,  Jake Mannix,  Pat Ferrel,  Paritosh Ranjan,  Trevor Grant,  Robin Anil,  Sebastian Schelter,  Stevo Slavić.\n\nReferences\nExternal links\nOfficial website",
    "Apache Spark": "Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n\nOverview\nApache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD API is not deprecated. The RDD technology still underlies the Dataset API.Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.Inside Apache Spark the workflow is managed as a directed acyclic graph (DAG). Nodes represent RDDs while edges represent the operations on the RDDs.\nSpark facilitates the implementation of both iterative algorithms, which visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications may be reduced by several orders of magnitude compared to Apache Hadoop MapReduce implementation.\nAmong the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster,  where you can launch a cluster either manually or use the launch scripts provided by the install package. It is also possible to run these daemons on a single machine for testing), Hadoop YARN, Apache Mesos or Kubernetes. For distributed storage, Spark can interface with a wide variety, including Alluxio, Hadoop Distributed File System (HDFS), MapR File System (MapR-FS), Cassandra, OpenStack Swift, Amazon S3, Kudu, Lustre file system, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.\n\nSpark Core\nSpark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, .NET and R) centered on the RDD abstraction (the Java API is available for other JVM languages, but is also usable for some other non-JVM languages that can connect to the JVM, such as Julia). This interface mirrors a functional/higher-order model of programming: a \"driver\" program invokes parallel operations such as map, filter or reduce on an RDD by passing a function to Spark, which then schedules the function's execution in parallel on the cluster. These operations, and additional ones such as joins, take RDDs as input and produce new RDDs. RDDs are immutable and their operations are lazy; fault-tolerance is achieved by keeping track of the \"lineage\" of each RDD (the sequence of operations that produced it) so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, .NET, Java, or Scala objects.\nBesides the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: broadcast variables reference read-only data that needs to be available on all nodes, while accumulators can be used to program reductions in an imperative style.A typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each map, flatMap (a variant of map) and reduceByKey takes an anonymous function that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.\n\nSpark SQL\nSpark SQL is a component on top of Spark Core that introduced a data abstraction called DataFrames, which provides support for structured and semi-structured data. Spark SQL provides a domain-specific language (DSL) to manipulate DataFrames in Scala, Java, Python or .NET. It also provides SQL language support, with command-line interfaces and ODBC/JDBC server. Although DataFrames lack the compile-time type-checking afforded by RDDs, as of Spark 2.0, the strongly typed DataSet is fully supported by Spark SQL as well.\n\nSpark Streaming\nSpark Streaming uses Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of lambda architecture. However, this convenience comes with the penalty of latency equal to the mini-batch duration. Other streaming data engines that process event by event rather than in mini-batches include Storm and the streaming component of Flink. Spark Streaming has support built-in to consume from Kafka, Flume, Twitter, ZeroMQ, Kinesis, and TCP/IP sockets.In Spark 2.x, a separate technology based on Datasets, called Structured Streaming, that has a higher-level interface is also provided to support streaming.Spark can be deployed in a traditional on-premises data center as well as in the cloud.\n\nMLlib Machine Learning Library\nSpark MLlib is a distributed machine-learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by Apache Mahout (according to benchmarks done by the MLlib developers against the alternating least squares (ALS) implementations, and before Mahout itself gained a Spark interface), and scales better than Vowpal Wabbit. Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning pipelines, including:\n\nsummary statistics, correlations, stratified sampling, hypothesis testing, random data generation\nclassification and regression: support vector machines, logistic regression, linear regression, naive Bayes classification, Decision Tree, Random Forest, Gradient-Boosted Tree\ncollaborative filtering techniques including alternating least squares (ALS)\ncluster analysis methods including k-means, and latent Dirichlet allocation (LDA)\ndimensionality reduction techniques such as singular value decomposition (SVD), and principal component analysis (PCA)\nfeature extraction and transformation functions\noptimization algorithms such as stochastic gradient descent, limited-memory BFGS (L-BFGS)\n\nGraphX\nGraphX is a distributed graph-processing framework on top of Apache Spark. Because it is based on RDDs, which are immutable, graphs are immutable and thus GraphX is unsuitable for graphs that need to be updated, let alone in a transactional manner like a graph database. GraphX provides two separate APIs for implementation of massively parallel algorithms (such as PageRank): a Pregel abstraction, and a more general MapReduce-style API. Unlike its predecessor Bagel, which was formally deprecated in Spark 1.6, GraphX has full support for property graphs (graphs where properties can be attached to edges and vertices).Like Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.\n\nLanguage support\nApache Spark has built-in support for Scala, Java, SQL, R, and Python with 3rd party support for the .NET CLR, Julia, and more.\n\nHistory\nSpark was initially started by Matei Zaharia at UC Berkeley's AMPLab in 2009, and open sourced in 2010 under a BSD license.In 2013, the project was donated to the Apache Software Foundation and switched its license to Apache 2.0. In February 2014, Spark became a Top-Level Apache Project.In November 2014, Spark founder M. Zaharia's company Databricks set a new world record in large scale sorting using Spark.Spark had in excess of 1000 contributors in 2015, making it one of the most active projects in the Apache Software Foundation and one of the most active open source big data projects.\n\nScala Version\nSpark 3.4.0 is based on Scala 2.13 (and thus works with Scala 2.12 and 2.13 out-of-the-box), but it can also be made to work with Scala 3.\n\nDevelopers\nApache Spark is developed by a community. The project is managed by a group called the \"Project Management Committee\" (PMC).\n\nSee also\nList of concurrent and parallel programming APIs/Frameworks\n\nNotes\nReferences\nExternal links\nOfficial website",
    "Apache SystemML": "Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. \nSystemDS's distinguishing characteristics are:\n\nAlgorithm customizability via R-like and Python-like languages.\nMultiple execution modes, including Standalone, Spark Batch, Spark MLContext, Hadoop Batch, and JMLC.\nAutomatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.\n\nHistory\nSystemML was created in 2010 by researchers at the IBM Almaden Research Center led by IBM Fellow Shivakumar Vaithyanathan. It was observed that data scientists would write machine learning algorithms in languages such as R and Python for small data. When it came time to scale to big data, a systems programmer would be needed to scale the algorithm in a language such as Scala. This process typically involved days or weeks per iteration, and errors would occur translating the algorithms to operate on big data. SystemML seeks to simplify this process. A primary goal of SystemML is to automatically scale an algorithm written in an R-like or Python-like language to operate on big data, generating the same answer without the error-prone, multi-iterative translation approach.\nOn June 15, 2015, at the Spark Summit in San Francisco, Beth Smith, General Manager of IBM Analytics, announced that IBM was open-sourcing SystemML as part of IBM's major commitment to Apache Spark and Spark-related projects. SystemML became publicly available on GitHub on August 27, 2015 and became an Apache Incubator project on November 2, 2015. On May 17, 2017, the Apache Software Foundation Board approved the graduation of Apache SystemML as an Apache Top Level Project.\n\nKey technologies\nThe following are some of the technologies built into the SystemDS engine.\n\nCompressed Linear Algebra for Large Scale Machine Learning\nDeclarative Machine Learning Language\n\nExamples\nPrincipal Component Analysis\nThe following code snippet does the Principal component analysis of input matrix \n  \n    A\n    A\n   , which returns the \n  \n    \n      \n        e\n        i\n        g\n        e\n        n\n        v\n        e\n        c\n        t\n        o\n        r\n        s\n      \n    \n    {\\displaystyle eigenvectors}\n   and the \n  \n    \n      \n        e\n        i\n        g\n        e\n        n\n        v\n        a\n        l\n        u\n        e\n        s\n      \n    \n    {\\textstyle eigenvalues}\n  .\n\nInvocation script\nDatabase functions\nDBSCAN clustering algorithm with Euclidean distance.\n\nImprovements\nSystemDS 2.0.0 is the first major release under the new name. This release contains a major refactoring, a few major features, a large number of improvements and fixes, and some experimental features to better support the end-to-end data science lifecycle. In addition to that, this release also removes several features that are not up date and outdated.\n\nNew mechanism for DML-bodied (script-level) builtin functions, and a wealth of new built-in functions for data preprocessing including data cleaning, augmentation and feature engineering techniques, new ML algorithms, and model debugging.\nSeveral methods for data cleaning have been implemented including multiple imputations with multivariate imputation by chained equations (MICE) and other techniques, SMOTE, an oversampling technique for class imbalance, forward and backward NA filling, cleaning using schema and length information, support for outlier detection using standard deviation and inter-quartile range, and functional dependency discovery.\nA complete framework for lineage tracing and reuse including support for loop deduplication, full and partial reuse, compiler assisted reuse, several new rewrites to facilitate reuse.\nNew federated runtime backend including support for federated matrices and frames, federated builtins (transform-encode, decode etc.).\nRefactor compression package and add functionalities including quantization for lossy compression, binary cell operations, left matrix multiplication. [experimental]\nNew python bindings with supports for several builtins, matrix operations, federated tensors and lineage traces.\nCuda implementation of cumulative aggregate operators (cumsum, cumprod etc.)\nNew model debugging technique with slice finder.\nNew tensor data model (basic tensors of different value types, data tensors with schema) [experimental]\nCloud deployment scripts for AWS and scripts to set up and start federated operations.\nPerformance improvements with parallel sort, gpu cum agg, append cbind etc.\nVarious compiler and runtime improvements including new and improved rewrites, reduced Spark context creation, new eval framework, list operations, updated native kernel libraries to name a few.\nNew data reader/writer for json frames and support for sql as a data source.\nMiscellaneous improvements: improved documentation, better testing, run/release scripts, improved packaging, Docker container for systemds, support for lambda expressions, bug fixes.\nRemoved MapReduce compiler and runtime backend, pydml parser, Java-UDF framework, script-level debugger.\nDeprecated ./scripts/algorithms, as those algorithms gradually will be part of SystemDS builtins.\n\nContributions\nApache SystemDS welcomes contributions in code, question and answer, community building, or spreading the word. The contributor guide is available at https://github.com/apache/systemds/blob/main/CONTRIBUTING.md\n\nSee also\nComparison of deep learning software\n\nReferences\nExternal links\nApache SystemML website\nIBM Research - SystemML\nQ & A with Shiv Vaithyanathan, Creator of SystemML and IBM Fellow\nA Universal Translator for Big Data and Machine Learning\nSystemML: Declarative Machine Learning at Scale presentation by Fred Reiss\nSystemML: Declarative Machine Learning on MapReduce\nHybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML\nSystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs\nIBM's SystemML machine learning system becomes Apache Incubator project\nIBM donates machine learning tech to Apache Spark open source community\nIBM's SystemML Moves Forward as Apache Incubator Project",
    "Application security": "Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.\n\nApproaches\nDifferent approaches will find different subsets of the security vulnerabilities lurking in an application and are most effective at different times in the software lifecycle. They each represent different tradeoffs of time, effort, cost and vulnerabilities found.\n\nDesign review. Before code is written the application's architecture and design can be reviewed for security problems. A common technique in this phase is the creation of a threat model.\nWhitebox security review, or code review. This is a security engineer deeply understanding the application through manually reviewing the source code and noticing security flaws. Through comprehension of the application, vulnerabilities unique to the application can be found.\nBlackbox security audit. This is only through the use of an application testing it for security vulnerabilities, no source code is required.\nAutomated Tooling. Many security tools can be automated through inclusion into the development or testing environment. Examples of those are automated DAST/SAST tools that are integrated into code editor or CI/CD platforms.\nCoordinated vulnerability platforms. These are hacker-powered application security solutions offered by many websites and software developers by which individuals can receive recognition and compensation for reporting bugs.\n\nWeb application security\nWeb application security is a branch of information security that deals specifically with the security of websites, web applications, and web services. At a high level, web application security draws on the principles of application security but applies them specifically to the internet and web systems.Web Application Security Tools are specialized tools for working with HTTP traffic, e.g., Web application firewalls.\n\nSecurity threats\nThe Open Web Application Security Project (OWASP) provides free and open resources. It is led by a non-profit called The OWASP Foundation. The OWASP Top 10 - 2017 results from recent research based on comprehensive data compiled from over 40 partner organizations. This data revealed approximately 2.3 million vulnerabilities across over 50,000 applications. According to the OWASP Top 10 - 2021, the ten most critical web application security risks include:\nBroken access control\nCryptographic Failures\nInjection\nInsecure Design\nSecurity Misconfiguration\nVulnerable and Outdated Components\nIdentification and Authentification Failures\nSoftware and Data Integrity Failures\nSecurity Logging and Monitoring Failures*\nServer-Side Request Forgery (SSRF)*\n\nTooling for security testing\nSecurity testing techniques scour for vulnerabilities or security holes in applications. These vulnerabilities leave applications open to exploitation. Ideally, security testing is implemented throughout the entire Software Development Life Cycle (SDLC) so that vulnerabilities may be addressed in a timely and thorough manner.\nThere are many kinds of automated tools for identifying vulnerabilities in applications. Common tool categories used for identifying application vulnerabilities include:\n\nStatic Application Security Testing (SAST) analyzes source code for security vulnerabilities during an application's development. Compared to DAST, SAST can be utilized even before the application is in an executable state. As SAST has access to the full source code it is a white-box approach. This can yield more detailed results but can result in many false positives that need to be manually verified.\nDynamic Application Security Testing (DAST, often called Vulnerability scanners) automatically detects vulnerabilities by crawling and analyzing websites. This method is highly scalable, easily integrated and quick. DAST tools are well suited for dealing with low-level attacks such as injection flaws but are not well suited to detect high-level flaws, e.g., logic or business logic flaws. Fuzzing tools are commonly used for input testing.\nInteractive Application Security Testing (IAST) assesses applications from within using software instrumentation. This combines the strengths of both SAST and DAST methods as well as providing access to code, HTTP traffic, library information, backend connections and configuration information. Some IAST products require the application to be attacked, while others can be used during normal quality assurance testing.\nRuntime application self-protection augments existing applications to provide intrusion detection and prevention from within an application runtime.\nDependency scanners (also called Software Composition Analysis) try to detect the usage of software components with known vulnerabilities. These tools can either work on-demand, e.g., during the source code build process, or periodically.\nAbstraction is the idea of making more complex things less complex.\n\nSecurity standards and regulations\nCERT Secure Coding\nISO/IEC 27034-1:2011 Information technology — Security techniques — Application security -- Part 1: Overview and concepts\nISO/IEC TR 24772:2013 Information technology — Programming languages — Guidance to avoiding vulnerabilities in programming languages through language selection and use\nNIST Special Publication 800-53\nOWASP ASVS: Web Application Security Verification Standard\n\nSee also\nApplication service architecture (ASA)\nCommon Weakness Enumeration\nData security\nMobile security\nOWASP\nMicrosoft Security Development Lifecycle\nUsable security\n\n\n== References ==",
    "Applications of artificial intelligence": "Artificial intelligence (AI) has been used in applications to alleviate certain problems throughout industry and academia. AI, like electricity or computers, is a general purpose technology that has a multitude of applications. It has been used in fields of language translation, image recognition, credit scoring, e-commerce and other domains.\n\nInternet and e-commerce\nRecommendation systems\nA recommendation system predicts the \"rating\" or \"preference\" a user would give to an item. Recommendation systems are used in a variety of areas, such as generating playlists for video and music services, product recommendations for online stores, or content recommendations for social media platforms and open web content recommendation.Companies to use such systems include Netflix, Amazon and YouTube.\n\nWeb feeds and posts\nMachine learning is also used in web feeds such as for determining which posts show up in social media feeds. Various types social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.\n\nTargeted advertising and increasing internet engagement\nAI is used to target web advertisements to those most likely to click or engage on them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints. Both AdSense and Facebook use AI for advertising.Online gambling companies use AI to improve customer targeting.Personality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.\n\nVirtual assistants\nIntelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.\n\nSearch engines\nSearch engines that use artificial intelligence include Google Search and Bing Chat.\n\nSpam filtering\nMachine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to identify any malicious elements. Numerous models built on machine learning algorithms exhibit exceptional performance with accuracies over 90% in distinguishing between spam and legitimate emails.\n\nLanguage translation\nAI has been used to automatically translate spoken language and textual content, in products such as Microsoft Translator, Google Translate and DeepL Translator. Additionally, research and development is in progress to decode and conduct animal communication.\n\nFacial recognition and image labeling\nAI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.Image labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.  Facebook's DeepFace identifies human faces in digital images.\n\nGames\nGames have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have produced superhuman results in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.\n\nEconomic and social challenges\nAI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.\n\nAgriculture\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n\nCyber security\nCyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.Applications of AI in cyber security include:\n\nNetwork protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.\nEndpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.\nApplication security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.\nSuspect user behavior: Machine learning can identify fraud or compromised applications as they occur.Google fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.\n\nEducation\nAI tutors allow students to get one-on-one help. They can reduce anxiety and stress for students stemming from tutor labs or human tutors.AI can create a dysfunctional environment with revenge effects such as technology that hinders students' ability to stay on task. In another scenario, AI can provide early prediction of student success in a virtual learning environment (VLE) such as Moodle.In the education process, students can personalize their training with the help of artificial intelligence. And for teaching professionals, the technology provided by AI can improve the quality of the educational process and teaching skills.AI text detectors can be used to scan essays generated by artificial intelligence in order to try to establish genuine authorship. However, a study found that seven of the most used of these detectors often wrongly flagged articles written by those whose first language was not English as AI-generated, thus discriminating against so called 'non-native' English speakers.\n\nFinance\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.\nBanks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.The use of AI in applications such as online trading and decision making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient. The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises. Especially for smaller and more innovative enterprises.\n\nTrading and investment\nAlgorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.\n\nUnderwriting\nOnline lender Upstart uses machine learning for underwriting.ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.\n\nAudit\nAI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.\n\nAnti-money laundering\nAI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to \"develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability\". A study about deep learning for AML identified \"key challenges for researchers\" to have \"access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced\" and suggests future research should bring-out \"explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data\".\n\nHistory\nIn the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Protrader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.\n\nGovernment\nAI facial recognition systems are used for mass surveillance, notably in China.In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.\n\nMilitary\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.Worldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. Many researchers avoid military applications.\n\nHealth\nHealthcare\nAI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.\nOther healthcare tasks thought suitable for an AI that are in development include:\n\nScreening\nHeart sound analysis\nCompanion robots for elder care\nMedical record analysis\nTreatment plan design\nMedication management\nAssisting blind people\nConsultations\nDrug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)\nClinical training\nOutcome prediction for surgical procedures\nHIV prognosis\nIdentifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)\nHelping link genes to their functions, otherwise analyzing genes and identification of novel biological targets\nHelp development of biomarkers\nHelp tailor therapies to individuals in personalized medicine/precision medicine\n\nWorkplace health and safety\nAI-enabled chatbots decrease the need for humans to perform basic call center tasks.Machine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.AI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.\n\nBiochemistry\nAlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).\n\nChemistry and biology\nMachine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\". It has been used for the design of proteins with prespecified functional sites.It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.There also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.\n\nNovel types of machine learning\nThere are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.\nA subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n\nDigital ghosts\nBiological computing in AI and as AI\nHowever, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this doesn't mean there being \"a technological solution to imitate natural intelligence\". Technologies that integrate biology and are often AI-based include biorobotics.\n\nAstronomy, space activities and ufology\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data – such as real-time observations – and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.\n\nFuture or non-human applications\nLoeb has speculated that one type of technological equipment the project may detect could be \"AI astronauts\" and in 2021 – in an opinion piece – that AI \"will\" \"supersede natural intelligence\", while Martin Rees stated that there \"may\" be more civilizations than thought with the \"majority of them\" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as \"safety of encounters with an alien AI\", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of \"AI astronauts\" that engage in \"supervised evolution\" (see also: directed evolution, uplift, directed panspermia and space colonization).\n\nAstrochemistry\nIt can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.\n\nOther fields of research\nArchaeology, history and imaging of sites\nMachine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.\nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".\n\nPhysics\nA deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.\n\nMaterials science\nAI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.\n\nReverse engineering\nMachine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.\n\nLaw\nLegal analysis\nAI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.The electronic discovery industry uses machine learning to reduce manual searching.\n\nLaw enforcement and legal proceedings\nCOMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.\n\nServices\nHuman resources\nAnother application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.\n\nJob search\nAI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.\n\nOnline and telephone customer service\nAI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.\n\nHospitality\nIn the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.\n\nMedia\nAI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\nTypical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n\nMotion interpolation\nPixel-art scaling algorithms\nImage scaling\nImage restoration\nPhoto colorization\nFilm restoration and video upscaling\nPhoto tagging\nAutomated species identification (such as identifying plants, fungi and animals with an app)\nText-to-image models such as DALL-E, Midjourney and Stable Diffusion\nImage to video\nText to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google\nText to music with AI models such as MusicLM\nText to speech such as ElevenLabs and 15.ai\nMotion capture\n\nDeep-fakes\nDeep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\nIn January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\nIn September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.In 2018, Vincent Nozick found a way to detect faked content by analyzing eyelid movements. DARPA gave 68 million dollars to work on deep-fake detection.Audio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.\n\nVideo content analysis, surveillance and manipulated media detection\nAI algorithms have been used to detect deepfake videos.\n\nMusic\nAI has been used to compose music of various genres.\nDavid Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.In 2012, AI Iamus created the first complete classical album.AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.Melomics creates computer-generated music for stress and pain relief.At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\nThe Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.\nSouth Korean singer Hayeon's debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.\n\nWriting and reporting\nNarrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.South Korean company Hanteo Global uses a journalism bot to write articles.Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n\nWikipedia\nMillions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.\nMachine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.\n\nVideo games\nIn video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.\n\nArt\nAI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing.\nIt started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.\nAI like \"Disco Diffusion\", \"DALL·E\" (1 and 2), Stable Diffusion, Imagen, \"Dream by Wombo\", Midjourney has been used for visualizing conceptual inputs such as song lyrics, certain texts or specific imagined concepts (or imaginations) in artistic ways or artistic images in general. Some of the tools also allow users to input images and various parameters e.g. to display an object or product in various environments, some can replicate artistic styles of popular artists, and some can create elaborate artistic images from rough sketches.\nSince their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.\n\nArt analysis\nIn addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\nResearchers have also introduced models that predict emotional responses to art.\n\nUtilities\nEnergy system\nPower electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).\n\nTelecommunications\nMany telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.\n\nManufacturing\nSensors\nArtificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.\n\nToys and games\nIn the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\nMattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.\n\nOil and gas\nOil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.\n\nTransport\nAutomotive\nAI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\nThere are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.Autonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).\n\nTraffic management\nAI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.\n\nSmart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.\n\nMilitary\nThe Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\nAI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\nSpeech recognition allows traffic controllers to give verbal directions to drones.\nArtificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n\nNASA\nIn 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.\n\nMaritime\nNeural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.\n\nEnvironmental monitoring\nAutonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.\n\nEarly-warning systems\nMachine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.\n\nComputer science\nProgramming assistance\nGitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.\n\nNeural network design\nAI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.\n\nQuantum computing\nMachine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).\n\nHistorical contributions\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:\nTime sharing\nInteractive interpreters\nGraphical user interfaces and the computer mouse\nRapid application development environments\nThe linked list data structure\nAutomatic storage management\nSymbolic programming\nFunctional programming\nDynamic programming\nObject-oriented programming\nOptical character recognition\nConstraint satisfaction\n\nBusiness\nCustomer service\nBusiness websites and social media platforms for businesses like use chatbots for customer interactions like helping in answering frequently asked questions. Chatbots offers 24/7 support and replaces humans thereby helping in cutting business costs.\n\nContent extraction\nAn optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.\n\nList of applications\nSee also\nApplications of artificial intelligence to legal informatics\nApplications of deep learning\nApplications of machine learning\nCollective intelligence#Applications\nList of artificial intelligence projects\nProgress in artificial intelligence\nOpen data\nTimeline of computing 2020–present\nData Sources\n\nFootnotes\nFurther reading\nKaplan, A.M.; Haenlein, M. (2018). \"Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence\". Business Horizons. 62 (1): 15–25. doi:10.1016/j.bushor.2018.08.004. S2CID 158433736.\nKurzweil, Ray (2005). The Singularity is Near: When Humans Transcend Biology. New York: Viking. ISBN 978-0-670-03384-3.\nNational Research Council (1999). \"Developments in Artificial Intelligence\". Funding a Revolution: Government Support for Computing Research. National Academy Press. ISBN 978-0-309-06278-7. OCLC 246584055.\nMoghaddam, M. J.; Soleymani, M. R.; Farsi, M. A. (2015). \"Sequence planning for stamping operations in progressive dies\". Journal of Intelligent Manufacturing. 26 (2): 347–357. doi:10.1007/s10845-013-0788-0. S2CID 7843287.\nFelten, Ed (3 May 2016). \"Preparing for the Future of Artificial Intelligence\".",
    "Apprenticeship learning": "In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert. It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.\n\nMapping function approach\nMapping methods try to mimic the expert by forming a direct mapping either from states to actions, or from states to reward values. For example, in 2002 researchers used such an approach to teach an AIBO robot basic soccer skills.\n\nInverse reinforcement learning approach\nInverse reinforcement learning (IRL) is the process of deriving a reward function from observed behavior. While ordinary \"reinforcement learning\" involves using rewards and punishments to learn behavior, in IRL the direction is reversed, and a robot observes a person's behavior to figure out what goal that behavior seems to be trying to achieve. The IRL problem can be defined as:\nGiven 1) measurements of an agent's behaviour over time, in a variety of circumstances; 2) measurements of the sensory inputs to that agent; 3) a model of the physical environment (including the agent's body): Determine the reward function that the agent is optimizing.\nIRL researcher Stuart J. Russell proposes that IRL might be used to observe humans and attempt to codify their complex \"ethical values\", in an effort to create \"ethical robots\" that might someday know \"not to cook your cat\" without needing to be explicitly told. The scenario can be modeled as a \"cooperative inverse reinforcement learning game\", where a \"person\" player and a \"robot\" player cooperate to secure the person's implicit goals, despite these goals not being explicitly known by either the person nor the robot.In 2017, OpenAI and DeepMind applied deep learning to the cooperative inverse reinforcement learning in simple domains such as Atari games and straightforward robot tasks such as backflips. The human role was limited to answering queries from the robot as to which of two different actions were preferred. The researchers found evidence that the techniques may be economically scalable to modern systems.Apprenticeship via inverse reinforcement learning (AIRP) was developed by in 2004 Pieter Abbeel, Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. AIRP deals with \"Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform\". AIRP has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.\nOne domain where AIRP has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - \"Autonomous Helicopter Aerobatics through Apprenticeship Learning\"\n\nSystem model approach\nSystem models try to mimic the expert by modeling world dynamics.\n\nPlan approach\nThe system learns rules to associate preconditions and postconditions with each action. In one 1994 demonstration, a humanoid learns a generalized plan from only two demonstrations of a repetitive ball\ncollection task.\n\nExample\nLearning from demonstration is often explained from a perspective that the working Robot-control-system is available and the human-demonstrator is using it. And indeed, if the software works, the Human operator takes the robot-arm, makes a move with it, and the robot will reproduce the action later. For example, he teaches the robot-arm how to put a cup under a coffeemaker and press the start-button. In the replay phase, the robot is imitating this behavior 1:1. But that is not how the system works internally; it is only what the audience can observe. In reality, Learning from demonstration is much more complex. One of the first works on learning by robot apprentices (anthropomorphic robots learning by imitation) was Adrian Stoica's PhD thesis in 1995.In 1997, robotics expert Stefan Schaal was working on the Sarcos robot-arm. The goal was simple: solve the pendulum swingup task. The robot itself can execute a movement, and as a result, the pendulum is moving. The problem is, that it is unclear what actions will result into which movement. It is an Optimal control-problem which can be described with mathematical formulas but is hard to solve. The idea from Schaal was, not to use a Brute-force solver but record the movements of a human-demonstration. The angle of the pendulum is logged over three seconds at the y-axis. This results into a diagram which produces a pattern.\nIn computer animation, the principle is called spline animation. That means, on the x-axis the time is given, for example 0.5 seconds, 1.0 seconds, 1.5 seconds, while on the y-axis is the variable given. In most cases it's the position of an object. In the inverted pendulum it is the angle.\nThe overall task consists of two parts: recording the angle over time and reproducing the recorded motion. The reproducing step is surprisingly simple. As an input we know, in which time step which angle the pendulum must have. Bringing the system to a state is called “Tracking control” or PID control. That means, we have a trajectory over time, and must find control actions to map the system to this trajectory. Other authors call the principle “steering behavior”, because the aim is to bring a robot to a given line.\n\nSee also\nInverse reinforcement learning\n\n\n== References ==",
    "Approximate computing": "Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design. It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose. One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.\nThe key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as program crash or erroneous output.\n\nStrategies\nSeveral strategies can be used for performing approximate computing.\n\nApproximate circuits\nApproximate arithmetic circuits: adders, multipliers and other logical circuits can reduce hardware overhead. For example, an approximate multi-bit adder can ignore the carry chain and thus, allow all its sub-adders to perform addition operation in parallel.\nApproximate storage and memory\nInstead of storing data values exactly, they can be stored approximately, e.g., by truncating the lower-bits in floating point data. Another method is to accept less reliable memory. For this, in DRAM and eDRAM, refresh rate assignments can be lowered or controlled. In SRAM, supply voltage can be lowered or controlled. Approximate storage can be applied to reduce MRAM's high write energy consumption. In general, any error detection and correction mechanisms should be disabled.\nSoftware-level approximation\nThere are several ways to approximate at software level. Memoization or fuzzy memoization (the use of a vector database for approximate retrieval from a cache, i.e. fuzzy caching) can be applied. Some iterations of loops can be skipped (termed as loop perforation) to achieve a result faster. Some tasks can also be skipped, for example when a run-time condition suggests that those tasks are not going to be useful (task skipping). Monte Carlo algorithms and Randomized algorithms trade correctness for execution time guarantees. The computation can be reformulated according to paradigms that allow easily the acceleration on specialized hardware, e.g. a neural processing unit.\nApproximate system\nIn an approximate system, different subsystems of the system such as the processor, memory, sensor, and communication modules are synergistically approximated to obtain a much better system-level Q-E trade-off curve compared to individual approximations to each of the subsystems.\n\nApplication areas\nApproximate computing has been used in a variety of domains where the applications are error-tolerant, such as multimedia processing, machine learning, signal processing, scientific computing. Therefore, approximate computing is mostly driven by applications that are related to human perception/cognition and have inherent error resilience. Many of these applications are based on statistical or probabilistic computation, such as different approximations can be made to better suit the desired objectives.\nOne notable application in machine learning is that Google is using this approach in their Tensor processing units (TPU, a custom ASIC).\n\nDerived paradigms\nThe main issue in approximate computing is the identification of the section of the application that can be approximated. In the case of large scale applications, it is very common to find people holding the expertise on approximate computing techniques not having enough expertise on the application domain (and vice versa). In order to solve this problem, programming paradigms have been proposed. They all have in common the clear role separation between application programmer and application domain expert. These approaches allow the spread of the most common optimizations and approximate computing techniques.\n\nSee also\nArtificial neural network\nMetaheuristic\nPCMOS\n\n\n== References ==",
    "ArXiv": "arXiv (pronounced \"archive\"—the X represents the Greek letter chi ⟨χ⟩) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer review. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of April 2021, the submission rate is about 16,000 articles per month.\n\nHistory\narXiv was made possible by the compact TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory (LANL) that could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles.\nIt began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. It is now hosted principally by Cornell, with five mirrors around the world.ArXiv was an early adopter and promoter of preprints. Its success in sharing preprints was one of the precipitating factors that led to the later movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv. \nThe annual budget for arXiv was approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions.In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it \"was supposed to be a three-hour tour, not a life sentence\". However, Ginsparg remains on the arXiv's Scientific Advisory Board and its Physics Advisory Committee.In January 2022, arXiv began assigning DOIs to articles, in collaboration with DataCite.\n\nData format\nEach arXiv paper has a unique identifier:\n\nYYMM.NNNNN, e.g. 1507.00123,\nYYMM.NNNN, e.g. 0704.0001,\narch-ive/YYMMNNN for older papers, e.g. hep-th/9901001.Different versions of the same paper are specified by a version number at the end. For example, 1709.08980v1. If no version number is specified, the default is the latest version.\narXiv uses a category system. Each paper is tagged with one or more categories. Some categories have two layers. For example, q-fin.TR is the  \"Trading and Market Microstructure\" category within \"quantitative finance\". Other categories have one layer. For example, hep-ex is \"high energy physics experiments\".\n\nModeration process and endorsement\nAlthough arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted.\nAdditionally, an \"endorsement\" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: \"If anybody is interested in my way of solving the problem, it's all there [on the arXiv] – let them go and read about it\". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.While arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, a 2002 article which appeared in Notices of the American Mathematical Society described those as \"surprisingly rare\". arXiv generally re-classifies these works, e.g. in \"General mathematics\", rather than deleting them; however, some authors have voiced concern over the lack of transparency in the arXiv screening process.\n\nSubmission formats\nPapers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.\n\nAccess\nThe standard access route is through the arXiv.org website or one of several mirrors. Other interfaces and access routes have also been created by other un-associated organisations.\nMetadata for arXiv is made available through OAI-PMH, the standard for open access repositories. Content is therefore indexed in all major consumers of such data, such as BASE, CORE and Unpaywall. As of 2020, the Unpaywall dump links over 500,000 arxiv URLs as the open access version of a work found in CrossRef data from the publishers, making arXiv a top 10 global host of green open access.\nFinally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them.\n\nCopyright status of files\nFiles on arXiv can have a number of different copyright statuses:\nSome are public domain, in which case they will have a statement saying so.\nSome are available under either the Creative Commons 4.0 Attribution-ShareAlike license or the Creative Commons 4.0 Attribution-Noncommercial-ShareAlike license.\nSome are copyright to the publisher, but the author has the right to distribute them and has given arXiv a non-exclusive irrevocable license to distribute them.\nMost are copyright to the author, and arXiv has only a non-exclusive irrevocable license to distribute them.\n\nSee also\nList of academic databases and search engines\nList of academic journals by preprint policy\nList of preprint repositories\nSci-Hub\n\nCitations\nGeneral and cited sources\nExternal links\n\nOfficial website",
    "ArXiv (identifier)": "arXiv (pronounced \"archive\"—the X represents the Greek letter chi ⟨χ⟩) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer review. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of April 2021, the submission rate is about 16,000 articles per month.\n\nHistory\narXiv was made possible by the compact TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory (LANL) that could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles.\nIt began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. It is now hosted principally by Cornell, with five mirrors around the world.ArXiv was an early adopter and promoter of preprints. Its success in sharing preprints was one of the precipitating factors that led to the later movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv. \nThe annual budget for arXiv was approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions.In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it \"was supposed to be a three-hour tour, not a life sentence\". However, Ginsparg remains on the arXiv's Scientific Advisory Board and its Physics Advisory Committee.In January 2022, arXiv began assigning DOIs to articles, in collaboration with DataCite.\n\nData format\nEach arXiv paper has a unique identifier:\n\nYYMM.NNNNN, e.g. 1507.00123,\nYYMM.NNNN, e.g. 0704.0001,\narch-ive/YYMMNNN for older papers, e.g. hep-th/9901001.Different versions of the same paper are specified by a version number at the end. For example, 1709.08980v1. If no version number is specified, the default is the latest version.\narXiv uses a category system. Each paper is tagged with one or more categories. Some categories have two layers. For example, q-fin.TR is the  \"Trading and Market Microstructure\" category within \"quantitative finance\". Other categories have one layer. For example, hep-ex is \"high energy physics experiments\".\n\nModeration process and endorsement\nAlthough arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted.\nAdditionally, an \"endorsement\" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: \"If anybody is interested in my way of solving the problem, it's all there [on the arXiv] – let them go and read about it\". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.While arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, a 2002 article which appeared in Notices of the American Mathematical Society described those as \"surprisingly rare\". arXiv generally re-classifies these works, e.g. in \"General mathematics\", rather than deleting them; however, some authors have voiced concern over the lack of transparency in the arXiv screening process.\n\nSubmission formats\nPapers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.\n\nAccess\nThe standard access route is through the arXiv.org website or one of several mirrors. Other interfaces and access routes have also been created by other un-associated organisations.\nMetadata for arXiv is made available through OAI-PMH, the standard for open access repositories. Content is therefore indexed in all major consumers of such data, such as BASE, CORE and Unpaywall. As of 2020, the Unpaywall dump links over 500,000 arxiv URLs as the open access version of a work found in CrossRef data from the publishers, making arXiv a top 10 global host of green open access.\nFinally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them.\n\nCopyright status of files\nFiles on arXiv can have a number of different copyright statuses:\nSome are public domain, in which case they will have a statement saying so.\nSome are available under either the Creative Commons 4.0 Attribution-ShareAlike license or the Creative Commons 4.0 Attribution-Noncommercial-ShareAlike license.\nSome are copyright to the publisher, but the author has the right to distribute them and has given arXiv a non-exclusive irrevocable license to distribute them.\nMost are copyright to the author, and arXiv has only a non-exclusive irrevocable license to distribute them.\n\nSee also\nList of academic databases and search engines\nList of academic journals by preprint policy\nList of preprint repositories\nSci-Hub\n\nCitations\nGeneral and cited sources\nExternal links\n\nOfficial website",
    "Array data structure": "In computer science, an array is a data structure consisting of a collection of elements (values or variables), of same memory size, each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.\nFor example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).\nThe memory address of the first element of an array is called first address, foundation address, or base address.\nBecause the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called \"matrices\". In some cases the term \"vector\" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word \"table\" is sometimes used as a synonym of array.\nArrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.\nArrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.\nThe term \"array\" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.\nThe term is also used, especially in the description of algorithms, to mean associative array or \"abstract array\", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.\n\nHistory\nThe first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. John von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer.p. 159 Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), Lisp (1958), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.\n\nApplications\nArrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.\nArrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient (implicit data structures), requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).\nOne or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate \"dynamic memory\" portably.\nArrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple IF statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.\n\nElement identifier and addressing formulas\nWhen data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indexes are also called subscripts. An index maps the array value to a stored object.\nThere are three ways in which the elements of an array can be indexed:\n\n0 (zero-based indexing)\nThe first element of the array is indexed by subscript of 0.\n1 (one-based indexing)\nThe first element of the array is indexed by subscript of 1.\nn (n-based indexing)\nThe base index of an array can be freely chosen. Usually programming languages allowing n-based indexing also allow negative index values and other scalar data types like enumerations, or characters may be used as an array index.Using zero based indexing is the design choice of many influential programming languages, including C, Java and Lisp. This leads to simpler implementation where the subscript refers to an offset from the starting position of an array, so the first element has an offset of zero.\nArrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example, a two-dimensional array A with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression A[1][3] in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and n for an n-dimensional array.\nThe number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.\nIn standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a \"linear\" formula on the indices.\n\nOne-dimensional arrays\nA one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.\nAs an example consider the C declaration int anArrayName[10]; which declares a one-dimensional array of ten integers. Here, the array can store ten elements of type int . This array has indices starting from zero through nine. For example, the expressions anArrayName[0] and anArrayName[9] are the first and last elements respectively.\nFor a vector with linear addressing, the element with index i is located at the address B + c × i, where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.\nIf the valid element indices begin at 0, the constant B is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element \"zeroth\" rather than \"first\".\nHowever, one can choose the index of the first element by an appropriate choice of the base address B. For example, if the array has five elements, indexed 1 through 5, and the base address B is replaced by B + 30c, then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant B may not be the address of any element.\n\nMultidimensional arrays\nFor a multidimensional array, the element with indices i,j would have address B + c · i + d · j, where the coefficients c and d are the row and column address increments, respectively.\nMore generally, in a k-dimensional array, the address of an element with indices i1, i2, ..., ik is\n\nB + c1 · i1 + c2 · i2 + … + ck · ik.For example: int a[2][3];\nThis means that array a has 2 rows and 3 columns, and the array is of integer type. Here we can store 6 elements they will be stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a11, a12, a13, a21, a22, a23.\nThis formula requires only k multiplications and k additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.\nThe coefficients ck must be chosen so that every valid index tuple maps to the address of a distinct element.\nIf the minimum legal value for every index is 0, then B is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address B. Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing B by B + c1 − 3c2 will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.\n\nDope vectors\nThe addressing formula is completely defined by the dimension d, the base address B, and the increments c1, c2, ..., ck. It is often useful to pack these parameters into a record called the array's descriptor or stride vector or dope vector. The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.\n\nCompact layouts\nOften the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.\n\nThere are two systematic compact layouts for a two-dimensional array. For example, consider the matrix\n\n  \n    \n      \n        A\n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n              \n              \n                \n                  4\n                \n                \n                  5\n                \n                \n                  6\n                \n              \n              \n                \n                  7\n                \n                \n                  8\n                \n                \n                  9\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle A={\\begin{bmatrix}1&2&3\\\\4&5&6\\\\7&8&9\\end{bmatrix}}.}\n  In the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:\n\nIn column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:\n\nFor arrays with three or more indices, \"row major order\" puts in consecutive positions any two elements whose index tuples differ only by one in the last index. \"Column major order\" is analogous with respect to the first index.\nIn systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. This is known as spatial locality, which is a type of locality of reference. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product A·B of two matrices, it would be best to have A stored in row-major order, and B in column-major order.\n\nResizing\nStatic arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a dynamic version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.\nSome array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.\n\nNon-linear formulas\nMore complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.\n\nEfficiency\nBoth store and select take (deterministic worst case) constant time. Arrays take linear (O(n)) space in the number of elements n that they hold.\nIn an array with element size k and on a machine with a cache line size of B bytes, iterating through an array of n elements requires the minimum of ceiling(nk/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/k better than the number of cache misses needed to access n elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does not mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.\nMemory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead (e.g., to store index bounds) but this is language-dependent. It can also happen that elements stored in an array require less memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called packed arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.\nArray accesses with statically predictable access patterns are a major source of data parallelism.\n\nComparison with other data structures\nDynamic arrays or growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ(n)) additional storage, whereas arrays do not reserve additional storage.\nAssociative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.\nBalanced trees require O(log n) time for indexed access, but also permit inserting or deleting elements in O(log n) time, whereas growable arrays require linear (Θ(n)) time to insert or delete elements at an arbitrary position.\nLinked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.\n\nAn Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row(pointer on c or c++). Thus an element in row i and column j of an array A would be accessed by double indexing (A[i][j] in typical notation). This alternative structure allows jagged arrays, where each row may have a different size—or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.\n\nDimension\nThe dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array is a rectangle of data, a three-dimensional array a block of data, etc.\nThis should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.\n\nSee also\nReferences\nExternal links\n\n Data Structures/Arrays at Wikibooks",
    "Arthur Samuel (computer scientist)": "Arthur Lee Samuel (December 5, 1901 – July 29, 1990) was an American pioneer in the field of computer gaming and artificial intelligence. He popularized the term \"machine learning\" in 1959. The Samuel Checkers-playing Program was among the world's first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI). He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.\n\nBiography\nSamuel was born on December 5, 1901, in Emporia, Kansas, and graduated from College of Emporia in Kansas in 1923.\nHe received a master's degree in Electrical Engineering from MIT in 1926, and taught for two years as instructor.  In 1928, he joined Bell Laboratories, where he worked mostly on vacuum tubes, including improvements of radar during World War II.  He developed a gas-discharge transmit-receive switch (TR tube) that allowed a single antenna to be used for both transmitting and receiving. After the war he moved to the University of Illinois at Urbana–Champaign, where he initiated the ILLIAC project, but left before its first computer was complete.\nSamuel went to IBM in Poughkeepsie, New York, in 1949, where he would conceive and carry out his most successful work. He is credited with one of the first software hash tables, and influencing early research in using transistors for computers at IBM. At IBM he made the first checkers program on IBM's first commercial computer, the IBM 701.  The program was a sensational demonstration of the advances in both hardware and skilled programming and caused IBM's stock to increase 15 points overnight.  His pioneering non-numerical programming helped shape the instruction set of processors, as he was one of the first to work with computers on projects other than computation. He was known for writing articles that made complex subjects easy to understand. He was chosen to write an introduction to one of the earliest journals devoted to computing in 1953.In 1966, Samuel retired from IBM and became a professor at Stanford University, where he worked the remainder of his life.  He worked with Donald Knuth on the TeX project, including writing some of the documentation. He continued to write software past his 88th birthday.He was given the Computer Pioneer Award by the IEEE Computer Society in 1987.\nHe died of complications from Parkinson's disease on July 29, 1990.\n\nComputer checkers (draughts) development\nSamuel is most known within the AI community for his groundbreaking work in computer checkers in 1959, and seminal research on machine learning, beginning in 1949. He graduated from MIT and taught at MIT and UIUC from 1946 to 1949. He believed teaching computers to play games was very fruitful for developing tactics appropriate to general problems, and he chose checkers as it is relatively simple though has a depth of strategy. The main driver of the machine was a search tree of the board positions reachable from the current state.  Since he had only a very limited amount of available computer memory, Samuel implemented what is now called alpha-beta pruning. Instead of searching each path until it came to the game's conclusion, Samuel developed a scoring function based on the position of the board at any given time.  This function tried to measure the chance of winning for each side at the given position.  It took into account such things as the number of pieces on each side, the number of kings, and the proximity of pieces to being “kinged”.  The program chose its move based on a minimax strategy, meaning it made the move that optimized the value of this function, assuming that the opponent was trying to optimize the value of the same function from its point of view.Samuel also designed various mechanisms by which his program could become better.  In what he called rote learning, the program remembered every position it had already seen, along with the terminal value of the reward function.  This technique effectively extended the search depth at each of these positions.  Samuel's later programs reevaluated the reward function based on input from professional games.  He also had it play thousands of games against itself as another way of learning.  With all of this work, Samuel's program reached a respectable amateur status, and was the first to play any board game at this high a level.  He continued to work on checkers until the mid-1970s, at which point his program achieved sufficient skill to challenge a respectable amateur.\n\nAwards\n1987. Computer Pioneer Award.For Adaptive non-numeric processing.\n\nSelected works\nComputing bit by bit, or Digital computers made easy (1953). Proceedings of the Institute of Radio Engineers 41, 1223-1230.\nSamuel, A. L. (2000). \"Some studies in machine learning using the game of checkers\". IBM Journal of Research and Development. IBM. 44: 206–226. doi:10.1147/rd.441.0206.Pioneer of machine learning.\nReprinted with an additional annotated game in Computers and Thought, edited by Edward Feigenbaum and Julian Feldman (New York: McGraw-Hill, 1963), 71-105.1983. First Grade TeX: A Beginner's TeX Manual. Stanford Computer Science Report STAN-CS-83-985 (November 1983).Senior member in TeX community.\n\nReferences\nExternal links\nCheckers AI: a look at Arthur Samuel's ideas on GitHub",
    "Artificial Intelligence: A Modern Approach": "Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020. It is used in over 1400 universities worldwide and has been called \"the most popular artificial intelligence textbook in the world\". It is considered the standard text in the field of artificial intelligence.The book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.  Programs in the book are presented in pseudo code with implementations in Java, Python, Lisp, JavaScript and Scala available online. There are also unsupported implementations in Prolog, C++, C#, and several other languages.\n\nEditions\n1st 1995, red cover\n2nd 2003, green cover\n3rd 2009, blue cover\n4th 2020, purple cover\n\nReferences\nExternal links\n\"AIMA\" (1st ed.). S Russell.\n\"AIMA\". Computer Science Division (4th ed.). Berkeley CoE.",
    "Artificial general intelligence": "An artificial general intelligence (AGI) is a type of hypothetical intelligent agent. The AGI concept is that it can learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies.\nThe timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern deep learning systems, such as GPT-4, are an early yet incomplete form of AGI or if new approaches are required.Contention exists over the potential for AGI to pose a threat to humanity; for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk.A 2020 survey identified 72 active AGI R&D projects spread across 37 countries.\n\nTerminology\nAGI is also known as strong AI, full AI, or general intelligent action. However, some academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.Related concepts include human-level AI, transformative AI, and superintelligence.\n\nCharacteristics\nVarious criteria for intelligence have been proposed (most famously the Turing test) but no definition is broadly accepted.\n\nIntelligence traits\nHowever, researchers generally hold that intelligence is required to do the following:\nreason, use strategy, solve puzzles, and make judgments under uncertainty;\nrepresent knowledge, including common sense knowledge;\nplan;\nlearn;\ncommunicate in natural language;and, if necessary, integrate these skills in completion of any given goal. \nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). However, no consensus holds that modern AI systems possess them to an adequate degree.\n\nPhysical traits\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:\nthe ability to sense (e.g. see, hear, etc.), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.)This includes the ability to detect and respond to hazard.\n\nMathematical formalisms\nA mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises “the ability to satisfy goals in a wide range of environments”. This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, is also called universal artificial intelligence.In 2015 Jan Leike and Marcus Hutter showed that Legg-Hutter intelligence - \"an agent’s ability to achieve goals in a wide range of environments\" - is measured with respect to \"a fixed Universal Turing Machine(UTM). AIXI is the most intelligent policy if it uses the same UTM\", a result which \"undermines all existing optimality properties for AIXI\".\n\nTests for testing human-level AGI\nSeveral tests meant to confirm human-level AGI have been considered, including:\nThe Turing Test (Turing)\nA machine and a human both converse unseen with a second human, who must evaluate which of the two is the machine, which passes the test if it can fool the evaluator a significant fraction of the time. Note: Turing does not prescribe what should qualify as intelligence, only that knowing that it is a machine should disqualify it. The AI Eugene Goostman achieved Turing's estimate of convincing 30% of judges that it was human in 2014.\nThe Robot College Student Test (Goertzel)\nA machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.\nThe Employment Test (Nilsson)\nA machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food, and marketing.\nThe Ikea test (Marcus)\nAlso known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak)\nA machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.\n\nAI-complete problems\nThere are many problems that may require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose-specific algorithm.AI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.AI-complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do; and for computer security to repel brute-force attacks.\n\nHistory\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".\n\nNarrow AI research\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as artificial neural networks and statistical machine learning. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018 development on this field was considered an emerging trend, and a mature stage was expected to happen in more than 10 years.\nMost mainstream AI researchers hope that strong AI can be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\nHowever, this is disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\n\nModern artificial general intelligence research\nThe term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course in AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\nAs of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open-ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences.\n\nFeasibility\nAs of 2022, AGI remains speculative. No such system has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\nA report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.\n\nTimescales\nIn the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007 the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to classify as a narrow AI system.In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.In 2023, the AI researcher Geoffrey Hinton stated that:\nThe idea that this stuff could actually get smarter than people — a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\n\nBrain simulation\nWhole brain emulation\nOne possible approach to achieving AGI is whole brain emulation: A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n\nEarly estimates\nFor low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nModelling the neurons in more detail\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.\n\nCurrent research\nSome research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 1011 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer-term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.Hans Moravec addressed the above arguments (\"brains are more complicated\", \"neurons have to be modeled in more detail\") in his 1997 paper \"When will computer hardware match the human brain?\". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.\nThe actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network (among about 1000 cells in total). The animal's neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons (partly on the molecular level), but the result cannot yet be called a total success.\n\nCriticisms of simulation-based approaches\nA fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in Second Life) as an option, but it is unknown whether this would be sufficient.\nDesktop computers using microprocessors capable of more than 109 cps (Kurzweil's non-standard unit \"computations per second\", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this:\n\nThe neuron model seems to be oversimplified (see next section).\nThere is insufficient understanding of higher cognitive processes to establish accurately what the brain's neural activity (observed using techniques such as functional magnetic resonance imaging) correlates with.\nEven if our understanding of cognition advances sufficiently, early simulation programs are likely to be very inefficient and will, therefore, need considerably more hardware.\nThe brain of an organism, while critical, may not be an appropriate boundary for a cognitive model. To simulate a bee brain, it may be necessary to simulate the body, and the environment. The Extended Mind thesis formalises this philosophical concept, and research into cephalopods demonstrated clear examples of a decentralized system.In addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.\n\nPhilosophical perspective\n\"Strong AI\" as defined in philosophy\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:\nStrong AI hypothesis: An artificial intelligence system can \"think\"—have \"a mind\" and \"consciousness\".\nWeak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two very different things.\nIn contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\". This is not the same as Searle's strong AI, unless you assume that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.\n\nConsciousness\nOther aspects of the human mind besides intelligence are relevant to the concept of strong AI, and these play a major role in science fiction and the ethics of artificial intelligence:\n\nconsciousness: To have subjective experience and thought.\nself-awareness: To be aware of oneself as a separate individual, especially to be aware of one's own thoughts.\nsentience: The ability to \"feel\" perceptions or emotions subjectively.\nsapience: The capacity for wisdom.These traits have a moral dimension, because a machine with this form of strong AI may have rights, analogous to the rights of non-human animals. Preliminary work has been conducted on integrating full ethical agents with existing legal and social frameworks, focusing on the legal position and rights of 'strong' AI. Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity.It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is possible that some of these traits naturally emerge from a fully intelligent machine. It is also possible that people will ascribe these properties to machines once they begin to act in a way that is clearly intelligent.\n\nArtificial consciousness research\nAlthough the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.\n\nResearch challenges\nProgress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks.A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes, \"No computer will be creative unless it can simulate all the nuances of human emotion.\"\n\nBenefits\nAGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.\nAGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.\n\nRisks\nPotential threat to human existence\nThe thesis that AI poses an existential risk for humans, and that this risk needs much more attention than it currently gets, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not \"understand why some people are not concerned\", and Hawking criticized widespread indifference in his 2014 editorial: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here–we'll leave the lights on?' Probably not–but this is more or less what is happening with AI.\nThe fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. Additional intelligence caused humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. The gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer:\nSuppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.\nA 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats: \"AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks\".Many scholars who are concerned about existential risk advocate (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation-state, resulting in AGI-enabled warfare, and in the case of AI misalignment, AGI-directed warfare, potentially against all humanity.The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then-current machines were in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares: \"I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way.\" Former Baidu Vice President and Chief Scientist Andrew Ng said in 2015 that worrying about AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"\n\nMass unemployment\nResearchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\nAccording to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:\nEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk considers that the automation of society will require governments to adopt a universal basic income.\n\nSee also\nNotes\nReferences\nSources\nFurther reading\nEka Roivainen, \"AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone\", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. \"Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.\"\n\nExternal links\nThe AGI portal maintained by Pei Wang\nThe Genesis Group at MIT's CSAIL – Modern research on the computations that underlay human intelligence\nOpenCog – open source project to develop a human-level AI\nSimulating logical human thought\nWhat Do We Know about AI Timelines? – Literature review",
    "Artificial immune system": "In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.\n\nDefinition\nThe field of artificial immune systems (AIS) is concerned with abstracting the structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. AIS is a sub-field of biologically inspired computing, and natural computation, with interests in machine learning and belonging to the broader field of artificial intelligence.\n\nArtificial immune systems (AIS) are adaptive systems, inspired by theoretical immunology and observed immune functions, principles and models, which are applied to problem solving.\nAIS is distinct from computational immunology and theoretical biology that are concerned with simulating immunology using computational and mathematical models towards better understanding the immune system, although such models initiated the field of AIS and continue to provide a fertile ground for inspiration. Finally, the field of AIS is not concerned with the investigation of the immune system as a substrate for computation, unlike other fields such as DNA computing.\n\nHistory\nAIS emerged in the mid-1980s with articles authored by Farmer, Packard and Perelson (1986) and Bersini and Varela (1990) on immune networks. However, it was only in the mid-1990s that AIS became a field in its own right. Forrest et al. (on negative selection) and Kephart et al. published their first papers on AIS in 1994, and Dasgupta conducted extensive studies on Negative Selection Algorithms. Hunt and Cooke started the works on Immune Network models in 1995; Timmis and Neal continued this work and made some improvements. De Castro & Von Zuben's and Nicosia & Cutello's work (on clonal selection) became notable in 2002. The first book on Artificial Immune Systems was edited by Dasgupta in 1999.\nCurrently, new ideas along AIS lines, such as danger theory and algorithms inspired by the innate immune system, are also being explored. Although some believe that these new ideas do not yet offer any truly 'new' abstract, over and above existing AIS algorithms. This, however, is hotly debated, and the debate provides one of the main driving forces for AIS development at the moment. Other recent developments involve the exploration of degeneracy in AIS models, which is motivated by its hypothesized role in open ended learning and evolution.Originally AIS set out to find efficient abstractions of processes found in the immune system but, more recently, it is becoming interested in modelling the biological processes and in applying immune algorithms to bioinformatics problems.\nIn 2008, Dasgupta and Nino  published a textbook on immunological computation which presents a compendium of up-to-date work related to immunity-based techniques and describes a wide variety of applications.\n\nTechniques\nThe common techniques are inspired by specific immunological theories that explain the function and behavior of the mammalian adaptive immune system.\n\nClonal selection algorithm: A class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation. These algorithms focus on the Darwinian attributes of the theory where selection is inspired by the affinity of antigen–antibody interactions, reproduction is inspired by cell division, and variation is inspired by somatic hypermutation. Clonal selection algorithms are most commonly applied to optimization and pattern recognition domains, some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator.\nNegative selection algorithm: Inspired by the positive and negative selection processes that occur during the maturation of T cells in the thymus called T cell tolerance. Negative selection refers to the identification and deletion (apoptosis) of self-reacting cells, that is T cells that may select for and attack self tissues. This class of algorithms are typically used for classification and pattern recognition problem domains where the problem space is modeled in the complement of available knowledge. For example, in the case of an anomaly detection domain the algorithm prepares a set of exemplar pattern detectors trained on normal (non-anomalous) patterns that model and detect unseen or anomalous patterns.\nImmune network algorithms: Algorithms inspired by the idiotypic network theory proposed by Niels Kaj Jerne that describes the regulation of the immune system by anti-idiotypic antibodies (antibodies that select for other antibodies). This class of algorithms focus on the network graph structures involved where antibodies (or antibody producing cells) represent the nodes and the training algorithm involves growing or pruning edges between the nodes based on affinity (similarity in the problems representation space). Immune network algorithms have been used in clustering, data visualization, control, and optimization domains, and share properties with artificial neural networks.\nDendritic cell algorithms: The dendritic cell algorithm (DCA) is an example of an immune inspired algorithm developed using a multi-scale approach. This algorithm is based on an abstract model of dendritic cells (DCs). The DCA is abstracted and implemented through a process of examining and modeling various aspects of DC function, from the molecular networks present within the cell to the behaviour exhibited by a population of cells as a whole. Within the DCA information is granulated at different layers, achieved through multi-scale processing.\n\nSee also\nBiologically inspired computing\nComputational immunology\nComputational intelligence\nEvolutionary computation\nImmunocomputing\nNatural computation\nSwarm intelligence\nLearning classifier system\nRule-based machine learning\n\nNotes\nReferences\nJ.D. Farmer, N. Packard and A. Perelson, (1986) \"The immune system, adaptation and machine learning\", Physica D, vol. 2, pp. 187–204\nH. Bersini, F.J. Varela, Hints for adaptive problem solving gleaned from immune networks. Parallel Problem Solving from Nature, First Workshop PPSW 1, Dortmund, FRG, October, 1990.\nD. Dasgupta (Editor), Artificial Immune Systems and Their Applications, Springer-Verlag, Inc. Berlin, January 1999, ISBN 3-540-64390-7\nV. Cutello and G. Nicosia (2002) \"An Immunological Approach to Combinatorial Optimization Problems\" Lecture Notes in Computer Science, Springer vol. 2527, pp. 361–370.\nL. N. de Castro and F. J. Von Zuben, (1999) \"Artificial Immune Systems: Part I -Basic Theory and Applications\", School of Computing and Electrical Engineering, State University of Campinas, Brazil, No. DCA-RT 01/99.\nS. Garrett (2005) \"How Do We Evaluate Artificial Immune Systems?\" Evolutionary Computation, vol. 13, no. 2, pp. 145–178. http://mitpress.mit.edu/journals/pdf/EVCO_13_2_145_0.pdf Archived 2011-06-29 at the Wayback Machine\nV. Cutello, G. Nicosia, M. Pavone, J. Timmis (2007) An Immune Algorithm for Protein Structure Prediction on Lattice Models, IEEE Transactions on Evolutionary Computation, vol. 11, no. 1, pp. 101–117. https://web.archive.org/web/20120208130715/http://www.dmi.unict.it/nicosia/papers/journals/Nicosia-IEEE-TEVC07.pdf\nVillalobos-Arias M., Coello C.A.C., Hernández-Lerma O. (2004) Convergence Analysis of a Multiobjective Artificial Immune System Algorithm. In: Nicosia G., Cutello V., Bentley P.J., Timmis J. (eds) Artificial Immune Systems. ICARIS 2004. Lecture Notes in Computer Science, vol 3239. Springer, Berlin, Heidelberg. DOI https://doi.org/10.1007/978-3-540-30220-9_19\n\nExternal links\nAISWeb: The Online Home of Artificial Immune Systems Information about AIS in general and links to a variety of resources including ICARIS conference series, code, teaching material and algorithm descriptions.\nARTIST: Network for Artificial Immune Systems Provides information about the UK AIS network, ARTIST. It provides technical and financial support for AIS in the UK and beyond, and aims to promote AIS projects.\nComputer Immune Systems Group at the University of New Mexico led by Stephanie Forrest.\nAIS: Artificial Immune Systems Group at the University of Memphis led by Dipankar Dasgupta.\nIBM Antivirus Research Early work in AIS for computer security.",
    "Artificial intelligence": "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of human beings or animals. AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), and competing at the highest level in strategic games (such as chess and Go).Artificial intelligence, that has emerged as an academic discipline in 1956, went through multiple cycles of unsubstantiated optimism (aka hype) followed by failures and subsequently losing funds in several AI winters.Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\n\nApproaches\nReplacing the initial goal (i.e. using highly formalized rule-based, step-by-step reasoning) with the seemingly less ambitious goal of probabilistic learning from experience has been backed up by evidence that humans most of the time use fast (either instinctive or \"intuitive\", i.e. probabilistic) judgments, too.\n\nFailure of rule-based approach\nRule-based step-by-step approach that was successful in highly formalized circumstances, such as in solving puzzles or making logical deductions, failed to provide useful results in more probabilistic, real-world circumstances.Neither methods for dealing with uncertain or incomplete information, such as used in probability and rational-choice economics, nor the computation power was available until the late 1980s and 1990s.Deep learning, especially large language models, has only begun to be possible using faster hardware, such as GPUs in late 2010s and early 2020s, as a result of both hardware improvements (faster computers, graphics processing units, cloud computing)  \nand access to large amounts of data (including curated datasets, such as ImageNet and vast textual corpora).\n\nHighly formalized knowledge representation\nInspired by highly formalized mathematics,  the researchers initially attempted to represent specific domains, such as objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge  (what we know about what other people know); and default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing), in a highly formalized way.\n\nLearning\nAfter the failure of AI to deliver results useful in real-world circumstances, a more pragmatic research, known as \"machine learning\", began to focus on probabilistic approaches that were previously not a focus of the field.Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and regression. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when the knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\nNatural language processing\nNatural language processing begun in the 1960s as part of symbolic AI, using highly formalized syntax to translate the human sentences into logical ones. As already mentioned, this approach failed to produce useful applications, due to the intractability of logic and the vastness of knowledge.Only with advent of deep learning approaches in late 2010s and early 2020s,, especially the generative pre-trained transformers (or \"GPTs\"), the models began to generate coherent text, and by 2023 begun to get human-level scores on the bar exam, SAT, GRE, and many other real-world applications.\n\nPerception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals,active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition,image classification,facial recognition, object recognition,robotic perception,\namong others.\n\nSocial intelligence\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.\nFor example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.\n\nGeneral intelligence\nA machine with general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\nTools\nLogic\nLogic\nis used for highly formalized kinds of knowledge representation and problem-solving, but it can be applied to other areas as well,  although it is not effective in real-world circumstances where faster, probabilistic reasoning is required.\n\nSearch and optimization\nThe goal of learning, i.e. finding and applying useful solutions to a problem, can be achieved by intelligently searching through many possible solutions. \nHowever, because the search space (the number of places to search) quickly grows to astronomical numbers, such a search doesn't deliver results in a timely fashion. Often \"heuristics\" or \"rules of thumb\" are needed to prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. \n\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. \nEvolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). \nAlternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\nProbabilistic methods for uncertain reasoning\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,\nand information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\n\nClassifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g. \"if shiny then diamond\"), on one hand, and controllers (e.g. \"if diamond then pick up\"), on the other hand. Controllers can, however, perform also the classification of the conditions before choosing an action to be implemented.\nClassifiers, on the other hand, are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples, which makes them very useful. \nA machine learning classifier can use either statistical or artificial neural networks approaches, and its performance depends on the  dataset size, distribution of samples across classes, dimensionality, and the level of noise.\n\nArtificial neural networks\nModern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.\nOther learning techniques for neural networks are Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.The main categories of networks include feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events).\n\nDeep learning\nDeep learning\nuses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.\n\nSpecialized hardware and software\nIn late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow  software, had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.Historically, specialized languages, such as Lisp, Prolog, and others, had been used.\n\nApplications\nIn the late 2010s and early 2020s, machine learning applications found their use in search engines (such as Google Search),\ntargeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace), image labeling (used by Facebook, Apple's iPhoto and TikTok), spam filtering and chatbots (such as ChatGPT).\nIn the early 2020s, generative AI gained widespread prominence. ChatGPT, based on GPT-3, and other large language models, were tried by 14% of Americans adults.In 2023, the increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, foreign policy, or supply chain management.\nIn the 2010s, experimental applications were developed, such as DeepMind's model that could learn many diverse Atari games on its own and AlphaFold 2 (2020), which demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. Other applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.\n\nIntellectual property\nIn 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four. The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.\n\nHistory\nNot counting the ancient precursors of mechanical reasoning in mythology, philosophy, and mathematics, advances in the modern mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate both mathematical deduction and formal reasoning, which is known as the Church–Turing thesis. This, along with at the time new discoveries in cybernetics and information theory, led researchers to consider the possibility of using electronic components to simulate human  problem-solving.Written in 1943 by McCullouch and Pitts, the formal design for Turing-complete \"artificial neurons\" is generally recognized as the first paper in AI.More than a decade later, in 1956, the field of AI research was officially born at a workshop at Dartmouth College. The attendees and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".They have, however, underestimated the vastness and probabilistic nature of most human knowledge, implicit in the \"remaining\" tasks.Both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects. Additionally, Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks approach would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific machine learning problems. Robotics researchers, such as Rodney Brooks, rejected the discredited rule-based approaches and trained robots that learned to move in their environment based on sensory information, using reasonable guesses (known as \"soft computing\") to deal with uncertain sensory information.Outside of academic discipline AI, which abandoned artificial neural networks altogether from its toolbox until the 1990s, people from other disciplines, such as psychologist Geoffrey Hinton and David Rumelhart, kept it alive under the name \"connectionism\".In the 1990s, Yann LeCun successfully showed that  convolutional neural networks can recognize handwritten digits, which made the neural networks approach a \"safe choice\" again.In the late 1990s and early 2000s, advanced mathematical methods and specific solutions to specific problems has emerged as a new academic discipline named machine learning. Machine learning researchers produced verifiable results and collaborated with other fields, such as statistics, economics and mathematics, culminating in \ndeep learning. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing)  \nand access to large amounts of data (including curated datasets, such as ImageNet). \nDeep learning soon proved to be highly successful at a wide range of tasks and was adopted throughout the field. For many specific tasks, other methods were abandoned.The machine learning achievements made it safe for media and businesses to refer to them as \"AI\" again. \nThe number of software projects that use machine learning at Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects in 2015.In a 2017 survey, one in five companies reported they had incorporated \"AI\" in some offerings or processes\". \nThe amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. According to 'AI Impacts', about $50 billion annually was invested in \"AI\" around 2022 in the US alone and about 20% of new US Computer Science PhD graduates have specialized in \"AI\";\nabout 800,000 \"AI\"-related US job openings existed in 2022.In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\n\nPhilosophy\nDefining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"\nHe advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".\nHe devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks\"Russell and Norvig agree with Turing that AI must be defined in terms of \"acting\" and not \"thinking\". However, they are critical that the test compares machines to people. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world.\" Another AI founder, Marvin Minsky similarly defines it as \"the ability to solve hard problems\". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\nA definition that has also been adopted by Google – major practitionary in the field of AI.\nThis definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n\nEvaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\nSymbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.\nPhilosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.\nAlthough his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\nNeat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely only on incremental testing to see if they work. This issue was actively discussed in the 70s and 80s,\nbut eventually was seen as irrelevant. In the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed in 2003 as \"the victory of the neats\". However in 2020 they wrote \"deep learning may represent a resurgence of the scruffies\". Modern AI has elements of both.\n\nSoft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\nNarrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.\nGeneral intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\n\nMachine consciousness, sentience and mind\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers \"don't care about the [philosophy of AI] – as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\nConsciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\nComputationalism and functionalism\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\nSearle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\n\nRobot rights\nIf a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.\nAny hypothetical robot rights would lie on a spectrum with animal rights and human rights.\nThis issue has been considered in fiction for centuries,\nand is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.\n\nFuture\nSuperintelligence\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.\nIts intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the \"singularity\".\nBecause it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\n\nRisks\nTechnological unemployment\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than social policy) creates unemployment (as opposed to redundancies).Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\n\nBad actors and weaponized AI\nAI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.\n\nAlgorithmic bias\nAI programs can become biased after learning from real-world data. It may not be introduced by the system designers but learned by the program, and thus the programmers may not be aware that the bias exists.\nBias can be inadvertently introduced by the way training data is selected and by the way a model is deployed. \nIt can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.Health equity issues may also be exacerbated when many-to-many mapping is done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating, CV screening, hiring and applications for public housing.At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\nExistential risk\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as the physicist Stephen Hawking puts it, \"spell the end of the human race\". According to the philosopher Nick Bostrom, for almost any goals that a sufficiently intelligent AI may have, it is instrumentally incentivized to protect itself from being shut down and to acquire more resources, as intermediary steps to better achieve these goals. Sentience or emotions are then not required for an advanced AI to be dangerous. In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". The political scientist Charles T. Rubin argued that \"any sufficiently advanced benevolence may be indistinguishable from malevolence\" and warned that we should not be confident that intelligent machines will by default treat us favorably.The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, Elon Musk have expressed concern about existential risk from AI. In 2023, AI pioneers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Sam Altman issued the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\"; some others such as Yann LeCun consider this to be unfounded. Mark Zuckerberg said that AI will \"unlock a huge amount of positive things\", including curing diseases and improving the safety of self-driving cars. Some experts have argued that the risks are too distant in the future to warrant research, or that humans will be valuable from the perspective of a superintelligent machine. Rodney Brooks, in particular, said in 2014 that \"malevolent\" AI is still centuries away.\n\nCopyright\nIn order to leverage as large a dataset as is feasible, generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under a rationale of \"fair use\". Experts disagree about how well, and under what circumstances, this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".\n\nEthical machines\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach's \"artificial moral agents\"\nand Stuart J. Russell's three principles for developing provably beneficial machines.\n\nRegulation\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.\nThe regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.\nBetween 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.\nMost EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.\nThe Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\n\nIn fiction\nThought-capable artificial beings have appeared as storytelling devices since antiquity,\nand have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;\nwhile almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\nSee also\nAI safety – Research area on making AI safe and beneficial\nAI alignment – Conformance to the intended objective\nArtificial intelligence in healthcare – Machine-learning algorithms and software in the analysis, presentation, and comprehension of complex medical and health care data\nArtificial intelligence arms race – Arms race for the most advanced AI-related technologies\nArtificial intelligence detection software\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Technology-enabled automation of complex business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\nOperations research – Discipline concerning the application of advanced analytical methods\nRobotic process automation – Form of business process automation technology\nSynthetic intelligence – Alternate term for or form of artificial intelligence\nUniversal basic income – Welfare system of unconditional income\nWeak artificial intelligence – Form of artificial intelligence\nData sources – The list of data sources for study and research\n\nExplanatory notes\nReferences\nAI textbooks\nThe two most widely used textbooks in 2023. (See the Open Syllabus). \n\nRussell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0070087705.These were the four the most widely used AI textbooks in 2008:\n\nHistory of AI\nOther sources\nFurther reading\nExternal links\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).",
    "Artificial intelligence in healthcare": "Artificial intelligence in healthcare is an overarching term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to mimic human cognition in the analysis, presentation, and comprehension of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease. Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data.\nThe primary aim of health-related AI applications is to analyze relationships between clinical data and patient outcomes. AI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care. What differentiates AI technology from traditional technologies in healthcare is the ability to gather larger and more diverse data, process it, and produce a well-defined output to the end-user. AI does this through machine learning algorithms and deep learning. These processes can recognize patterns in behavior and create their own logic. To gain useful insights and predictions, machine learning models must be trained using extensive amounts of input data. AI algorithms behave differently from humans in two ways: (1) algorithms are literal: once a goal is set, the algorithm learns exclusively from the input data and can only understand what it has been programmed to do, (2) and some deep learning algorithms are black boxes; algorithms can predict with extreme precision, but offer little to no comprehensible explanation to the logic behind its decisions aside from the data and type of algorithm used.As widespread use of AI in healthcare is relatively new, research is ongoing into its application in various fields of medicine and industry. Additionally, greater consideration is being given to the unprecedented ethical concerns related to its practice such as data privacy, automation of jobs, and representation biases. Furthermore, new technologies brought about by AI in healthcare are often resisted by healthcare leaders, leading to slow and erratic adoption.\n\nHistory\nResearch in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral. While it was designed for applications in organic chemistry, it provided the basis for a subsequent system MYCIN, considered one of the most significant early uses of artificial intelligence in medicine. MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians. Approaches involving fuzzy set theory, Bayesian networks, and artificial neural networks, have been applied to intelligent computing systems in healthcare.\nMedical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: \n\nImprovements in computing power resulting in faster data collection and data processing\nGrowth of genomic sequencing databases\nWidespread implementation of electronic health record systems\nImprovements in natural language processing and computer vision, enabling machines to replicate human perceptual processes\nEnhanced the precision of robot-assisted surgery\nIncreased tree-based machine learning models that allow flexibility in establishing health predictors\nImprovements in deep learning techniques and data logs in rare diseasesAI algorithms can also be used to analyze large amounts of data through electronic health records for disease prevention and diagnosis. Medical institutions such as The Mayo Clinic, Memorial Sloan Kettering Cancer Center, and the British National Health Service, have developed AI algorithms for their departments. Large technology companies such as IBM and Google, have also developed AI algorithms for healthcare. Additionally, hospitals are looking to AI software to support operational initiatives that increase cost saving, improve patient satisfaction, and satisfy their staffing and workforce needs. Currently, the United States government is investing billions of dollars to progress the development of AI in healthcare. Companies are developing technologies that help healthcare managers improve business operations through increasing utilization, decreasing patient boarding, reducing length of stay and optimizing staffing levels.\n\nClinical applications\nCardiovascular\nArtificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool, though few studies have directly compared the accuracy of machine learning models to clinician diagnostic ability. Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment for acute coronary syndrome. Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital. Another growing area of research is the utility of AI in classifying heart sounds and diagnosing valvular disease. Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data on social determinants of health as they pertain to cardiovascular disease.A key limitation of studies to date is that they have not compared algorithmic performance to humans. Two exceptions include showing AI is noninferior to humans in interpretation of cardiac echocardiograms and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.\n\nDermatology\nDermatology is an imaging abundant speciality and the development of deep learning has been strongly tied to image processing. Therefore, there is a natural fit between the dermatology and deep learning. There are 3 main imaging types in dermatology: contextual images, macro images, micro images. For each modality, deep learning showed great progress. Han et al. showed keratinocytic skin cancer detection from face photographs. Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images. Noyan et al. demonstrated a convolutional neural network that achieved 94% accuracy at identifying skin cells from microscopic Tzanck smear images. A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.Recent advances have suggested the use of AI to describe and evaluate the outcome of maxillo-facial surgery or the assessment of cleft palate therapy in regard to facial attractiveness or age appearance.In 2018, a paper published in the journal Annals of Oncology mentioned that skin cancer could be detected more accurately by an artificial intelligence system (which used a deep learning convolutional neural network) than by dermatologists. On average, the human dermatologists accurately detected 86.6% of skin cancers from the images, compared to 95% for the CNN machine.\n\nGastroenterology\nAI can play a role in various facets of the field of gastroenterology. Endoscopic exams such as esophagogastroduodenoscopies (EGD) and colonoscopies rely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of early gastric cancer have shown sensitivity close to expert endoscopists.\n\nInfectious diseases\nAI has shown potential in both the laboratory and clinical spheres of infectious disease medicine. As the novel coronavirus ravages through the globe, the United States is estimated to invest more than $2 billion in AI-related healthcare research by 2025, more than 4 times the amount spent in 2019 ($463 million). While neural networks have been developed to rapidly and accurately detect a host response to COVID-19 from mass spectrometry samples, a scoping review of the literature found few examples of AI being used directly in clinical practice during the COVID-19 pandemic itself. Other applications include support-vector machines identifying antimicrobial resistance, machine learning analysis of blood smears to detect malaria, and improved point-of-care testing of Lyme disease based on antigen detection. Additionally, AI has been investigated for improving diagnosis of meningitis, sepsis, and tuberculosis, as well as predicting treatment complications in hepatitis B and hepatitis C patients.\n\nMusculoskeletal\nAI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients.  Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients’ pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.\n\nOncology\nAI has been explored for use in cancer diagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics. Through its ability to translate images to mathematical sequences, AI has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides. In January 2020, researchers demonstrated an AI system, based on a Google DeepMind algorithm, capable of surpassing human experts in breast cancer detection. In July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date in identifying prostate cancer, with 98% sensitivity and 97% specificity.\n\nOphthalmology\nThe eyes are not to be left out in the use of artificial intelligence-enhanced technology to aid in the screening of eye disease. In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm.  Moreover, AI technology may be utilized to further improve \"diagnosis rates\" because of the potential to decrease detection time.\n\nPathology\nFor many diseases, pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods of digital pathology allows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B, gastric cancer, and colorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes. AI is well-suited for use in low-complexity pathological analysis of large-scale screening samples, such as colorectal or breast cancer screening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis. Several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists, and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone. Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years, though savings attributed to AI specifically have not yet been widely researched. The use of augmented and virtual reality could prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review. AI also has the potential to identify histological findings at levels beyond what the human eye can see, and has shown the ability to utilize genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer. One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlled trials in determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.\n\nPrimary care\nPrimary care has become one key development area for AI technologies. AI in primary care has been used for supporting decision making, predictive modelling, and business analytics. There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.\n\nPsychiatry\nIn psychiatry, AI applications are still in a phase of proof-of-concept. Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes, chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.Challenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017. Such applications outside the healthcare system raise various professional, ethical and regulatory questions. Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.\n\nRadiology\nAI is being studied within the field of radiology to detect and diagnose diseases through Computerized Tomography (CT) and Magnetic Resonance (MR) Imaging. It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers. Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated. AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality, and automatically assessing image quality. Further research investigating the use of AI in nuclear medicine focuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies.\n\nSystems applications\nDisease diagnosis\nAn article by Jiang, et al. (2017) demonstrated that there are several types of AI techniques that have been used for a variety of different diseases, such as support vector machines, neural networks, and decision trees. Each of these techniques is described as having a \"training goal\" so \"classifications agree with the outcomes as much as possible…\".\nTo demonstrate some specifics for disease diagnosis/classification there are two different techniques used in the classification of these diseases including using \"Artificial Neural Networks (ANN) and Bayesian Networks (BN)\". It was found that ANN was better and could more accurately classify diabetes and CVD.\nThrough the use of Medical Learning Classifiers (MLC's), Artificial Intelligence has been able to substantially aid doctors in patient diagnosis through the manipulation of mass Electronic Health Records (EHR's). Medical conditions have grown more complex, and with a vast history of electronic medical records building, the likelihood of case duplication is high. Although someone today with a rare illness is less likely to be the only person to have had any given disease, the inability to access cases from similarly symptomatic origins is a major roadblock for physicians. The implementation of AI to not only help find similar cases and treatments, such as through early predictors of Alzheimer’s disease and dementias, but also factor in chief symptoms and help the physicians ask the most appropriate questions helps the patient receive the most accurate diagnosis and treatment possible.Recent developments in statistical physics, machine learning, and inference algorithms are being explored for their potential in improving medical diagnostic approaches. Combining the skills of medical professionals and machines can help overcome decision-making weaknesses in medical practice. To do so, one needs precise disease definitions and a probabilistic analysis of symptoms and molecular profiles. Physicists have been studying similar problems for years, utilizing microscopic elements and their interactions to extract macroscopic states of various physical systems. Physics inspired machine learning approaches can thus be applied to study disease processes and to perform biomarker analysis.\n\nTelemedicine\nThe increase of telemedicine, the treatment of patients remotely, has shown the rise of possible AI applications. AI can assist in caring for patients remotely by monitoring their information through sensors. A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.Another application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.Since the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations. Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal. Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.\n\nElectronic health records\nElectronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, the next step is to use artificial intelligence to interpret the records and provide new information to physicians.One application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms. For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences. NLP algorithms consolidate these differences so that larger datasets can be analyzed. Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read. Other applications use concept processing to analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.Beyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record and predict a risk for a disease based on their previous information and family history. One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts. This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses. Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease. Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time. One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70–72% accuracy in predicting individualized treatment response. These methods are helpful due to the fact that the amount of online health records doubles every five years. Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.\n\nDrug Interactions\nImprovements in natural language processing led to the development of algorithms to identify drug-drug interactions in medical literature. Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken. To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers at Carlos III University assembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms. Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.  Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.Other algorithms identify drug-drug interactions from patterns in user-generated content, especially electronic health records and/or adverse event reports. Organizations such as the FDA Adverse Event Reporting System (FAERS) and the World Health Organization's VigiBase allow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.\n\nIndustry\nThe trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms.\nA large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions. Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of \"data assessment, storage, management, and analysis technologies\" which are all crucial parts of the healthcare industry.The following are examples of large companies that have contributed to AI algorithms for use in healthcare:\n\nIBM's Watson Oncology is in development at Memorial Sloan Kettering Cancer Center and Cleveland Clinic. IBM is also working with CVS Health on AI applications in chronic disease treatment and with Johnson & Johnson on analysis of scientific papers to find new connections for drug development. In May 2017, IBM and Rensselaer Polytechnic Institute began a joint project entitled Health Empowerment by Analytics, Learning and Semantics (HEALS), to explore using AI technology to enhance healthcare.\nMicrosoft's Hanover project, in partnership with Oregon Health & Science University's Knight Cancer Institute, analyzes medical research to predict the most effective cancer drug treatment options for patients. Other projects include medical image analysis of tumor progression and the development of programmable cells.\nGoogle's DeepMind platform is being used by the UK National Health Service to detect certain health risks through data collected via a mobile app. A second project with the NHS involves the analysis of medical images collected from NHS patients to develop computer vision algorithms to detect cancerous tissues.\nTencent is working on several medical systems and services. These include AI Medical Innovation System (AIMIS), an AI-powered diagnostic medical imaging service; WeChat Intelligent Healthcare; and Tencent Doctorwork\nIntel's venture capital arm Intel Capital recently invested in startup Lumiata which uses AI to identify at-risk patients and develop care options.Neuralink has come up with a next-generation neuroprosthetic which intricately interfaces with thousands of neural pathways in the brain. Their process allows a chip, roughly the size of a quarter, to be inserted in the place of a chunk of a skull by a precision surgical robot to avoid accidental injury .Digital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).\nIFlytek launched a service robot \"Xiao Man\", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH (\"Cruzr\") and Softbank Robotics (\"Pepper\").\nThe Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India.\nWith the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies. Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well. Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances or in emotional distress.\n\nExpanding care to developing nations\nArtificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.  Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before. With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient. The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.\n\nRegulation\nWhile research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection. These challenges of the clinical use of AI have brought about a potential need for regulations.\n\nCurrently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR). The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States. In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.\nThe only agency that has expressed concern is the FDA. Bakul Patel, the Associate Center Director for Digital Health of the FDA, is quoted saying in May 2017:\n\"We're trying to get people who have hands-on development experience with a product's full life cycle. We already have some scientists who know artificial intelligence and machine learning, but we want complementary people who can look forward and see how this technology will evolve.\"\n\nUnited Nations (WHO/ITU)\nThe joint ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) has built a platform - known as the ITU-WHO AI for Health Framework - for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.\n\nUS FDA\nIn January 2021, the US FDA published a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan. This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.\nAccording to the U.S. Department of Health and Human Services, the Office for Civil Rights (OCR) has issued guidance on the ethical use of AI in healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the \"implementation of the HHS AI strategy\". The OCR also has issued rules and regulations to protect the privacy of individuals’ health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals’ privacy and ethical issues related to AI in healthcareThe U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processesThe European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency. With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.\n\nEthical concerns\nData collection\nIn order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology. The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare.\n\nAutomation\nAccording to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years. However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.Automation can provide benefits alongside doctors as well. It is expected that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not. AI will likely not completely replace healthcare workers but rather give them more time to attend to their patients. AI may avert healthcare worker burnout and cognitive overload\nAI will ultimately help contribute to the progression of societal goals which include better communication, improved quality of healthcare, and autonomy.Recently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand\n\nBias\nSince AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care. A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping.There can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.  Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. White males are overly represented in medical data sets. Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations. Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients. In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult. However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.\nA final source of bias, which has been called “label choice bias,” arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely-used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients. Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program\n\nSee also\nReferences\n\n\n== Further reading ==",
    "Artificial intelligence systems integration": "The core idea of artificial intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.\nMost artificial intelligence systems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that of speech recognition. However, in recent years, there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\n\nIntegration Focus\nThe focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.\n\nChallenges & solutions\nCollaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.\nThe outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:\n\nSpeech synthesis\nFreeTTS from CMU\nSpeech recognition\nSphinx from CMU\nLogical reasoning\nOpenCyc from Cycorp\nOpen Mind Common Sense Net from MITWith the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures.\nMany online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with any ease.\n\nMethodologies\nConstructionist Design Methodology\nThe Constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM.\n\nExamples of Integrated Systems\nASIMO, Honda's humanoid robot, and QRIO, Sony's version of a humanoid robot.\nCog, M.I.T. humanoid robot project under the direction of Rodney Brooks.\nAIBO, Sony's robot dog, integrates vision, hearing and motorskills.\nTOPIO, TOSY's humanoid robot can play ping-pong with human\n\nSee also\nHybrid intelligent system, systems that combine the methods of traditional symbolic AI & that of Computational intelligence.\nNeurosymbolic AI\nHumanoid robots utilize systems integration intensely.\nConstructionist design methodology\nCognitive architectures\n\nReferences\nNotes\nConstructionist Design Methodology, published in A.I. magazine\nMissionEngine: Multi-system integration using Python in the Tactical Language Project\n\nExternal links\nCOG, a humanoid robot at M.I.T.\nThe Open Knowledge Initiative Library",
    "Artificial neural network": "Artificial neural networks (ANNs, also shortened to neural networks (NNs) or neural nets) are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.\n\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n\nTraining\nNeural networks learn (or are trained) by processing examples, each of which contains a known \"input\" and \"result\", forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This difference is the error. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output that is increasingly similar to the target output. After a sufficient number of these adjustments, the training can be terminated based on certain criteria. This is a form of supervised learning.\nSuch systems \"learn\" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers, and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.\n\nHistory\nThe simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artificial neural network, funded by the United States Office of Naval Research.Some say that research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However, by the time this book came out, methods for training multilayer perceptrons (MLPs) were already known.\nThe first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling. \nThe first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.\nIn computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision.\nThe backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as \nthe reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt, but he did not have an implementation of this procedure, although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory. \nIn 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. \nIn 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart, Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988, Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989, Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. \nIn 1992, max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. \nLeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In the 1980s, backpropagation did not work well for deep FNNs and RNNs. To overcome this problem, Juergen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a chunker solved a deep learning task whose depth exceeded 1000.In 1992, Juergen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern.\nThe modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper \"Attention Is All You Need.\" \nIt combines this with a softmax operator and a projection matrix.\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.In 1991, Juergen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\"\nIn 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.\nExcellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras, Timo Aila, Samuli  Laine, and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\nSepp Hochreiter's diploma thesis (1991) was called \"one of the most important documents in the history of machine learning\" by his supervisor Juergen Schmidhuber. Hochreiter identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. This led to the deep learning method called long short-term memory (LSTM), published in Neural Computation (1997). LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used the LSTM principle to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012).\n\nModels\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\n\nArtificial neurons\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\n\nOrganization\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\n\nHyperparameter\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\n\nLearning\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\n\nLearning rate\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\n\nCost function\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).\n\nBackpropagation\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.The following formulas update the weights using the computed gradient and learning rate, α:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  Weights 1\n                \n              \n              \n                \n                :\n                \n                  w\n                  \n                    \n                      \n                        (\n                      \n                    \n                    1\n                    \n                      \n                        )\n                      \n                    \n                  \n                \n                :=\n                \n                  w\n                  \n                    \n                      \n                        (\n                      \n                    \n                    1\n                    \n                      \n                        )\n                      \n                    \n                  \n                \n                −\n                \n                  \n                    1\n                    m\n                  \n                \n                ×\n                α\n                Δ\n                \n                  w\n                  \n                    \n                      \n                        (\n                      \n                    \n                    1\n                    \n                      \n                        )\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                \n                  Weights 2\n                \n              \n              \n                \n                :\n                \n                  w\n                  \n                    \n                      \n                        (\n                      \n                    \n                    2\n                    \n                      \n                        )\n                      \n                    \n                  \n                \n                :=\n                \n                  w\n                  \n                    \n                      \n                        (\n                      \n                    \n                    2\n                    \n                      \n                        )\n                      \n                    \n                  \n                \n                −\n                \n                  \n                    1\n                    m\n                  \n                \n                ×\n                α\n                Δ\n                \n                  w\n                  \n                    \n                      \n                        (\n                      \n                    \n                    2\n                    \n                      \n                        )\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Weights 1}}&:w^{{\\bigl (}1{\\bigr )}}:=w^{{\\bigl (}1{\\bigr )}}-{1 \\over m}\\times \\alpha \\Delta w^{{\\bigl (}1{\\bigr )}}\\\\{\\text{Weights 2}}&:w^{{\\bigl (}2{\\bigr )}}:=w^{{\\bigl (}2{\\bigr )}}-{1 \\over m}\\times \\alpha \\Delta w^{{\\bigl (}2{\\bigr )}}\\end{aligned}}}\n\nLearning paradigms\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\n\nSupervised learning\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\nUnsupervised learning\nIn unsupervised learning, input data is given along with the cost function, some function of the data \n  \n    \n      x\n    \n    \\textstyle x\n   and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n  \n    \n      f\n      (\n      x\n      )\n      =\n      a\n    \n    \\textstyle f(x)=a\n   where \n  \n    \n      a\n    \n    \\textstyle a\n   is a constant and the cost \n  \n    \n      C\n      =\n      E\n      [\n      (\n      x\n      −\n      f\n      (\n      x\n      )\n      \n        )\n        \n          2\n        \n      \n      ]\n    \n    \\textstyle C=E[(x-f(x))^{2}]\n  . Minimizing this cost produces a value of \n  \n    \n      a\n    \n    \\textstyle a\n   that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n  \n    \n      x\n    \n    \\textstyle x\n   and \n  \n    \n      f\n      (\n      x\n      )\n    \n    \\textstyle f(x)\n  , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\nReinforcement learning\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\nFormally the environment is modeled as a Markov decision process (MDP) with states \n  \n    \n      \n        \n          s\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          s\n          \n            n\n          \n        \n      \n      ∈\n      S\n    \n    \\textstyle {s_{1},...,s_{n}}\\in S\n   and actions \n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          a\n          \n            m\n          \n        \n      \n      ∈\n      A\n    \n    \\textstyle {a_{1},...,a_{m}}\\in A\n  . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n  \n    \n      P\n      (\n      \n        c\n        \n          t\n        \n      \n      \n        |\n      \n      \n        s\n        \n          t\n        \n      \n      )\n    \n    \\textstyle P(c_{t}|s_{t})\n  , the observation distribution \n  \n    \n      P\n      (\n      \n        x\n        \n          t\n        \n      \n      \n        |\n      \n      \n        s\n        \n          t\n        \n      \n      )\n    \n    \\textstyle P(x_{t}|s_{t})\n   and the transition distribution \n  \n    \n      P\n      (\n      \n        s\n        \n          t\n          +\n          1\n        \n      \n      \n        |\n      \n      \n        s\n        \n          t\n        \n      \n      ,\n      \n        a\n        \n          t\n        \n      \n      )\n    \n    \\textstyle P(s_{t+1}|s_{t},a_{t})\n  , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\nSelf-learning\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n\n  In situation s perform action a;\n  Receive consequence situation s';\n  Compute emotion of being in consequence situation v(s');\n  Update crossbar memory w'(a,s) = w(a,s) + v(s').\n\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\n\nNeuroevolution\nNeuroevolution can create neural network topologies and weights using evolutionary computation. With modern enhancements, neuroevolution is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\nStochastic neural network\nStochastic neural networks originating from  Sherrington–Kirkpatrick models  are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\n\nOther\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\n\nModes\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n\nTypes\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\nSome of the main breakthroughs include: convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; long short-term memory avoid the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\n\nNetwork design\nNeural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\nDesign issues include deciding the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc. ).\n\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:\n\nUse\nUsing artificial neural networks requires an understanding of their characteristics.\n\nChoice of model: This depends on the data representation and the application. Overly complex models are slow learning.\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.ANN capabilities fall within the following broad categories:\nFunction approximation, or regression analysis, including time series prediction, fitness approximation and modeling.\nClassification, including pattern and sequence recognition, novelty detection and sequential decision making.\nData processing, including filtering, clustering, blind source separation and compression.\nRobotics, including directing manipulators and prostheses.\n\nApplications\nBecause of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sensor data analysis, sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. ex-ante models for specific financial long-run forecasts and artificial financial markets), data mining, visualization, machine translation, social network filtering and e-mail spam filtering. ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\n\nTheoretical properties\nComputational power\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\nCapacity\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form.  As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\n\nConvergence\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\nAnother issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\n\nGeneralization and statistics\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.\nThe second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\nThe softmax activation function is:\n\n  \n    \n      \n        y\n        \n          i\n        \n      \n      =\n      \n        \n          \n            e\n            \n              \n                x\n                \n                  i\n                \n              \n            \n          \n          \n            \n              ∑\n              \n                j\n                =\n                1\n              \n              \n                c\n              \n            \n            \n              e\n              \n                \n                  x\n                  \n                    j\n                  \n                \n              \n            \n          \n        \n      \n    \n    y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}\n\nCriticism\nTraining\nA common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\n\nTheory\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney commented that, as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\". One response to Dewdney is that neural networks handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\nHardware\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.\nSchmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\nPractical counterexamples\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\n\nHybrid approaches\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\n\nGallery\nSee also\nNotes\nReferences\n\n\n== Bibliography ==",
    "Artificial neuron": "An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network. The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often  monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.The artificial neuron transfer function should not be confused with a linear system's transfer function.\nArtificial neurons can also refer to artificial cells in neuromorphic engineering (see below) that are similar to natural physical neurons.\n\nBasic structure\nFor a given artificial neuron k, let there be m + 1 inputs with signals x0 through xm and weights wk0 through wkm. Usually, the x0 input is assigned the value +1, which makes it a bias input with wk0 = bk. This leaves only m actual inputs to the neuron: from x1 to xm.\nThe output of the kth neuron is:\n\n  \n    \n      \n        \n          y\n          \n            k\n          \n        \n        =\n        φ\n        \n          (\n          \n            \n              ∑\n              \n                j\n                =\n                0\n              \n              \n                m\n              \n            \n            \n              w\n              \n                k\n                j\n              \n            \n            \n              x\n              \n                j\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle y_{k}=\\varphi \\left(\\sum _{j=0}^{m}w_{kj}x_{j}\\right)}\n  Where \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n   (phi) is the  transfer function (commonly  a threshold function).\n\nThe output is analogous to the axon of a biological neuron, and its value propagates to the input of the next layer, through a synapse. It may also exit the system, possibly as part of an output vector.\nIt has no learning process as such. Its transfer function weights are calculated and threshold value are predetermined.\n\nTypes\nDepending on the specific model used they may be called a semi-linear unit,  Nv neuron, binary neuron, linear threshold function, or McCulloch–Pitts (MCP) neuron.\nSimple artificial neurons, such as the McCulloch–Pitts model, are sometimes described as \"caricature models\", since they are intended to reflect one or more neurophysiological observations, but without regard to realism.\n\nBiological models\nArtificial neurons are designed to mimic aspects of their biological counterparts. However a significant performance gap exists between biological and artificial neural networks. In particular single biological neurons in the human brain with oscillating activation function capable of learning the XOR function have been discovered.\nDendrites – In a biological neuron, the dendrites act as the input vector. These dendrites allow the cell to receive signals from a large (>1000) number of neighboring neurons. As in the above mathematical treatment, each dendrite is able to perform \"multiplication\" by that dendrite's \"weight value.\" The multiplication is accomplished by increasing or decreasing the ratio of synaptic neurotransmitters to signal chemicals introduced into the dendrite in response to the synaptic neurotransmitter. A negative multiplication effect can be achieved by transmitting signal inhibitors (i.e. oppositely charged ions) along the dendrite in response to the reception of synaptic neurotransmitters.\nSoma – In a biological neuron, the soma acts as the summation function, seen in the above mathematical description. As positive and negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites, the positive and negative ions are effectively added in summation, by simple virtue of being mixed together in the solution inside the cell's body.\nAxon – The axon gets its signal from the summation behavior which occurs inside the soma. The opening to the axon essentially samples the electrical potential of the solution inside the soma. Once the soma reaches a certain potential, the axon will transmit an all-in signal pulse down its length. In this regard, the axon behaves as the ability for us to connect our artificial neuron to other artificial neurons.Unlike most artificial neurons, however, biological neurons fire in discrete pulses. Each time the electrical potential inside the soma reaches a certain threshold, a pulse is transmitted down the axon. This pulsing can be translated into continuous values. The rate (activations per second, etc.) at which an axon fires converts directly into the rate at which neighboring cells get signal ions introduced into them. The faster a biological neuron fires, the faster nearby neurons accumulate electrical potential (or lose electrical potential, depending on the \"weighting\" of the dendrite that connects to the neuron that fired). It is this conversion that allows computer scientists and mathematicians to simulate biological neural networks using artificial neurons which can output distinct values (often from −1 to 1).\n\nEncoding\nResearch has shown that unary coding is used in the neural circuits responsible for birdsong production. The use of unary in biological networks is presumably due to the inherent simplicity of the coding. Another contributing factor could be that unary coding provides a certain degree of error correction.\n\nPhysical artificial cells\nThere is research and development into physical artificial neurons – organic and inorganic.\nFor example, some artificial neurons can receive and release dopamine (chemical signals rather than electrical signals) and communicate with natural rat muscle and brain cells, with potential for use in BCIs/prosthetics.Low-power biocompatible memristors may enable construction of artificial neurons which function at voltages of biological action potentials and could be used to directly process biosensing signals, for neuromorphic computing and/or direct communication with biological neurons.Organic neuromorphic circuits made out of polymers, coated with an ion-rich gel to enable a material to carry an electric charge like real neurons, have been built into a robot, enabling it to learn sensorimotorically within the real world, rather than via simulations or virtually. Moreover, artificial spiking neurons made of soft matter (polymers) can operate in biologically relevant environments and enable the synergetic communication between the artificial and biological domains.\n\nHistory\nThe first artificial neuron was the Threshold Logic Unit (TLU), or Linear Threshold Unit, first proposed by Warren McCulloch and Walter Pitts in 1943. The model was specifically targeted as a computational model of the \"nerve net\" in the brain. As a transfer function, it employed a threshold, equivalent to using the Heaviside step function. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen from the fact that one can implement the AND and OR functions, and use them in the disjunctive or the conjunctive normal form.\nResearchers also soon realized that cyclic networks, with feedbacks through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.\nOne important and pioneering artificial neural network that used the linear threshold function was the perceptron, developed by Frank Rosenblatt. This model already considered more flexible weight values in the neurons, and was used in machines with adaptive capabilities. The representation of the threshold values as a bias term was introduced by Bernard Widrow in 1960 – see ADALINE.\nIn the late 1980s, when research on neural networks regained strength, neurons with more continuous shapes started to be considered. The possibility of differentiating the activation function allows the direct use of the gradient descent and other optimization algorithms for the adjustment of the weights. Neural networks also started to be used as a general function approximation model. The best known training algorithm called backpropagation has been rediscovered several times but its first development goes back to the work of Paul Werbos.\n\nTypes of transfer functions\nThe transfer function (activation function) of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron.  Crucially, for instance, any multilayer perceptron using a linear transfer function has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.Below, u refers in all cases to the weighted sum of all the inputs to the neuron, i.e. for n inputs,\n\n  \n    \n      \n        u\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          w\n          \n            i\n          \n        \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle u=\\sum _{i=1}^{n}w_{i}x_{i}}\n  where w is a vector of synaptic weights and x is a vector of inputs.\n\nStep function\nThe output y of this transfer function is binary, depending on whether the input meets a specified threshold, θ. The \"signal\" is sent, i.e. the output is set to one, if the activation meets the threshold.\n\n  \n    \n      \n        y\n        =\n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    if \n                  \n                  u\n                  ≥\n                  θ\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    if \n                  \n                  u\n                  <\n                  θ\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle y={\\begin{cases}1&{\\text{if }}u\\geq \\theta \\\\0&{\\text{if }}u<\\theta \\end{cases}}}\n  This function is used in perceptrons and often shows up in many other models. It performs a division of the space of inputs by a hyperplane. It is specially useful in the last layer of a network intended to perform binary classification of the inputs. It can be approximated from other sigmoidal functions by assigning large values to the weights.\n\nLinear combination\nIn this case, the output unit is simply the weighted sum of its inputs plus a bias term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as harmonic analysis, and they can all be used in neural networks with this linear neuron. The bias term allows us to make affine transformations to the data.\nSee: Linear transformation, Harmonic analysis, Linear filter, Wavelet, Principal component analysis, Independent component analysis, Deconvolution.\n\nSigmoid\nA fairly simple non-linear function, the sigmoid function such as the logistic function also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It was previously commonly seen in multilayer perceptrons. However, recent work has shown sigmoid neurons to be less effective than rectified linear neurons. The reason is that the gradients computed by the backpropagation algorithm tend to diminish towards zero as activations propagate through layers of sigmoidal neurons, making it difficult to optimize neural networks using multiple layers of sigmoidal neurons.\n\nRectifier\nIn the context of artificial neural networks, the rectifier or ReLU (Rectified Linear Unit) is an activation function defined as the positive part of its argument:\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          x\n          \n            +\n          \n        \n        =\n        max\n        (\n        0\n        ,\n        x\n        )\n        ,\n      \n    \n    {\\displaystyle f(x)=x^{+}=\\max(0,x),}\n  where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature with strong biological motivations and mathematical justifications. It has been demonstrated for the first time in 2011 to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, i.e., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent.\nA commonly used variant of the ReLU activation function is the Leaky ReLU which allows a small, positive gradient when the unit is not active:\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  x\n                \n                \n                  \n                    if \n                  \n                  x\n                  >\n                  0\n                  ,\n                \n              \n              \n                \n                  a\n                  x\n                \n                \n                  \n                    otherwise\n                  \n                  .\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0,\\\\ax&{\\text{otherwise}}.\\end{cases}}}\n  \nwhere x is the input to the neuron and a is a small positive constant (in the original paper the value 0.01 was used for a).\n\nPseudocode algorithm\nThe following is a simple pseudocode implementation of a single TLU which takes boolean inputs (true or false), and returns a single boolean output when activated. An object-oriented model is used. No method of training is defined, since several exist. If a purely functional model were used, the class TLU below would be replaced with a function TLU with input parameters threshold, weights, and inputs that returned a boolean value.\n\nclass TLU defined as:\n    data member threshold : number\n    data member weights : list of numbers of size X\n\n    function member fire(inputs : list of booleans of size X) : boolean defined as:\n        variable T : number\n        T ← 0\n        for each i in 1 to X do\n            if inputs(i) is true then\n                T ← T + weights(i)\n            end if\n        end for each\n        if T > threshold then\n            return true\n        else:\n            return false\n        end if\n    end function\nend class\n\nSee also\nBinding neuron\nConnectionism\n\nReferences\nFurther reading\nExternal links\nArtifical [sic] neuron mimicks function of human cells\nMcCulloch-Pitts Neurons (Overview)",
    "Association for Computational Linguistics": "The Association for Computational Linguistics (ACL) is a scientific and professional organization for people working on natural language processing. Its namesake conference is one of the primary high impact conferences for natural language processing research, along with EMNLP. The conference is held each summer in locations where significant computational linguistics research is carried out.\nIt was founded in 1962, originally named the Association for Machine Translation and Computational Linguistics (AMTCL). It became the ACL in 1968. The ACL has a European (EACL), a North American (NAACL), and an Asian (AACL) chapter.\n\nHistory\nThe ACL was founded in 1962 as the Association for Machine Translation and Computational Linguistics (AMTCL). The initial membership was about 100. In 1965 the AMTCL took over the journal Mechanical Translation and Computational Linguistics. This journal was succeeded by many other journals: American Journal of Computational Linguistics (1974—1978, 1980—1983), and then Computational Linguistics (1984—present). Since 1988, the journal has been published for the ACL by MIT Press.The annual meeting was first held in 1963 in conjunction with the Association for Computing Machinery National Conference. The annual meeting was, for much time, relatively informal and did not publish anything lengthier than abstracts. By 1968, the society took on its current name, the Association for Computational Linguistics (ACL). The publishing of the annual meeting's Proceedings of the ACL began in 1979, and gradually matured into its modern form. Many of the meetings were held in conjunction with the Linguistic Society of America, and a few with the American Society for Information Science and Cognitive Science Society.The United States government sponsored much research from 1989 to 1994, leading to a maturing of the ACL, characterized by an increase in author retention rates and an increase in research in some key topics, such as speech recognition. By the 21st century, the society was able to maintain authors at a high rate who coalesced in a more stable arrangement around individual research topics.In 2020, the annual meeting of the ACL for the first time received more submissions from China than from the United States, reflecting the increasing geographical diversity of the society.\n\nConference Locations\nACL 2023, Toronto, Canada\n ACL 2022, Dublin, Ireland\n ACL 2021, Bangkok, Thailand (moved online due to COVID-19)\n ACL 2020, originally Seattle, Washington, United States (moved online due to COVID-19)\n ACL 2019, Florence, Italy\n ACL 2018, Melbourne, Australia\n ACL 2017, Vancouver, CanadaRef\n\nActivities\nThe ACL organizes several of the top conferences and workshops in the field of computational linguistics and natural language processing. These include:\n\nAnnual Meeting of the Association for Computational Linguistics (ACL), the flagship conference of the organization\nEmpirical Methods in Natural Language Processing (EMNLP)\nInternational Joint Conference on Natural Language Processing (IJCNLP), held jointly one of the other conferences on a rotating basis\nConference on Computational Natural Language Learning (CoNLL)\nLexical and Computational Semantics and Semantic Evaluation (SemEval)\nJoint Conference on Lexical and Computational Semantics (*SEM)\nWorkshop on Statistical Machine Translation (WMT)Besides conferences, the ACL also sponsors the journals Computational Linguistics and Transactions of the Association for Computational Linguistics (TACL). Papers and other presentations at ACL and ACL-affiliated venues are archived online in the open-access ACL Anthology.\n\nSpecial Interest Groups\nACL has a large number of Special Interest Groups (SIGs), focusing on specific areas of natural language processing. Some current SIGs within ACL are:\n\nPresidents\nEach year the ACL elects a distinguished computational linguist who becomes vice-president of the organization in the next calendar year and president one year later. Recent ACL presidents are:\n\nReferences\nExternal links\nOfficial website\nACL Anthology\nACL Wiki\nEACL\nNAACL\nAACL",
    "Association rule learning": "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      {\n      \n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n      \n      }\n      ⇒\n      {\n      \n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n      \n      }\n    \n    \\{{\\mathrm {onions,potatoes} }\\}\\Rightarrow \\{{\\mathrm {burger} }\\}\n   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.\nIn addition to the above example from market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nThe association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.\n\nDefinition\nFollowing the original definition by Agrawal, Imieliński, Swami the problem of association rule mining is defined as:\nLet \n  \n    \n      I\n      =\n      {\n      \n        i\n        \n          1\n        \n      \n      ,\n      \n        i\n        \n          2\n        \n      \n      ,\n      …\n      ,\n      \n        i\n        \n          n\n        \n      \n      }\n    \n    I=\\{i_{1},i_{2},\\ldots ,i_{n}\\}\n   be a set of \n  \n    n\n    n\n   binary attributes called items.\nLet \n  \n    \n      D\n      =\n      {\n      \n        t\n        \n          1\n        \n      \n      ,\n      \n        t\n        \n          2\n        \n      \n      ,\n      …\n      ,\n      \n        t\n        \n          m\n        \n      \n      }\n    \n    D=\\{t_{1},t_{2},\\ldots ,t_{m}\\}\n   be a set of transactions called the database.\nEach transaction in \n  \n    D\n    D\n   has a unique transaction ID and contains a subset of the items in \n  \n    I\n    I\n  .\nA rule is defined as an implication of the form:\n\n  \n    \n      X\n      ⇒\n      Y\n    \n    X\\Rightarrow Y\n  , where \n  \n    \n      X\n      ,\n      Y\n      ⊆\n      I\n    \n    X,Y\\subseteq I\n  .\nIn Agrawal, Imieliński, Swami a rule is defined only between a set and a single item, \n  \n    \n      \n        X\n        ⇒\n        \n          i\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X\\Rightarrow i_{j}}\n   for \n  \n    \n      \n        \n          i\n          \n            j\n          \n        \n        ∈\n        I\n      \n    \n    {\\displaystyle i_{j}\\in I}\n  .\nEvery rule is composed by two different sets of items, also known as itemsets, \n  \n    X\n    X\n   and \n  \n    Y\n    Y\n  , where \n  \n    X\n    X\n   is called antecedent or left-hand-side (LHS) and \n  \n    Y\n    Y\n   consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent. The statement \n  \n    \n      X\n      ⇒\n      Y\n    \n    X\\Rightarrow Y\n   is often read as if \n  \n    X\n    X\n   then \n  \n    Y\n    Y\n  , where the antecedent (\n  \n    X\n    X\n   ) is the if and the consequent (\n  \n    Y\n    Y\n  ) is the then. This simply implies that, in theory, whenever \n  \n    X\n    X\n   occurs in a dataset, then \n  \n    Y\n    Y\n   will as well.\n\nProcess\nAssociation rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true. However, there is a third criteria that can be used, it is called Lift and it can be used to compare the expected Confidence and the actual Confidence. Lift will show how many times the if-then statement is expected to be found to be true.\nAssociation rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn’t have any meaning. That is why Association rules are typically made from rules that are well represented by the data.\nThere are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis. What technique you should use depends on what you are looking for with your data. Association rules are primarily used to find analytics and a prediction of customer behavior. For Classification analysis, it would most likely be used to question, make decisions, and predict behavior. Clustering analysis is primarily used when there are no assumptions made about the likely relationships within the data. Regression analysis Is used when you want to predict the value of a continuous dependent from a number of independent variables.Benefits\nThere are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine. Medicine uses Association rules to help diagnose patients. When diagnosing patients there are many variables to consider as many diseases will share similar symptoms. With the use of the Association rules, doctors can determine the conditional probability of an illness by comparing symptom relationships from past cases.Downsides\nHowever, Association rules also lead to many different downsides such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downside of having a large number of discovered rules. The reason is that this does not guarantee that the rules will be found relevant, but it could also cause the algorithm to have low performance. Sometimes the implemented algorithms will contain too many variables and parameters. For someone that doesn’t have a good concept of data mining, this might cause them to have trouble understanding it.\nThresholdsWhen using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Usually, the Association rule generation is split into two different steps that needs to be applied:\nA minimum Support threshold to find all the frequent itemsets that are in the database.\nA minimum Confidence threshold to the frequent itemsets found to create rules.The Support Threshold is 30%, Confidence Threshold is 50%\nThe Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first. Item A is second because its threshold values are spot on. Item D has met the threshold for Support but not Confidence. Item B has not met the threshold for either Support or Confidence and that is why it is last.\nTo find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size \n  \n    \n      \n        2\n        \n          n\n        \n      \n      −\n      1\n    \n    2^{n}-1\n   , of course this means to exclude the empty set which is not considered to be a valid itemset. However, the size of the power set will grow exponentially in the number of item n that is within the power set I. An efficient search is possible by using the downward-closure property of support (also called anti-monotonicity). This would guarantee that a frequent itemset and all its subsets are also frequent and thus will have no infrequent itemsets as a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori and Eclat) can find all frequent itemsets.\n\nUseful Concepts\nTo illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction. The set of items is \n  \n    \n      \n        I\n        =\n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n          ,\n          b\n          u\n          t\n          t\n          e\n          r\n          ,\n          b\n          e\n          e\n          r\n          ,\n          d\n          i\n          a\n          p\n          e\n          r\n          s\n          ,\n          e\n          g\n          g\n          s\n          ,\n          f\n          r\n          u\n          i\n          t\n        \n        }\n      \n    \n    {\\displaystyle I=\\{\\mathrm {milk,bread,butter,beer,diapers,eggs,fruit} \\}}\n  .\nAn example rule for the supermarket could be \n  \n    \n      {\n      \n        \n          b\n          u\n          t\n          t\n          e\n          r\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n      \n      }\n      ⇒\n      {\n      \n        \n          m\n          i\n          l\n          k\n        \n      \n      }\n    \n    \\{{\\mathrm {butter,bread} }\\}\\Rightarrow \\{{\\mathrm {milk} }\\}\n   meaning that if butter and bread are bought, customers also buy milk.\nIn order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.\nLet \n  \n    \n      X\n      ,\n      Y\n    \n    X,Y\n   be itemsets, \n  \n    \n      X\n      ⇒\n      Y\n    \n    X\\Rightarrow Y\n   an association rule and T a set of transactions of a given database.\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\nSupport\nSupport is an indication of how frequently the itemset appears in the dataset.\nIn our example, it can be easier to explain support by writing \n  \n    \n      \n        s\n        u\n        p\n        p\n        o\n        r\n        t\n        =\n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        \n          \n            \n              (\n              \n                number of transactions containing \n              \n              A\n              \n                 and \n              \n              B\n              )\n            \n             (total number of transactions)\n          \n        \n      \n    \n    {\\displaystyle support=P(A\\cap B)={\\frac {({\\text{number of transactions containing }}A{\\text{ and }}B)}{\\text{ (total number of transactions)}}}}\n    where A and B are separate item sets that occur in at the same time in a transaction.\nUsing Table 2 as an example, the itemset \n  \n    \n      \n        X\n        =\n        {\n        \n          b\n          e\n          e\n          r\n          ,\n          d\n          i\n          a\n          p\n          e\n          r\n          s\n        \n        }\n      \n    \n    {\\displaystyle X=\\{\\mathrm {beer,diapers} \\}}\n   has a support of \n  \n    \n      1\n      \n        /\n      \n      5\n      =\n      0.2\n    \n    1/5=0.2\n   since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of support of X is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).Furthermore, the itemset \n  \n    \n      \n        Y\n        =\n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n          ,\n          b\n          u\n          t\n          t\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle Y=\\{\\mathrm {milk,bread,butter} \\}}\n   has a support of \n  \n    \n      1\n      \n        /\n      \n      5\n      =\n      0.2\n    \n    1/5=0.2\n   as it appears in 20% of all transactions as well.\nWhen using antecedents and consequents, it allows a data miner to determine the support of multiple items being bought together in comparison to the whole data set. For example, Table 2 shows that if milk is bought, then bread is bought has a support of 0.4 or 40%. This because in 2 out 5 of the transactions, milk as well as bread are bought. In smaller data sets like this example, it is harder to see a strong correlation when there are few samples, but when the data set grows larger, support can be used to find correlation between two or more products in the supermarket example.\nMinimum support thresholds are useful for determining which itemsets are preferred or interesting.\nIf we set the support threshold to ≥0.4 in Table 3, then the \n  \n    \n      \n        {\n        \n          m\n          i\n          l\n          k\n        \n        }\n        ⇒\n        {\n        \n          e\n          g\n          g\n          s\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {milk} \\}\\Rightarrow \\{\\mathrm {eggs} \\}}\n   would be removed since it did not meet the minimum threshold of 0.4. Minimum threshold is used to remove samples where there is not a strong enough support or confidence to deem the sample as important or interesting in the dataset.\nAnother way of finding interesting samples is to find the value of (support)X(confidence); this allows a data miner to see the samples where support and confidence are high enough to be highlighted in the dataset and prompt a closer look at the sample to find more information on the connection between the items.\nSupport can be beneficial for finding the connection between products in comparison to the whole dataset, whereas confidence looks at the connection between one or more items and another item. Below is a table that shows the comparison and contrast between support and support x confidence, using the information from Table 4 to derive the confidence values.\n\nThe support of X with respect to T is defined as the proportion of transactions in the dataset which contains the itemset X. Denoting a transaction by \n  \n    \n      (\n      i\n      ,\n      t\n      )\n    \n    (i,t)\n   where i is the unique identifier of the transaction and t is its itemset, the support may be written as:\n\n  \n    \n      \n        \n          s\n          u\n          p\n          p\n          o\n          r\n          t\n          \n          o\n          f\n          \n          X\n        \n        =\n        \n          \n            \n              \n                |\n              \n              {\n              (\n              i\n              ,\n              t\n              )\n              ∈\n              T\n              :\n              X\n              ⊆\n              t\n              }\n              \n                |\n              \n            \n            \n              \n                |\n              \n              T\n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {support\\,of\\,X} ={\\frac {|\\{(i,t)\\in T:X\\subseteq t\\}|}{|T|}}}\n  \nThis notation can be used when defining more complicated datasets where the items and itemsets may not be as easy as our supermarket example above. Other examples of where support can be used is in finding groups of genetic mutations that work collectively to cause a disease, investigating the number of subscribers that respond to upgrade offers, and discovering which products in a drug store are never bought together.\n\nConfidence\nConfidence is the percentage of all transactions satisfying X that also satisfy Y.With respect to T, the confidence value of an association rule, often denoted as \n  \n    \n      X\n      ⇒\n      Y\n    \n    X\\Rightarrow Y\n  , is the ratio of transactions containing both X and Y to the total amount of X values present, where X is the antecedent and Y is the consequent.\nConfidence can also be interpreted as an estimate of the conditional probability \n  \n    \n      P\n      (\n      \n        E\n        \n          Y\n        \n      \n      \n        |\n      \n      \n        E\n        \n          X\n        \n      \n      )\n    \n    P(E_{Y}|E_{X})\n  , the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.It is commonly depicted as:\n\n  \n    \n      \n        \n          c\n          o\n          n\n          f\n        \n        (\n        X\n        ⇒\n        Y\n        )\n        =\n        P\n        (\n        Y\n        \n          |\n        \n        X\n        )\n        =\n        \n          \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              ∩\n              Y\n              )\n            \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              )\n            \n          \n        \n        =\n        \n          \n            \n              \n                number of transactions containing \n              \n              X\n              \n                 and \n              \n              Y\n            \n            \n              \n                number of transactions containing \n              \n              X\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {conf} (X\\Rightarrow Y)=P(Y|X)={\\frac {\\mathrm {supp} (X\\cap Y)}{\\mathrm {supp} (X)}}={\\frac {{\\text{number of transactions containing }}X{\\text{ and }}Y}{{\\text{number of transactions containing }}X}}}\n  \nThe equation illustrates that confidence can be computed by calculating the co-occurrence of transactions X and Y within the dataset in ratio to transactions containing only X. This means that the number of transactions in both  X and Y  is divided by those just in X .\nFor example, Table 2 shows the rule \n  \n    \n      {\n      \n        \n          b\n          u\n          t\n          t\n          e\n          r\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n      \n      }\n      ⇒\n      {\n      \n        \n          m\n          i\n          l\n          k\n        \n      \n      }\n    \n    \\{{\\mathrm {butter,bread} }\\}\\Rightarrow \\{{\\mathrm {milk} }\\}\n   which has a confidence of \n  \n    \n      \n        \n          \n            \n              1\n              \n                /\n              \n              5\n            \n            \n              1\n              \n                /\n              \n              5\n            \n          \n        \n        =\n        \n          \n            0.2\n            0.2\n          \n        \n        =\n        1.0\n      \n    \n    {\\displaystyle {\\frac {1/5}{1/5}}={\\frac {0.2}{0.2}}=1.0}\n   in the dataset, which denotes that every time a customer buys butter and bread, they also buy milk. This particular example demonstrates the rule being correct 100% of the time for transactions containing both butter and bread. The rule \n  \n    \n      \n        {\n        \n          f\n          r\n          u\n          i\n          t\n        \n        }\n        ⇒\n        {\n        \n          e\n          g\n          g\n          s\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {fruit} \\}\\Rightarrow \\{\\mathrm {eggs} \\}}\n  , however, has a confidence of \n  \n    \n      \n        \n          \n            \n              2\n              \n                /\n              \n              5\n            \n            \n              3\n              \n                /\n              \n              5\n            \n          \n        \n        =\n        \n          \n            0.4\n            0.6\n          \n        \n        =\n        0.67\n      \n    \n    {\\displaystyle {\\frac {2/5}{3/5}}={\\frac {0.4}{0.6}}=0.67}\n  . This suggests that eggs are bought 67% of the times that fruit is brought. Within this particular dataset, fruit is purchased a total of 3 times, with two of those times consisting of egg purchases.\nFor larger datasets, a minimum threshold, or a percentage cutoff, for the confidence can be useful for determining item relationships. When applying this method to some of the data in Table 2, information that does not meet the requirements are removed. Table 4 shows association rule examples where the minimum threshold for confidence is 0.5 (50%). Any data that does not have a confidence of at least 0.5 is omitted. Generating thresholds allow for the association between items to become stronger as the data is further researched by emphasizing those that co-occur the most. The table uses the confidence information from Table 3 to implement the Support x Confidence column, where the relationship between items via their both confidence and support, instead of just one concept, is highlighted. Ranking the rules by Support x Confidence multiples the confidence of a particular rule to its support and is often implemented for a more in-depth understanding of the relationship between the items.\n\nOverall, using confidence in association rule mining is great way to bring awareness to data relations. Its greatest benefit is highlighting the relationship between particular items to one another within the set, as it compares co-occurrences of items to the total occurrence of the antecedent in the specific rule. However, confidence is not the optimal method for every concept in association rule mining. The disadvantage of using it is that it does not offer multiple difference outlooks on the associations. Unlike support, for instance, confidence does not provide the perspective of relationships between certain items in comparison to the entire dataset, so while milk and bread, for example, may occur 100% of the time for confidence, it only has a support of 0.4 (40%). This is why it is important to look at other viewpoints, such as Support x Confidence, instead of solely relying on one concept incessantly to define the relationships.\n\nLift\nThe lift of a rule is defined as:\n\n  \n    \n      \n        \n          l\n          i\n          f\n          t\n        \n        (\n        X\n        ⇒\n        Y\n        )\n        =\n        \n          \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              ∩\n              Y\n              )\n            \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              )\n              ×\n              \n                s\n                u\n                p\n                p\n              \n              (\n              Y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {lift} (X\\Rightarrow Y)={\\frac {\\mathrm {supp} (X\\cap Y)}{\\mathrm {supp} (X)\\times \\mathrm {supp} (Y)}}}\n  \nor the ratio of the observed support to that expected if X and Y were independent.\nFor example, the rule \n  \n    \n      {\n      \n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n      \n      }\n      ⇒\n      {\n      \n        \n          b\n          u\n          t\n          t\n          e\n          r\n        \n      \n      }\n    \n    \\{{\\mathrm {milk,bread} }\\}\\Rightarrow \\{{\\mathrm {butter} }\\}\n   has a lift of \n  \n    \n      \n        \n          0.2\n          \n            0.4\n            ×\n            0.4\n          \n        \n      \n      =\n      1.25\n    \n    {\\frac {0.2}{0.4\\times 0.4}}=1.25\n  .\nIf the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\nIf the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.\nIf the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.\nThe value of lift is that it considers both the support of the rule and the overall data set.\n\nConviction\nThe conviction of a rule is defined as \n  \n    \n      \n        \n          c\n          o\n          n\n          v\n        \n      \n      (\n      X\n      ⇒\n      Y\n      )\n      =\n      \n        \n          \n            1\n            −\n            \n              \n                s\n                u\n                p\n                p\n              \n            \n            (\n            Y\n            )\n          \n          \n            1\n            −\n            \n              \n                c\n                o\n                n\n                f\n              \n            \n            (\n            X\n            ⇒\n            Y\n            )\n          \n        \n      \n    \n    {\\mathrm {conv} }(X\\Rightarrow Y)={\\frac {1-{\\mathrm {supp} }(Y)}{1-{\\mathrm {conf} }(X\\Rightarrow Y)}}\n  .For example, the rule \n  \n    \n      {\n      \n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n      \n      }\n      ⇒\n      {\n      \n        \n          b\n          u\n          t\n          t\n          e\n          r\n        \n      \n      }\n    \n    \\{{\\mathrm {milk,bread} }\\}\\Rightarrow \\{{\\mathrm {butter} }\\}\n   has a conviction of \n  \n    \n      \n        \n          \n            1\n            −\n            0.4\n          \n          \n            1\n            −\n            0.5\n          \n        \n      \n      =\n      1.2\n    \n    {\\frac {1-0.4}{1-0.5}}=1.2\n  , and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule \n  \n    \n      {\n      \n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n      \n      }\n      ⇒\n      {\n      \n        \n          b\n          u\n          t\n          t\n          e\n          r\n        \n      \n      }\n    \n    \\{{\\mathrm {milk,bread} }\\}\\Rightarrow \\{{\\mathrm {butter} }\\}\n   would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.\n\nAlternative measures of interestingness\nIn addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:\n\nAll-confidence\nCollective strength\nLeverageSeveral more measures are presented and compared by Tan et al. and by Hahsler. Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of \"Subjective Interestingness.\"\n\nHistory\nThe concept of association rules was popularized particularly due to the 1993 article of Agrawal et al., which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called \"association rules\" is introduced already in the 1966 paper on GUHA, a general data mining method developed by Petr Hájek et al.An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with \n  \n    \n      \n        \n          s\n          u\n          p\n          p\n        \n      \n      (\n      X\n      )\n    \n    {\\mathrm {supp} }(X)\n   and \n  \n    \n      \n        \n          c\n          o\n          n\n          f\n        \n        (\n        X\n        ⇒\n        Y\n        )\n      \n    \n    {\\displaystyle \\mathrm {conf} (X\\Rightarrow Y)}\n   greater than user defined constraints.\n\nStatistically sound associations\nOne limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.\n\nAlgorithms\nMany algorithms for generating association rules have been proposed.\nSome well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.\n\nApriori algorithm\nApriori is given by R. Agrawal and R. Srikant in 1994 for frequent item set mining and association rule learning. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often. The name of the algorithm is Apriori because it uses prior knowledge of frequent itemset properties.\n\nOverview: Apriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length  from item sets of length . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\nExample: Assume that each row is a cancer sample with a certain combination of mutations labeled by a character in the alphabet. For example a row could have {a, c} which means it is affected by mutation 'a' and mutation 'c'. \n\nNow we will generate the frequent item set by counting the number of occurrences of each character. This is also known as finding the support values. Then we will prune the item set by picking a minimum support threshold. For this pass of the algorithm we will pick 3. \n\nSince all support values are three or above there is no pruning. The frequent item set is {a}, {b}, {c}, and {d}. After this we will repeat the process by counting pairs of mutations in the input set. \n\nNow we will make our minimum support value 4 so only {a, d} and {c, d} will remain after pruning. Now we will use the frequent item set to make combinations of triplets.  We will then repeat the process by counting occurrences of triplets of mutations in the input set. \n\nSince we only have one item the next set of combinations of quadruplets is empty so the algorithm will stop.\nAdvantages and Limitations:\nApriori has some limitations. Candidate generation can result in large candidate sets. For example a 10^4 frequent 1-itemset will generate a 10^7 candidate 2-itemset. The algorithm also needs to frequently scan the database, to be specific n+1 scans where n is the length of the longest pattern. Apriori is slower than the Eclat algorithm. However, Apriori performs well compared to Eclat when the dataset is large. This is because in the Eclat algorithm if the dataset is too large the tid-lists become too large for memory. FP-growth outperforms the Apriori and Eclat. This is due to the FP-growth algorithm not having candidate generation or test, using a compact data structure, and only having one database scan.\n\nEclat algorithm\nEclat (alt. ECLAT, stands for Equivalence Class Transformation) is a backtracking algorithm, which traverses the frequent itemset lattice graph in a depth-first search (DFS) fashion. Whereas the breadth-first search (BFS) traversal used in the Apriori algorithm will end up checking every subset of an itemset before checking it, DFS traversal checks larger itemsets and can save on checking the support of some of its subsets by virtue of the downward-closer property. Furthermore it will almost certainly use less memory as DFS has a lower space complexity than BFS.\nTo illustrate this, let there be a frequent itemset {a, b, c}. a DFS may check the nodes in the frequent itemset lattice in the following order: {a} → {a, b} → {a, b, c}, at which point it is known that {b}, {c}, {a, c}, {b, c} all satisfy the support constraint by the downward-closure property. BFS would explore each subset of {a, b, c} before finally checking it. As the size of an itemset increases, the number of its subsets undergoes combinatorial explosion.\nIt is suitable for both sequential as well as parallel execution with locality-enhancing properties.\n\nFP-growth algorithm\nFP stands for frequent pattern.In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.\nItems in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.\nItems in each transaction that do not meet the minimum support requirement are discarded.\nIf many transactions share most frequent items, the FP-tree provides high compression close to tree root.\nRecursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).\nGrowth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item \n  \n    I\n    I\n  .\nA new conditional tree is created which is the original FP-tree projected onto \n  \n    I\n    I\n  . The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on \n  \n    I\n    I\n   meet the minimum support threshold. The resulting paths from root to \n  \n    I\n    I\n   will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.\nOnce the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.\n\nOthers\nASSOC\nThe ASSOC procedure is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example \"items\" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.\n\nOPUS search\nOPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support. Initially used to find rules for a fixed consequent it has subsequently been extended to find rules with any item as a consequent. OPUS search is the core technology in the popular Magnum Opus association discovery system.\n\nLore\nA famous story about association rule mining is the \"beer and diaper\" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true. Daniel Powers says:\nIn 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis \"did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers\". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.\n\nOther types of association rule mining\nMulti-Relation Association Rules (MRAR): These are association rules where each item may have several relations. These relations indicate indirect relationships between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is nearby a city with humid climate type and also are younger than 20 \n  \n    \n      \n      ⟹\n      \n    \n    \\implies\n   their health condition is good”. Such association rules can be extracted from RDBMS data or semantic web data.Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.Weighted class learning is another form of associative learning where weights may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.\nHigh-order pattern discovery facilitates the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.\nK-optimal pattern discovery provides an alternative to the standard approach to association rule learning which requires that each pattern appear frequently in the data.\nApproximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.Generalized Association Rules hierarchical taxonomy (concept hierarchy)\nQuantitative Association Rules categorical and quantitative data\nInterval Data Association Rules e.g. partition the age into 5-year-increment ranged\nSequential pattern mining  discovers subsequences that are common to more than minsup (minimum support threshold) sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.Subspace Clustering, a specific type of clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.Warmr, shipped as part of the ACE data mining suite, allows association rule learning for first order relational rules.\n\nSee also\nSequence mining\nProduction system (computer science)\nLearning classifier system\nRule-based machine learning\n\nReferences\nBibliographies\nAnnotated Bibliography on Association Rules by M. Hahsler",
    "Astroinformatics": "Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies. The field is closely related to astrostatistics.\n\nBackground\nAstroinformatics is primarily focused on developing the tools, methods, and applications of computational science, data science, machine learning, and statistics for research and education in data-oriented astronomy. Early efforts in this direction included data discovery, metadata standards development, data modeling, astronomical data dictionary development, data access, information retrieval, data integration, and data mining in the astronomical Virtual Observatory initiatives. Further development of the field, along with astronomy community endorsement, was presented to the National Research Council (United States) in 2009 in the astroinformatics \"state of the profession\" position paper for the 2010 Astronomy and Astrophysics Decadal Survey. That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper Astroinformatics: Data-Oriented Astronomy Research and Education.Astroinformatics as a distinct field of research was inspired by work in the fields of Bioinformatics and Geoinformatics, and through the eScience work of Jim Gray (computer scientist) at Microsoft Research, whose legacy was remembered and continued through the Jim Gray eScience Awards.Although the primary focus of astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to digitize historical and recent astronomical observations and images in a large database for efficient retrieval through web-based interfaces. Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.Astroinformatics is described as the \"fourth paradigm\" of astronomical research. There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science. Data mining and machine learning play significant roles in astroinformatics as a scientific research discipline due to their focus on \"knowledge discovery from data\" (KDD) and \"learning from data\".The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the Large Synoptic Survey Telescope and into the exabytes with the Square Kilometre Array. This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part due to this, data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing information-intensive and data-intensive sub-disciplines to an extent that these sub-disciplines are now becoming (or have already become) standalone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, such programs most likely will be developed in the near future.\nInformatics has been recently defined as \"the use of digital data, information, and related services for research and knowledge generation\". However the usual, or commonly used definition is \"informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support.\" Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., taxonomies, ontologies, folksonomies, and/or collaborative tagging) plus Astrostatistics will also be heavily involved. Citizen science projects (such as Galaxy Zoo) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.\nIn 2012, two position papers were presented to the Council of the American Astronomical Society that led to the establishment of formal working groups in astroinformatics and Astrostatistics for the profession of astronomy within the US and elsewhere.Astroinformatics provides a natural context for the integration of education and research. The experience of research can now be implemented within the classroom to establish and grow data literacy through the easy re-use of data. It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.\n\nConferences\nAdditional conferences and conference lists:\n\nSee also\nAstronomy and Computing\nAstrophysics Data System\nAstrophysics Source Code Library\nAstrostatistics\nCommittee on Data for Science and Technology\nGalaxy Zoo\nInternational Astrostatistics Association\nInternational Virtual Observatory Alliance (IVOA)\nMilkyWay@home\nVirtual Observatory\nWorldWide Telescope\nZooniverse\n\nReferences\nExternal links\nInternational AstroInformatics Association (IAIA)\nAstronomical Data Analysis Software and Systems (ADASS)\nAstrostatistics and Astroinformatics Portal\nCosmostatistics Initiative (COIN)\nAstroinformatics and Astrostatistics Commission of the International Astronomical Union",
    "Attention (machine learning)": "In artificial neural networks, attention is a technique that is meant to mimic cognitive attention. This effect enhances some parts of the input data while diminishing other parts—the motivation being that the network should devote more focus to the important parts of the data, even though they may be a small portion of an image or sentence.  Learning which part of the data is more important than another depends on the context, and this is trained by gradient descent.\nAttention-like mechanisms were introduced in the 1990s under names like multiplicative modules, sigma pi units, and hyper-networks. The flexibility of these models comes from the introduction of \"soft\" weights that can change during runtime, in contrast to standard weights that must remain fixed at runtime. Uses of attention include memory in fast weight controllers, neural Turing machines, reasoning tasks in differentiable neural computers, language processing in transformers, and LSTMs, and multi-sensory data processing (sound, images, video, and text) in perceivers.\n\nOverview\nCorrelating the different parts within a sentence or a picture can help capture its structure. The attention scheme gives a neural network an opportunity to do that. For example, in the sentence \"See that girl run.\", when the network processes \"that\" we want it to know that this word refers to \"girl\".  The next diagram shows how a well-trained network can make this happen.\n\nThe structure of the input data is captured in the Qw and Kw weights, and the Vw weights express that structure in terms of more meaningful features for the task being trained for.  For this reason, the attention head components are called Query (Q), Key (K), and Value (V)—a loose and possibly misleading analogy with relational database systems.\nWe can greatly speed up calculations by parallelizing the above.  Note that the context vector for \"that\" does not rely on context vectors for the other words; therefore we can calculate all context vectors at once by simply using the matrix X of stacked word embeddings instead of x in the formula above.  Now, the softmax should be interpreted as a matrix softmax acting on separate rows.  This is a huge advantage over recurrent networks which must operate sequentially.\n\nA language translation example\nTo build a machine that translates English to French, an attention unit  is grafted to the basic Encoder-Decoder (diagram below).  In the simplest case, the attention unit consists of dot products of the recurrent encoder states and does not need training.  In practice, the attention unit consists of 3 trained, fully-connected neural network layers called query, key, and value.\n\nViewed as a matrix, the attention weights show how the network adjusts its focus according to context.\n\nThis view of the attention weights addresses the neural network \"explainability\" problem.  Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix.  The off-diagonal dominance shows that the attention mechanism is more nuanced.  On the first pass through the decoder, 94% of the attention weight is on the first English word \"I\", so the network offers the word \"je\".  On the second pass of the decoder, 88% of the attention weight is on the third English word \"you\", so it offers \"t'\".  On the last pass, 95% of the attention weight is on the second English word \"love\", so it offers \"aime\".\n\nVariants\nMany variants of attention implement soft weights, such as \n\n\"internal spotlights of attention\" generated by fast weight programmers or fast weight controllers (1992) (also known as transformers with \"linearized self-attention\"). A slow neural network learns by gradient descent to program the fast weights of another neural network through outer products of self-generated activation patterns called \"FROM\" and \"TO\" which in transformer terminology are called \"key\" and \"value.\" This fast weight \"attention mapping\" is applied to queries.\nBahdanau Attention, also referred to as additive attention,\nLuong Attention  which is known as multiplicative attention, built on top of additive attention,\nhighly parallelizable self-attention introduced in 2016 as decomposable attention  and successfully used in transformers a year later.For convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations.These variants recombine the encoder-side inputs to redistribute those effects to each target output.  Often, a correlation-style matrix of dot products provides the re-weighting coefficients.\n\nSee also\nTransformer (machine learning model) § Scaled dot-product attention\nPerceiver § Components for query-key-value (QKV) attention\n\nReferences\nExternal links\nDan Jurafsky and James H. Martin (2022) Speech and Language Processing (3rd ed. draft, January 2022), ch. 10.4 Attention and ch. 9.7 Self-Attention Networks: Transformers\nAlex Graves (4 May 2020), Attention and Memory in Deep Learning (video lecture), DeepMind / UCL, via YouTube\nRasa Algorithm Whiteboard - Attention via YouTube",
    "Auto-GPT": "Auto-GPT is an \"AI agent\" that, given a goal in natural language, will attempt to achieve it by breaking it into sub-tasks and using the internet and other tools in an automatic loop. It uses OpenAI's GPT-4 or GPT-3.5 APIs, and is among the first examples of an application using GPT-4 to perform autonomous tasks.\n\nDetails\nUnlike interactive systems such as ChatGPT, which require manual commands for every task, Auto-GPT assigns itself new objectives to work on with the aim of reaching a greater goal, without a mandatory need for human input. It is able to execute responses to prompts to accomplish a goal task, and in doing so will create and revise its own prompts to recursive instances in response to new information. It manages short-term and long-term memory by writing to and reading from databases and files; manages LLM input length restrictions using summarization; can perform internet-based actions such as web searching, web form, and API interactions unattended; and includes text-to-speech for voice output.Observers tout  Auto-GPT's ability to write, debug, test, and edit code, even suggesting this ability may extend to Auto-GPT's own source code enabling self-improvement. However, as the underlying GPT models it uses are proprietary, Auto-GPT cannot modify them, and it does not ordinarily have access to its own base system code.\n\nBackground\nOn March 14, 2023, OpenAI released the large language model GPT-4. Observers were impressed by the model's substantially improved performance across a wide range of tasks. As a text prediction model, GPT-4 itself has no ability to perform actions autonomously, but during pre-release safety testing red-team researchers found GPT-4 could be enabled to perform actions in the real world like convincing a TaskRabbit worker to solve a CAPTCHA challenge for it. A team of Microsoft researchers argued that, given GPT-4's breadth of abilities at levels approaching those of humans, GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" The researchers emphasized their experiments also found significant limitations in the system.Auto-GPT was released March 30, 2023 by Toran Bruce Richards, the founder of video game company Significant Gravitas Ltd. It became the top trending repository on GitHub shortly after its release, and has repeatedly trended on Twitter since.\n\nIssues\nWhether Auto-GPT will find practical applications is uncertain. In addition to being plagued by confabulatory \"hallucinations\" of the underlying large language models upon which it is based, Auto-GPT often also has trouble staying on task, both problems which developers continue to try to address. After successfully completing a task, it usually does not remember how to perform it for later use, and when it does, for example when it writes a program, it often forgets to use the program later. Auto-GPT struggles to effectively decompose tasks and has trouble understanding problem contexts and how goals overlap.\n\nChaosGPT\nAuto-GPT was used to create ChaosGPT, which, given the goal of destroying humanity, was not immediately successful in doing so.\n\nReferences\nFurther reading\nPounder, Les (April 15, 2023). \"How To Create Your Own Auto-GPT AI Agent\". Tom's Hardware. Retrieved April 16, 2023.\nWiggers, Kyle (April 22, 2023). \"What is Auto-GPT and why does it matter?\". TechCrunch. Retrieved April 23, 2023.\n\nExternal links\nOfficial Website\nOfficial repository at GitHub",
    "Autoencoder": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).\n\nMathematical principles\nDefinition\nAn autoencoder is defined by the following components: Two sets: the space of decoded messages \n  \n    \n      \n        X\n      \n    \n    {\\mathcal {X}}\n  ; the space of encoded messages \n  \n    \n      \n        Z\n      \n    \n    {\\mathcal {Z}}\n  . Almost always, both \n  \n    \n      \n        X\n      \n    \n    {\\mathcal {X}}\n   and \n  \n    \n      \n        Z\n      \n    \n    {\\mathcal {Z}}\n   are Euclidean spaces, that is, \n  \n    \n      \n        \n          \n            X\n          \n        \n        =\n        \n          \n            R\n          \n          \n            m\n          \n        \n        ,\n        \n          \n            Z\n          \n        \n        =\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {X}}=\\mathbb {R} ^{m},{\\mathcal {Z}}=\\mathbb {R} ^{n}}\n   for some \n  \n    \n      m\n      ,\n      n\n    \n    m,n\n  .    Two parametrized families of functions: the encoder family \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n        :\n        \n          \n            X\n          \n        \n        →\n        \n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }:{\\mathcal {X}}\\rightarrow {\\mathcal {Z}}}\n  , parametrized by \n  \n    ϕ\n    \\phi\n  ; the decoder family \n  \n    \n      \n        \n          D\n          \n            θ\n          \n        \n        :\n        \n          \n            Z\n          \n        \n        →\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle D_{\\theta }:{\\mathcal {Z}}\\rightarrow {\\mathcal {X}}}\n  , parametrized by \n  \n    θ\n    \\theta\n  .For any \n  \n    \n      \n        x\n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\mathcal {X}}}\n  , we usually write \n  \n    \n      \n        z\n        =\n        \n          E\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle z=E_{\\phi }(x)}\n  , and refer to it as the code, the latent variable, latent representation, latent vector, etc. Conversely, for any \n  \n    \n      \n        z\n        ∈\n        \n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle z\\in {\\mathcal {Z}}}\n  , we usually write \n  \n    \n      \n        \n          x\n          ′\n        \n        =\n        \n          D\n          \n            θ\n          \n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle x'=D_{\\theta }(z)}\n  , and refer to it as the (decoded) message.\nUsually, both the encoder and the decoder are defined as multilayer perceptrons. For example, a one-layer-MLP encoder \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }}\n   is:\n\n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n        (\n        \n          x\n        \n        )\n        =\n        σ\n        (\n        W\n        x\n        +\n        b\n        )\n      \n    \n    {\\displaystyle E_{\\phi }(\\mathbf {x} )=\\sigma (Wx+b)}\n  where \n  \n    σ\n    \\sigma\n   is an element-wise activation function such as a sigmoid function or a rectified linear unit, \n  \n    W\n    W\n   is a matrix called \"weight\", and \n  \n    b\n    b\n   is a vector called \"bias\".\n\nTraining an autoencoder\nAn autoencoder, by itself, is simply a tuple of two functions. To judge its quality, we need a task. A task is defined by a reference probability distribution \n  \n    \n      \n        \n          μ\n          \n            r\n            e\n            f\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ref}}\n   over \n  \n    \n      \n        X\n      \n    \n    {\\mathcal {X}}\n  , and a \"reconstruction quality\" function \n  \n    \n      \n        d\n        :\n        \n          \n            X\n          \n        \n        ×\n        \n          \n            X\n          \n        \n        →\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle d:{\\mathcal {X}}\\times {\\mathcal {X}}\\to [0,\\infty ]}\n  , such that \n  \n    \n      \n        d\n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        )\n      \n    \n    {\\displaystyle d(x,x')}\n   measures how much \n  \n    \n      x\n      ′\n    \n    x'\n   differs from \n  \n    x\n    x\n  .\nWith those, we can define the loss function for the autoencoder asThe optimal autoencoder for the given task \n  \n    \n      \n        (\n        \n          μ\n          \n            r\n            e\n            f\n          \n        \n        ,\n        d\n        )\n      \n    \n    {\\displaystyle (\\mu _{ref},d)}\n   is then \n  \n    \n      \n        arg\n        ⁡\n        \n          min\n          \n            θ\n            ,\n            ϕ\n          \n        \n        L\n        (\n        θ\n        ,\n        ϕ\n        )\n      \n    \n    {\\displaystyle \\arg \\min _{\\theta ,\\phi }L(\\theta ,\\phi )}\n  . The search for the optimal autoencoder can be accomplished by any mathematical optimization technique, but usually by gradient descent. This search process is referred to as \"training the autoencoder\".\nIn most situations, the reference distribution is just the empirical distribution given by a dataset \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            N\n          \n        \n        }\n        ⊂\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\{x_{1},...,x_{N}\\}\\subset {\\mathcal {X}}}\n  , so that\nwhere and \n  \n    \n      \n        \n          δ\n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\delta _{x_{i}}}\n   is the Dirac measure, and the quality function is just L2 loss: \n  \n    \n      \n        d\n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        )\n        =\n        ‖\n        x\n        −\n        \n          x\n          ′\n        \n        \n          ‖\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle d(x,x')=\\|x-x'\\|_{2}^{2}}\n  . Then the problem of searching for the optimal autoencoder is just a least-squares optimization:\n\nInterpretation\nAn autoencoder has two main parts: an encoder that maps the message to a code, and a decoder that reconstructs the message from the code. An optimal autoencoder would perform as close to perfect reconstruction as possible, with \"close to perfect\" defined by the reconstruction quality function \n  \n    d\n    d\n  .\nThe simplest way to perform the copying task perfectly would be to duplicate the signal. To suppress this behavior, the code space \n  \n    \n      \n        Z\n      \n    \n    {\\mathcal {Z}}\n   usually has fewer dimensions than the message space \n  \n    \n      \n        X\n      \n    \n    {\\mathcal {X}}\n  .\nSuch an autoencoder is called undercomplete. It can be interpreted as compressing the message, or reducing its dimensionality.At the limit of an ideal undercomplete autoencoder, every possible code \n  \n    z\n    z\n   in the code space is used to encode a message \n  \n    x\n    x\n   that really appears in the distribution \n  \n    \n      \n        \n          μ\n          \n            r\n            e\n            f\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ref}}\n  , and the decoder is also perfect: \n  \n    \n      \n        \n          D\n          \n            θ\n          \n        \n        (\n        \n          E\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        )\n        =\n        x\n      \n    \n    {\\displaystyle D_{\\theta }(E_{\\phi }(x))=x}\n  . This ideal autoencoder can then be used to generate messages indistinguishable from real messages, by feeding its decoder arbitrary code \n  \n    z\n    z\n   and obtaining \n  \n    \n      \n        \n          D\n          \n            θ\n          \n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle D_{\\theta }(z)}\n  , which is a message that really appears in the distribution \n  \n    \n      \n        \n          μ\n          \n            r\n            e\n            f\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ref}}\n  .\nIf the code space \n  \n    \n      \n        Z\n      \n    \n    {\\mathcal {Z}}\n   has dimension larger than (overcomplete), or equal to, the message space \n  \n    \n      \n        X\n      \n    \n    {\\mathcal {X}}\n  , or the hidden units are given enough capacity, an autoencoder can learn the identity function and become useless. However, experimental results found that overcomplete autoencoders might still learn useful features.In the ideal setting, the code dimension and the model capacity could be set on the basis of the complexity of the data distribution to be modeled. A standard way to do so is to add modifications to the basic autoencoder, to be detailed below.\n\nHistory\nThe autoencoder was first proposed as a nonlinear generalization of principal components analysis (PCA) by Kramer. The autoencoder has also been called the autoassociator, or Diabolo network. Its first applications date to early 1990s. Their most traditional application was dimensionality reduction or feature learning, but the concept became widely used for learning generative models of data. Some of the most powerful AIs in the 2010s involved autoencoders stacked inside deep neural networks.\n\nVariations\nRegularized autoencoders\nVarious techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.\n\nSparse autoencoder (SAE)\nInspired by the sparse coding hypothesis in neuroscience, sparse autoencoders are variants of autoencoders, such that the codes \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle E_{\\phi }(x)}\n   for messages tend to be sparse codes, that is, \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle E_{\\phi }(x)}\n   is close to zero in most entries. Sparse autoencoders may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time. Encouraging sparsity improves performance on classification tasks. There are two main ways to enforce sparsity. One way is to simply clamp all but the highest-k activations of the latent code to zero. This is the k-sparse autoencoder.The k-sparse autoencoder inserts the following \"k-sparse function\" in the latent layer of a standard autoencoder:where \n  \n    \n      \n        b\n        \n          i\n        \n      \n      =\n      1\n    \n    b_{i}=1\n   if \n  \n    \n      \n        \n          |\n        \n        \n          x\n          \n            i\n          \n        \n        \n          |\n        \n      \n    \n    {\\displaystyle |x_{i}|}\n   ranks in the top k, and 0 otherwise.\nBackpropagating through \n  \n    \n      f\n      \n        k\n      \n    \n    f_{k}\n   is simple: set gradient to 0 for \n  \n    \n      \n        b\n        \n          i\n        \n      \n      =\n      0\n    \n    b_{i}=0\n   entries, and keep gradient for \n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle b_{i}=1}\n   entries. This is essentially a generalized ReLU function.The other way is a relaxed version of the k-sparse autoencoder. Instead of forcing sparsity, we add a sparsity regularization loss, then optimize forwhere \n  \n    \n      λ\n      >\n      0\n    \n    \\lambda >0\n   measures how much sparsity we want to enforce.Let the autoencoder architecture have \n  \n    K\n    K\n   layers. To define a sparsity regularization loss, we need a \"desired\" sparsity \n  \n    \n      \n        \n          \n            \n              \n                ρ\n                ^\n              \n            \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\rho }}_{k}}\n   for each layer, a weight \n  \n    \n      w\n      \n        k\n      \n    \n    w_{k}\n   for how much to enforce each sparsity, and a function \n  \n    \n      \n        s\n        :\n        [\n        0\n        ,\n        1\n        ]\n        ×\n        [\n        0\n        ,\n        1\n        ]\n        →\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle s:[0,1]\\times [0,1]\\to [0,\\infty ]}\n   to measure how much two sparsities differ.\nFor each input \n  \n    x\n    x\n  , let the actual sparsity of activation in each layer \n  \n    k\n    k\n   bewhere \n  \n    \n      \n        \n          a\n          \n            k\n            ,\n            i\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle a_{k,i}(x)}\n   is the activation in the \n  \n    i\n    i\n   -th neuron of the \n  \n    k\n    k\n   -th layer upon input \n  \n    x\n    x\n  .\nThe sparsity loss upon input \n  \n    x\n    x\n   for one layer is \n  \n    \n      \n        s\n        (\n        \n          \n            \n              \n                ρ\n                ^\n              \n            \n          \n          \n            k\n          \n        \n        ,\n        \n          ρ\n          \n            k\n          \n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle s({\\hat {\\rho }}_{k},\\rho _{k}(x))}\n  , and the sparsity regularization loss for the entire autoencoder is the expected weighted sum of sparsity losses:Typically, the function \n  \n    s\n    s\n   is either the Kullback-Leibler (KL) divergence, as\n  \n    \n      \n        s\n        (\n        ρ\n        ,\n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        )\n        =\n        K\n        L\n        (\n        ρ\n        \n          |\n        \n        \n          |\n        \n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        )\n        =\n        ρ\n        log\n        ⁡\n        \n          \n            ρ\n            \n              \n                ρ\n                ^\n              \n            \n          \n        \n        +\n        (\n        1\n        −\n        ρ\n        )\n        log\n        ⁡\n        \n          \n            \n              1\n              −\n              ρ\n            \n            \n              1\n              −\n              \n                \n                  \n                    ρ\n                    ^\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle s(\\rho ,{\\hat {\\rho }})=KL(\\rho ||{\\hat {\\rho }})=\\rho \\log {\\frac {\\rho }{\\hat {\\rho }}}+(1-\\rho )\\log {\\frac {1-\\rho }{1-{\\hat {\\rho }}}}}\n  or the L1 loss, as \n  \n    \n      \n        s\n        (\n        ρ\n        ,\n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        )\n        =\n        \n          |\n        \n        ρ\n        −\n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        \n          |\n        \n      \n    \n    {\\displaystyle s(\\rho ,{\\hat {\\rho }})=|\\rho -{\\hat {\\rho }}|}\n  , or the L2 loss, as \n  \n    \n      \n        s\n        (\n        ρ\n        ,\n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        )\n        =\n        \n          |\n        \n        ρ\n        −\n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s(\\rho ,{\\hat {\\rho }})=|\\rho -{\\hat {\\rho }}|^{2}}\n  .\nAlternatively, the sparsity regularization loss may be defined without reference to any \"desired sparsity\", but simply force as much sparsity as possible. In this case, one can sparsity regularization loss as where \n  \n    \n      h\n      \n        k\n      \n    \n    h_{k}\n   is the activation vector in the \n  \n    k\n    k\n  -th layer of the autoencoder. The norm \n  \n    \n      ‖\n      ⋅\n      ‖\n    \n    \\|\\cdot \\|\n   is usually the L1 norm (giving the L1 sparse autoencoder) or the L2 norm (giving the L2 sparse autoencoder).\n\nDenoising autoencoder (DAE)\nDenoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion.A DAE is defined by adding a noise process to the standard autoencoder. A noise process is defined by a probability distribution \n  \n    \n      \n        \n          μ\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mu _{T}}\n   over functions \n  \n    \n      \n        T\n        :\n        \n          \n            X\n          \n        \n        →\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle T:{\\mathcal {X}}\\to {\\mathcal {X}}}\n  . That is, the function \n  \n    T\n    T\n   takes a message \n  \n    \n      \n        x\n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\mathcal {X}}}\n  , and corrupts it to a noisy version \n  \n    \n      T\n      (\n      x\n      )\n    \n    T(x)\n  . The function \n  \n    T\n    T\n   is selected randomly, with a probability distribution \n  \n    \n      \n        \n          μ\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mu _{T}}\n  .\nGiven a task \n  \n    \n      \n        (\n        \n          μ\n          \n            r\n            e\n            f\n          \n        \n        ,\n        d\n        )\n      \n    \n    {\\displaystyle (\\mu _{ref},d)}\n  , the problem of training a DAE is the optimization problem:That is, the optimal DAE should take any noisy message and attempt to recover the original message without noise, thus the name \"denoising\".\nUsually, the noise process \n  \n    T\n    T\n   is applied only during training and testing, not during downstream use.\nThe use of DAE depends on two assumptions:\n\nThere exist representations to the messages that are relatively stable and robust to the type of noise we are likely to encounter;\nThe said representations capture structures in the input distribution that are useful for our purposes.Example noise processes include:\n\nadditive isotropic Gaussian noise,\nmasking noise (a fraction of the input is randomly chosen and set to 0)\nsalt-and-pepper noise (a fraction of the input is randomly chosen and randomly set to its minimum or maximum value).\n\nContractive autoencoder (CAE)\nA contractive autoencoder adds the contractive regularization loss to the standard autoencoder loss:where \n  \n    \n      λ\n      >\n      0\n    \n    \\lambda >0\n   measures how much contractive-ness we want to enforce. The contractive regularization loss itself is defined as the expected Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input:To understand what \n  \n    \n      \n        \n          L\n          \n            c\n            o\n            n\n            t\n            r\n            a\n            c\n            t\n            i\n            v\n            e\n          \n        \n      \n    \n    {\\displaystyle L_{contractive}}\n   measures, note the factfor any message \n  \n    \n      \n        x\n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\mathcal {X}}}\n  , and small variation \n  \n    \n      δ\n      x\n    \n    \\delta x\n   in it. Thus, if \n  \n    \n      \n        ‖\n        \n          ∇\n          \n            x\n          \n        \n        \n          E\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        \n          ‖\n          \n            F\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|\\nabla _{x}E_{\\phi }(x)\\|_{F}^{2}}\n   is small, it means that a small neighborhood of the message maps to a small neighborhood of its code. This is a desired property, as it means small variation in the message leads to small, perhaps even zero, variation in its code, like how two pictures may look the same even if they are not exactly the same.\nThe DAE can be understood as an infinitesimal limit of CAE: in the limit of small Gaussian input noise, DAEs make the reconstruction function resist small but finite-sized input perturbations, while CAEs make the extracted features resist infinitesimal input perturbations.\n\nMinimal description length autoencoder\nConcrete autoencoder\nThe concrete autoencoder is designed for discrete feature selection. A concrete autoencoder forces the latent space to consist only of a user-specified number of features. The concrete autoencoder uses a continuous relaxation of the categorical distribution to allow gradients to pass through the feature selector layer, which makes it possible to use standard backpropagation to learn an optimal subset of input features that minimize reconstruction loss.\n\nVariational autoencoder (VAE)\nVariational autoencoders (VAEs) belong to the families of variational Bayesian methods. Despite the architectural similarities with basic autoencoders, VAEs are architecture with different goals and with a completely different mathematical formulation. The latent space is in this case composed by a mixture of distributions instead of a fixed vector.\nGiven an input dataset \n  \n    x\n    x\n   characterized by an unknown probability function \n  \n    \n      P\n      (\n      x\n      )\n    \n    P(x)\n   and a multivariate latent encoding vector \n  \n    z\n    z\n  , the objective is to model the data as a distribution \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x)}\n  , with \n  \n    θ\n    \\theta\n   defined as the set of the network parameters so that \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n        =\n        \n          ∫\n          \n            z\n          \n        \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        ,\n        z\n        )\n        d\n        z\n      \n    \n    {\\displaystyle p_{\\theta }(x)=\\int _{z}p_{\\theta }(x,z)dz}\n  .\n\nAdvantages of depth\nAutoencoders are often trained with a single layer encoder and a single layer decoder, but using many-layered (deep) encoders and decoders offers many advantages.\nDepth can exponentially reduce the computational cost of representing some functions.\nDepth can exponentially decrease the amount of training data needed to learn some functions.\nExperimentally, deep autoencoders yield better compression compared to shallow or linear autoencoders.\n\nTraining\nGeoffrey Hinton developed the deep belief network technique for training many-layered deep autoencoders. His method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that pretraining approximates a good solution, then using backpropagation to fine-tune the results.Researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders. A 2015 study showed that joint training learns better data models along with more representative features for classification as compared to the layerwise method. However, their experiments showed that the success of joint training depends heavily on the regularization strategies adopted.\n\nApplications\nThe two main applications of autoencoders are dimensionality reduction and information retrieval, but modern variations have been applied to other tasks.\n\nDimensionality reduction\nDimensionality reduction was one of the first deep learning applications.For Hinton's 2006 study, he pretrained a multi-layer autoencoder with a stack of RBMs and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until hitting a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 components of a principal component analysis (PCA), and learned a representation that was qualitatively easier to interpret, clearly separating data clusters.Representing dimensions can improve performance on tasks such as classification. Indeed, the hallmark of dimensionality reduction is to place semantically related examples near each other.\n\nPrincipal component analysis\nIf linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). The weights of an autoencoder with a single hidden layer of size \n  \n    p\n    p\n   (where \n  \n    p\n    p\n   is less than the size of the input) span the same vector subspace as the one spanned by the first \n  \n    p\n    p\n   principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition.However, the potential of autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct the input with significantly lower information loss.\n\nInformation retrieval\nInformation retrieval benefits particularly from dimensionality reduction in that search can become more efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to semantic hashing, proposed by Salakhutdinov and Hinton in 2007. By training the algorithm to produce a low-dimensional binary code, all database entries could be stored in a hash table mapping binary code vectors to entries. This table would then support information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the query encoding.\n\nAnomaly detection\nAnother application for autoencoders is anomaly detection. By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn to precisely reproduce the most frequently observed characteristics. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is small compared to the observation set so that its contribution to the learned representation could be ignored. After training, the autoencoder will accurately reconstruct \"normal\" data, while failing to do so with unfamiliar anomalous data. Reconstruction error (the error between the original data and its low dimensional reconstruction) is used as an anomaly score to detect anomalies.Recent literature has however shown that certain autoencoding models can, counterintuitively, be very good at reconstructing anomalous examples and consequently not able to reliably perform anomaly detection.\n\nImage processing\nThe characteristics of autoencoders are useful in image processing.\nOne example can be found in lossy image compression, where autoencoders outperformed other approaches and proved competitive against JPEG 2000.Another useful application of autoencoders in image preprocessing is image denoising.Autoencoders found use in more demanding contexts such as medical imaging where they have been used for image denoising as well as super-resolution. In image-assisted diagnosis, experiments have applied autoencoders for breast cancer detection and for modelling the relation between the cognitive decline of Alzheimer's disease and the latent features of an autoencoder trained with MRI.\n\nDrug discovery\nIn 2019 molecules generated with variational autoencoders were validated experimentally in mice.\n\nPopularity prediction\nRecently, a stacked autoencoder framework produced promising results in predicting popularity of social media posts, which is helpful for online advertising strategies.\n\nMachine translation\nAutoencoders have been applied to machine translation, which is usually referred to as neural machine translation (NMT). Unlike traditional autoencoders, the output does not match the input - it is in another language. In NMT, texts are treated as sequences to be encoded into the learning procedure, while on the decoder side sequences in the target language(s) are generated. Language-specific autoencoders incorporate further linguistic features into the learning procedure, such as Chinese decomposition features. Machine translation is rarely still done with autoencoders, but rather transformer networks.\n\nSee also\nRepresentation learning\nSparse dictionary learning\nDeep learning\n\n\n== References ==",
    "Automata theory": "Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science. The word automata comes from the Greek word αὐτόματος, which means \"self-acting, self-willed, self-moving\". An automaton (automata in plural) is an abstract self-propelled computing device which follows a predetermined sequence of operations automatically. An automaton with a finite number of states is called a Finite Automaton (FA) or Finite-State Machine (FSM). The figure on the right illustrates a finite-state machine, which is a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the previous state and current input symbol as its arguments.\nAutomata theory is closely related to formal language theory. In this context, automata are used as finite representations of formal languages that may be infinite. Automata are often classified by the class of formal languages they can recognize, as in the Chomsky hierarchy, which describes a nesting relationship between major classes of automata. Automata play a major role in the theory of computation, compiler construction, artificial intelligence, parsing and formal verification.\n\nHistory\nThe theory of abstract automata was developed in the mid-20th century in connection with finite automata. Automata theory was initially considered a branch of mathematical systems theory, studying the behavior of discrete-parameter systems. Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems. The theory of the finite-state transducer was developed under different names by different research communities. The earlier concept of Turing machine was also included in the discipline along with new forms of infinite-state automata, such as pushdown automata.\n1956 saw the publication of Automata Studies, which collected work by scientists including Claude Shannon, W. Ross Ashby, John von Neumann, Marvin Minsky, Edward F. Moore, and Stephen Cole Kleene. With the publication of this volume, \"automata theory emerged as a relatively autonomous discipline\". The book included Kleene's description of the set of regular events, or regular languages, and a relatively stable measure of complexity in Turing machine programs by Shannon. \nIn the same year, Noam Chomsky described the Chomsky hierarchy, a correspondence between automata and formal grammars, and Ross Ashby published An Introduction to Cybernetics, an accessible textbook explaining automata and information using basic set theory.\nThe study of linear bounded automata led to the Myhill–Nerode theorem, which gives a necessary and sufficient condition for a formal language to be regular, and an exact count of the number of states in a minimal machine for the language. The pumping lemma for regular languages, also useful in regularity proofs, was proven in this period by Michael O. Rabin and Dana Scott, along with the computational equivalence of deterministic and nondeterministic finite automata.In the 1960s, a body of algebraic results known as \"structure theory\" or \"algebraic decomposition theory\" emerged, which dealt with the realization of sequential machines from smaller machines by interconnection. While any finite automaton can be simulated using a  universal gate set, this requires that the simulating circuit contain loops of arbitrary complexity. Structure theory deals with the \"loop-free\" realizability of machines.\nThe theory of computational complexity also took shape in the 1960s. By the end of the decade, automata theory came to be seen as \"the pure mathematics of computer science\".\n\nAutomata\nWhat follows is a general definition of an automaton, which restricts a broader definition of a system to one viewed as acting in discrete time-steps, with its state behavior and outputs defined at each step by unchanging functions of only its state and input.\n\nInformal description\nAn automaton runs when it is given some sequence of inputs in discrete (individual) time steps (or just steps). An automaton processes one input picked from a set of symbols or letters, which is called an input alphabet. The symbols received by the automaton as input at any step are a sequence of symbols called words. An automaton has a set of states. At each moment during a run of the automaton, the automaton is in one of its states. When the automaton receives new input it moves to another state (or transitions) based on a transition function that takes the previous state and current input symbol as parameters. At the same time, another function called the output function produces symbols from the output alphabet, also according to the previous state and current input symbol. The automaton reads the symbols of the input word and transitions between states until the word is read completely, if it is finite in length, at which point the automaton halts. A state at which the automaton halts is called the final state.\nTo investigate the possible state/input/output sequences in an automaton using formal language theory, a machine can be assigned a starting state and a set of accepting states. Then, depending on whether a run starting from the starting state ends in an accepting state, the automaton can be said to accept or reject an input sequence. The set of all the words accepted by an automaton is called the language recognized by the automaton. A familiar example of a machine recognizing a language is an electronic lock, which accepts or rejects attempts to enter the correct code.\n\nFormal definition\nAutomatonAn automaton can be represented formally by a 5-tuple \n  \n    \n      \n        M\n        =\n        ⟨\n        Σ\n        ,\n        Γ\n        ,\n        Q\n        ,\n        δ\n        ,\n        λ\n        ⟩\n      \n    \n    {\\displaystyle M=\\langle \\Sigma ,\\Gamma ,Q,\\delta ,\\lambda \\rangle }\n  , where:\n\n  \n    Σ\n    \\Sigma\n   is a finite set of symbols, called the input alphabet of the automaton,\n\n  \n    Γ\n    \\Gamma\n   is another finite set of symbols, called the output alphabet of the automaton,\n\n  \n    Q\n    Q\n   is a set of states,\n\n  \n    δ\n    \\delta\n   is the next-state function or transition function \n  \n    \n      \n        δ\n        :\n        Q\n        ×\n        Σ\n        →\n        Q\n      \n    \n    {\\displaystyle \\delta :Q\\times \\Sigma \\to Q}\n   mapping state-input pairs to successor states,\n\n  \n    λ\n    \\lambda\n   is the next-output function \n  \n    \n      \n        λ\n        :\n        Q\n        ×\n        Σ\n        →\n        Γ\n      \n    \n    {\\displaystyle \\lambda :Q\\times \\Sigma \\to \\Gamma }\n   mapping state-input pairs to outputs.\nIf \n  \n    Q\n    Q\n   is finite, then \n  \n    M\n    M\n   is a finite automaton.Input word\nAn automaton reads a finite string of symbols \n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          a\n          \n            2\n          \n        \n        .\n        .\n        .\n        \n          a\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a_{1}a_{2}...a_{n}}\n  , where \n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n        ∈\n        Σ\n      \n    \n    {\\displaystyle a_{i}\\in \\Sigma }\n  , which is called an input word. The set of all words is denoted by \n  \n    \n      Σ\n      \n        ∗\n      \n    \n    \\Sigma ^{*}\n  .Run\nA sequence of states \n  \n    \n      \n        \n          q\n          \n            0\n          \n        \n        ,\n        \n          q\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          q\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle q_{0},q_{1},...,q_{n}}\n  , where \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        ∈\n        Q\n      \n    \n    {\\displaystyle q_{i}\\in Q}\n   such that \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        δ\n        (\n        \n          q\n          \n            i\n            −\n            1\n          \n        \n        ,\n        \n          a\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle q_{i}=\\delta (q_{i-1},a_{i})}\n   for \n  \n    \n      \n        0\n        <\n        i\n        ≤\n        n\n      \n    \n    {\\displaystyle 0<i\\leq n}\n  , is a run of the automaton on an input \n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          a\n          \n            2\n          \n        \n        .\n        .\n        .\n        \n          a\n          \n            n\n          \n        \n        ∈\n        \n          Σ\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle a_{1}a_{2}...a_{n}\\in \\Sigma ^{*}}\n   starting from state \n  \n    \n      q\n      \n        0\n      \n    \n    q_{0}\n  . In other words, at first the automaton is at the start state \n  \n    \n      q\n      \n        0\n      \n    \n    q_{0}\n  , and receives input \n  \n    \n      a\n      \n        1\n      \n    \n    a_{1}\n  . For \n  \n    \n      a\n      \n        1\n      \n    \n    a_{1}\n   and every following \n  \n    \n      a\n      \n        i\n      \n    \n    a_{i}\n   in the input string, the automaton picks the next state \n  \n    \n      q\n      \n        i\n      \n    \n    q_{i}\n   according to the transition function \n  \n    \n      \n        δ\n        (\n        \n          q\n          \n            i\n            −\n            1\n          \n        \n        ,\n        \n          a\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta (q_{i-1},a_{i})}\n  , until the last symbol \n  \n    \n      a\n      \n        n\n      \n    \n    a_{n}\n   has been read, leaving the machine in the final state of the run, \n  \n    \n      q\n      \n        n\n      \n    \n    q_{n}\n  . Similarly, at each step, the automaton emits an output symbol according to the output function \n  \n    \n      \n        λ\n        (\n        \n          q\n          \n            i\n            −\n            1\n          \n        \n        ,\n        \n          a\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\lambda (q_{i-1},a_{i})}\n  .The transition function \n  \n    δ\n    \\delta\n   is extended inductively into \n  \n    \n      \n        \n          \n            δ\n            ¯\n          \n        \n        :\n        Q\n        ×\n        \n          Σ\n          \n            ∗\n          \n        \n        →\n        Q\n      \n    \n    {\\displaystyle {\\overline {\\delta }}:Q\\times \\Sigma ^{*}\\to Q}\n   to describe the machine's behavior when fed whole input words. For the empty string \n  \n    ε\n    \\varepsilon\n  , \n  \n    \n      \n        \n          \n            δ\n            ¯\n          \n        \n        (\n        q\n        ,\n        ε\n        )\n        =\n        q\n      \n    \n    {\\displaystyle {\\overline {\\delta }}(q,\\varepsilon )=q}\n   for all states \n  \n    q\n    q\n  , and for strings \n  \n    \n      \n        w\n        a\n      \n    \n    {\\displaystyle wa}\n   where \n  \n    a\n    a\n   is the last symbol and \n  \n    w\n    w\n   is the (possibly empty) rest of the string, \n  \n    \n      \n        \n          \n            δ\n            ¯\n          \n        \n        (\n        q\n        ,\n        w\n        a\n        )\n        =\n        δ\n        (\n        \n          \n            δ\n            ¯\n          \n        \n        (\n        q\n        ,\n        w\n        )\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle {\\overline {\\delta }}(q,wa)=\\delta ({\\overline {\\delta }}(q,w),a)}\n  . The output function \n  \n    λ\n    \\lambda\n   may be extended similarly into \n  \n    \n      \n        \n          \n            λ\n            ¯\n          \n        \n        (\n        q\n        ,\n        w\n        )\n      \n    \n    {\\displaystyle {\\overline {\\lambda }}(q,w)}\n  , which gives the complete output of the machine when run on word \n  \n    w\n    w\n   from state \n  \n    q\n    q\n  .\nAcceptorIn order to study an automaton with the theory of formal languages, an automaton may be considered as an acceptor, replacing the output alphabet and function \n  \n    Γ\n    \\Gamma\n   and \n  \n    λ\n    \\lambda\n   with\n\n  \n    \n      \n        q\n        \n          0\n        \n      \n      ∈\n      Q\n    \n    q_{0}\\in Q\n  , a designated start state, and\n\n  \n    F\n    F\n  , a set of states of \n  \n    Q\n    Q\n   (i.e. \n  \n    \n      F\n      ⊆\n      Q\n    \n    F\\subseteq Q\n  ) called accept states.\nThis allows the following to be defined:Accepting word\nA word \n  \n    \n      \n        w\n        =\n        \n          a\n          \n            1\n          \n        \n        \n          a\n          \n            2\n          \n        \n        .\n        .\n        .\n        \n          a\n          \n            n\n          \n        \n        ∈\n        \n          Σ\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle w=a_{1}a_{2}...a_{n}\\in \\Sigma ^{*}}\n   is an accepting word for the automaton if \n  \n    \n      \n        \n          \n            δ\n            ¯\n          \n        \n        (\n        \n          q\n          \n            0\n          \n        \n        ,\n        w\n        )\n        ∈\n        F\n      \n    \n    {\\displaystyle {\\overline {\\delta }}(q_{0},w)\\in F}\n  , that is, if after consuming the whole string \n  \n    w\n    w\n   the machine is in an accept state.Recognized language\nThe language \n  \n    \n      \n        L\n        ⊆\n        \n          Σ\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle L\\subseteq \\Sigma ^{*}}\n   recognized by an automaton is the set of all the words that are accepted by the automaton, \n  \n    \n      \n        L\n        =\n        {\n        w\n        ∈\n        \n          Σ\n          \n            ∗\n          \n        \n         \n        \n          |\n        \n         \n        \n          \n            δ\n            ¯\n          \n        \n        (\n        \n          q\n          \n            0\n          \n        \n        ,\n        w\n        )\n        ∈\n        F\n        }\n      \n    \n    {\\displaystyle L=\\{w\\in \\Sigma ^{*}\\ |\\ {\\overline {\\delta }}(q_{0},w)\\in F\\}}\n  .Recognizable languages\nThe recognizable languages are the set of languages that are recognized by some automaton. For finite automata the recognizable languages are regular languages. For different types of automata, the recognizable languages are different.\n\nVariant definitions of automata\nAutomata are defined to study useful machines under mathematical formalism. So the definition of an automaton is open to variations according to the \"real world machine\" that we want to model using the automaton. People have studied many variations of automata. The following are some popular variations in the definition of different components of automata.\n\nInputFinite input: An automaton that accepts only finite sequences of symbols. The above introductory definition only encompasses finite words.\nInfinite input: An automaton that accepts infinite words (ω-words). Such automata are called ω-automata.\nTree input: The input may be a tree of symbols instead of sequence of symbols. In this case after reading each symbol, the automaton reads all the successor symbols in the input tree. It is said that the automaton makes one copy of itself for each successor and each such copy starts running on one of the successor symbols from the state according to the transition relation of the automaton. Such an automaton is called a tree automaton.\nInfinite tree input : The two extensions above can be combined, so the automaton reads a tree structure with (in)finite branches. Such an automaton is called an infinite tree automaton.StatesSingle state: An automaton with one state, also called a combinational circuit, performs a transformation which may implement combinational logic.\nFinite states: An automaton that contains only a finite number of states.\nInfinite states: An automaton that may not have a finite number of states, or even a countable number of states. Different kinds of abstract memory may be used to give such machines finite descriptions.\nStack memory: An automaton may also contain some extra memory in the form of a stack in which symbols can be pushed and popped. This kind of automaton is called a pushdown automaton.\nQueue memory: An automaton may have memory in the form of a queue. Such a machine is called queue machine and is Turing-complete.\nTape memory: The inputs and outputs of automata are often described as input and output tapes. Some machines have additional working tapes, including the Turing machine, linear bounded automaton, and log-space transducer.Transition functionDeterministic: For a given current state and an input symbol, if an automaton can only jump to one and only one state then it is a deterministic automaton.\nNondeterministic: An automaton that, after reading an input symbol, may jump into any of a number of states, as licensed by its transition relation. The term transition function is replaced by transition relation: The automaton non-deterministically decides to jump into one of the allowed choices. Such automata are called nondeterministic automata.\nAlternation: This idea is quite similar to tree automata but orthogonal. The automaton may run its multiple copies on the same next read symbol. Such automata are called alternating automata. The acceptance condition must be satisfied on all runs of such copies to accept the input.Acceptance conditionAcceptance of finite words: Same as described in the informal definition above.\nAcceptance of infinite words: an ω-automaton cannot have final states, as infinite words never terminate. Rather, acceptance of the word is decided by looking at the infinite sequence of visited states during the run.\nProbabilistic acceptance: An automaton need not strictly accept or reject an input. It may accept the input with some probability between zero and one. For example, quantum finite automata, geometric automata and metric automata have probabilistic acceptance.Different combinations of the above variations produce many classes of automata.\nAutomata theory is a subject matter that studies properties of various types of automata. For example, the following questions are studied about a given type of automata.\n\nWhich class of formal languages is recognizable by some type of automata? (Recognizable languages)\nAre certain automata closed under union, intersection, or complementation of formal languages? (Closure properties)\nHow expressive is a type of automata in terms of recognizing a class of formal languages? And, their relative expressive power? (Language hierarchy)Automata theory also studies the existence or nonexistence of any effective algorithms to solve problems similar to the following list:\n\nDoes an automaton accept at least one input word? (Emptiness checking)\nIs it possible to transform a given non-deterministic automaton into a deterministic automaton without changing the language recognized? (Determinization)\nFor a given formal language, what is the smallest automaton that recognizes it? (Minimization)\n\nTypes of automata\nThe following is an incomplete list of types of automata.\n\nDiscrete, continuous, and hybrid automata\nNormally automata theory describes the states of abstract machines but there are discrete automata, analog automata or continuous automata, or hybrid discrete-continuous automata, which use digital data, analog data or continuous time, or digital and analog data, respectively.\n\nHierarchy in terms of powers\nThe following is an incomplete hierarchy in terms of powers of different types of virtual machines. The hierarchy reflects the nested categories of languages the machines are able to accept.\n\nApplications\nEach model in automata theory plays important roles in several applied areas. Finite automata are used in text processing, compilers, and hardware design. Context-free grammar (CFGs) are used in programming languages and artificial intelligence. Originally, CFGs were used in the study of human languages. Cellular automata are used in the field of artificial life, the most famous example being John Conway's Game of Life. Some other examples which could be explained using automata theory in biology include mollusk and pine cone growth and pigmentation patterns. Going further, a theory suggesting that the whole universe is computed by some sort of a discrete automaton, is advocated by some scientists. The idea originated in the work of Konrad Zuse, and was popularized in America by Edward Fredkin. Automata also appear in the theory of finite fields: the set of irreducible polynomials that can be written as composition of degree two polynomials is in fact a regular language.\nAnother problem for which automata can be used is the induction of regular languages.\n\nAutomata simulators\nAutomata simulators are pedagogical tools used to teach, learn and research automata theory. An automata simulator takes as input the description of an automaton and then simulates its working for an arbitrary input string. The description of the automaton can be entered in several ways. An automaton can be defined in a symbolic language  or its specification may be entered in a predesigned form or its transition diagram may be drawn by clicking and dragging the mouse. Well known automata simulators include Turing's World, JFLAP, VAS, TAGS and SimStudio.\n\nConnection to category theory\nOne can define several distinct categories of automata following the automata classification into different types described in the previous section. The mathematical category of deterministic automata, sequential machines or sequential automata, and Turing machines with automata homomorphisms defining the arrows between automata is a Cartesian closed category, it has both categorical limits and colimits. An automata homomorphism maps a quintuple of an automaton Ai onto the quintuple of another automaton \n Aj. Automata homomorphisms can also be considered as automata transformations or as semigroup homomorphisms, when the state space, S, of the automaton is defined as a semigroup Sg. Monoids are also considered as a suitable setting for automata in monoidal categories.\nCategories of variable automataOne could also define a variable automaton, in the sense of Norbert Wiener in his book on The Human Use of Human Beings via the endomorphisms \n  \n    \n      \n        A\n        \n          i\n        \n      \n      →\n      \n        A\n        \n          i\n        \n      \n    \n    A_{i}\\to A_{i}\n  . Then one can show that such variable automata homomorphisms form a mathematical group. In the case of non-deterministic, or other complex kinds of automata, the latter set of endomorphisms may become, however, a variable automaton groupoid. Therefore, in the most general case, categories of variable automata of any kind are categories of groupoids or groupoid categories. Moreover, the category of reversible automata is then a \n2-category, and also a subcategory of the 2-category of groupoids, or the groupoid category.\n\nSee also\nBoolean differential calculus\n\nReferences\nFurther reading\nJohn E. Hopcroft; Rajeev Motwani; Jeffrey D. Ullman (2000). Introduction to Automata Theory, Languages, and Computation (2nd ed.). Pearson Education. ISBN 978-0-201-44124-6.\nMichael Sipser (1997). Introduction to the Theory of Computation. PWS Publishing. ISBN 978-0-534-94728-6. Part One: Automata and Languages, chapters 1–2, pp. 29–122. Section 4.1: Decidable Languages, pp. 152–159. Section 5.1: Undecidable Problems from Language Theory, pp. 172–183.\nElaine Rich (2008). Automata, Computability and Complexity: Theory and Applications. Pearson. ISBN 978-0-13-228806-4.\nSalomaa, Arto (1985). Computation and automata. Encyclopedia of Mathematics and Its Applications. Vol. 25. Cambridge University Press. ISBN 978-0-521-30245-6. Zbl 0565.68046.\nAnderson, James A. (2006). Automata theory with modern applications. With contributions by Tom Head. Cambridge: Cambridge University Press. ISBN 978-0-521-61324-8. Zbl 1127.68049.\nConway, J.H. (1971). Regular algebra and finite machines. Chapman and Hall Mathematics Series. London: Chapman & Hall. Zbl 0231.94041.\nJohn M. Howie (1991) Automata and Languages, Clarendon Press ISBN 0-19-853424-8 MR1254435\nSakarovitch, Jacques (2009). Elements of automata theory. Translated from the French by Reuben Thomas. Cambridge University Press. ISBN 978-0-521-84425-3. Zbl 1188.68177.\nJames P. Schmeiser; David T. Barnard (1995). Producing a top-down parse order with bottom-up parsing. Elsevier North-Holland.\nIgor Aleksander; F. Keith Hanna (1975). Automata Theory: An Engineering Approach. New York: Crane Russak. ISBN 978-0-8448-0657-0.\nMarvin Minsky (1967). Computation: Finite and infinite machines. Princeton, N.J.: Prentice Hall.\nJohn C. Martin (2011). Introduction to Languages and The Theory of Computation. New York: McGraw Hill. ISBN 978-0-07-319146-1.\n\nExternal links\ndk.brics.automaton\nlibfa",
    "Automated decision-making": "Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.\n\nOverview\nThere are different definitions of ADM based on the level of automation involved. Some definitions suggests ADM involves decisions made through purely technological means without human input, such as the EU's General Data Protection Regulation (Article 22). However, ADM technologies and applications can take many forms ranging from decision-support systems that make recommendations for human decision-makers to act on, sometimes known as augmented intelligence or 'shared decision-making', to fully automated decision-making processes that make decisions on behalf of individuals or organizations without human involvement. Models used in automated decision-making systems can be as simple as checklists and decision trees through to artificial intelligence and deep neural networks (DNN).\nSince the 1950s computers have gone from being able to do basic processing to having the capacity to undertake complex, ambiguous and highly skilled tasks such as image and speech recognition, gameplay, scientific and medical analysis and inferencing across multiple data sources. ADM is now being increasingly deployed across all sectors of society and many diverse domains from entertainment to transport.\nAn ADM system (ADMS) may involve multiple decision points, data sets, and technologies (ADMT) and may sit within a larger administrative or technical system such as a criminal justice system or business process.\n\nData\nAutomated decision-making involves the use of data as an input, either to be analysed within a process, model or algorithm, or for learning and generating new models. ADM systems may use and connect a wide range of data types and sources depending on the goals and contexts of the system, for example sensor data for self-driving cars and robotics, identity data for security systems, demographic and financial data for public administration, medical records in health, criminal records in law. This can sometimes involve vast amounts of data and computing power.\n\nData quality\nThe quality of the data that is available and able to be used in ADM systems is fundamental to the outcomes and is often highly problematic for many reasons. Datasets are often highly variable, large-scale data may be controlled by corporations or governments, restricted for privacy or security reasons, incomplete, biased, limited in terms of time or coverage, measuring and describing terms in different ways, and many other issues.\nFor machines to learn from data, large corpuses are often required which can be difficult to obtain or compute, however where available, have provided significant breakthroughs, for example in diagnosing chest x-rays.\n\nADM Technologies\nAutomated decision-making technologies (ADMT) are software-coded digital tools that automate the translation of input data to output data, contributing to the function of automated decision-making systems. There are a wide range of technologies in use across ADM applications and systems.\nADMTs involving basic computational operations\n\nSearch (includes 1-2-1, 1-2-many, data matching/merge)\nMatching (two different things)\nMathematical Calculation (formula)ADMTs for assessment and grouping:\n\nUser profiling\nRecommender systems\nClustering\nClassification\nFeature learning\nPredictive analytics (includes forecasting)ADMTs relating to space and flows:\n\nSocial network analysis (includes link prediction)\nMapping\nRoutingADMTs for processing of complex data formats\n\nImage processing\nAudio processing\nNatural Language Processing (NLP)Other ADMT\n\nBusiness rules management systems\nTime series analysis\nAnomaly detection\nModelling/Simulation\n\nMachine learning\nMachine learning (ML) involves training computer programs through exposure to large data sets and examples to learn from experience and solve problems. Machine learning can be used to generate and analyse data as well as make algorithmic calculations and has been applied to image and speech recognition, translations, text, data and simulations. While machine learning has been around for some time, it is becoming increasingly powerful due to recent breakthroughs in training deep neural networks (DNNs), and dramatic increases in data storage capacity and computational power with GPU coprocessors and cloud computing.Machine learning systems based on foundation models run on deep neural networks and use pattern matching to train a single huge system on large amounts of general data such as text and images. Early models tended to start from scratch for each new problem however since the early 2020s many are able to be adapted to new problems. Examples of these technologies include Open AI's DALL-E (an image creation program) and their various GPT language models, and Google's PaLM language model program.\n\nApplications\nADM is being used to replace or augment human decision-making by both public and private-sector organisations for a range of reasons including to help increase consistency, improve efficiency, reduce costs and enable new solutions to complex problems.\n\nDebate\nResearch and development are underway into uses of technology to assess argument quality, assess argumentative essays and judge debates. Potential applications of these argument technologies span education and society. Scenarios to consider, in these regards, include those involving the assessment and evaluation of conversational, mathematical, scientific, interpretive, legal, and political argumentation and debate.\n\nLaw\nIn legal systems around the world, algorithmic tools such as risk assessment instruments (RAI), are being used to supplement or replace the human judgment of judges, civil servants and police officers in many contexts. In the United States RAI are being used to generate scores to predict the risk of recidivism in pre-trial detention and sentencing decisions, evaluate parole for prisoners and to predict \"hot spots\" for future crime. These scores may result in automatic effects or may be used to inform decisions made by officials within the justice system. In Canada ADM has been used since 2014 to automate certain activities conducted by immigration officials and to support the evaluation of some immigrant and visitor applications.\n\nEconomics\nAutomated decision-making systems are used in certain computer programs to create buy and sell orders related to specific financial transactions and automatically submit the orders in the international markets. Computer programs can automatically generate orders based on predefined set of rules using trading strategies which are based on technical analyses, advanced statistical and mathematical computations, or inputs from other electronic sources.\n\nBusiness\nContinuous auditing\nContinuous auditing uses advanced analytical tools to automate auditing processes. It can be utilized in the private sector by business enterprises and in the public sector by governmental organizations and municipalities. As artificial intelligence and machine learning continue to advance, accountants and auditors may make use of increasingly sophisticated algorithms which make decisions such as those involving determining what is anomalous, whether to notify personnel, and how to prioritize those tasks assigned to personnel.\n\nMedia and Entertainment\nDigital media, entertainment platforms and information services increasingly provide content to audiences via automated recommender systems based on demographic information, previous selections, collaborative filtering or content-based filtering. This includes music and video platforms, publishing, health information, product databases and search engines. Many recommender systems also provide some agency to users in accepting recommendations and incorporate data-driven algorithmic feedback loops based on the actions of the system user.Large scale machine learning language models and image creation programs being developed by companies such as OpenAI and Google in the 2020s have restricted access however they are likely to have wide-spread application in fields such as advertising, copywriting, stock imagery and graphic design as well as other fields such as journalism and law.\n\nAdvertising\nOnline advertising is closely integrated with many digital media platforms, websites and search engines and often involves automated delivery of display advertisements in diverse formats. 'Programmatic' online advertising involves automating the sale and delivery of digital advertising on websites and platforms via software rather than direct human decision-making. This is sometimes known as the waterfall model which involves a sequence of steps across various systems and players: publishers and data management platforms, user data, ad servers and their delivery data, inventory management systems, ad traders and ad exchanges. There are various issues with this system including lack of transparency for advertisers, unverifiable metrics, lack of control over ad venues, audience tracking and privacy concerns. Internet users who dislike ads have adopted counter measures such as ad blocking technologies which allow users to automatically filter unwanted advertising from websites and some internet applications. In 2017, 24% of Australian internet users had ad blockers.\n\nHealth\nDeep learning AI image models are being used for reviewing x-rays and detecting the eye condition macular degeneration.\n\nSocial Services\nGovernments have been implementing digital technologies to provide more efficient administration and social services since the early 2000s, often referred to as e-government. Many governments around the world are now using automated, algorithmic systems for profiling and targeting policies and services including algorithmic policing based on risks, surveillance sorting of people such as airport screening, providing services based on risk profiles in child protection, providing employment services and governing the unemployed. A significant application of ADM in social services relates to the use of predictive analytics – eg predictions of risks to children from abuse/neglect in child protection, predictions of recidivism or crime in policing and criminal justice, predictions of welfare/tax fraud in compliance systems, predictions of long term unemployment in employment services. Historically these systems were based on standard statistical analyses, however from the early 2000s machine learning has increasingly been developed and deployed. Key issues with the use of ADM in social services include bias, fairness, accountability and explainability which refers to transparency around the reasons for a decision and the ability to explain the basis on which a machine made a decision. For example Australia's federal social security delivery agency, Centrelink, developed and implemented an automated processes for detecting and collecting debt which led to many cases of wrongful debt collection in what became known as the RoboDebt scheme.\n\nTransport and Mobility\nConnected and automated mobility (CAM) involves autonomous vehicles such as self-driving cars and other forms of transport which use automated decision-making systems to replace various aspects of human control of the vehicle. This can range from level 0 (complete human driving) to level 5 (completely autonomous). At level 5 the machine is able to make decisions to control the vehicle based on data models and geospatial mapping and real-time sensors and processing of the environment. Cars with levels 1 to 3 are already available on the market in 2021. In 2016 The German government established an 'Ethics Commission on Automated and Connected Driving' which recommended connected and automated vehicles (CAVs) be developed if the systems cause fewer accidents than human drivers (positive balance of risk). It also provided 20 ethical rules for the adaptation of automated and connected driving. In 2020 the European Commission strategy on CAMs recommended that they be adopted in Europe to reduce road fatalities and lower emissions however self-driving cars also raise many policy, security and legal issues in terms of liability and ethical decision-making in the case of accidents, as well as privacy issues. Issues of trust in autonomous vehicles and community concerns about their safety are key factors to be addressed if AVs are to be widely adopted.\n\nSurveillance\nAutomated digital data collections via sensors, cameras, online transactions and social media have significantly expanded the scope, scale, and goals of surveillance practices and institutions in government and commercial sectors. As a result there has been a major shift from targeted monitoring of suspects to the ability to monitor entire populations. The level of surveillance now possible as a result of automated data collection has been described as surveillance capitalism or surveillance economy to indicate the way digital media involves large-scale tracking and accumulation of data on every interaction.\n\nEthical and legal issues\nThere are many social, ethical and legal implications of automated decision-making systems. Concerns raised include lack of transparency and contestability of decisions, incursions on privacy and surveillance, exacerbating systemic bias and inequality due to data and algorithmic bias, intellectual property rights, the spread of misinformation via media platforms, administrative discrimination, risk and responsibility, unemployment and many others. As ADM becomes more ubiquitous there is greater need to address the ethical challenges to ensure good governance in information societies.ADM systems are often based on machine learning and algorithms which are not easily able to be viewed or analysed, leading to concerns that they are 'black box' systems which are not transparent or accountable.A report from Citizen lab in Canada argues for a critical human rights analysis of the application of ADM in various areas to ensure the use of automated decision-making does not result in infringements on rights, including the rights to equality and non-discrimination; freedom of movement, expression, religion, and association; privacy rights and the rights to life, liberty, and security of the person.Legislative responses to ADM include:\n\nThe European General Data Protection Regulation (GDPR), introduced in 2016, is a regulation in EU law on data protection and privacy in the European Union (EU). Article 22(1) enshrines the right of data subjects not to be subject to decisions, which have legal or other significant effects, being based solely on automatic individual decision making. GDPR also includes some rules on the right to explanation however the exact scope and nature of these is currently subject to pending review by the Court of Justice of the European Union. These provisions were not first introduced in the GDPR, but have been present in a similar form across Europe since the Data Protection Directive in 1995, and the 1978 French law, the loi informatique et libertés. Similarly scoped and worded provisions with varying attached rights and obligations are present in the data protection laws of many other jurisdictions across the world, including Uganda, Morocco and the US state of Virginia.\nRights for the explanation of public sector automated decisions forming 'algorithmic treatment' under the French loi pour une République numérique.\n\nBias\nADM may incorporate algorithmic bias arising from:\n\nData sources, where data inputs are biased in their collection or selection\nTechnical design of the algorithm, for example where assumptions have been made about how a person will behave\nEmergent bias, where the application of ADM in unanticipated circumstances creates a biased outcome\n\nExplainability\nQuestions of biased or incorrect data or algorithms and concerns that some ADMs are black box technologies, closed to human scrutiny or interrogation, has led to what is referred to as the issue of explainability, or the right to an explanation of automated decisions and AI. This is also known as Explainable AI (XAI), or Interpretable AI, in which the results of the solution can be analysed and understood by humans. XAI algorithms are considered to follow three principles - transparency, interpretability and explainability.\n\nInformation asymmetry\nAutomated decision-making may increase the information asymmetry between individuals whose data feeds into the system and the platforms and decision-making systems capable of inferring information from that data. On the other hand it has been observed that in financial trading the information asymmetry between two artificial intelligent agents may be much less than between two human agents or between human and machine agents.\n\nResearch fields\nMany academic disciplines and fields are increasingly turning their attention to the development, application and implications of ADM including business, computer sciences, human computer interaction (HCI), law, public administration, and media and communications. The automation of media content and algorithmically driven news, video and other content via search systems and platforms is a major focus of academic research in media studies.The ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include ADM and AI.\nKey research centres investigating ADM include: \n\nAlgorithm Watch, Germany\nARC Centre of Excellence for Automated Decision-Making and Society, Australia\nCitizen Lab, Canada\nInformatics Europe\n\nSee also\nAutomated decision support\nAlgorithmic bias\nDecision-making software\nDecision Management\nEthics of artificial intelligence\nGovernment by algorithm\nMachine learning\nRecommender systems\n\n\n== References ==",
    "Automated machine learning": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n\nComparison to the standard approach\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen by the machine learning expert. \nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation.\n\nTargets of automation\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\n\nData preparation and ingestion (from raw data and miscellaneous formats)\nColumn type detection; e.g., boolean, discrete numerical, continuous numerical, or text\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\nTask detection; e.g., binary classification, regression, clustering, or ranking\nFeature engineering\nFeature selection\nFeature extraction\nMeta-learning and transfer learning\nDetection and handling of skewed data and/or missing values\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\nHyperparameter optimization of the learning algorithm and featurization\nPipeline selection under time, memory, and complexity constraints\nSelection of evaluation metrics and validation procedures\nProblem checking\nLeakage detection\nMisconfiguration detection\nAnalysis of obtained results\nCreating user interfaces and visualizations\n\nSee also\nNeural architecture search\nNeuroevolution\nSelf-tuning\nNeural Network Intelligence\nAutoAI\nModelOps\n\nReferences\nFurther reading\n\"Open Source AutoML Tools: AutoGluon, TransmogrifAI, Auto-sklearn, and NNI\". Bizety. 2020-06-16.\nFerreira, Luís, et al. \"A comparison of AutoML tools for machine learning, deep learning and XGBoost.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021. https://repositorium.sdum.uminho.pt/bitstream/1822/74125/1/automl_ijcnn.pdf",
    "Automated medical diagnosis": "Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.\nCAD also has potential future applications in digital pathology with the advent of whole-slide imaging and machine learning algorithms. So far its application has been limited to quantifying immunostaining but is also being investigated for the standard H&E stain.CAD is an interdisciplinary technology combining elements of artificial intelligence and computer vision with radiological and pathology image processing. A typical application is the detection of a tumor. For instance, some hospitals use CAD to support preventive medical check-ups in mammography (diagnosis of breast cancer), the detection of polyps in Colonoscopy, and lung cancer.\nComputer-aided detection (CADe) systems are usually confined to marking conspicuous structures and sections. Computer-aided diagnosis (CADx) systems evaluate the conspicuous structures. For example, in mammography CAD highlights microcalcification clusters and hyperdense structures in the soft tissue. This allows the radiologist to draw conclusions about the condition of the pathology. Another application is CADq, which quantifies, e.g., the size of a tumor or the tumor's behavior in contrast medium uptake. Computer-aided simple triage (CAST) is another type of CAD, which performs a fully automatic initial interpretation and triage of studies into some meaningful categories (e.g. negative and positive). CAST is particularly applicable in emergency diagnostic imaging, where a prompt diagnosis of critical, life-threatening condition is required.\nAlthough CAD has been used in clinical environments for over 40 years, CAD usually does not substitute the doctor or other professional, but rather plays a supporting role. The professional (generally a radiologist) is generally responsible for the final interpretation of a medical image. However, the goal of some CAD systems is to detect earliest signs of abnormality in patients that human professionals cannot, as in diabetic retinopathy, architectural distortion in mammograms, ground-glass nodules in thoracic CT, and non-polypoid (“flat”) lesions in CT colonography.\n\nTopics\nA Brief History\nIn the late 1950s, with the dawn of modern computers researchers in various fields started exploring the possibility of building computer-aided medical diagnostic (CAD) systems. These first CAD systems used flow-charts, statistical pattern-matching, probability theory or knowledge bases to drive their decision-making process.Since the early 1970s, some of the very early CAD systems in medicine, which were often referred as “expert systems” in medicine, were developed and used mainly for educational purposes.  The MYCIN expert system, the Internist-I expert system and the CADUCEUS (expert system) are some of such examples.\nDuring the beginning of the early developments, the researchers were aiming at building entirely automated CAD / expert systems.  The expectation of what computers can do was unrealistically optimistic among these scientists. However, after the breakthrough paper, “Reducibility among Combinatorial Problems” by Richard M. Karp, it became clear that there were limitations but also potential opportunities when one develops algorithms to solve groups of important computational problems.As result of the new understanding of the various algorithmic limitations that Karp discovered in the early 1970s, researchers started realizing the serious limitations that CAD and expert systems in medicine have. The recognition of these limitations brought the investigators to develop new kinds of CAD systems by using advanced approaches.  Thus, by the late 1980s and early 1990s the focus sifted in the use of data mining approaches for the purpose of using more advanced and flexible CAD systems.\nIn 1998, the first commercial CAD system for mammography, the ImageChecker system, was approved by the US Food and Drug Administration (FDA). In the following years several commercial CAD systems for analyzing mammography, breast MRI, medical imagining of lung, colon, and heart also received FDA approvals.  Currently, CAD systems are used as a diagnostic aid to provide physicians for better medical decision-making.\n\nMethodology\nCAD is fundamentally based on highly complex pattern recognition. X-ray or other types of images are scanned for suspicious structures. Normally a few thousand images are required to optimize the algorithm. Digital image data are copied to a CAD server in a DICOM-format and are prepared and analyzed in several steps.\n1. Preprocessing for\n\nReduction of artifacts (bugs in images)\nImage noise reduction\nLeveling (harmonization) of image quality (increased contrast) for clearing the image's different basic conditions e.g. different exposure parameter.\nFiltering2. Segmentation for\n\nDifferentiation of different structures in the image, e.g. heart, lung, ribcage, blood vessels, possible round lesions\nMatching with anatomic databank\nSample gray-values in volume of interest3. Structure/ROI (Region of Interest) Analyze\nEvery detected region is analyzed individually for special characteristics:\n\nCompactness\nForm, size and location\nReference to close by structures / ROIs\nAverage grey level value analyze within a ROI\nProportion of grey levels to border of the structure inside the ROI4. Evaluation / classification\nAfter the structure is analyzed, every ROI is evaluated individually (scoring) for the probability of a TP. The following procedures are examples of classification algorithms.\n\nNearest-Neighbor Rule (e.g. k-nearest neighbors)\nMinimum distance classifier\nCascade classifier\nNaive Bayes classifier\nArtificial neural network\nRadial basis function network (RBF)\nSupport vector machine (SVM)\nPrincipal component analysis (PCA)If the detected structures have reached a certain threshold level, they are highlighted in the image for the radiologist. Depending on the CAD system these markings can be permanently or temporary saved. The latter's advantage is that only the markings which are approved by the radiologist are saved. False hits should not be saved, because an examination at a later date becomes more difficult then.\n\nSensitivity and specificity\nCAD systems seek to highlight suspicious structures. Today's CAD systems cannot detect 100% of pathological changes. The hit rate (sensitivity) can be up to 90% depending on system and application.\nA correct hit is termed a True Positive (TP), while the incorrect marking of healthy sections constitutes a False Positive (FP). The less FPs indicated, the higher the specificity is. A low specificity reduces the acceptance of the CAD system because the user has to identify all of these wrong hits. The FP-rate in lung overview examinations (CAD Chest) could be reduced to 2 per examination. In other segments (e.g. CT lung examinations) the FP-rate could be 25 or more. In CAST systems the FP rate must be extremely low (less than 1 per examination) to allow a meaningful study triage.\n\nAbsolute detection rate\nThe absolute detection rate of the radiologist is an alternative metric to sensitivity and specificity. Overall, results of clinical trials about sensitivity, specificity, and the absolute detection rate can vary markedly. Each study result depends on its basic conditions and has to be evaluated on those terms. The following facts have a strong influence:\n\nRetrospective or prospective design\nQuality of the used images\nCondition of the x-ray examination\nRadiologist's experience and education\nType of lesion\nSize of the considered lesion\n\nChallenges that CAD in Medicine Faces Today\nDespite the many developments that CAD has achieved since the dawn of computers, there are still certain challenges that CAD systems face today.Some challenges are related to various algorithmic limitations in the procedures of a CAD system including input data collection, preprocessing, processing and system assessments. Algorithms are generally designed to select a single likely diagnosis, thus providing suboptimal results for patients with multiple, concurrent disorders. Today input data for CAD mostly come from electronic health records (EHR). Effective designing, implementing and analyzing for EHR is a major necessity on any CAD systems.Due to the massive availability of data and the need to analyze such data, big data is also one of the biggest challenges that CAD systems face today. The increasingly vast amount of patient data is a serious problem. Often the patient data are complex and can be semi-structured or unstructured data. It requires highly developed approaches to store, retrieve and analyze them in reasonable time.During the preprocessing stage, input data requires to be normalized. The normalization of input data includes noise reduction, and filtering. Processing may contain a few sub-steps depending on applications.  Basic three sub-steps on medical imaging are segmentation, feature extraction / selection and classification. These sub-steps require advanced techniques to analyze input data with less computational time. Although much effort has been devoted on creating innovative techniques for these procedures of CAD systems, there is still not the single best algorithm for each step. Ongoing studies in building innovative algorithms for all the aspects of CAD systems is essential.There is also a lack of standardized assessment measures for CAD Systems. This fact may cause the difficulty for obtaining FDA approval for commercial use. Moreover, while many positive developments of CAD systems have been proven, studies for validating their algorithms for clinical practice has hardly been confirmed.Other challenges are related to the problem for healthcare providers to adopt new CAD systems in clinical practice.  Some negative studies may discourage the use of CAD.  In addition, the lack of training of health professionals on the use of CAD sometimes brings the incorrect interpretation of the system outcomes.  These challenges are described in more detail in.\n\nApplications\nCAD is used in the diagnosis of breast cancer, lung cancer, colon cancer, prostate cancer, bone metastases, coronary artery disease, congenital heart defect, pathological brain detection, fracture detection, Alzheimer's disease, and diabetic retinopathy.\n\nBreast cancer\nCAD is used in screening mammography (X-ray examination of the female breast). Screening mammography is used for the early detection of breast cancer. CAD systems are often utilized to help classify a tumor as malignant or benign. CAD is especially established in US and the Netherlands and is used in addition to human evaluation, usually by a radiologist. The first CAD system for mammography was developed in a research project at the University of Chicago. Today it is commercially offered by iCAD and Hologic. However, while achieving high sensitivities, CAD systems tend to have very low specificity and the benefits of using CAD remain uncertain. A 2008 systematic review on computer-aided detection in screening mammography concluded that CAD does not have a significant effect on cancer detection rate, but does undesirably increase recall rate (i.e. the rate of false positives). However, it noted considerable heterogeneity in the impact on recall rate across studies.Recent advances in machine learning, deep-learning and artificial intelligence technology have enabled the development of CAD systems that are clinically proven to assist radiologists in addressing the challenges of reading mammographic images by improving cancer detection rates and reducing false positives and unnecessary patient recalls, while significantly decreasing reading times.Procedures to evaluate mammography based on magnetic resonance imaging exist too.\n\nLung cancer (bronchial carcinoma)\nIn the diagnosis of lung cancer, computed tomography with special three-dimensional CAD systems are established and considered as appropriate second opinions. At this a volumetric dataset with up to 3,000 single images is prepared and analyzed. Round lesions (lung cancer, metastases and benign changes) from 1 mm are detectable. Today all well-known vendors of medical systems offer corresponding solutions.\nEarly detection of lung cancer is valuable. However, the random detection of lung cancer in the early stage (stage 1) in the X-ray image is difficult. Round lesions that vary from 5–10 mm are easily overlooked.\nThe routine application of CAD Chest Systems may help to detect small changes without initial suspicion. A number of researchers developed CAD systems for detection of lung nodules (round lesions less than 30 mm) in chest radiography and CT, and CAD systems for diagnosis (e.g., distinction between malignant and benign) of lung nodules in CT. Virtual dual-energy imaging improved the performance of CAD systems in chest radiography.\n\nColon cancer\nCAD is available for detection of colorectal polyps in the colon in CT colonography. Polyps are small growths that arise from the inner lining of the colon. CAD detects the polyps by identifying their characteristic \"bump-like\" shape. To avoid excessive false positives, CAD ignores the normal colon wall, including the haustral folds.\n\nCardiovascular disease\nState-of-the-art methods in cardiovascular computing, cardiovascular informatics, and mathematical and computational modeling can provide valuable tools in clinical decision-making. CAD systems with novel image-analysis-based markers as input can aid vascular physicians to decide with higher confidence on best suitable treatment for cardiovascular disease patients.\nReliable early-detection and risk-stratification of carotid atherosclerosis is of outmost importance for predicting strokes in asymptomatic patients. \nTo this end, various noninvasive and low-cost markers have been proposed, using ultrasound-image-based features. These combine echogenicity, texture, and motion characteristics to assist clinical decision towards improved prediction, assessment and management of cardiovascular risk.CAD is available for the automatic detection of significant (causing more than 50% stenosis) coronary artery disease in coronary CT angiography (CCTA) studies.\n\nCongenital heart defect\nEarly detection of pathology can be the difference between life and death. CADe can be done by auscultation with a digital stethoscope and specialized software, also known as Computer-aided auscultation. Murmurs, irregular heart sounds, caused by blood flowing through a defective heart, can be detected with high sensitivity and specificity. Computer-aided auscultation is sensitive to external noise and bodily sounds and requires an almost silent environment to function accurately.\n\nPathological brain detection (PBD)\nChaplot et al. was the first to use Discrete Wavelet Transform (DWT) coefficients to detect pathological brains. Maitra and Chatterjee employed the Slantlet transform, which is an improved version of DWT. Their feature vector of each image is created by considering the magnitudes of Slantlet transform outputs corresponding to six spatial positions chosen according to a specific logic.In 2010, Wang and Wu presented a forward neural network (FNN) based method to classify a given MR brain image as normal or abnormal. The parameters of FNN were optimized via adaptive chaotic particle swarm optimization (ACPSO). Results over 160 images showed that the classification accuracy was 98.75%.In 2011, Wu and Wang proposed using DWT for feature extraction, PCA for feature reduction, and FNN with scaled chaotic artificial bee colony (SCABC) as classifier.In 2013, Saritha et al. were the first to apply wavelet entropy (WE) to detect pathological brains. Saritha also suggested to use spider-web plots. Later, Zhang et al. proved removing spider-web plots did not influence the performance. Genetic pattern search method was applied to identify abnormal brain from normal controls. Its classification accuracy was reported as 95.188%. Das et al. proposed to use Ripplet transform. Zhang et al. proposed to use particle swarm optimization (PSO). Kalbkhani et al. suggested to use GARCH model.In 2014, El-Dahshan et al. suggested to use pulse coupled neural network.In 2015, Zhou et al. suggested to apply naive Bayes classifier to detect pathological brains.\n\nAlzheimer's disease\nCADs can be used to identify subjects with Alzheimer's and mild cognitive impairment from normal elder controls.\nIn 2014, Padma et al. used combined wavelet statistical texture features to segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel support vector machine decision tree had 80% classification accuracy, with an average computation time of 0.022s for each image classification.In 2019, Signaevsky et al. have first reported a trained Fully Convolutional Network (FCN) for detection and quantification of neurofibrillary tangles (NFT) in Alzheimer's disease and an array of other tauopathies. The trained FCN achieved high precision and recall in naive digital whole slide image (WSI) semantic segmentation, correctly identifying NFT objects using a SegNet model trained for 200 epochs. The FCN reached near-practical efficiency with average processing time of 45 min per WSI per Graphic Processing Unit (GPU), enabling reliable and reproducible large-scale detection of NFTs. The measured performance on test data of eight naive WSI across various tauopathies resulted in the recall, precision, and an F1 score of 0.92, 0.72, and 0.81, respectively.Eigenbrain is a novel brain feature that can help to detect AD, based on Principal Component Analysis or Independent Component Analysis decomposition. Polynomial kernel SVM has been shown to achieve good accuracy. The polynomial KSVM performs better than linear SVM and RBF kernel SVM. Other approaches with decent results involve the use of texture analysis, morphological features, or high-order statistical features\n\nNuclear medicine\nCADx is available for nuclear medicine images. Commercial CADx systems for the diagnosis of bone metastases in whole-body bone scans and coronary artery disease in myocardial perfusion images exist.With a high sensitivity and an acceptable false lesions detection rate, computer-aided automatic lesion detection system is demonstrated as useful and will probably in the future be able to help nuclear medicine physicians to identify possible bone lesions.\n\nDiabetic retinopathy\nDiabetic retinopathy is a disease of the retina that is diagnosed predominantly by fundoscopic images. Diabetic patients in industrialised countries generally undergo regular screening for the condition. Imaging is used to recognize early signs of abnormal retinal blood vessels. Manual analysis of these images can be time-consuming and unreliable. CAD has been employed to enhance the accuracy, sensitivity, and specificity of automated detection method. The use of some CAD systems to replace human graders can be safe and cost effective.Image pre-processing, and feature extraction and classification are two main stages of these CAD algorithms.\n\nPre-processing methods\nImage normalization is minimizing the variation across the entire image. Intensity variations in areas between periphery and central macular region of the eye have been reported to cause inaccuracy of vessel segmentation. Based on the 2014 review, this technique was the most frequently used and appeared in 11 out of 40 recently (since 2011) published primary research.\nHistogram equalization is useful in enhancing contrast within an image. This technique is used to increase local contrast. At the end of the processing, areas that were dark in the input image would be brightened, greatly enhancing the contrast among the features present in the area. On the other hand, brighter areas in the input image would remain bright or be reduced in brightness to equalize with the other areas in the image. Besides vessel segmentation, other features related to diabetic retinopathy can be further separated by using this pre-processing technique. Microaneurysm and hemorrhages are red lesions, whereas exudates are yellow spots. Increasing contrast between these two groups allow better visualization of lesions on images. With this technique, 2014 review found that 10 out of the 14 recently (since 2011) published primary research.Green channel filtering is another technique that is useful in differentiating lesions rather than vessels. This method is important because it provides the maximal contrast between diabetic retinopathy-related lesions. Microaneurysms and hemorrhages are red lesions that appear dark after application of green channel filtering. In contrast, exudates, which appear yellow in normal image, are transformed into bright white spots after green filtering. This technique is mostly used according to the 2014 review, with appearance in 27 out of 40 published articles in the past three years. In addition, green channel filtering can be used to detect center of optic disc in conjunction with double-windowing system.Non-uniform illumination correction is a technique that adjusts for non-uniform illumination in fundoscopic image. Non-uniform illumination can be a potential error in automated detection of diabetic retinopathy because of changes in statistical characteristics of image. These changes can affect latter processing such as feature extraction and are not observable by humans. Correction of non-uniform illumination (f') can be achieved by modifying the pixel intensity using known original pixel intensity (f), and average intensities of local (λ) and desired pixels (μ) (see formula below). Walter-Klein transformation is then applied to achieve the uniform illumination. This technique is the least used pre-processing method in the review from 2014.\n\n  \n    \n      \n        \n          f\n          ′\n        \n        =\n        f\n        +\n        μ\n        −\n        λ\n      \n    \n    {\\displaystyle f'=f+\\mu -\\lambda }\n  \nMorphological operations is the second least used pre-processing method in 2014 review. The main objective of this method is to provide contrast enhancement, especially darker regions compared to background.\n\nFeature extractions and classifications\nAfter pre-processing of funduscopic image, the image will be further analyzed using different computational methods. However, the current literature agreed that some methods are used more often than others during vessel segmentation analyses. These methods are SVM, multi-scale, vessel-tracking, region growing approach, and model-based approaches.\n\nSupport vector machine is by far the most frequently used classifier in vessel segmentation, up to 90% of cases. SVM is a supervised learning model that belongs to the broader category of pattern recognition technique. The algorithm works by creating a largest gap between distinct samples in the data. The goal is to create the largest gap between these components that minimize the potential error in classification. In order to successfully segregate blood vessel information from the rest of the eye image, SVM algorithm creates support vectors that separate the blood vessel pixel from the rest of the image through a supervised environment. Detecting blood vessel from new images can be done through similar manner using support vectors. Combination with other pre-processing technique, such as green channel filtering, greatly improves the accuracy of detection of blood vessel abnormalities. Some beneficial properties of SVM include\nFlexibility – Highly flexible in terms of function\nSimplicity – Simple, especially with large datasets (only support vectors are needed to create separation between data)Multi-scale approach is a multiple resolution approach in vessel segmentation. At low resolution, large-diameter vessels can first be extracted. By increasing resolution, smaller branches from the large vessels can be easily recognized. Therefore, one advantage of using this technique is the increased analytical speed. Additionally, this approach can be used with 3D images. The surface representation is a surface normal to the curvature of the vessels, allowing the detection of abnormalities on vessel surface.Vessel tracking is the ability of the algorithm to detect \"centerline\" of vessels. These centerlines are maximal peak of vessel curvature. Centers of vessels can be found using directional information that is provided by Gaussian filter. Similar approaches that utilize the concept of centerline are the skeleton-based and differential geometry-based.Region growing approach is a method of detecting neighboring pixels with similarities. A seed point is required for such method to start. Two elements are needed for this technique to work: similarity and spatial proximity. A neighboring pixel to the seed pixel with similar intensity is likely to be the same type and will be added to the growing region. One disadvantage of this technique is that it requires manual selection of seed point, which introduces bias and inconsistency in the algorithm. This technique is also being used in optic disc identification.Model-based approaches employ representation to extract vessels from images. Three broad categories of model-based are known: deformable, parametric, and template matching. Deformable methods uses objects that will be deformed to fit the contours of the objects on the image. Parametric uses geometric parameters such as tubular, cylinder, or ellipsoid representation of blood vessels. Classical snake contour in combination with blood vessel topological information can also be used as a model-based approach. Lastly, template matching is the usage of a template, fitted by stochastic deformation process using Hidden Markov Mode 1.\n\nEffects on employment\nAutomation of medical diagnosis labor (for example, quantifying red blood cells) has some historical precedent. The deep learning revolution of the 2010s has already produced AIs that are more accurate in many areas of visual diagnosis than radiologists and dermatologists, and this gap is expected to grow. Some experts, including many doctors, are dismissive of the effects that AI will have on medical specialties. In contrast, many economists and artificial intelligence experts believe that fields such as radiology will be massively disrupted, with unemployment or downward pressure on the wages of radiologists; hospitals will need fewer radiologists overall, and many of the radiologists who still exist will require substantial retraining. Geoffrey Hinton, the \"Godfather of deep learning\", argues that (in view of the likely advances expected in the next five or ten years) hospitals should immediately stop training radiologists, as their time-consuming and expensive training on visual diagnosis will soon be mostly obsolete, leading to a glut of traditional radiologists. An op-ed in JAMA argues that pathologists and radiologists should merge into a single \"information specialist\" role, and state that \"To avoid being replaced by computers, radiologists must allow themselves to be displaced by computers.\" Information specialists would be trained in \"Bayesian logic, statistics, data science\", and some genomics and biometrics; manual visual pattern recognition would be greatly de-emphasized compared with current onerous radiology training.\n\nSee also\nComputerized Systems Used In Clinical Trials\nDiagnostic robot\n\nReferences\nExternal links\nDigital Retinal Images for Vessel Extraction (DRIVE)\nSTructured Analysis of the REtina (STARE)\nHigh-Resolution Fundus (HRF) Image Database",
    "Automated planning and scheduling": "Automated planning and scheduling, sometimes denoted as simply AI planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\nIn known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.\n\nOverview\nGiven a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).\nThe difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.\n\nAre the actions deterministic or non-deterministic? For nondeterministic actions, are the associated probabilities available?\nAre the state variables discrete or continuous? If they are discrete, do they have only a finite number of possible values?\nCan the current state be observed unambiguously? There can be full observability and partial observability.\nHow many initial states are there, finite or arbitrarily many?\nDo actions have a duration?\nCan several actions be taken concurrently, or is only one action possible at a time?\nIs the objective of a plan to reach a designated goal state, or to maximize a reward function?\nIs there only one agent or are there several agents? Are the agents cooperative or selfish? Do all of the agents construct their own plans separately, or are the plans constructed centrally for all agents?The simplest possible planning problem, known as the Classical Planning Problem, is determined by:\n\na unique known initial state,\ndurationless actions,\ndeterministic actions,\nwhich can be taken only one at a time,\nand a single agent.Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.\nFurther, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.\nWith nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.\nDiscrete-time Markov decision processes (MDP) are planning problems with:\n\ndurationless actions,\nnondeterministic actions with probabilities,\nfull observability,\nmaximization of a reward function,\nand a single agent.When full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP).\nIf there are more than one agent, we have multi-agent planning, which is closely related to game theory.\n\nDomain independent planning\nIn AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.\n\nPlanning domain modelling languages\nThe most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.\nAn alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.\n\nAlgorithms for planning\nClassical planning\nforward chaining state space search, possibly enhanced with heuristics\nbackward chaining search, possibly enhanced by the use of state constraints (see STRIPS, graphplan)\npartial-order planning\n\nReduction to other problems\nreduction to the propositional satisfiability problem (satplan).\nreduction to Model checking - both are essentially problems of traversing state spaces, and the classical planning problem corresponds to a subclass of model checking problems.\n\nTemporal planning\nTemporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied.\n\nProbabilistic planning\nProbabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.\nWith partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.\n\nPreference-based planning\nIn preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.\n\nConditional planning\nDeterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree. The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.An early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s. What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed. A major advantage of conditional planning is the ability to handle partial plans. An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems.\n\nContingent planning\nWe speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning. The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.\nMichael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete. A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete and 2EXPTIME-complete if the goal is specified with LDLf.\n\nConformant planning\nConformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning, but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete, and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.\n\nDeployment of planning systems\nThe Hubble Space Telescope uses a short-term system called SPSS and a long-term planning system called Spike.\n\nSee also\nAction description language\nActor model\nApplications of artificial intelligence\nConstraint satisfaction problem\nReactive planning\nScheduling (computing)\nStrategy (game theory)ListsList of SMT solvers\nList of constraint programming languages\nList of emerging technologies\nOutline of artificial intelligence\n\nReferences\nFurther reading\nVlahavas, I. \"Planning and Scheduling\". EETN. Archived from the original on 2013-12-22.\n\nExternal links\nInternational Conference on Automated Planning and Scheduling",
    "Automated theorem proving": "Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs.  Automated reasoning over mathematical proof was a major impetus for the development of computer science.\n\nLogical foundations\nWhile the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege's Begriffsschrift (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic.  His Foundations of Arithmetic, published in 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential Principia Mathematica, first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.In 1929, Mojżesz Presburger showed that the theory of natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.\nHowever, shortly after this positive result, Kurt Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems (1931), showing that in any sufficiently strong axiomatic system there are true statements which cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples for undecidable questions.\n\nFirst implementations\nShortly after World War II, the first general purpose computers became available. In 1954, Martin Davis programmed Presburger's algorithm for a JOHNNIAC vacuum tube computer at the Institute for Advanced Study in Princeton, New Jersey. According to Davis, \"Its great triumph was to prove that the sum of two even numbers is even\". More ambitious was the Logic Theory Machine in 1956, a deduction system for the propositional logic of the Principia Mathematica, developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theory Machine constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the Principia.The \"heuristic\" approach of the Logic Theory Machine tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle.  In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore's program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.\n\nDecidability of the problem\nDepending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the frequent case of propositional logic, the problem is decidable but co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first order predicate calculus, Gödel's completeness theorem states that the theorems (provable statements) are exactly the logically valid well-formed formulas, so identifying valid formulas is recursively enumerable: given unbounded resources, any valid formula can eventually be proven. However, invalid formulas (those that are not entailed by a given theory), cannot always be recognized.\nThe above applies to first order theories, such as Peano arithmetic. However, for a specific model that may be described by a first order theory, some statements may be true but undecidable in the theory used to describe the model. For example, by Gödel's incompleteness theorem, we know that any theory whose proper axioms are true for the natural numbers cannot prove all first order statements true for the natural numbers, even if the list of proper axioms is allowed to be infinite enumerable. It follows that an automated theorem prover will fail to terminate while searching for a proof precisely when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve many hard problems, even in models that are not fully described by any first order theory (such as the integers).\n\nRelated problems\nA simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.\nSince the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial and various techniques aiming at making the prover's output smaller, and consequently more easily understandable and checkable, have been developed.\nProof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.\nAnother distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference.  Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).\nThere are hybrid theorem proving systems which use model checking as an inference rule. There are also programs which were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof which was essentially impossible to verify by humans due to the enormous size of the program's calculation (such proofs are called non-surveyable proofs).  Another example of a program-assisted proof is the one that shows that the game of Connect Four can always be won by the first player.\n\nIndustrial uses\nCommercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification.  Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.\n\nFirst-order theorem proving\nIn the late 1960s agencies funding research in automated deduction began to emphasize the need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied to the problem of verifying the correctness of computer programs in languages such as Pascal, Ada, etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using John Alan Robinson's resolution principle. This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.First-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling fully automated systems. More expressive logics, such as Higher-order logics, allow the convenient expression of a wider range of problems than first order logic, but theorem proving for these logics is less well developed.\n\nBenchmarks, competitions, and sources\nThe quality of implemented systems has benefited from the existence of a large library of standard benchmark examples — the Thousands of Problems for Theorem Provers (TPTP) Problem Library  — as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.\nSome important systems (all have won at least one CASC competition division) are listed below.\n\nE is a high-performance prover for full first-order logic, but built on a purely equational calculus, originally developed in the automated reasoning group of Technical University of Munich under the direction of Wolfgang Bibel, and now at Baden-Württemberg Cooperative State University in Stuttgart.\nOtter, developed at the Argonne National Laboratory, is based on first-order resolution and paramodulation. Otter has since been replaced by Prover9, which is paired with Mace4.\nSETHEO is a high-performance system based on the goal-directed model elimination calculus, originally developed by a team under direction of Wolfgang Bibel. E and SETHEO have been combined (with other systems) in the composite theorem prover E-SETHEO.\nVampire was originally developed and implemented at Manchester University by Andrei Voronkov and Krystof Hoder. It is now developed by a growing international team. It has won the FOF division (among other divisions) at the CADE ATP System Competition regularly since 2001.\nWaldmeister is a specialized system for unit-equational first-order logic developed by Arnim Buch and Thomas Hillenbrand. It won the CASC UEQ division for fourteen consecutive years (1997–2010).\nSPASS is a first order logic theorem prover with equality. This is developed by the research group Automation of Logic, Max Planck Institute for Computer Science.The Theorem Prover Museum is an initiative to conserve the sources of theorem prover systems for future analysis, since they are important cultural/scientific artefacts. It has the sources of many of the systems mentioned above.\n\nPopular techniques\nFirst-order resolution with unification\nModel elimination\nMethod of analytic tableaux\nSuperposition and term rewriting\nModel checking\nMathematical induction\nBinary decision diagrams\nDPLL\nHigher-order unification\nQuantifier elimination\n\nSoftware systems\nFree software\nAlt-Ergo\nAutomath\nCVC\nE\nGödel machine\nIsaPlanner\nLCF\nMizar\nNuPRL\nParadox\nProver9\nPVS\nSPARK (programming language)\nTwelf\nZ3 Theorem Prover\n\nProprietary software\nCARINE\nWolfram Mathematica\nResearchCyc\n\nSee also\nNotes\nReferences\nExternal links\nA list of theorem proving tools",
    "Automatic differentiation": "In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\nAutomatic differentiation exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor of more arithmetic operations than the original program.\n\nDifference from other differentiation methods\nAutomatic differentiation is distinct from symbolic differentiation and numerical differentiation. \nSymbolic differentiation faces the difficulty of converting a computer program into a single mathematical expression and can lead to inefficient code. Numerical differentiation (the method of finite differences) can introduce round-off errors in the discretization process and cancellation. Both of these classical methods have problems with calculating higher derivatives, where complexity and errors increase. Finally, both of these classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems.\n\nForward and reverse accumulation\nChain rule of partial derivatives of composite functions\nFundamental to automatic differentiation is the decomposition of differentials provided by the chain rule of partial derivatives of composite functions. For the simple composition\n\nthe chain rule gives\n\nTwo types of automatic differentiation\nUsually, two distinct modes of automatic differentiation are presented.\n\nforward accumulation (also called bottom-up, forward mode, or tangent mode)\nreverse accumulation (also called top-down, reverse mode, or adjoint mode)Forward accumulation specifies that one traverses the chain rule from inside to outside (that is, first compute \n  \n    \n      \n        ∂\n        \n          w\n          \n            1\n          \n        \n        \n          /\n        \n        ∂\n        x\n      \n    \n    {\\displaystyle \\partial w_{1}/\\partial x}\n   and then \n  \n    \n      \n        ∂\n        \n          w\n          \n            2\n          \n        \n        \n          /\n        \n        ∂\n        \n          w\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\partial w_{2}/\\partial w_{1}}\n   and at last \n  \n    \n      \n        ∂\n        y\n        \n          /\n        \n        ∂\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\partial y/\\partial w_{2}}\n  ), while reverse accumulation has the traversal from outside to inside (first compute \n  \n    \n      \n        ∂\n        y\n        \n          /\n        \n        ∂\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\partial y/\\partial w_{2}}\n   and then \n  \n    \n      \n        ∂\n        \n          w\n          \n            2\n          \n        \n        \n          /\n        \n        ∂\n        \n          w\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\partial w_{2}/\\partial w_{1}}\n   and at last \n  \n    \n      \n        ∂\n        \n          w\n          \n            1\n          \n        \n        \n          /\n        \n        ∂\n        x\n      \n    \n    {\\displaystyle \\partial w_{1}/\\partial x}\n  ). More succinctly,\n\nForward accumulation computes the recursive relation: \n  \n    \n      \n        \n          \n            \n              ∂\n              \n                w\n                \n                  i\n                \n              \n            \n            \n              ∂\n              x\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                w\n                \n                  i\n                \n              \n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  −\n                  1\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                w\n                \n                  i\n                  −\n                  1\n                \n              \n            \n            \n              ∂\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial w_{i}}{\\partial x}}={\\frac {\\partial w_{i}}{\\partial w_{i-1}}}{\\frac {\\partial w_{i-1}}{\\partial x}}}\n   with \n  \n    \n      \n        \n          w\n          \n            3\n          \n        \n        =\n        y\n      \n    \n    {\\displaystyle w_{3}=y}\n  , and,\nReverse accumulation computes the recursive relation: \n  \n    \n      \n        \n          \n            \n              ∂\n              y\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              y\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  +\n                  1\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                w\n                \n                  i\n                  +\n                  1\n                \n              \n            \n            \n              ∂\n              \n                w\n                \n                  i\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial y}{\\partial w_{i}}}={\\frac {\\partial y}{\\partial w_{i+1}}}{\\frac {\\partial w_{i+1}}{\\partial w_{i}}}}\n   with \n  \n    \n      \n        \n          w\n          \n            0\n          \n        \n        =\n        x\n      \n    \n    {\\displaystyle w_{0}=x}\n  .The value of the partial derivative, called seed, is propagated forward or backward and is initially \n  \n    \n      \n        \n          \n            \n              ∂\n              x\n            \n            \n              ∂\n              x\n            \n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle {\\frac {\\partial x}{\\partial x}}=1}\n   or \n  \n    \n      \n        \n          \n            \n              ∂\n              y\n            \n            \n              ∂\n              y\n            \n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle {\\frac {\\partial y}{\\partial y}}=1}\n  . Forward accumulation evaluates the function and calculates the derivative with respect to one independent variable in one pass. For each independent variable \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      \n        x\n        \n          2\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n    \n    x_{1},x_{2},\\dots ,x_{n}\n   a separate pass is therefore necessary in which the derivative with respect to that independent variable is set to one (\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                x\n                \n                  1\n                \n              \n            \n            \n              ∂\n              \n                x\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle {\\frac {\\partial x_{1}}{\\partial x_{1}}}=1}\n  ) and of all others to zero (\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                x\n                \n                  2\n                \n              \n            \n            \n              ∂\n              \n                x\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        ⋯\n        =\n        \n          \n            \n              ∂\n              \n                x\n                \n                  n\n                \n              \n            \n            \n              ∂\n              \n                x\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial x_{2}}{\\partial x_{1}}}=\\dots ={\\frac {\\partial x_{n}}{\\partial x_{1}}}=0}\n  ). In contrast, reverse accumulation requires the evaluated partial functions for the partial derivatives. Reverse accumulation therefore evaluates the function first and calculates the derivatives with respect to all independent variables in an additional pass.\nWhich of these two types should be used depends on the sweep count. The computational complexity of one sweep is proportional to the complexity of the original code.\n\nForward accumulation is more efficient than reverse accumulation for functions f : Rn → Rm with n ≪ m as only n sweeps are necessary, compared to m sweeps for reverse accumulation.\nReverse accumulation is more efficient than forward accumulation for functions f : Rn → Rm with n ≫ m as only m sweeps are necessary, compared to n sweeps for forward accumulation.Backpropagation of errors in multilayer perceptrons, a technique used in machine learning, is a special case of reverse accumulation.Forward accumulation was introduced by R.E. Wengert in 1964. According to Andreas Griewank, reverse accumulation has been suggested since the late 1960s, but the inventor is unknown. Seppo Linnainmaa published reverse accumulation in 1976.\n\nForward accumulation\nIn forward accumulation AD, one first fixes the independent variable with respect to which differentiation is performed and computes the derivative of each sub-expression recursively. In a pen-and-paper calculation, this involves repeatedly substituting the derivative of the inner functions in the chain rule:\n\nThis can be generalized to multiple variables as a matrix product of Jacobians.\nCompared to reverse accumulation, forward accumulation is natural and easy to implement as the flow of derivative information coincides with the order of evaluation. Each variable \n  \n    \n      w\n      \n        i\n      \n    \n    w_{i}\n   is augmented with its derivative \n  \n    \n      \n        \n          \n            \n              \n                w\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\dot {w}}_{i}}\n   (stored as a numerical value, not a symbolic expression),\n\nas denoted by the dot. The derivatives are then computed in sync with the evaluation steps and combined with other derivatives via the chain rule.\nUsing the chain rule, if \n  \n    \n      w\n      \n        i\n      \n    \n    w_{i}\n   has predecessors in the computational graph:\n\n  \n    \n      \n        \n          \n            \n              \n                w\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          ∑\n          \n            j\n            ∈\n            {\n            \n              predecessors of i\n            \n            }\n          \n        \n        \n          \n            \n              ∂\n              \n                w\n                \n                  i\n                \n              \n            \n            \n              ∂\n              \n                w\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                w\n                ˙\n              \n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\dot {w}}_{i}=\\sum _{j\\in \\{{\\text{predecessors of i}}\\}}{\\frac {\\partial w_{i}}{\\partial w_{j}}}{\\dot {w}}_{j}}\n  As an example, consider the function:\n\nFor clarity, the individual sub-expressions have been labeled with the variables \n  \n    \n      w\n      \n        i\n      \n    \n    w_{i}\n  .\nThe choice of the independent variable to which differentiation is performed affects the seed values ẇ1 and ẇ2. Given interest in the derivative of this function with respect to x1, the seed values should be set to:\n\nWith the seed values set, the values propagate using the chain rule as shown. Figure 2 shows a pictorial depiction of this process as a computational graph.\n\nTo compute the gradient of this example function, which requires not only \n  \n    \n      \n        \n          \n            \n              \n                ∂\n                y\n              \n              \n                ∂\n                \n                  x\n                  \n                    1\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {\\partial y}{\\partial x_{1}}}}\n   but also \n  \n    \n      \n        \n          \n            \n              \n                ∂\n                y\n              \n              \n                ∂\n                \n                  x\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {\\partial y}{\\partial x_{2}}}}\n  , an additional sweep is performed over the computational graph using the seed values \n  \n    \n      \n        \n          \n            \n              w\n              ˙\n            \n          \n        \n        \n          1\n        \n      \n      =\n      0\n      ;\n      \n        \n          \n            \n              w\n              ˙\n            \n          \n        \n        \n          2\n        \n      \n      =\n      1\n    \n    {\\dot {w}}_{1}=0;{\\dot {w}}_{2}=1\n  .\n\nImplementation\nPseudo Code\nForward accumulation calculates the function and the derivative (but only for one independent variable each) in one pass. The associated method call expects the expression Z to be derived with regard to a variable V. The method returns a pair of the evaluated function and its derivation. The method traverses the expression tree recursively until a variable is reached. If the derivative with respect to this variable is requested, its derivative is 1, 0 otherwise. Then the partial function as well as the partial derivative are evaluated.\n\nC++\nReverse accumulation\nIn reverse accumulation AD, the dependent variable to be differentiated is fixed and the derivative is computed with respect to each sub-expression recursively. In a pen-and-paper calculation, the derivative of the outer functions is repeatedly substituted in the chain rule:\n\nIn reverse accumulation, the quantity of interest is the adjoint, denoted with a bar \n  \n    \n      \n        \n          \n            \n              \n                w\n                ¯\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\bar {w}}_{i}}\n  ; it is a derivative of a chosen dependent variable with respect to a subexpression \n  \n    \n      w\n      \n        i\n      \n    \n    w_{i}\n  :\n\nUsing the chain rule, if \n  \n    \n      w\n      \n        i\n      \n    \n    w_{i}\n   has successors in the computational graph:\n\n  \n    \n      \n        \n          \n            \n              \n                w\n                ¯\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          ∑\n          \n            j\n            ∈\n            {\n            \n              successors of i\n            \n            }\n          \n        \n        \n          \n            \n              \n                w\n                ¯\n              \n            \n          \n          \n            j\n          \n        \n        \n          \n            \n              ∂\n              \n                w\n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                w\n                \n                  i\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {w}}_{i}=\\sum _{j\\in \\{{\\text{successors of i}}\\}}{\\bar {w}}_{j}{\\frac {\\partial w_{j}}{\\partial w_{i}}}}\n  Reverse accumulation traverses the chain rule from outside to inside, or in the case of the computational graph in Figure 3, from top to bottom. The example function is scalar-valued, and thus there is only one seed for the derivative computation, and only one sweep of the computational graph is needed to calculate the (two-component) gradient. This is only half the work when compared to forward accumulation, but reverse accumulation requires the storage of the intermediate variables wi as well as the instructions that produced them in a data structure known as a \"tape\" or a Wengert list (however, Wengert published forward accumulation, not reverse accumulation), which may consume significant memory if the computational graph is large. This can be mitigated to some extent by storing only a subset of the intermediate variables and then reconstructing the necessary work variables by repeating the evaluations, a technique known as rematerialization. Checkpointing is also used to save intermediary states.\n\nThe operations to compute the derivative using reverse accumulation are shown in the table below (note the reversed order):\n\nThe data flow graph of a computation can be manipulated to calculate the gradient of its original calculation. This is done by adding an adjoint node for each primal node, connected by adjoint edges which parallel the primal edges but flow in the opposite direction. The nodes in the adjoint graph represent multiplication by the derivatives of the functions calculated by the nodes in the primal. For instance, addition in the primal causes fanout in the adjoint; fanout in the primal causes addition in the adjoint; a unary function y = f(x) in the primal causes x̄ = ȳ f′(x) in the adjoint; etc.\n\nImplementation\nPseudo Code\nReverse accumulation requires two passes: In the forward pass, the function is evaluated first and the partial results are cached. In the reverse pass, the partial derivatives are calculated and the previously derived value is backpropagated. The corresponding method call expects the expression Z to be derived and seed with the derived value of the parent expression. For the top expression, Z derived with regard to Z, this is 1. The method traverses the expression tree recursively until a variable is reached and adds the current seed value to the derivative expression.\n\nPython\nImplementation in Python without tapes.\n\nC++\nBeyond forward and reverse accumulation\nForward and reverse accumulation are just two (extreme) ways of traversing the chain rule. The problem of computing a full Jacobian of f : Rn → Rm with a minimum number of arithmetic operations is known as the optimal Jacobian accumulation (OJA) problem, which is NP-complete. Central to this proof is the idea that algebraic dependencies may exist between the local partials that label the edges of the graph. In particular, two or more edge labels may be recognized as equal. The complexity of the problem is still open if it is assumed that all edge labels are unique and algebraically independent.\n\nAutomatic differentiation using dual numbers\nForward mode automatic differentiation is accomplished by augmenting the algebra of real numbers and obtaining a new arithmetic. An additional component is added to every number to represent the derivative of a function at the number, and all arithmetic operators are extended for the augmented algebra. The augmented algebra is the algebra of dual numbers.\nReplace every number \n  \n    \n      \n      x\n    \n    \\,x\n   with the number \n  \n    \n      x\n      +\n      \n        x\n        ′\n      \n      ε\n    \n    x+x'\\varepsilon\n  , where \n  \n    \n      x\n      ′\n    \n    x'\n   is a real number, but \n  \n    ε\n    \\varepsilon\n   is an abstract number with the property \n  \n    \n      \n        ε\n        \n          2\n        \n      \n      =\n      0\n    \n    \\varepsilon ^{2}=0\n   (an infinitesimal; see Smooth infinitesimal analysis). Using only this, regular arithmetic gives\n\nusing \n  \n    \n      \n        (\n        1\n        +\n        \n          y\n          ′\n        \n        ε\n        \n          /\n        \n        y\n        )\n        ⋅\n        (\n        1\n        −\n        \n          y\n          ′\n        \n        ε\n        \n          /\n        \n        y\n        )\n        =\n        1\n      \n    \n    {\\displaystyle (1+y'\\varepsilon /y)\\cdot (1-y'\\varepsilon /y)=1}\n  .\nNow, polynomials can be calculated in this augmented arithmetic. If \n  \n    \n      P\n      (\n      x\n      )\n      =\n      \n        p\n        \n          0\n        \n      \n      +\n      \n        p\n        \n          1\n        \n      \n      x\n      +\n      \n        p\n        \n          2\n        \n      \n      \n        x\n        \n          2\n        \n      \n      +\n      ⋯\n      +\n      \n        p\n        \n          n\n        \n      \n      \n        x\n        \n          n\n        \n      \n    \n    P(x)=p_{0}+p_{1}x+p_{2}x^{2}+\\cdots +p_{n}x^{n}\n  , then\n\nwhere \n  \n    \n      P\n      \n        (\n        1\n        )\n      \n    \n    P^{(1)}\n   denotes the derivative of \n  \n    P\n    P\n   with respect to its first argument, and \n  \n    \n      x\n      ′\n    \n    x'\n  , called a seed, can be chosen arbitrarily.\nThe new arithmetic consists of ordered pairs, elements written \n  \n    \n      ⟨\n      x\n      ,\n      \n        x\n        ′\n      \n      ⟩\n    \n    \\langle x,x'\\rangle\n  , with ordinary arithmetics on the first component, and first order differentiation arithmetic on the second component, as described above. Extending the above results on polynomials to analytic functions gives a list of the basic arithmetic and some standard functions for the new arithmetic:\n\nand in general for the primitive function \n  \n    g\n    g\n  ,\n\nwhere \n  \n    \n      g\n      \n        u\n      \n    \n    g_{u}\n   and \n  \n    \n      g\n      \n        v\n      \n    \n    g_{v}\n   are the derivatives of \n  \n    g\n    g\n   with respect to its first and second arguments, respectively.\nWhen a binary basic arithmetic operation is applied to mixed arguments—the pair \n  \n    \n      ⟨\n      u\n      ,\n      \n        u\n        ′\n      \n      ⟩\n    \n    \\langle u,u'\\rangle\n   and the real number \n  \n    c\n    c\n  —the real number is first lifted to \n  \n    \n      ⟨\n      c\n      ,\n      0\n      ⟩\n    \n    \\langle c,0\\rangle\n  . The derivative of a function \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} }\n   at the point \n  \n    \n      x\n      \n        0\n      \n    \n    x_{0}\n   is now found by calculating \n  \n    \n      f\n      (\n      ⟨\n      \n        x\n        \n          0\n        \n      \n      ,\n      1\n      ⟩\n      )\n    \n    f(\\langle x_{0},1\\rangle )\n   using the above arithmetic, which gives \n  \n    \n      ⟨\n      f\n      (\n      \n        x\n        \n          0\n        \n      \n      )\n      ,\n      \n        f\n        ′\n      \n      (\n      \n        x\n        \n          0\n        \n      \n      )\n      ⟩\n    \n    \\langle f(x_{0}),f'(x_{0})\\rangle\n   as the result.\n\nVector arguments and functions\nMultivariate functions can be handled with the same efficiency and mechanisms as univariate functions by adopting a directional derivative operator. That is, if it is sufficient to compute \n  \n    \n      \n        y\n        ′\n      \n      =\n      ∇\n      f\n      (\n      x\n      )\n      ⋅\n      \n        x\n        ′\n      \n    \n    y'=\\nabla f(x)\\cdot x'\n  , the directional derivative \n  \n    \n      \n        \n          y\n          ′\n        \n        ∈\n        \n          \n            R\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle y'\\in \\mathbb {R} ^{m}}\n   of \n  \n    \n      \n        f\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        →\n        \n          \n            R\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} ^{n}\\to \\mathbb {R} ^{m}}\n   at \n  \n    \n      \n        x\n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x\\in \\mathbb {R} ^{n}}\n   in the direction \n  \n    \n      \n        \n          x\n          ′\n        \n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x'\\in \\mathbb {R} ^{n}}\n   may be calculated as \n  \n    \n      (\n      ⟨\n      \n        y\n        \n          1\n        \n      \n      ,\n      \n        y\n        \n          1\n        \n        ′\n      \n      ⟩\n      ,\n      …\n      ,\n      ⟨\n      \n        y\n        \n          m\n        \n      \n      ,\n      \n        y\n        \n          m\n        \n        ′\n      \n      ⟩\n      )\n      =\n      f\n      (\n      ⟨\n      \n        x\n        \n          1\n        \n      \n      ,\n      \n        x\n        \n          1\n        \n        ′\n      \n      ⟩\n      ,\n      …\n      ,\n      ⟨\n      \n        x\n        \n          n\n        \n      \n      ,\n      \n        x\n        \n          n\n        \n        ′\n      \n      ⟩\n      )\n    \n    (\\langle y_{1},y'_{1}\\rangle ,\\ldots ,\\langle y_{m},y'_{m}\\rangle )=f(\\langle x_{1},x'_{1}\\rangle ,\\ldots ,\\langle x_{n},x'_{n}\\rangle )\n   using the same arithmetic as above. If all the elements of \n  \n    \n      ∇\n      f\n    \n    \\nabla f\n   are desired, then \n  \n    n\n    n\n   function evaluations are required. Note that in many optimization applications, the directional derivative is indeed sufficient.\n\nHigh order and many variables\nThe above arithmetic can be generalized to calculate second order and higher derivatives of multivariate functions. However, the arithmetic rules quickly grow complicated: complexity is quadratic in the highest derivative degree. Instead, truncated Taylor polynomial algebra can be used. The resulting arithmetic, defined on generalized dual numbers, allows efficient computation using functions as if they were a data type. Once the Taylor polynomial of a function is known, the derivatives are easily extracted.\n\nImplementation\nForward-mode AD is implemented by a nonstandard interpretation of the program in which real numbers are replaced by dual numbers, constants are lifted to dual numbers with a zero epsilon coefficient, and the numeric primitives are lifted to operate on dual numbers. This nonstandard interpretation is generally implemented using one of two strategies: source code transformation or operator overloading.\n\nSource code transformation (SCT)\nThe source code for a function is replaced by an automatically generated source code that includes statements for calculating the derivatives interleaved with the original instructions.\nSource code transformation can be implemented for all programming languages, and it is also easier for the compiler to do compile time optimizations. However, the implementation of the AD tool itself is more difficult and the build system is more complex. Examples of source code transformation tools include the Enzyme tool for LLVM/MLIR (and thus differentiates C/C++, Julia, Rust, Fortran, Python, etc) and the \nTapenade tool for Fortran/C.\n\nOperator overloading (OO)\nOperator overloading is a possibility for source code written in a language supporting it. Objects for real numbers and elementary mathematical operations must be overloaded to cater for the augmented arithmetic depicted above. This requires no change in the form or sequence of operations in the original source code for the function to be differentiated, but often requires changes in basic data types for numbers and vectors to support overloading and often also involves the insertion of special flagging operations. Due to the inherent operator overloading overhead on each loop, this approach usually demonstrates weaker speed performance.\nExamples of operator-overloading implementations of automatic differentiation in C++ are:\n\nAdept\nNAG's dco library\nStan libraries\nXAD  open-source tool\n\nOperator overloading and Source Code Transformation\nOverloaded Operators can be used to extract the valuation graph, followed by automatic generation of the AD-version of the primal function at run-time. Unlike the classic OO AAD, such AD-function does not change from one iteration to the next one. Hence there is any OO or tape interpretation run-time overhead per Xi sample.\nWith the AD-function being generated at runtime, it can be optimised to take into account the current state of the program and precompute certain values. In addition, it can be generated in a way to consistently utilize native CPU vectorization to process 4(8)-double chunks of user data (AVX2\\AVX512 speed up x4-x8). With multithreading added into account, such approach can lead to a final acceleration of order 8 × #Cores compared to the traditional AAD tools. A reference implementation is available on GitHub.\n\nSee also\nDifferentiable programming\n\nNotes\nReferences\nFurther reading\nRall, Louis B. (1981). Automatic Differentiation: Techniques and Applications. Lecture Notes in Computer Science. Vol. 120. Springer. ISBN 978-3-540-10861-0.\nGriewank, Andreas; Walther, Andrea (2008). Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Other Titles in Applied Mathematics. Vol. 105 (2nd ed.). SIAM. ISBN 978-0-89871-659-7.\nNeidinger, Richard (2010). \"Introduction to Automatic Differentiation and MATLAB Object-Oriented Programming\" (PDF). SIAM Review. 52 (3): 545–563. CiteSeerX 10.1.1.362.6580. doi:10.1137/080743627. S2CID 17134969. Retrieved 2013-03-15.\nNaumann, Uwe (2012). The Art of Differentiating Computer Programs. Software-Environments-tools. SIAM. ISBN 978-1-611972-06-1.\nHenrard, Marc (2017). Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Palgrave Macmillan. ISBN 978-3-319-53978-2.\n\nExternal links\nwww.autodiff.org, An \"entry site to everything you want to know about automatic differentiation\"\nAutomatic Differentiation of Parallel OpenMP Programs\nAutomatic Differentiation, C++ Templates and Photogrammetry\nAutomatic Differentiation, Operator Overloading Approach\nCompute analytic derivatives of any Fortran77, Fortran95, or C program through a web-based interface Automatic Differentiation of Fortran programs\nDescription and example code for forward Automatic Differentiation in Scala\nfinmath-lib stochastic automatic differentiation, Automatic differentiation for random variables (Java implementation of the stochastic automatic differentiation).\nAdjoint Algorithmic Differentiation: Calibration and Implicit Function Theorem\nC++ Template-based automatic differentiation article and implementation\nTangent Source-to-Source Debuggable Derivatives\nExact First- and Second-Order Greeks by Algorithmic Differentiation\nAdjoint Algorithmic Differentiation of a GPU Accelerated Application\nAdjoint Methods in Computational Finance Software Tool Support for Algorithmic Differentiationop\nMore than a Thousand Fold Speed Up for xVA Pricing Calculations with Intel Xeon Scalable Processors",
    "Autonomous car": "A self-driving car, also known as an autonomous car, driverless car, or robotic car (robo-car), is a car that is capable of traveling without human input. Self-driving cars use sensors to perceive their surroundings, such as optical and thermographic cameras, radar, lidar, ultrasound/sonar, GPS, odometry and inertial measurement units. Control systems interpret sensory information to create a three-dimensional model of the vehicle's surroundings. Based on the model, the car then identifies an appropriate navigation path and strategies for managing traffic controls (stop signs, traffic lights, speed limits, yield signs, etc.) and obstacles.Once the technology matures, autonomous vehicles are predicted to impact the automotive industry, health, welfare, urban planning, traffic, insurance, labor market, and other fields. Their regulation is becoming an increasingly important issue.\nAutonomy in vehicles is often divided into six levels, according to a system developed by SAE International (SAE J3016). The SAE levels can be roughly understood as Level 0 – no automation; Level 1 – hands on/shared control; Level 2 – hands off; Level 3 – eyes off; Level 4 – mind off, and Level 5 – steering wheel optional.\nAs of April 2023, vehicles operating at Level 3 and above are an insignificant market factor. In December 2020, Waymo became the first service provider to offer driverless taxi rides to the general public, in a part of Phoenix, Arizona. In March 2021, Honda was the first manufacturer to sell a legally approved Level 3 car. Nuro began autonomous commercial delivery operations in California in 2021. In December 2021, Mercedes-Benz received approval for a Level 3 car. In February 2022, Cruise became the second service provider to offer driverless taxi rides to the general public, in San Francisco.\nIn December 2022, several manufacturers had scaled back plans for self-driving technology, including Ford and Volkswagen.\n\nHistory\nExperiments have been conducted on automated driving systems (ADS) since at least the 1920s; trials began in the 1950s. The first semi-automated car was developed in 1977, by Japan's Tsukuba Mechanical Engineering Laboratory, which required specially marked streets that were interpreted by two cameras on the vehicle and an analog computer. The vehicle reached speeds up to 30 kilometres per hour (19 mph) with the support of an elevated rail.A landmark autonomous car appeared in the 1980s, with Carnegie Mellon University's Navlab and ALV projects funded by the United States' Defense Advanced Research Projects Agency (DARPA) starting in 1984 and Mercedes-Benz and Bundeswehr University Munich's EUREKA Prometheus Project in 1987. By 1985, the ALV had demonstrated self-driving speeds on two-lane roads of 31 kilometres per hour (19 mph), with obstacle avoidance added in 1986, and off-road driving in day and night time conditions by 1987. A major milestone was achieved in 1995, with Carnegie Mellon University's Navlab 5 completing the first autonomous coast-to-coast drive of the United States. Of the 2,849 mi (4,585 km) between Pittsburgh, Pennsylvania and San Diego, California, 2,797 mi (4,501 km) were autonomous (98.2%), completed with an average speed of 63.8 mph (102.7 km/h). From the 1960s through the second DARPA Grand Challenge in 2005, automated vehicle research in the United States was primarily funded by DARPA, the US Army, and the US Navy, yielding incremental advances in speeds, driving competence in more complex conditions, controls, and sensor systems. Companies and research organizations have developed prototypes.The US allocated US$650 million in 1991 for research on the National Automated Highway System, which demonstrated automated driving through a combination of automation embedded in the highway with automated technology in vehicles, and cooperative networking between the vehicles and with the highway infrastructure. The programme concluded with a successful demonstration in 1997 but without clear direction or funding to implement the system on a larger scale. Partly funded by the National Automated Highway System and DARPA, the Carnegie Mellon University Navlab drove 4,584 kilometres (2,848 mi) across America in 1995, 4,501 kilometres (2,797 mi) or 98% of it autonomously. Navlab's record achievement stood unmatched for two decades until 2015, when Delphi improved it by piloting an Audi, augmented with Delphi technology, over 5,472 kilometres (3,400 mi) through 15 states while remaining in self-driving mode 99% of the time. In 2015, the US states of Nevada, Florida, California, Virginia, and Michigan, together with Washington, DC, allowed the testing of automated cars on public roads.From 2016 to 2018, the European Commission funded an innovation strategy development for connected and automated driving through the Coordination Actions CARTRE and SCOUT. Moreover, the Strategic Transport Research and Innovation Agenda (STRIA) Roadmap for Connected and Automated Transport was published in 2019.In November 2017, Waymo announced that it had begun testing driverless cars without a safety driver in the driver position; however, there was still an employee in the car. An October 2017 report by the Brookings Institution found that $80 billion had been reported as invested in all facets of self driving technology up to that point, but that it was \"reasonable to presume that total global investment in autonomous vehicle technology is significantly more than this\".In October 2018, Waymo announced that its test vehicles had traveled in automated mode for over 10,000,000 miles (16,000,000 km), increasing by about 1,000,000 miles (1,600,000 kilometres) per month. In December 2018, Waymo was the first to commercialize a fully autonomous taxi service in the US, in Phoenix, Arizona. In October 2020, Waymo launched a geo-fenced driverless ride hailing service in Phoenix. The cars are being monitored in real-time by a team of remote engineers, and there are cases where the remote engineers need to intervene.In March 2019, ahead of the autonomous racing series Roborace, Robocar set the Guinness World Record for being the fastest autonomous car in the world. In pushing the limits of self-driving vehicles, Robocar reached 282.42 km/h (175.49 mph) – an average confirmed by the UK Timing Association at Elvington in Yorkshire, UK.In 2020, a National Transportation Safety Board chairman stated that no self-driving cars (SAE level 3+) were available for consumers to purchase in the US in 2020:\n\nThere is not a vehicle currently available to US consumers that is self-driving. Period. Every vehicle sold to US consumers still requires the driver to be actively engaged in the driving task, even when advanced driver assistance systems are activated. If you are selling a car with an advanced driver assistance system, you're not selling a self-driving car. If you are driving a car with an advanced driver assistance system, you don't own a self-driving car.\nOn 5 March 2021, Honda began leasing in Japan a limited edition of 100 Legend Hybrid EX sedans equipped with the newly approved Level 3 automated driving equipment which had been granted the safety certification by Japanese government to their autonomous \"Traffic Jam Pilot\" driving technology, and legally allow drivers to take their eyes off the road.\n\nDefinitions\nThere is some inconsistency in the terminology used in the self-driving car industry. Various organizations have proposed to define an accurate and consistent vocabulary.\nIn 2014, such confusion was documented in SAE J3016 which states that \"some vernacular usages associate autonomous specifically with full driving automation (Level 5), while other usages apply it to all levels of driving automation, and some state legislation has defined it to correspond approximately to any ADS [automated driving system] at or above Level 3 (or to any vehicle equipped with such an ADS).\"\n\nTerminology and safety considerations\nModern vehicles provide features such as keeping the car within its lane, speed controls, or emergency braking. Those features alone are just considered as driver assistance technologies because they still require a human driver control while fully automated vehicles drive themselves without human driver input.\nAccording to Fortune, some newer vehicles' technology names—such as AutonoDrive, PilotAssist, Full-Self Driving or DrivePilot—might confuse the driver, who may believe no driver input is expected when in fact the driver needs to remain involved in the driving task. According to the BBC, confusion between those concepts leads to deaths.For this reason, some organizations such as the AAA try to provide standardized naming conventions for features such as ALKS which aim to have capacity to manage the driving task, but which are not yet approved to be an automated vehicles in any countries. The Association of British Insurers considers the usage of the word autonomous in marketing for modern cars to be dangerous because car ads make motorists think \"autonomous\" and \"autopilot\" mean a vehicle can drive itself when they still rely on the driver to ensure safety. Technology able to drive a car is still in its beta stage.\nSome car makers suggest or claim vehicles are self-driving when they are not able to manage some driving situations. Despite being called Full Self-Driving, Tesla stated that its offering should not be considered as a fully autonomous driving system. This makes drivers risk becoming excessively confident, taking distracted driving behavior, leading to crashes. While in Great-Britain, a fully self-driving car is only a car registered in a specific list. There have also been proposals to adopt the aviation automation safety knowledge into the discussions of safe implementation of autonomous vehicles, due to the experience that has been gained over the decades by the aviation sector on safety topics.According to the SMMT, \"There are two clear states – a vehicle is either assisted with a driver being supported by technology or automated where the technology is effectively and safely replacing the driver.\"\n\nAutonomous vs. automated\nAutonomous means self-governing. Many historical projects related to vehicle automation have been automated (made automatic) subject to a heavy reliance on artificial aids in their environment, such as magnetic strips. Autonomous control implies satisfactory performance under significant uncertainties in the environment, and the ability to compensate for system failures without external intervention.One approach is to implement communication networks both in the immediate vicinity (for collision avoidance) and farther away (for congestion management). Such outside influences in the decision process reduce an individual vehicle's autonomy, while still not requiring human intervention.\nAs of 2017, most commercial projects focused on automated vehicles that did not communicate with other vehicles or with an enveloping management regime. Euro NCAP defines autonomous in \"Autonomous Emergency Braking\" as: \"the system acts independently of the driver to avoid or mitigate the accident\", which implies the autonomous system is not the driver.In Europe, the words automated and autonomous might be used together. For instance, Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles (...) defines \"automated vehicle\" and \"fully automated vehicle\" based on their autonomous capacity:\n\"automated vehicle\" means a motor vehicle designed and constructed to move autonomously for certain periods of time without continuous driver supervision but in respect of which driver intervention is still expected or required;\n\"fully automated vehicle\" means a motor vehicle that has been designed and constructed to move autonomously without any driver supervision;In British English, the word automated alone might have several meaning, such in the sentence: \"Thatcham also found that the automated lane keeping systems could only meet two out of the twelve principles required to guarantee safety, going on to say they cannot, therefore, be classed as 'automated driving', instead it claims the tech should be classed as \"assisted driving\".\": The first occurrence of the \"automated\" word refers to an Unece automated system, while the second occurrence refers to the British legal definition of an automated vehicle. The British law interprets the meaning of \"automated vehicle\" based on the interpretation section related to a vehicle \"driving itself\" and an insured vehicle.\n\nAutonomous versus cooperative\nTo enable a car to travel without any driver embedded within the vehicle, some companies use a remote driver.\nAccording to SAE J3016, Some driving automation systems may indeed be autonomous if they perform all of their functions independently and self-sufficiently, but if they depend on communication and/or cooperation with outside entities, they should be considered cooperative rather than autonomous.\n\nClassifications\nSelf-driving car\nPC Magazine defines a self-driving car as \"a computer-controlled car that drives itself\". The Union of Concerned Scientists states that self-driving cars are \"cars or trucks in which human drivers are never required to take control to safely operate the vehicle. Also known as autonomous or \"driverless\" cars, they combine sensors and software to control, navigate, and drive the vehicle.\"The British Automated and Electric Vehicles Act 2018 law defines a vehicle as \"driving itself\" if the vehicle \"is operating in a mode in which it is not being controlled, and does not need to be monitored, by an individual\".Another British definition assumes: \"Self-driving vehicles are vehicles that can safely and lawfully drive themselves.\"\n\nSAE classification\nA classification system with six levels – ranging from fully manual to fully automated systems – was published in 2014 by standardization body SAE International as J3016, Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems; the details are revised periodically. This classification is based on the amount of driver intervention and attentiveness required, rather than the vehicle's capabilities, although these are loosely related. In the United States in 2013, the National Highway Traffic Safety Administration (NHTSA) had released its original formal classification system. After SAE updated its classification in 2016, called J3016_201609, NHTSA adopted the SAE standard, and SAE classification became widely accepted. The SAE standard plays a major role but it has certain limitations.\n\nLevels of driving automation\nIn SAE's automation level definitions, \"driving mode\" means \"a type of driving scenario with characteristic dynamic driving task requirements (e.g., expressway merging, high speed cruising, low speed traffic jam, closed-campus operations, etc.)\"\nLevel 0: The automated system issues warnings and may momentarily intervene but has no sustained vehicle control.\nLevel 1 (\"hands on\"): The driver and the automated system share control of the vehicle. Examples are systems where the driver controls steering and the automated system controls engine power to maintain a set speed (Cruise control) or engine and brake power to maintain and vary speed (Adaptive cruise control or ACC); and Parking Assistance, where steering is automated while speed is under manual control. The driver must be ready to retake full control at any time. Lane Keeping Assistance (LKA) Type II is a further example of Level 1 self-driving. Automatic emergency braking which alerts the driver to a crash and permits full braking capacity is also a Level 1 feature, according to Autopilot Review magazine.\nLevel 2 (\"hands off\"): The automated system takes full control of the vehicle: accelerating, braking, and steering. The driver must monitor the driving and be prepared to intervene immediately at any time if the automated system fails to respond properly. The shorthand \"hands off\" is not meant to be taken literally – contact between hand and wheel is often mandatory during SAE 2 driving, to confirm that the driver is ready to intervene. The eyes of the driver may be monitored by cameras to confirm that the driver is keeping their attention to traffic. Actual hands off driving is sometimes considered level 2.5, although there are no official half-levels. A common example is adaptive cruise control combined with lane keeping assist technology so that the driver simply monitors the vehicle, such as \"Super-Cruise\" in the Cadillac CT6 by General Motors or Ford's F-150 BlueCruise.\nLevel 3 (\"eyes off\"): The driver can safely turn their attention away from the driving tasks, e.g. the driver can text or watch a film. The vehicle will handle situations that call for an immediate response, like emergency braking. The driver must still be prepared to intervene within some limited time, specified by the manufacturer, when called upon by the vehicle to do so. This level of automation can be thought of as a co-driver or co-pilot that's ready to alert the driver in an orderly fashion when swapping their turn to drive. An example would be a Traffic Jam Chauffeur (a car satisfying the international Automated Lane Keeping Systems (ALKS) regulations).\nLevel 4 (\"mind off\"): As level 3, but no driver attention is ever required for safety, e.g. the driver may safely go to sleep or leave the driver's seat. However, self-driving is supported only in limited spatial areas (geofenced) or under special circumstances. Outside of these areas or circumstances, the vehicle must be able to safely abort the trip, e.g. slow down and park the car, if the driver does not retake control. An example would be a robotic taxi or a robotic delivery service that covers selected locations in an area, at a specific time and quantities. Automated valet parking is another example.\nLevel 5 (\"steering wheel optional\"): No human intervention is required at all. An example would be a robotic vehicle that works on all kinds of surfaces, all over the world, all year around, in all weather conditions.In the formal SAE definition below, an important transition is from SAE Level 2 to SAE Level 3 in which the human driver is no longer expected to monitor the environment continuously. At SAE 3, the human driver still has responsibility to intervene when asked to do so by the automated system. At SAE 4 the human driver is always relieved of that responsibility and at SAE 5 the automated system will never need to ask for an intervention.\n\nCriticism of SAE\nThe SAE Automation Levels have been criticized for their technological focus. It has been argued that the structure of the levels suggests that automation increases linearly and that more automation is better, which may not always be the case. The SAE Levels also do not account for changes that may be required to infrastructure and road user behavior.\n\nTechnology\nGeneral perspectives\nSeveral classifications have been proposed to deal with the broad range of technological discussions pertaining to self-driving cars. One such proposal is to classify based on the following categories; car navigation, path planning, environment perception and car control. In the 2020s, it became apparent that these technologies are far more complex than initially thought. Even video games have been used as a platform to test autonomous vehicles.\n\nHybrid navigation\nHybrid navigation is the simultaneous use of more than one navigation system for location data determination, needed for navigation.\nSensing\nTo reliably and safely operate an autonomous vehicle, usually a mixture of sensors is utilized.\nTypical sensors include lidar (Light Detection and Ranging), stereo vision, GPS and IMU.\nModern self-driving cars generally use Bayesian simultaneous localization and mapping (SLAM) algorithms,\nwhich fuse data from multiple sensors and an off-line map into current location estimates and map updates. \nWaymo has developed a variant of SLAM with detection and tracking of other moving objects (DATMO), which also handles obstacles such as cars and pedestrians. Simpler systems may use roadside real-time locating system (RTLS) technologies to aid localization.\nMaps\nSelf-driving cars require a new class of high-definition maps (HD maps) that represent the world at up to two orders of magnitude more detail. In May 2018, researchers from the Massachusetts Institute of Technology (MIT) announced that they had built an automated car that can navigate unmapped roads. Researchers at their Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a new system, called MapLite, which allows self-driving cars to drive on roads that they have never been on before, without using 3D maps. The system combines the GPS position of the vehicle, a \"sparse topological map\" such as OpenStreetMap (i.e. having 2D features of the roads only), and a series of sensors that observe the road conditions.Sensor fusion\nControl systems on automated cars may use sensor fusion, which is an approach that integrates information from a variety of sensors on the car to produce a more consistent, accurate, and useful view of the environment. Self-driving cars tend to use a combination of cameras, LiDAR sensors, and radar sensors in order to enhance performance and ensure the safety of the passenger and other drivers on the road. An increased consistency in self-driving performance prevents accidents that may occur because of one faulty sensor.Path planningPath planning is a computational problem to find a sequence of valid configurations that moves the object from the source to destination. Self-driving cars rely on path planning technology in order to follow the rules of traffic and prevent accidents from occurring. The large scale path of the vehicle can be determined by using a voronoi diagram, an occupancy grid mapping, or with a driving corridors algorithm. A driving corridors algorithm allows the vehicle to locate and drive within open free space that is bounded by lanes or barriers. While these algorithms work in a simple situation, path planning has not been proven to be effective in a complex scenario. Two techniques used for path planning are graph-based search and variational-based optimization techniques. Graph-based techniques can make harder decisions such as how to pass another vehicle/obstacle. Variational-based optimization techniques require a higher level of planning in setting restrictions on the vehicle's driving corridor to prevent collisions.\n\nDrive by wire\nDrive by wire technology in the automotive industry is the use of electrical or electro-mechanical systems for performing vehicle functions traditionally achieved by mechanical linkages.\n\nDriver monitoring system\nDriver monitoring system is a vehicle safety system to assess the driver's alertness and warn the driver if needed. It is recognized in developer side that the role of the systems will increase as SAE Level 2 systems become more common-place, and becomes more challenging at Level 3 and above to predict the driver's readiness for handover.\n\nVehicular communication\nVehicular communications is a growing area of communications between vehicles and including roadside communication infrastructure. Vehicular communication systems use vehicles and roadside units as the communicating nodes in a peer-to-peer network, providing each other with information. This connectivity enables autonomous vehicles to interact with non-autonomous traffic and pedestrians to increase safety.\nAnd autonomous vehicles will need to connect to the cloud to update their software and maps, and feedback information to improve the used maps and software of their manufacturer.\n\nRe-programmable\nAutonomous vehicles have software systems that drive the vehicle, meaning that updates through reprogramming or editing the software can enhance the benefits of the owner (e.g. update in better distinguishing blind person vs. non-blind person so that the vehicle will take extra caution when approaching a blind person). A characteristic of this re-programmable part of autonomous vehicles is that the updates need not only to come from the supplier, because through machine learning, smart autonomous vehicles can generate certain updates and install them accordingly (e.g. new navigation maps or new intersection computer systems). These reprogrammable characteristics of the digital technology and the possibility of smart machine learning give manufacturers of autonomous vehicles the opportunity to differentiate themselves on software.\nIn March 2021, UNECE regulation on software update and software update management system was published.\n\nModularity\nAutonomous vehicles are more modular since they are made up out of several modules which will be explained hereafter through a Layered Modular Architecture. The Layered Modular Architecture extends the architecture of purely physical vehicles by incorporating four loosely coupled layers of devices, networks, services and contents into Autonomous Vehicles. These loosely coupled layers can interact through certain standardized interfaces.\n\nThe first layer of this architecture consists of the device layer. This layer consists of the following two parts: logical capability and physical machinery. The physical machinery refers to the actual vehicle itself (e.g. chassis and carrosserie). When it comes to digital technologies, the physical machinery is accompanied by a logical capability layer in the form of operating systems that helps to guide the vehicles itself and make it autonomous. The logical capability provides control over the vehicle and connects it with the other layers;\nOn top of the device layer comes the network layer. This layer also consists of two different parts: physical transport and logical transmission. The physical transport layer refers to the radars, sensors and cables of the autonomous vehicles which enable the transmission of digital information. Next to that, the network layer of autonomous vehicles also has a logical transmission which contains communication protocols and network standard to communicate the digital information with other networks and platforms or between layers. This increases the accessibility of the autonomous vehicles and enables the computational power of a network or platform;\nThe service layer contains the applications and their functionalities that serves the autonomous vehicle (and its owners) as they extract, create, store and consume content with regards to their own driving history, traffic congestion, roads or parking abilities for example.;\nThe final layer of the model is the contents layer. This layer contains the sounds, images and videos. The autonomous vehicles store, extract and use to act upon and improve their driving and understanding of the environment. The contents layer also provides metadata and directory information about the content's origin, ownership, copyright, encoding methods, content tags, Geo-time stamps, and so on (Yoo et al., 2010).\n\nHomogenization\nIn order for autonomous vehicles to perceive their surroundings, they have to use different techniques each with their own accompanying digital information (e.g. radar, GPS, motion sensors and computer vision). Homogenization requires that the digital information from these different sources is transmitted and stored in the same form. This means their differences are decoupled, and digital information can be transmitted, stored, and computed in a way that the vehicles and their operating system can better understand and act upon it.\nIn international standardization field, ISO/TC 22 is in charge of in-vehicle transport information and control systems, and ISO/TC 204 is in charge of information, communication and control systems in the field of urban and rural surface transportation. International standards have been actively developed in the domains of AD/ADAS functions, connectivity, human interaction, in-vehicle systems, management/engineering, dynamic map and positioning, privacy and security.\n\nMathematical safety model\nIn 2017, Mobileye published a mathematical model for automated vehicle safety which is called \"Responsibility-Sensitive Safety (RSS)\".\nIt is under standardization at IEEE Standards Association as \"IEEE P2846: A Formal Model for Safety Considerations in Automated Vehicle Decision Making\".In 2022, a research group of National Institute of Informatics (NII, Japan) expanded RSS and developed \"Goal-Aware RSS\" to make RSS rules possible to deal with complex scenarios via program logic.\n\nChallenges\nObstacles\nThe potential benefits from increased vehicle automation described may be limited by foreseeable challenges such as disputes over liability, the time needed to turn over the existing stock of vehicles from non-automated to automated, and thus a long period of humans and autonomous vehicles sharing the roads, resistance by individuals to forfeiting control of their cars, concerns about safety, and the implementation of a legal framework and consistent global government regulations for self-driving cars. In addition, cyberattacks could be a potential threat to autonomous driving in the future.Other obstacles could include de-skilling and lower levels of driver experience for dealing with potentially dangerous situations and anomalies, ethical problems where an automated vehicle's software is forced during an unavoidable crash to choose between multiple harmful courses of action (the trolley problem), concerns about making large numbers of people currently employed as drivers unemployed, the potential for more intrusive mass surveillance of location, association and travel as a result of police and intelligence agency access to large data sets generated by sensors and pattern-recognition AI, and possibly insufficient understanding of verbal sounds, gestures and non-verbal cues by police, other drivers or pedestrians.\nPossible technological obstacles for automated cars are:\nArtificial intelligence is still not able to function properly in chaotic inner-city environments.\nA car's computer could potentially be compromised, as could a communication system between cars.\nSusceptibility of the car's sensing and navigation systems to different types of weather (such as snow) or deliberate interference, including jamming and spoofing.\nAvoidance of large animals requires recognition and tracking, and Volvo found that software suited to caribou, deer, and elk was ineffective with kangaroos.\nAutonomous cars may require high-definition maps to operate properly. Where these maps may be out of date, they would need to be able to fall back to reasonable behaviors.\nCompetition for the radio spectrum desired for the car's communication.\nField programmability for the systems will require careful evaluation of product development and the component supply chain.\nCurrent road infrastructure may need changes for automated cars to function optimally.\nValidation challenge of Automated Driving and need for novel simulation-based approaches comprising digital twins and agent-based traffic simulation.\n\nConcerns\nDeceptive marketing\nAs Tesla's \"Full Self-Driving (FSD)\" actually corresponds to Level 2, \nsenators called for investigation to the Federal Trade Commission (FTC) about their marketing claims in August 2021.\nAnd in December 2021 in Japan, Mercedes-Benz Japan Co., Ltd. was punished by the Consumer Affairs Agency for the descriptions in their handouts that are different from the fact.In July 2016, following a fatal crash by a Tesla car operating in \"Autopilot\" mode, Mercedes-Benz was also criticized for a misleading commercial advertising E-Class models which had been available with \"Drive Pilot\".\nAt that time, Mercedes-Benz rejected the claims and stopped its \"self-driving car\" ad campaign which had been running in the United States.\nIn August 2022, the California Department of Motor Vehicles (DMV) accused Tesla of deceptive marketing  practices.Employment\nCompanies working on the technology have an increasing recruitment problem in that the available talent pool has not grown with demand. As such, education and training by third-party organizations such as providers of online courses and self-taught community-driven projects such as DIY Robocars and Formula Pi have quickly grown in popularity, while university level extra-curricular programmes such as Formula Student Driverless have bolstered graduate experience. Industry is steadily increasing freely available information sources, such as code, datasets and glossaries to widen the recruitment pool.\nNational security\nIn the 2020s, from the importance of the automotive sector to the nation, the self-driving car has become a topic of national security. The concerns regarding cybersecurity and data protection are not only important for user protection, but also in the context of national security. The trove of data collected by self-driving cars, paired with cybersecurity vulnerabilities, creates an appealing target for intelligence collection. Self-driving cars are required to be considered in a new way when it comes to espionage risk.It was in July 2018 that a former Apple engineer was arrested by Federal Bureau of Investigation (FBI) at San Jose International Airport (SJC) while preparing to board a flight to China and charged with stealing proprietary information related to Apple's self-driving car project.\nAnd in January 2019, another Apple employee was charged with stealing self-driving car project secrets.\nIn July 2021, United States Department of Justice (DOJ) accused Chinese security officials of a hacking attack seeking data on  of coordinating a vast hacking campaign to steal sensitive and secret information from government entities including research related to autonomous vehicles.\nOn the China side, they have already prepared \"the Provisions on Management of Automotive Data Security (Trial)\".It is concerned that leapfrogging ability can be applied to autonomous car technology.\nAlso, emerging Cellular V2X (Cellular Vehicle-to-Everything) technologies are based on 5G wireless networks.\nAs of November 2022, US Congress is applying fresh scrutiny to the possibility that imported Chinese technology could be a Trojan horse.\n\nHuman factors\nMoving obstacles\nSelf-driving cars are already exploring the difficulties of determining the intentions of pedestrians, bicyclists, and animals, and models of  behavior must be programmed into driving algorithms. Human road users also have the challenge of determining the intentions of autonomous vehicles, where there is no driver with which to make eye contact or exchange hand signals. Drive.ai is testing a solution to this problem that involves LED signs mounted on the outside of the vehicle, announcing status such as \"going now, don't cross\" vs. \"waiting for you to cross\".Handover and risk compensation\nTwo human-factor challenges are important for safety. One is the handover from automated driving to manual driving.\nHuman factors research on automated systems has shown that people are slow to detect a problem with automation and slow to understand the problem after it is detected. When automation failures occur, unexpected transitions that require a driver to take over will occur suddenly and the driver may not be ready to take over.The second challenge is known as risk compensation: as a system is perceived to be safer, instead of benefiting entirely from all of the increased safety, people engage in riskier  behavior and enjoy other benefits. Semi-automated cars have been shown to suffer from this problem, for example with users of Tesla Autopilot ignoring the road and using electronic devices or other activities against the advice of the company that the car is not capable of being completely autonomous. In the near future, pedestrians and bicyclists may travel in the street in a riskier fashion if they believe self-driving cars are capable of avoiding them.\nTrust\nIn order for people to buy self-driving cars and vote for the government to allow them on roads, the technology must be trusted as safe. Self-driving elevators were invented in 1900, but the high number of people refusing to use them slowed adoption for several decades until operator strikes increased demand and trust was built with advertising and features like the emergency stop button. There are three types of trust between human and automation. There is dispositional trust, the trust between the driver and the company's product; there is situational trust, or the trust from different scenarios; and there is learned trust where the trust is built between similar events.\n\nMoral issues\nRationale for liabilityThere are different opinions on who should be held liable in case of a crash, especially with people being hurt. One study suggests requesting the owners of self-driving cars to sign end-user license agreements (EULAs), assigning to them accountability for any accidents. Other studies suggest introducing a tax or insurance that would protect owners and users of automated vehicles of claims made by victims of an accident. Other possible parties that can be held responsible in case of a technical failure include software engineers that programmed the code for the automated operation of the vehicles, and suppliers of components of the AV.Implications from the Trolley ProblemA moral dilemma that a software engineer or car manufacturer might face in programming the operating software of a self-driving vehicle is captured in a variation of the traditional ethical thought experiment, the trolley problem: An AV is driving with passengers when suddenly a person appears in its way and the car has to commit between one of two options, either to run the person over or to avoid hitting the person by swerving into a wall, killing the passengers. Researchers have suggested, in particular, two ethical theories to be applicable to the  behavior of automated vehicles in cases of emergency: deontology and utilitarianism. Deontological theory suggests that an automated car needs to follow strict written-out rules that it needs to follow in any situation. Utilitarianism, on the other hand, promotes maximizing the number of people surviving in a crash. Critics suggest that automated vehicles should adapt a mix of multiple theories to be able to respond morally right in the instance of a crash. Recently, some specific ethical frameworks i.e., utilitarianism, deontology, relativism, absolutism (monism), and pluralism, are investigated empirically with respect to the acceptance of self-driving cars in unavoidable accidents.According to research, people overwhelmingly express a preference for autonomous vehicles to be programmed with utilitarian ideas, that is, in a manner that generates the least harm and minimizes driving casualties. While people want others to purchase utilitarian promoting vehicles, they themselves prefer to ride in vehicles that prioritize the lives of people inside the vehicle at all costs. This presents a paradox in which people prefer that others drive utilitarian vehicles designed to maximize the lives preserved in a fatal situation but want to ride in cars that prioritize the safety of passengers at all costs. People disapprove of regulations that promote utilitarian views and would be less willing to purchase a self-driving car that may opt to promote the greatest good at the expense of its passengers.Bonnefon et al. concluded that the regulation of autonomous vehicle ethical prescriptions may be counterproductive to societal safety. This is because, if the government mandates utilitarian ethics and people prefer to ride in self-protective cars, it could prevent the large scale implementation of self-driving cars. Delaying the adoption of autonomous cars vitiates the safety of society as a whole because this technology is projected to save so many lives.PrivacyPrivacy-related issues arise mainly from the interconnectivity of automated cars, making it just another mobile device that can gather any information about an individual (see data mining). This information gathering ranges from tracking of the routes taken, voice recording, video recording, preferences in media that is consumed in the car,  behavioral patterns, to many more streams of information. The data and communications infrastructure needed to support these vehicles may also be capable of surveillance, especially if coupled to other data sets and advanced analytics.\n\nLevel 4 infrastructure\nThe technology needed to upgrade a nation's road network to accommodate Level 4 autonomy has yet to be fully scoped. It is said to include creating new standards, approaches and legislation that meet the differing needs of specific nations.\nIn March 2023, The Japanese government unveiled a plan to set up a dedicated lane for self-driving vehicles on a highway. The project is a part of an initiative to expand digital technologies nationwide, and will start in the fiscal year that begins in April 2024.\nAnd in April 2023, JR East announced their challenge to raise their self-driving level of Kesennuma Line Bus rapid transit (BRT) in rural area from the current Level 2 to Level 4 at 60 km/h.\n\nApplications\nRobotaxi\n\nRobotaxi is an application of self-driving car which is supposed to be operated by taxi company or ridesharing company. \nThrough the massive investments by Big Tech companies in the mid-2010s, research and development of robotaxi became active in the U.S.Self-driving shuttle and bus\n\nSelf-driving shuttle is an application of self-driving car with considerations of multiple passengers supposing the use cases mainly in cities. Through the European Union funded \"CityMobil2\" project in the mid-2010s, research and development of self-driving shuttle became active in Europe.\nContinuously, under the funding programme Horizon 2020, \"Avenue\" project was conducted from 2018 to 2022 in four cities (Geneva, Lyon, Copenhagen and Luxembourg).Self-driving truck and van\n\nCompanies such as Otto and Starsky Robotics have focused on autonomous trucks. Automation of trucks is important, not only due to the improved safety aspects of these very heavy vehicles, but also due to the ability of fuel savings through platooning. Autonomous vans are being developed for use by online grocers such as Ocado.Autonomous micro-mobility\nResearch has indicated that goods distribution on the macro (urban distribution) and micro level (last mile delivery) could be made more efficient with the use of autonomous vehicles thanks to the possibility of smaller vehicle sizes.\nAlso, simulation studies in MIT Media Lab indicate that ultra-lightweight systems can become more helping to remove cars from our cities by applying autonomous driving technologies.In November 2022, Honda unveiled the \"Honda CI Micro-mobility\" machines and their core technologies. Honda starts demonstration testing using \"Honda CI Micro-mobility\" machines, \"CiKoMa\" and \"WaPOCH\", at two locations in Jōsō City of Ibaraki Prefecture.Autonomous work vehicle\nIn 2021, Honda and Black & Veatch have successfully tested their second generation prototype Autonomous Work Vehicle (AWV) at a Black & Veatch construction site in New Mexico.In December 2022, eve autonomy in Japan, a company backed by Yamaha Motor and TIER IV, launched the all-in-one autonomous transportation commercial service \"eve auto\" with EV work vehicle as the first SAE Level 4 service in Japan at nine sites, including Yamaha Motor's three factories, Prime Polymer's Anesaki Works, Panasonic's cold chain factory in the Oizumi area, Fuji Electric's Suzuka factory, Japan Logistic Systems Corp.'s Ageo Center, and ENEOS Corp.'s Negishi refinery.\n\nTesting\nApproaches\nThe testing of vehicles with varying degrees of automation can be carried out either physically, in a closed environment or, where permitted, on public roads (typically requiring a license or permit, or adhering to a specific set of operating principles), or in a virtual environment, i.e. using computer simulations. When driven on public roads, automated vehicles require a person to monitor their proper operation and \"take over\" when needed. For example, New York has strict requirements for the test driver, such that the vehicle can be corrected at all times by a licensed operator; highlighted by Cardian Cube Company's application and discussions with New York State officials and the NYS DMV.\n\nDisengagements in the 2010s\nIn California, self-driving car manufacturers are required to submit annual reports to share how often their vehicles disengaged from autonomous mode during tests.\nIt has been believed that we would learn how reliable the vehicles are becoming based on how often they needed \"disengagements\".In 2017, Waymo reported 63 disengagements over 352,545 mi (567,366 km) of testing, an average distance of 5,596 mi (9,006 km) between disengagements, the highest among companies reporting such figures. Waymo also traveled a greater total distance than any of the other companies. Their 2017 rate of 0.18 disengagements per 1,000 mi (1,600 km) was an improvement over the 0.2 disengagements per 1,000 mi (1,600 km) in 2016, and 0.8 in 2015. In March 2017, Uber reported an average of just 0.67 mi (1.08 km) per disengagement. In the final three months of 2017, Cruise (now owned by GM) averaged 5,224 mi (8,407 km) per disengagement over a total distance of 62,689 mi (100,888 km). In July 2018, the first electric driverless racing car, \"Robocar\", completed a 1.8-kilometer track, using its navigation system and artificial intelligence.\n\nIn the 2020s\nDisengagements\nAs of 2022, \"disengagements\" are at the center of the controversy. The problem is that reporting companies have varying definitions of what qualifies as a disengagement, and that definition can change over time.\nExecutives of self-driving car companies have criticized disengagements as a deceptive metric, because it does not take into account the higher degree of difficulty navigating urban streets compared with interstates highway.Compliance\nIn April 2021, WP.29 GRVA issued the master document on \"Test Method for Automated Driving (NATM)\".In October 2021, the Europe's comprehensive pilot test of automated driving on public roads, L3Pilot, demonstrated automated systems for cars in Hamburg, Germany, in conjunction with ITS World Congress 2021. SAE Level 3 and 4 functions were tested on ordinary roads.\nAt the end of February 2022, the final results of the L3Pilot project were published.In November 2022, an International Standard ISO 34502 on \"Scenario based safety evaluation framework\" was published.Collision avoidance\nIn April 2022, collision avoidance testing was demonstrated by Nissan.\nAlso, Waymo published a document about collision avoidance testing in December 2022.Simulation and validation\nIn September 2022, Biprogy released a software system of \"Driving Intelligence Validation Platform (DIVP)\" as the achievement of Japanese national project \"SIP-adus\" led by Cabinet Office with the same name of its subproject which is interoperable with  Open Simulation Interface (OSI) of ASAM.Topics\nIn November 2021, the California Department of Motor Vehicles (DMV) notified Pony.ai that it was suspending its driverless testing permit following a reported collision in Fremont on 28 October. This incident stands out because the vehicle was in autonomous mode and didn't involve any other vehicle.\nIn May 2022, DMV revoked Pony.ai's permit for failing to monitor the driving records of the safety drivers on its testing permit.In April 2022, it is reported that Cruise's testing vehicle blocked fire engine on emergency call, and sparked questions about an autonomous vehicle's ability to handle unexpected roadway issues.In November 2022, Toyota gave a demonstration of one of its GR Yaris test car equipped with AI, which had been trained on the skills and knowledge of professional rally drivers to enhance the safety of self-driving cars. Toyota has been using the learnings from the collaborative activities with Microsoft in FIA World Rally Championship since 2017 season.Pedestrian reaction\nIn 2023 David R. Large, senior research fellow with the Human Factors Research Group at the University of Nottingham, disguised himself as a car seat in a study to test people's reactions to driverless cars. He said, \"We wanted to explore how pedestrians would interact with a driverless car and developed this unique methodology to explore their reactions.\" The study found that, in the absence of someone in the driving seat, pedestrians trust certain visual prompts more than others when deciding whether to cross the road.\n\nIncidents\nTesla Autopilot\nAs of November 2021, Tesla's advanced driver-assistance system (ADAS) Autopilot is classified as a Level 2.On 20 January 2016, the first of five known fatal crashes of a Tesla with Autopilot occurred in China's Hubei province. According to China's 163.com news channel, this marked \"China's first accidental death due to Tesla's automatic driving (system)\". Initially, Tesla pointed out that the vehicle was so badly damaged from the impact that their recorder was not able to conclusively prove that the car had been on autopilot at the time; however, 163.com pointed out that other factors, such as the car's absolute failure to take any evasive actions prior to the high speed crash, and the driver's otherwise good driving record, seemed to indicate a strong likelihood that the car was on autopilot at the time. A similar fatal crash occurred four months later in Florida. In 2018, in a subsequent civil suit between the father of the driver killed and Tesla, Tesla did not deny that the car had been on autopilot at the time of the accident, and sent evidence to the victim's father documenting that fact.The second known fatal accident involving a vehicle being driven by itself took place in Williston, Florida on 7 May 2016 while a Tesla Model S electric car was engaged in Autopilot mode. The occupant was killed in a crash with an 18-wheel tractor-trailer. On 28 June 2016 the US National Highway Traffic Safety Administration (NHTSA) opened a formal investigation into the accident working with the Florida Highway Patrol. According to NHTSA, preliminary reports indicate the crash occurred when the tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway, and the car failed to apply the brakes. The car continued to travel after passing under the truck's trailer. NHTSA's preliminary evaluation was opened to examine the design and performance of any automated driving systems in use at the time of the crash, which involved a population of an estimated 25,000 Model S cars. On 8 July 2016, NHTSA requested Tesla Motors provide the agency detailed information about the design, operation and testing of its Autopilot technology. The agency also requested details of all design changes and updates to Autopilot since its introduction, and Tesla's planned updates schedule for the next four months.According to Tesla, \"neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.\" The car attempted to drive full speed under the trailer, \"with the bottom of the trailer impacting the windshield of the Model S\". Tesla also claimed that this was Tesla's first known autopilot death in over 130 million miles (210 million kilometers) driven by its customers with Autopilot engaged, however by this statement, Tesla was apparently refusing to acknowledge claims that the January 2016 fatality in Hubei China had also been the result of an autopilot system error. According to Tesla there is a fatality every 94 million miles (151 million kilometers) among all type of vehicles in the US. However, this number also includes fatalities of the crashes, for instance, of motorcycle drivers with pedestrians.In July 2016, the US National Transportation Safety Board (NTSB) opened a formal investigation into the fatal accident while the Autopilot was engaged. The NTSB is an investigative body that has the power to make only policy recommendations. An agency spokesman said \"It's worth taking a look and seeing what we can learn from that event, so that as that automation is more widely introduced we can do it in the safest way possible.\" In January 2017, the NTSB released the report that concluded Tesla was not at fault; the investigation revealed that for Tesla cars, the crash rate dropped by 40 percent after Autopilot was installed.In 2021, NTSB Chair called on Tesla to change the design of its Autopilot to ensure it cannot be misused by drivers, according to a letter sent to the company's CEO.\n\nWaymo\nWaymo originated as a self-driving car project within Google. In August 2012, Google announced that their vehicles had completed over 300,000 automated-driving miles (500,000 km) accident-free, typically involving about a dozen cars on the road at any given time, and that they were starting to test with single drivers instead of in pairs. In late-May 2014, Google revealed a new prototype that had no steering wheel, gas pedal, or brake pedal, and was fully automated. As of March 2016, Google had test-driven their fleet in automated mode a total of 1,500,000 mi (2,400,000 km). In December 2016, Google Corporation announced that its technology would be spun off to a new company called Waymo, with both Google and Waymo becoming subsidiaries of a new parent company called Alphabet.According to Google's accident reports as of early 2016, their test cars had been involved in 14 collisions, of which other drivers were at fault 13 times, although in 2016 the car's software caused a crash.In June 2015, Brin confirmed that 12 vehicles had suffered collisions as of that date. Eight involved rear-end collisions at a stop sign or traffic light, two in which the vehicle was side-swiped by another driver, one in which another driver rolled through a stop sign, and one where a Google employee was controlling the car manually. In July 2015, three Google employees suffered minor injuries when their vehicle was rear-ended by a car whose driver failed to brake at a traffic light. This was the first time that a collision resulted in injuries. On 14 February 2016 a Google vehicle attempted to avoid sandbags blocking its path. During the maneuver it struck a bus. Google stated, \"In this case, we clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision.\" Google characterized the crash as a misunderstanding and a learning experience. No injuries were reported in the crash.\n\nUber's Advanced Technologies Group (ATG)\nIn March 2018, Elaine Herzberg died after being hit by a self-driving car being tested by Uber's Advanced Technologies Group (ATG) in the US state of Arizona. There was a safety driver in the car. Herzberg was crossing the road about 400 feet from an intersection. This marks the first time an individual is known to have been killed by an autonomous vehicle, and the incident raised questions about regulation of the self-driving car industry. Some experts said a human driver could have avoided the fatal crash. Arizona governor Doug Ducey suspended the company's ability to test and operate its automated cars on public roadways citing an \"unquestionable failure\" of the expectation that Uber make public safety its top priority. Uber then stopped self-driving tests in California until it was issued a new permit in 2020.In May 2018, the US National Transportation Safety Board (NTSB) issued a preliminary report. The final report 18 months later determined that the immediate cause of the accident was the safety driver's failure to monitor the road because she was distracted by her phone. However, Uber ATG's \"inadequate safety culture\" contributed to the crash. The report noted from the post-mortem that the victim had \"a very high level\" of methamphetamine in her body. The board also called on federal regulators to carry out a review before allowing automated test vehicles to operate on public roads.In September 2020, the backup driver, Rafael Vasquez, was charged with negligent homicide, because she did not look at the road for several seconds while her phone was streaming The Voice broadcast by Hulu. She pleaded not guilty and was released to await trial. Uber does not face any criminal charge because in the USA there is no basis for criminal liability for the corporation. The safety driver is assumed to be responsible for the accident, because she was in the driving seat in a capacity to avoid an accident (like in a Level 3). The trial was originally planned for February 2021 but is now scheduled to begin in June 2023.\n\nNavya Arma driving system\nOn 9 November 2017, a Navya Arma automated self-driving bus with passengers was involved in a crash with a truck. The truck was found to be at fault of the crash, reversing into the stationary automated bus. The automated bus did not take evasive actions or apply defensive driving techniques such as flashing its headlights, or sounding the horn. As one passenger commented, \"The shuttle didn't have the ability to move back. The shuttle just stayed still.\"\n\nNIO Navigate on Pilot\nOn 12 August 2021, a 31-year-old Chinese man was killed after his NIO ES8 collided with a construction vehicle. NIO's self-driving feature is still in beta and cannot yet deal with static obstacles. Though the vehicle's manual clearly states that the driver must take over when nearing construction sites, the issue is whether the feature was improperly marketed and unsafe. Lawyers of the deceased's family have also called into question NIO's private access to the vehicle, which they argue may lead to the data ending up forged.\n\nToyota e-Palette operation\nOn 26 August 2021, a Toyota e-Palette, a mobility vehicle used to support mobility within the Athletes' Village at the Olympic and Paralympic Games Tokyo 2020, collided with a visually impaired pedestrian about to cross a pedestrian crossing.\nThe Toyota bus service was suspended after the accident, and resumed on 31 August 2021 with improved safety measures.\n\nPublic opinion surveys\nIn the 2010s\nIn a 2011 online survey of 2,006 US and UK consumers by Accenture, 49% said they would be comfortable using a \"driverless car\".A 2012 survey of 17,400 vehicle owners by J.D. Power and Associates found 37% initially said they would be interested in purchasing a \"fully autonomous car\". However, that figure dropped to 20% if told the technology would cost US$3,000 more.In a 2012 survey of about 1,000 German drivers by automotive researcher Puls, 22% of the respondents had a positive attitude towards these cars, 10% were undecided, 44% were skeptical and 24% were hostile.A 2013 survey of 1,500 consumers across 10 countries by Cisco Systems found 57% \"stated they would be likely to ride in a car controlled entirely by technology that does not require a human driver\", with Brazil, India and China the most willing to trust automated technology.In a 2014 US telephone survey by Insurance.com, over three-quarters of licensed drivers said they would at least consider buying a self-driving car, rising to 86% if car insurance were cheaper. 31.7% said they would not continue to drive once an automated car was available instead.In a February 2015 survey of top auto journalists, 46% predicted that either Tesla or Daimler would be the first to the market with a fully autonomous vehicle, while (at 38%) Daimler was predicted to be the most functional, safe, and in-demand autonomous vehicle.\nIn 2015, a questionnaire survey by Delft University of Technology explored the opinion of 5,000 people from 109 countries on automated driving. Results showed that respondents, on average, found manual driving the most enjoyable mode of driving. 22% of the respondents did not want to spend any money for a fully automated driving system. Respondents were found to be most concerned about software hacking/misuse, and were also concerned about legal issues and safety. Finally, respondents from more developed countries (in terms of lower accident statistics, higher education, and higher income) were less comfortable with their vehicle transmitting data. The survey also gave results on potential consumer opinion on interest of purchasing an automated car, stating that 37% of surveyed current owners were either \"definitely\" or \"probably\" interested in purchasing an automated car.In 2016, a survey in Germany examined the opinion of 1,603 people, who were representative in terms of age, gender, and education for the German population, towards partially, highly, and fully automated cars. Results showed that men and women differ in their willingness to use them. Men felt less anxiety and more joy towards automated cars, whereas women showed the exact opposite. The gender difference towards anxiety was especially pronounced between young men and women but decreased with participants' age.In 2016, a PwC survey, in the United States, showing the opinion of 1,584 people, highlights that \"66 percent of respondents said they think autonomous cars are probably smarter than the average human driver\". People are still worried about safety and mostly the fact of having the car hacked. Nevertheless, only 13% of the interviewees see no advantages in this new kind of cars.In 2017, Pew Research Center surveyed 4,135 US adults from 1–15 May and found that many Americans anticipate significant impacts from various automation technologies in the course of their lifetimes—from the widespread adoption of automated vehicles to the replacement of entire job categories with robot workers.In 2019, results from two opinion surveys of 54 and 187 US adults respectively were published. A new standardized questionnaire, the autonomous vehicle acceptance model (AVAM) was developed, including additional description to help respondents better understand the implications of different automation levels. Results showed that users were less accepting of high autonomy levels and displayed significantly lower intention to use highly autonomous vehicles. Additionally, partial autonomy (regardless of level) was perceived as requiring uniformly higher driver engagement (usage of hands, feet and eyes) than full autonomy.\n\nIn the 2020s\nIn 2022, research by safety charity Lloyd's Register Foundation uncovered that only a quarter (27%) of the world's population would feel safe in self-driving cars.\n\nRegulation\nRegulation of self-driving cars is an increasingly important issue which includes multiple subtopics. Among them are self-driving car liability, regulations regarding approval and international conventions.\nIn the 2010s, researchers openly worried about the potential of future regulation to delay deployment of automated cars on the road. In 2020, international regulation in the form of UNECE WP.29 GRVA was established, regulating Level 3 automated driving. As of 2022, it is considered very challenging to be approved as Level 3.\n\nCommercialization\nBetween manually driven vehicles (SAE Level 0) and fully autonomous vehicles (SAE Level 5), there are a variety of vehicle types that have some degree of automation. These are collectively known as semi-automated vehicles. As it could be a while before the technology and infrastructure are developed for full automation, it is likely that vehicles will have increasing levels of automation. These semi-automated vehicles could potentially harness many of the advantages of fully automated vehicles, while still keeping the driver in charge of the vehicle.As of 2023 nearly all commercially available vehicles with autonomous features are considered SAE Level 2. Development is ongoing at many car companies on further automation features that function at Level 2 and Level 3. Other companies offer services of autonomous Level 4 robotaxis in a few cities in the United States.\n\nLevel 2 commercialization\nSAE Level 2 features are available as part of the advanced driver-assistance system (ADAS) abilities in many commercially available vehicles. These systems often require a subscription to an ongoing service or paid upgrade with the car purchase.\nFord started offering the \"BlueCruise\" service on certain electric and gas-powered vehicles in 2022; it is named \"ActiveGlide\" in Lincoln vehicles. The system provides features such as lane centering, street sign recognition and hands-free highway driving on more than 130,000 miles of divided highways in the US. The version 1.2 update of the service was released in September 2022, and added features like hands-free lane changing, in-lane repositioning, and predictive speed assist. In April 2023 BlueCruise technology was approved in the UK, for use on certain motorways. The technology will at first only be available for 2023 models of Ford's electric Mustang Mach-E SUV.Tesla vehicles are equipped with hardware that Tesla claims will allow full self-driving in the future. The Tesla Autopilot suite of ADAS features are included in all Tesla vehicle models. More advanced driving features are available at an extra cost, under the \"Enhanced Autopilot\" and \"Full Self-Driving\" names. The marketing names have been criticized as misleading, as all Tesla ADAS features provide only Level 2 capabilities.\n\nLevel 2 development\nGeneral Motors is developing the \"Ultra Cruise\" ADAS system, that will be a dramatic improvement over their current \"Super Cruise\" system. Ultra Cruise will cover \"95 percent\" of driving scenarios on 2 million miles of roads in the US, according to the company. The system hardware in and around the car includes multiple cameras, short- and long-range radar, and a lidar sensor, and will be powered by the Qualcomm Snapdragon Ride Platform. The luxury Cadillac Celestiq electric vehicle will be one of the first vehicles to feature Ultra Cruise.\n\nLevel 3 commercialization\nLevel 3 development\nIn 2017, BMW had been trying to make 7 Series as an automated car in public urban motorways of the United States, Germany and Israel before commercializing them in 2021. Although it was not realized, BMW is still preparing 7 Series to become the next manufacturer to reach Level 3 in the second half of 2022.In September 2021, Stellantis has presented its findings from a pilot programme testing Level 3 autonomous vehicles on public Italian highways.\nStellantis's Highway Chauffeur claims Level 3 capabilities, which was tested on the Maserati Ghibli and Fiat 500X prototypes.\nStellantis is going to roll out Level 3 capability within its cars in 2024.In January 2022, Polestar, a Volvo Cars' brand, indicated its plan to offer Level 3 autonomous driving system in the Polestar 3 SUV, Volvo XC90 successor, with technologies from Luminar Technologies, Nvidia, and Zenseact.In the same month, Bosch and the Volkswagen Group subsidiary CARIAD released a collaboration for autonomous driving up to level 3. This Joint development targets to be explored and evalauted for Level 4.As of February 2022, Hyundai Motor Company is in the stage of enhancing cybersecurity of connected cars to put Level 3 self-driving Genesis G90 on Korean roads.In December 2022, Honda announced that it will enhance its Level 3 technology to function at any speed below legal limits on highways by 2029.In early 2023, Mercedes-Benz received authorization for its Level 3 Drive Pilot in Nevada, and plans to apply for approval in California by mid-2023. Drive Pilot is planned to be available in the US market as an option for some models in the second half of 2023.\n\nLevel 4 commercialization\nCruise and Waymo offer limited robotaxi services in a handful of American cities, as fully autonomous vehicles without any human safety drivers in the vehicles.On 1 April 2023 in Japan, Level 4 legal scheme of the amended \"Road Traffic Act\" was nation-wide enforced, and one service level-upped to the Level 4 service. The approved self-driving shuttle is \"ZEN drive Pilot Level 4\" custom-made by AIST.\n\nLevel 4 development\nIn July 2020, Toyota started testing with public demonstration rides on Lexus LS (fifth generation) based TRI-P4 with Level 4 capability.\nIn August 2021, Toyota operated potentially Level 4 service using e-Palette around the Tokyo 2020 Olympic Village.In September 2020, Mercedes-Benz introduced world's first commercial Level 4 Automated Valet Parking (AVP) system named Intelligent Park Pilot for its new S-Class. The system can be pre-installed but is conditional on future national legal approval.In September 2021, Honda started testing programme toward launch of Level 4 mobility service business in Japan under collaboration with Cruise and General Motors, using Cruise AV.\nIn October 2021 at World Congress on Intelligent Transport Systems, Honda presented that they are already testing Level 4 technology on modified Legend Hybrid EX. \nAt the end of the month, Honda explained that they are conducting verification project on Level 4 technology on a test course in Tochigi prefecture. Honda plans to test on public roads in early 2022.In February 2022, General Motors and Cruise have petitioned NHTSA for permission to build and deploy a self-driving vehicle, the Cruise Origin, which is without human controls like steering wheels or brake pedals. The car was developed with GM and Cruise investor Honda, and its production is expected to begin in late 2022 in Detroit at GM's Factory Zero.\nAs of April 2022, the petition is pending.In April 2022, Honda unveiled its Level 4 mobility service partners to roll out in central Tokyo in the mid-2020s using the Cruise Origin.\nBy September 2022, Japan version prototype of Cruise Origin for Tokyo was completed and started testing.In January 2023, Holon, the new brand from the Benteler Group, unvield its self-driving shuttle autonomous during the Consumer Electronics Show (CES) 2023 in Las Vegas. The company claims the vehicle is the world's first Level 4 shuttle built to automotive standard. Production of the Holon mover is scheduled to start in the US at the end of 2025.\n\nSee also\nReferences\nFurther reading\n Media related to Self-driving cars at Wikimedia Commons",
    "Autoregressive model": "In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation which should not be confused with differential equation). Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\nContrary to the moving-average (MA) model, the autoregressive model is not always stationary as it may contain a unit root.\n\nDefinition\nThe notation \n  \n    \n      \n        A\n        R\n        (\n        p\n        )\n      \n    \n    {\\displaystyle AR(p)}\n   indicates an autoregressive model of order p. The AR(p) model is defined as\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            i\n          \n        \n        \n          X\n          \n            t\n            −\n            i\n          \n        \n        +\n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}=\\sum _{i=1}^{p}\\varphi _{i}X_{t-i}+\\varepsilon _{t}}\n  where \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          φ\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1},\\ldots ,\\varphi _{p}}\n   are the parameters of the model, and \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   is white noise. This can be equivalently written using the backshift operator B as\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            i\n          \n        \n        \n          B\n          \n            i\n          \n        \n        \n          X\n          \n            t\n          \n        \n        +\n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}=\\sum _{i=1}^{p}\\varphi _{i}B^{i}X_{t}+\\varepsilon _{t}}\n  so that, moving the summation term to the left side and using polynomial notation, we have\n\n  \n    \n      \n        ϕ\n        [\n        B\n        ]\n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\phi [B]X_{t}=\\varepsilon _{t}}\n  An autoregressive model can thus be viewed as the output of an all-pole infinite impulse response filter whose input is white noise.\nSome parameter constraints are necessary for the model to remain weak-sense stationary.  For example, processes in the AR(1) model with \n  \n    \n      \n        \n          |\n        \n        \n          φ\n          \n            1\n          \n        \n        \n          |\n        \n        ≥\n        1\n      \n    \n    {\\displaystyle |\\varphi _{1}|\\geq 1}\n   are not stationary. More generally, for an AR(p) model to be weak-sense stationary, the roots of the polynomial \n  \n    \n      \n        Φ\n        (\n        z\n        )\n        :=\n        \n          1\n          −\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              p\n            \n          \n          \n            φ\n            \n              i\n            \n          \n          \n            z\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\Phi (z):=\\textstyle 1-\\sum _{i=1}^{p}\\varphi _{i}z^{i}}\n   must lie outside the unit circle, i.e., each (complex) root \n  \n    \n      \n        \n          z\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle z_{i}}\n   must satisfy \n  \n    \n      \n        \n          |\n        \n        \n          z\n          \n            i\n          \n        \n        \n          |\n        \n        >\n        1\n      \n    \n    {\\displaystyle |z_{i}|>1}\n   (see pages 89,92 ).\n\nIntertemporal effect of shocks\nIn an AR process, a one-time shock affects values of the evolving variable infinitely far into the future. For example, consider the AR(1) model \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          φ\n          \n            1\n          \n        \n        \n          X\n          \n            t\n            −\n            1\n          \n        \n        +\n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}=\\varphi _{1}X_{t-1}+\\varepsilon _{t}}\n  . A non-zero value for \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   at say time t=1 affects \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle X_{1}}\n   by the amount  \n  \n    \n      \n        \n          ε\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{1}}\n  . Then by the AR equation for \n  \n    \n      \n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{2}}\n   in terms of \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle X_{1}}\n  , this affects \n  \n    \n      \n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{2}}\n   by the amount \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n        \n          ε\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1}\\varepsilon _{1}}\n  . Then by the AR equation for \n  \n    \n      \n        \n          X\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle X_{3}}\n   in terms of \n  \n    \n      \n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{2}}\n  , this affects \n  \n    \n      \n        \n          X\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle X_{3}}\n   by the amount \n  \n    \n      \n        \n          φ\n          \n            1\n          \n          \n            2\n          \n        \n        \n          ε\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1}^{2}\\varepsilon _{1}}\n  . Continuing this process shows that the effect of \n  \n    \n      \n        \n          ε\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{1}}\n   never ends, although if the process is stationary then the effect diminishes toward zero in the limit.\nBecause each shock affects X values infinitely far into the future from when they occur, any given value Xt is affected by shocks occurring infinitely far into the past. This can also be seen by rewriting the autoregression\n\n  \n    \n      \n        ϕ\n        (\n        B\n        )\n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ε\n          \n            t\n          \n        \n        \n      \n    \n    {\\displaystyle \\phi (B)X_{t}=\\varepsilon _{t}\\,}\n  (where the constant term has been suppressed by assuming that the variable has been measured as deviations from its mean) as\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            \n              ϕ\n              (\n              B\n              )\n            \n          \n        \n        \n          ε\n          \n            t\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle X_{t}={\\frac {1}{\\phi (B)}}\\varepsilon _{t}\\,.}\n  When the polynomial division on the right side is carried out, the polynomial in the backshift operator applied to \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   has an infinite order—that is, an infinite number of lagged values of \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   appear on the right side of the equation.\n\nCharacteristic polynomial\nThe autocorrelation function of an AR(p) process can be expressed as\n\n  \n    \n      \n        ρ\n        (\n        τ\n        )\n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          a\n          \n            k\n          \n        \n        \n          y\n          \n            k\n          \n          \n            −\n            \n              |\n            \n            τ\n            \n              |\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho (\\tau )=\\sum _{k=1}^{p}a_{k}y_{k}^{-|\\tau |},}\n  where \n  \n    \n      \n        \n          y\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle y_{k}}\n   are the roots of the polynomial\n\n  \n    \n      \n        ϕ\n        (\n        B\n        )\n        =\n        1\n        −\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            k\n          \n        \n        \n          B\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\phi (B)=1-\\sum _{k=1}^{p}\\varphi _{k}B^{k}}\n  where B is the backshift operator, where \n  \n    \n      \n        ϕ\n        (\n        ⋅\n        )\n      \n    \n    {\\displaystyle \\phi (\\cdot )}\n   is the function defining the autoregression, and where \n  \n    \n      \n        \n          φ\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{k}}\n   are the coefficients in the autoregression. The formula is valid only if all the roots have multiplicity 1.The autocorrelation function of an AR(p) process is a sum of decaying exponentials.\n\nEach real root contributes a component to the autocorrelation function that decays exponentially.\nSimilarly, each pair of complex conjugate roots contributes an exponentially damped oscillation.\n\nGraphs of AR(p) processes\nThe simplest AR process is AR(0), which has no dependence between the terms.  Only the error/innovation/noise term contributes to the output of the process, so in the figure, AR(0) corresponds to white noise.\nFor an AR(1) process with a positive \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  , only the previous term in the process and the noise term contribute to the output.  If \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n   is close to 0, then the process still looks like white noise, but as \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n   approaches 1, the output gets a larger contribution from the previous term relative to the noise. This results in a \"smoothing\" or integration of the output, similar to a low pass filter.\nFor an AR(2) process, the previous two terms and the noise term contribute to the output. If both \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1}}\n   and \n  \n    \n      \n        \n          φ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{2}}\n   are positive, the output will resemble a low pass filter, with the high frequency part of the noise decreased. If \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1}}\n   is positive while \n  \n    \n      \n        \n          φ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{2}}\n   is negative, then the process favors changes in sign between terms of the process.  The output oscillates. This can be likened to edge detection or detection of change in direction.\n\nExample: An AR(1) process\nAn AR(1) process is given by:where \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   is a white noise process with zero mean and constant variance \n  \n    \n      \n        \n          σ\n          \n            ε\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{\\varepsilon }^{2}}\n  .\n(Note: The subscript on \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1}}\n   has been dropped.) The process is weak-sense stationary if \n  \n    \n      \n        \n          |\n        \n        φ\n        \n          |\n        \n        <\n        1\n      \n    \n    {\\displaystyle |\\varphi |<1}\n   since it is obtained as the output of a stable filter whose input is white noise.  (If \n  \n    \n      \n        φ\n        =\n        1\n      \n    \n    {\\displaystyle \\varphi =1}\n   then the variance of \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}}\n   depends on time lag t, so that the variance of the series diverges to infinity as t goes to infinity, and is therefore not weak sense stationary.) Assuming \n  \n    \n      \n        \n          |\n        \n        φ\n        \n          |\n        \n        <\n        1\n      \n    \n    {\\displaystyle |\\varphi |<1}\n  , the mean \n  \n    \n      \n        E\n        ⁡\n        (\n        \n          X\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (X_{t})}\n   is identical for all values of t by the very definition of weak sense stationarity. If the mean is denoted by \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  , it follows fromthatand hence\n\n  \n    \n      \n        μ\n        =\n        0.\n      \n    \n    {\\displaystyle \\mu =0.}\n  The variance is\n\n  \n    \n      \n        \n          \n            var\n          \n        \n        (\n        \n          X\n          \n            t\n          \n        \n        )\n        =\n        E\n        ⁡\n        (\n        \n          X\n          \n            t\n          \n          \n            2\n          \n        \n        )\n        −\n        \n          μ\n          \n            2\n          \n        \n        =\n        \n          \n            \n              σ\n              \n                ε\n              \n              \n                2\n              \n            \n            \n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\textrm {var}}(X_{t})=\\operatorname {E} (X_{t}^{2})-\\mu ^{2}={\\frac {\\sigma _{\\varepsilon }^{2}}{1-\\varphi ^{2}}},}\n  where \n  \n    \n      \n        \n          σ\n          \n            ε\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{\\varepsilon }}\n   is the standard deviation of \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n  . This can be shown by noting that\n\n  \n    \n      \n        \n          \n            var\n          \n        \n        (\n        \n          X\n          \n            t\n          \n        \n        )\n        =\n        \n          φ\n          \n            2\n          \n        \n        \n          \n            var\n          \n        \n        (\n        \n          X\n          \n            t\n            −\n            1\n          \n        \n        )\n        +\n        \n          σ\n          \n            ε\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\textrm {var}}(X_{t})=\\varphi ^{2}{\\textrm {var}}(X_{t-1})+\\sigma _{\\varepsilon }^{2},}\n  and then by noticing that the quantity above is a stable fixed point of this relation.\nThe autocovariance is given by\n\n  \n    \n      \n        \n          B\n          \n            n\n          \n        \n        =\n        E\n        ⁡\n        (\n        \n          X\n          \n            t\n            +\n            n\n          \n        \n        \n          X\n          \n            t\n          \n        \n        )\n        −\n        \n          μ\n          \n            2\n          \n        \n        =\n        \n          \n            \n              σ\n              \n                ε\n              \n              \n                2\n              \n            \n            \n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n        \n        \n        \n          φ\n          \n            \n              |\n            \n            n\n            \n              |\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{n}=\\operatorname {E} (X_{t+n}X_{t})-\\mu ^{2}={\\frac {\\sigma _{\\varepsilon }^{2}}{1-\\varphi ^{2}}}\\,\\,\\varphi ^{|n|}.}\n  It can be seen that the autocovariance function decays with a decay time (also called time constant) of \n  \n    \n      \n        τ\n        =\n        1\n        −\n        φ\n      \n    \n    {\\displaystyle \\tau =1-\\varphi }\n  .The spectral density function is the Fourier transform of the autocovariance function. In discrete terms this will be the discrete-time Fourier transform:\n\n  \n    \n      \n        Φ\n        (\n        ω\n        )\n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n        \n          ∑\n          \n            n\n            =\n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        \n          B\n          \n            n\n          \n        \n        \n          e\n          \n            −\n            i\n            ω\n            n\n          \n        \n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n        \n          (\n          \n            \n              \n                σ\n                \n                  ε\n                \n                \n                  2\n                \n              \n              \n                1\n                +\n                \n                  φ\n                  \n                    2\n                  \n                \n                −\n                2\n                φ\n                cos\n                ⁡\n                (\n                ω\n                )\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\Phi (\\omega )={\\frac {1}{\\sqrt {2\\pi }}}\\,\\sum _{n=-\\infty }^{\\infty }B_{n}e^{-i\\omega n}={\\frac {1}{\\sqrt {2\\pi }}}\\,\\left({\\frac {\\sigma _{\\varepsilon }^{2}}{1+\\varphi ^{2}-2\\varphi \\cos(\\omega )}}\\right).}\n  This expression is periodic due to the discrete nature of the \n  \n    \n      \n        \n          X\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X_{j}}\n  , which is manifested as the cosine term in the denominator.  If we assume that the sampling time (\n  \n    \n      \n        Δ\n        t\n        =\n        1\n      \n    \n    {\\displaystyle \\Delta t=1}\n  ) is much smaller than the decay time (\n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  ), then we can use a continuum approximation to \n  \n    \n      \n        \n          B\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle B_{n}}\n  :\n\n  \n    \n      \n        B\n        (\n        t\n        )\n        ≈\n        \n          \n            \n              σ\n              \n                ε\n              \n              \n                2\n              \n            \n            \n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n        \n        \n        \n          φ\n          \n            \n              |\n            \n            t\n            \n              |\n            \n          \n        \n      \n    \n    {\\displaystyle B(t)\\approx {\\frac {\\sigma _{\\varepsilon }^{2}}{1-\\varphi ^{2}}}\\,\\,\\varphi ^{|t|}}\n  which yields a Lorentzian profile for the spectral density:\n\n  \n    \n      \n        Φ\n        (\n        ω\n        )\n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n        \n          \n            \n              σ\n              \n                ε\n              \n              \n                2\n              \n            \n            \n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n        \n        \n          \n            γ\n            \n              π\n              (\n              \n                γ\n                \n                  2\n                \n              \n              +\n              \n                ω\n                \n                  2\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Phi (\\omega )={\\frac {1}{\\sqrt {2\\pi }}}\\,{\\frac {\\sigma _{\\varepsilon }^{2}}{1-\\varphi ^{2}}}\\,{\\frac {\\gamma }{\\pi (\\gamma ^{2}+\\omega ^{2})}}}\n  where \n  \n    \n      \n        γ\n        =\n        1\n        \n          /\n        \n        τ\n      \n    \n    {\\displaystyle \\gamma =1/\\tau }\n   is the angular frequency associated with the decay time \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  .\nAn alternative expression for \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}}\n   can be derived by first substituting \n  \n    \n      \n        φ\n        \n          X\n          \n            t\n            −\n            2\n          \n        \n        +\n        \n          ε\n          \n            t\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi X_{t-2}+\\varepsilon _{t-1}}\n   for \n  \n    \n      \n        \n          X\n          \n            t\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle X_{t-1}}\n   in the defining equation. Continuing this process N times yields\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          φ\n          \n            N\n          \n        \n        \n          X\n          \n            t\n            −\n            N\n          \n        \n        +\n        \n          ∑\n          \n            k\n            =\n            0\n          \n          \n            N\n            −\n            1\n          \n        \n        \n          φ\n          \n            k\n          \n        \n        \n          ε\n          \n            t\n            −\n            k\n          \n        \n        .\n      \n    \n    {\\displaystyle X_{t}=\\varphi ^{N}X_{t-N}+\\sum _{k=0}^{N-1}\\varphi ^{k}\\varepsilon _{t-k}.}\n  For N approaching infinity, \n  \n    \n      \n        \n          φ\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\varphi ^{N}}\n   will approach zero and:\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            0\n          \n          \n            ∞\n          \n        \n        \n          φ\n          \n            k\n          \n        \n        \n          ε\n          \n            t\n            −\n            k\n          \n        \n        .\n      \n    \n    {\\displaystyle X_{t}=\\sum _{k=0}^{\\infty }\\varphi ^{k}\\varepsilon _{t-k}.}\n  It is seen that \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}}\n   is white noise convolved with the \n  \n    \n      \n        \n          φ\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\varphi ^{k}}\n   kernel plus the constant mean. If the white noise \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   is a Gaussian process then \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}}\n   is also a Gaussian process. In other cases, the central limit theorem indicates that \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}}\n   will be approximately normally distributed when \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n   is close to one.\nFor \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\varepsilon _{t}=0}\n  , the process \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        φ\n        \n          X\n          \n            t\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle X_{t}=\\varphi X_{t-1}}\n   will be a geometric progression (exponential growth or decay). In this case, the solution can be found analytically: \n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        a\n        \n          φ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle X_{t}=a\\varphi ^{t}}\n   whereby \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   is an unknown constant (initial condition).\n\nExplicit mean/difference form of AR(1) process\nThe AR(1) model is the discrete time analogy of the continuous Ornstein-Uhlenbeck process.  It is therefore sometimes useful to understand the properties of the AR(1) model cast in an equivalent form. In this form, the AR(1) model, with process parameter \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   is given by:\n\n  \n    \n      \n        \n          X\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          X\n          \n            t\n          \n        \n        +\n        (\n        1\n        −\n        θ\n        )\n        (\n        μ\n        −\n        \n          X\n          \n            t\n          \n        \n        )\n        +\n        \n          ε\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle X_{t+1}=X_{t}+(1-\\theta )(\\mu -X_{t})+\\varepsilon _{t+1}}\n  , where \n  \n    \n      \n        \n          |\n        \n        θ\n        \n          |\n        \n        <\n        1\n        \n      \n    \n    {\\displaystyle |\\theta |<1\\,}\n   and \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n   is the model mean.By putting this in the form \n  \n    \n      \n        \n          X\n          \n            t\n            +\n            1\n          \n        \n        =\n        ϕ\n        \n          X\n          \n            t\n          \n        \n        +\n        \n          ε\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle X_{t+1}=\\phi X_{t}+\\varepsilon _{t+1}}\n  , and then expanding the series for \n  \n    \n      \n        \n          X\n          \n            t\n            +\n            n\n          \n        \n      \n    \n    {\\displaystyle X_{t+n}}\n  , one can show that:\n\n  \n    \n      \n        E\n        ⁡\n        (\n        \n          X\n          \n            t\n            +\n            n\n          \n        \n        \n          |\n        \n        \n          X\n          \n            t\n          \n        \n        )\n        =\n        μ\n        \n          [\n          \n            1\n            −\n            \n              θ\n              \n                n\n              \n            \n          \n          ]\n        \n        +\n        \n          X\n          \n            t\n          \n        \n        \n          θ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} (X_{t+n}|X_{t})=\\mu \\left[1-\\theta ^{n}\\right]+X_{t}\\theta ^{n}}\n  , and\n\n  \n    \n      \n        Var\n        ⁡\n        (\n        \n          X\n          \n            t\n            +\n            n\n          \n        \n        \n          |\n        \n        \n          X\n          \n            t\n          \n        \n        )\n        =\n        \n          σ\n          \n            2\n          \n        \n        \n          \n            \n              1\n              −\n              \n                θ\n                \n                  2\n                  n\n                \n              \n            \n            \n              1\n              −\n              \n                θ\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\operatorname {Var} (X_{t+n}|X_{t})=\\sigma ^{2}{\\frac {1-\\theta ^{2n}}{1-\\theta ^{2}}}}\n  .\n\nChoosing the maximum lag\nThe partial autocorrelation of an AR(p) process equals zero at lags larger than p, so the appropriate maximum lag p is the one after which the partial autocorrelations are all zero.\n\nCalculation of the AR parameters\nThere are many ways to estimate the coefficients, such as the ordinary least squares procedure or method of moments (through Yule–Walker equations).\nThe AR(p) model is given by the equation\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            i\n          \n        \n        \n          X\n          \n            t\n            −\n            i\n          \n        \n        +\n        \n          ε\n          \n            t\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle X_{t}=\\sum _{i=1}^{p}\\varphi _{i}X_{t-i}+\\varepsilon _{t}.\\,}\n  It is based on parameters \n  \n    \n      \n        \n          φ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{i}}\n   where i = 1, ..., p. There is a direct correspondence between these parameters and the covariance function of the process, and this correspondence can be inverted to determine the parameters from the autocorrelation function (which is itself obtained from the covariances). This is done using the Yule–Walker equations.\n\nYule–Walker equations\nThe Yule–Walker equations, named for Udny Yule and Gilbert Walker, are the following set of equations.\n\n  \n    \n      \n        \n          γ\n          \n            m\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            k\n          \n        \n        \n          γ\n          \n            m\n            −\n            k\n          \n        \n        +\n        \n          σ\n          \n            ε\n          \n          \n            2\n          \n        \n        \n          δ\n          \n            m\n            ,\n            0\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\gamma _{m}=\\sum _{k=1}^{p}\\varphi _{k}\\gamma _{m-k}+\\sigma _{\\varepsilon }^{2}\\delta _{m,0},}\n  where m = 0, …, p, yielding p + 1 equations. Here \n  \n    \n      \n        \n          γ\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{m}}\n   is the autocovariance function of Xt, \n  \n    \n      \n        \n          σ\n          \n            ε\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{\\varepsilon }}\n   is the standard deviation of the input noise process, and \n  \n    \n      \n        \n          δ\n          \n            m\n            ,\n            0\n          \n        \n      \n    \n    {\\displaystyle \\delta _{m,0}}\n   is the Kronecker delta function.\nBecause the last part of an individual equation is non-zero only if m = 0, the set of equations can be solved by representing the equations for m > 0 in matrix form, thus getting the equation\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    γ\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    γ\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    γ\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    γ\n                    \n                      p\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    γ\n                    \n                      0\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      −\n                      1\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      −\n                      2\n                    \n                  \n                \n                \n                  ⋯\n                \n              \n              \n                \n                  \n                    γ\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      0\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      −\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n              \n              \n                \n                  \n                    γ\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      0\n                    \n                  \n                \n                \n                  ⋯\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n              \n              \n                \n                  \n                    γ\n                    \n                      p\n                      −\n                      1\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      p\n                      −\n                      2\n                    \n                  \n                \n                \n                  \n                    γ\n                    \n                      p\n                      −\n                      3\n                    \n                  \n                \n                \n                  ⋯\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    φ\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    φ\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    φ\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    φ\n                    \n                      p\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\gamma _{1}\\\\\\gamma _{2}\\\\\\gamma _{3}\\\\\\vdots \\\\\\gamma _{p}\\\\\\end{bmatrix}}={\\begin{bmatrix}\\gamma _{0}&\\gamma _{-1}&\\gamma _{-2}&\\cdots \\\\\\gamma _{1}&\\gamma _{0}&\\gamma _{-1}&\\cdots \\\\\\gamma _{2}&\\gamma _{1}&\\gamma _{0}&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\ddots \\\\\\gamma _{p-1}&\\gamma _{p-2}&\\gamma _{p-3}&\\cdots \\\\\\end{bmatrix}}{\\begin{bmatrix}\\varphi _{1}\\\\\\varphi _{2}\\\\\\varphi _{3}\\\\\\vdots \\\\\\varphi _{p}\\\\\\end{bmatrix}}}\n  which can be solved for all \n  \n    \n      \n        {\n        \n          φ\n          \n            m\n          \n        \n        ;\n        m\n        =\n        1\n        ,\n        2\n        ,\n        …\n        ,\n        p\n        }\n        .\n      \n    \n    {\\displaystyle \\{\\varphi _{m};m=1,2,\\dots ,p\\}.}\n   The remaining equation for m = 0 is\n\n  \n    \n      \n        \n          γ\n          \n            0\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            k\n          \n        \n        \n          γ\n          \n            −\n            k\n          \n        \n        +\n        \n          σ\n          \n            ε\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\gamma _{0}=\\sum _{k=1}^{p}\\varphi _{k}\\gamma _{-k}+\\sigma _{\\varepsilon }^{2},}\n  which, once  \n  \n    \n      \n        {\n        \n          φ\n          \n            m\n          \n        \n        ;\n        m\n        =\n        1\n        ,\n        2\n        ,\n        …\n        ,\n        p\n        }\n      \n    \n    {\\displaystyle \\{\\varphi _{m};m=1,2,\\dots ,p\\}}\n   are known, can be solved for \n  \n    \n      \n        \n          σ\n          \n            ε\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{\\varepsilon }^{2}.}\n  \nAn alternative formulation is in terms of the autocorrelation function. The AR parameters are determined by the first p+1 elements \n  \n    \n      \n        ρ\n        (\n        τ\n        )\n      \n    \n    {\\displaystyle \\rho (\\tau )}\n   of the autocorrelation function. The full autocorrelation function can then be derived by recursively calculating\n\n  \n    \n      \n        ρ\n        (\n        τ\n        )\n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            k\n          \n        \n        ρ\n        (\n        k\n        −\n        τ\n        )\n      \n    \n    {\\displaystyle \\rho (\\tau )=\\sum _{k=1}^{p}\\varphi _{k}\\rho (k-\\tau )}\n  Examples for some Low-order AR(p) processes\n\np=1\n\n  \n    \n      \n        \n          γ\n          \n            1\n          \n        \n        =\n        \n          φ\n          \n            1\n          \n        \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{1}=\\varphi _{1}\\gamma _{0}}\n  \nHence \n  \n    \n      \n        \n          ρ\n          \n            1\n          \n        \n        =\n        \n          γ\n          \n            1\n          \n        \n        \n          /\n        \n        \n          γ\n          \n            0\n          \n        \n        =\n        \n          φ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\rho _{1}=\\gamma _{1}/\\gamma _{0}=\\varphi _{1}}\n  \np=2\nThe Yule–Walker equations for an AR(2) process are\n\n  \n    \n      \n        \n          γ\n          \n            1\n          \n        \n        =\n        \n          φ\n          \n            1\n          \n        \n        \n          γ\n          \n            0\n          \n        \n        +\n        \n          φ\n          \n            2\n          \n        \n        \n          γ\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{1}=\\varphi _{1}\\gamma _{0}+\\varphi _{2}\\gamma _{-1}}\n  \n\n  \n    \n      \n        \n          γ\n          \n            2\n          \n        \n        =\n        \n          φ\n          \n            1\n          \n        \n        \n          γ\n          \n            1\n          \n        \n        +\n        \n          φ\n          \n            2\n          \n        \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{2}=\\varphi _{1}\\gamma _{1}+\\varphi _{2}\\gamma _{0}}\n  Remember that \n  \n    \n      \n        \n          γ\n          \n            −\n            k\n          \n        \n        =\n        \n          γ\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{-k}=\\gamma _{k}}\n  \nUsing the first equation yields \n  \n    \n      \n        \n          ρ\n          \n            1\n          \n        \n        =\n        \n          γ\n          \n            1\n          \n        \n        \n          /\n        \n        \n          γ\n          \n            0\n          \n        \n        =\n        \n          \n            \n              φ\n              \n                1\n              \n            \n            \n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho _{1}=\\gamma _{1}/\\gamma _{0}={\\frac {\\varphi _{1}}{1-\\varphi _{2}}}}\n  \nUsing the recursion formula yields \n  \n    \n      \n        \n          ρ\n          \n            2\n          \n        \n        =\n        \n          γ\n          \n            2\n          \n        \n        \n          /\n        \n        \n          γ\n          \n            0\n          \n        \n        =\n        \n          \n            \n              \n                φ\n                \n                  1\n                \n                \n                  2\n                \n              \n              −\n              \n                φ\n                \n                  2\n                \n                \n                  2\n                \n              \n              +\n              \n                φ\n                \n                  2\n                \n              \n            \n            \n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho _{2}=\\gamma _{2}/\\gamma _{0}={\\frac {\\varphi _{1}^{2}-\\varphi _{2}^{2}+\\varphi _{2}}{1-\\varphi _{2}}}}\n\nEstimation of AR parameters\nThe above equations (the Yule–Walker equations) provide several routes to estimating the parameters of an AR(p) model, by replacing the theoretical covariances with estimated values. Some of these variants can be described as follows:\n\nEstimation of autocovariances or autocorrelations. Here each of these terms is estimated separately, using conventional estimates. There are different ways of doing this and the choice between these affects the properties of the estimation scheme. For example, negative estimates of the variance can be produced by some choices.\nFormulation as a least squares regression problem in which an ordinary least squares prediction problem is constructed, basing prediction of values of Xt on the p previous values of the same series. This can be thought of as a forward-prediction scheme. The normal equations for this problem can be seen to correspond to an approximation of the matrix form of the Yule–Walker equations in which each appearance of an autocovariance of the same lag is replaced by a slightly different estimate.\nFormulation as an extended form of ordinary least squares prediction problem. Here two sets of prediction equations are combined into a single estimation scheme and a single set of normal equations. One set is the set of forward-prediction equations and the other is a corresponding set of backward prediction equations, relating to the backward representation of the AR model:\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            i\n          \n        \n        \n          X\n          \n            t\n            −\n            i\n          \n        \n        +\n        \n          ε\n          \n            t\n          \n          \n            ∗\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle X_{t}=\\sum _{i=1}^{p}\\varphi _{i}X_{t-i}+\\varepsilon _{t}^{*}\\,.}\n  \nHere predicted values of Xt would be based on the p future values of the same series. This way of estimating the AR parameters is due to John Parker Burg, and is called the Burg method: Burg and later authors called these particular estimates \"maximum entropy estimates\", but the reasoning behind this applies to the use of any set of estimated AR parameters. Compared to the estimation scheme using only the forward prediction equations, different estimates of the autocovariances are produced, and the estimates have different stability properties. Burg estimates are particularly associated with maximum entropy spectral estimation.Other possible approaches to estimation include maximum likelihood estimation. Two distinct variants of maximum likelihood are available: in one (broadly equivalent to the forward prediction least squares scheme) the likelihood function considered is that corresponding to the conditional distribution of later values in the series given the initial p values in the series; in the second, the likelihood function considered is that corresponding to the unconditional joint distribution of all the values in the observed series. Substantial differences in the results of these approaches can occur if the observed series is short, or if the process is close to non-stationarity.\n\nSpectrum\nThe power spectral density (PSD) of an AR(p) process with noise variance \n  \n    \n      \n        \n          V\n          a\n          r\n        \n        (\n        \n          Z\n          \n            t\n          \n        \n        )\n        =\n        \n          σ\n          \n            Z\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {Var} (Z_{t})=\\sigma _{Z}^{2}}\n   is\n\n  \n    \n      \n        S\n        (\n        f\n        )\n        =\n        \n          \n            \n              σ\n              \n                Z\n              \n              \n                2\n              \n            \n            \n              \n                |\n              \n              1\n              −\n              \n                ∑\n                \n                  k\n                  =\n                  1\n                \n                \n                  p\n                \n              \n              \n                φ\n                \n                  k\n                \n              \n              \n                e\n                \n                  −\n                  i\n                  2\n                  π\n                  f\n                  k\n                \n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle S(f)={\\frac {\\sigma _{Z}^{2}}{|1-\\sum _{k=1}^{p}\\varphi _{k}e^{-i2\\pi fk}|^{2}}}.}\n\nAR(0)\nFor white noise (AR(0))\n\n  \n    \n      \n        S\n        (\n        f\n        )\n        =\n        \n          σ\n          \n            Z\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle S(f)=\\sigma _{Z}^{2}.}\n\nAR(1)\nFor AR(1)\n\n  \n    \n      \n        S\n        (\n        f\n        )\n        =\n        \n          \n            \n              σ\n              \n                Z\n              \n              \n                2\n              \n            \n            \n              \n                |\n              \n              1\n              −\n              \n                φ\n                \n                  1\n                \n              \n              \n                e\n                \n                  −\n                  2\n                  π\n                  i\n                  f\n                \n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              σ\n              \n                Z\n              \n              \n                2\n              \n            \n            \n              1\n              +\n              \n                φ\n                \n                  1\n                \n                \n                  2\n                \n              \n              −\n              2\n              \n                φ\n                \n                  1\n                \n              \n              cos\n              ⁡\n              2\n              π\n              f\n            \n          \n        \n      \n    \n    {\\displaystyle S(f)={\\frac {\\sigma _{Z}^{2}}{|1-\\varphi _{1}e^{-2\\pi if}|^{2}}}={\\frac {\\sigma _{Z}^{2}}{1+\\varphi _{1}^{2}-2\\varphi _{1}\\cos 2\\pi f}}}\n  If \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle \\varphi _{1}>0}\n    there is a single spectral peak at f=0, often referred to as red noise. As \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\varphi _{1}}\n    becomes nearer 1, there is stronger power at low frequencies, i.e. larger time lags. This is then a low-pass filter, when applied to full spectrum light, everything except for the red light will be filtered.\nIf \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n        <\n        0\n      \n    \n    {\\displaystyle \\varphi _{1}<0}\n   there is a minimum at f=0, often referred to as blue noise. This similarly acts as a high-pass filter, everything except for blue light will be filtered.\n\nAR(2)\nAR(2) processes can be split into three groups depending on the characteristics of their roots:\n\n  \n    \n      \n        \n          z\n          \n            1\n          \n        \n        ,\n        \n          z\n          \n            2\n          \n        \n        =\n        −\n        \n          \n            1\n            \n              2\n              \n                φ\n                \n                  2\n                \n              \n            \n          \n        \n        \n          (\n          \n            \n              φ\n              \n                1\n              \n            \n            ±\n            \n              \n                \n                  φ\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                +\n                4\n                \n                  φ\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle z_{1},z_{2}=-{\\frac {1}{2\\varphi _{2}}}\\left(\\varphi _{1}\\pm {\\sqrt {\\varphi _{1}^{2}+4\\varphi _{2}}}\\right)}\n  When \n  \n    \n      \n        \n          φ\n          \n            1\n          \n          \n            2\n          \n        \n        +\n        4\n        \n          φ\n          \n            2\n          \n        \n        <\n        0\n      \n    \n    {\\displaystyle \\varphi _{1}^{2}+4\\varphi _{2}<0}\n  , the process has a pair of complex-conjugate roots, creating a mid-frequency peak at:\n  \n    \n      \n        \n          f\n          \n            ∗\n          \n        \n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n          cos\n          \n            −\n            1\n          \n        \n        ⁡\n        \n          (\n          \n            \n              \n                \n                  φ\n                  \n                    1\n                  \n                \n                (\n                \n                  φ\n                  \n                    2\n                  \n                \n                −\n                1\n                )\n              \n              \n                4\n                \n                  φ\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f^{*}={\\frac {1}{2\\pi }}\\cos ^{-1}\\left({\\frac {\\varphi _{1}(\\varphi _{2}-1)}{4\\varphi _{2}}}\\right)}\n  Otherwise the process has real roots, and:\n\nWhen \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle \\varphi _{1}>0}\n   it acts as a low-pass filter on the white noise with a spectral peak at \n  \n    \n      \n        f\n        =\n        0\n      \n    \n    {\\displaystyle f=0}\n  \nWhen \n  \n    \n      \n        \n          φ\n          \n            1\n          \n        \n        <\n        0\n      \n    \n    {\\displaystyle \\varphi _{1}<0}\n   it acts as a high-pass filter on the white noise with a spectral peak at \n  \n    \n      \n        f\n        =\n        1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle f=1/2}\n  .The process is non-stationary when the roots are outside the unit circle.\nThe process is stable when the roots are within the unit circle, or equivalently when the coefficients are in the triangle \n  \n    \n      \n        −\n        1\n        ≤\n        \n          φ\n          \n            2\n          \n        \n        ≤\n        1\n        −\n        \n          |\n        \n        \n          φ\n          \n            1\n          \n        \n        \n          |\n        \n      \n    \n    {\\displaystyle -1\\leq \\varphi _{2}\\leq 1-|\\varphi _{1}|}\n  .\nThe full PSD function can be expressed in real form as:\n\n  \n    \n      \n        S\n        (\n        f\n        )\n        =\n        \n          \n            \n              σ\n              \n                Z\n              \n              \n                2\n              \n            \n            \n              1\n              +\n              \n                φ\n                \n                  1\n                \n                \n                  2\n                \n              \n              +\n              \n                φ\n                \n                  2\n                \n                \n                  2\n                \n              \n              −\n              2\n              \n                φ\n                \n                  1\n                \n              \n              (\n              1\n              −\n              \n                φ\n                \n                  2\n                \n              \n              )\n              cos\n              ⁡\n              (\n              2\n              π\n              f\n              )\n              −\n              2\n              \n                φ\n                \n                  2\n                \n              \n              cos\n              ⁡\n              (\n              4\n              π\n              f\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle S(f)={\\frac {\\sigma _{Z}^{2}}{1+\\varphi _{1}^{2}+\\varphi _{2}^{2}-2\\varphi _{1}(1-\\varphi _{2})\\cos(2\\pi f)-2\\varphi _{2}\\cos(4\\pi f)}}}\n\nImplementations in statistics packages\nR, the stats package includes an ar function.\nMATLAB's Econometrics Toolbox and System Identification Toolbox includes autoregressive models\nMatlab and Octave: the TSA toolbox contains several estimation functions for uni-variate, multivariate and adaptive autoregressive models.\nPyMC3: the Bayesian statistics and probabilistic programming framework supports autoregressive modes with p lags.\nbayesloop supports parameter inference and model selection for the AR-1 process with time-varying parameters.\nPython: implementation in statsmodels.\n\nImpulse response\nThe impulse response of a system is the change in an evolving variable in response to a change in the value of a shock term k periods earlier, as a function of k. Since the AR model is a special case of the vector autoregressive model, the computation of the impulse response in vector autoregression#impulse response applies here.\n\nn-step-ahead forecasting\nOnce the parameters of the autoregression\n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          φ\n          \n            i\n          \n        \n        \n          X\n          \n            t\n            −\n            i\n          \n        \n        +\n        \n          ε\n          \n            t\n          \n        \n        \n      \n    \n    {\\displaystyle X_{t}=\\sum _{i=1}^{p}\\varphi _{i}X_{t-i}+\\varepsilon _{t}\\,}\n  have been estimated, the autoregression can be used to forecast an arbitrary number of periods into the future. First use t to refer to the first period for which data is not yet available; substitute the known preceding values Xt-i for i=1, ..., p into the autoregressive equation while setting the error term \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}}\n   equal to zero (because we forecast Xt to equal its expected value, and the expected value of the unobserved error term is zero). The output of the autoregressive equation is the forecast for the first unobserved period. Next, use t to refer to the next period for which data is not yet available; again the autoregressive equation is used to make the forecast, with one difference: the value of X one period prior to the one now being forecast is not known, so its expected value—the predicted value arising from the previous forecasting step—is used instead. Then for future periods the same procedure is used, each time using one more forecast value on the right side of the predictive equation until, after  p predictions, all p right-side values are predicted values from preceding steps.\nThere are four sources of uncertainty regarding predictions obtained in this manner: (1) uncertainty as to whether the autoregressive model is the correct model; (2) uncertainty about the accuracy of the forecasted values that are used as lagged values in the right side of the autoregressive equation; (3) uncertainty about the true values of the autoregressive coefficients; and (4) uncertainty about the value of the error term \n  \n    \n      \n        \n          ε\n          \n            t\n          \n        \n        \n      \n    \n    {\\displaystyle \\varepsilon _{t}\\,}\n   for the period being predicted. Each of the last three can be quantified and combined to give a confidence interval for the n-step-ahead predictions; the confidence interval will become wider as n increases because of the use of an increasing number of estimated values for the right-side variables.\n\nSee also\nMoving average model\nLinear difference equation\nPredictive analytics\nLinear predictive coding\nResonance\nLevinson recursion\nOrnstein–Uhlenbeck process\nInfinite impulse response\n\nNotes\nReferences\nMills, Terence C. (1990). Time Series Techniques for Economists. Cambridge University Press. ISBN 9780521343398.\nPercival, Donald B.; Walden, Andrew T. (1993). Spectral Analysis for Physical Applications. Cambridge University Press. Bibcode:1993sapa.book.....P.\nPandit, Sudhakar M.; Wu, Shien-Ming (1983). Time Series and System Analysis with Applications. John Wiley & Sons.\n\nExternal links\nAutoRegression Analysis (AR) by Paul Bourke\nEconometrics lecture (topic: Autoregressive models) on YouTube by Mark Thoma",
    "Azure Machine Learning": "Microsoft Azure, often referred to as Azure (/ˈæʒər, ˈeɪʒər/ AZH-ər, AY-zhər, UK also /ˈæzjʊər, ˈeɪzjʊər/ AZ-ure, AY-zure), is a cloud computing platform run by Microsoft, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.\nMicrosoft's cloud computing platform, Azure, was introduced at the Professional Developers Conference (PDC) in October 2008 under the codename \"Project Red Dog\". It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014.\n\nServices\nMicrosoft Azure uses large-scale virtualization at Microsoft data centers worldwide and it offers more than 600 services.\n\nComputer services\nVirtual machines, infrastructure as a service (IaaS) allowing users to launch general-purpose Microsoft Windows and Linux virtual machines, software as a service (SaaS) as well as preconfigured machine images for popular software packages.Starting in 2022, VMs are powered by Ampere Cloud-native processors.\nMost users run Linux on Azure, some of the many Linux distributions offered, including Microsoft's own Linux-based Azure Sphere.\nApp services, platform as a service (PaaS) environment letting developers easily publish and manage websites.\nWebsites, Azure Web Sites allows developers to build sites using ASP.NET, PHP, Node.js, Java, or Python and can be deployed using FTP, Git, Mercurial, Team Foundation Server or uploaded through the user portal. This feature was announced in preview form in June 2012 at the Meet Microsoft Azure event. Customers can create websites in PHP, ASP.NET, Node.js, or Python, or select from several open source applications from a gallery to deploy. This comprises one aspect of the platform as a service (PaaS) offerings for the Microsoft Azure Platform. It was renamed Web Apps in April 2015.\nWebJobs, are applications that can be deployed to an App Service environment to implement background processing that can be invoked on a schedule, on demand, or run continuously. The Blob, Table and Queue services can be used to communicate between WebApps, XYZ, iOS Software and WebJobs and to provide state.\nAzure Kubernetes Service (AKS) allows you to quickly deploy a production ready kubernetes cluster in Azure. Azure is responsible for managing the control plane and customers get the flexibility to choose/scale the data place (kubernetes worker nodes).\n\nIdentity\nAzure Active Directory Connect is used to synchronize on-premises directories and enable SSO (Single Sign On).\nAzure Active Directory B2C allows the use of consumer identity and access management in the cloud.\nAzure Active Directory Domain Services is used to join Azure virtual machines to a domain without domain controllers.\nAzure information protection can be used to protect sensitive information.\nAzure Active Directory External Identities are a set of capabilities which allow organizations to collaborate with external users including customers and partners.\n\nMobile services\nMobile Engagement collects real-time analytics that highlight users’ behavior. It also provides push notifications to mobile devices.\nHockeyApp can be used to develop, distribute, and beta-test mobile apps.\n\nStorage services\nStorage Services provides REST and SDK APIs for storing and accessing data on the cloud.\nTable Service lets programs store structured text in partitioned collections of entities that are accessed by partition key and primary key. Azure Table Service is a NoSQL non-relational database.\nBlob Service allows programs to store unstructured text and binary data as blobs that can be accessed by an HTTP(S) path. Blob service also provides security mechanisms to control access to data.\nQueue Service lets programs communicate asynchronously by message using queues.\nFile Service allows storing and access of data on the cloud using the REST APIs or the SMB protocol.\n\nCommunication services\nAzure Communication Services offers an SDK for creating web and mobile communications applications that include SMS, video calling, VOIP and PSTN calling, and web based chat.\n\nData management\nAzure Data Explorer provides big data analytics and data-exploration capabilities\nAzure Search provides text search and a subset of OData's structured filters using REST or SDK APIs.\nCosmos DB is a NoSQL database service that implements a subset of the SQL SELECT statement on JSON documents.\nAzure Cache for Redis is a managed implementation of Redis.\nStorSimple manages storage tasks between on-premises devices and cloud storage.\nAzure SQL Database works to create, scale and extend applications into the cloud using Microsoft SQL Server technology. It also integrates with Active Directory, Microsoft System Center and Hadoop.\nAzure Synapse Analytics is a fully managed cloud data warehouse.\nAzure Data Factory, is a data integration service that allows creation of data-driven workflows in the cloud for orchestrating and automating data movement and data transformation.\nAzure Data Lake is a scalable data storage and analytic service for big data analytics workloads that require developers to run massively parallel queries.\nAzure HDInsight is a big data relevant service, that deploys Hortonworks Hadoop on Microsoft Azure, and supports the creation of Hadoop clusters using Linux with Ubuntu.\nAzure Stream Analytics  is a Serverless scalable event processing engine that enables users to develop and run real-time analytics on multiple streams of data from sources such as devices, sensors, web sites, social media, and other applications.\n\nMessaging\nThe Microsoft Azure Service Bus allows applications running on Azure premises or off-premises devices to communicate with Azure. This helps to build scalable and reliable applications in a service-oriented architecture (SOA). The Azure service bus supports four different types of communication mechanisms:\nEvent Hubs, which provide event and telemetry ingress to the cloud at massive scale, with low latency and high reliability. For example, an event hub can be used to track data from cell phones such as coordinating with a GPS in real time.\nQueues, which allow one-directional communication. A sender application would send the message to the service bus queue, and a receiver would read from the queue. Though there can be multiple readers for the queue only one would process a single message.\nTopics, which provide one-directional communication using a subscriber pattern. It is similar to a queue, however, each subscriber will receive a copy of the message sent to a Topic. Optionally the subscriber can filter out messages based on specific criteria defined by the subscriber.\nRelays, which provide bi-directional communication. Unlike queues and topics, a relay doesn't store in-flight messages in its own memory. Instead, it just passes them on to the destination application.\n\nMedia services\nA PaaS offering that can be used for encoding, content protection, streaming, or analytics.\n\nCDN\nAzure has a worldwide content delivery network (CDN) designed to efficiently deliver audio, video, applications, images, and other static files. It improves the performance of websites by caching static files closer to users based on their geographic location. Users can manage the network using a REST-based HTTP API.Azure has 118 point of presence locations, across 100 cities, worldwide (also known as Edge locations) as of January 2023.\n\nDeveloper\nApplication Insights\nAzure DevOps\n\nManagement\nWith Azure Automation, users can easily automate repetitive and time-consuming tasks, often prone to cloud or enterprise setting errors. They can accomplish it using runbooks or desired state configurations for process automation.\nMicrosoft SMA\n\nAzure AI\nMicrosoft Azure Machine Learning (Azure ML) provides tools and ML frameworks for developers to create their own machine learning and artificial intelligence (AI) services.\nAzure Cognitive Services by Microsoft comprise prebuilt APIs, SDKs, and services developers can customize. These services encompass perceptual and cognitive intelligence features such as speech recognition, speaker recognition, neural speech synthesis, face recognition, computer vision, OCR/form understanding, natural language processing, machine translation, and business decision services. Many AI characteristics in Microsoft's products and services, namely Bing, Office, Teams, Xbox, and Windows, are driven by Azure Cognitive Services.\n\nAzure Blockchain Workbench\nThrough Azure Blockchain Workbench, Microsoft is providing the required infrastructure to set up a consortium network in multiple topologies using a variety of consensus mechanisms. Microsoft provides integration from these blockchain platforms to other Microsoft services to streamline the development of distributed applications. Microsoft supports many general-purpose blockchains including Ethereum and Hyperledger Fabric and purpose-built blockchains like Corda.\n\nFunctions\nAzure functions are used in serverless computing architectures where subscribers can execute code as an event driven Function-as-a-Service (FaaS) without managing the underlying server resources. Customers using Azure functions are billed based on per-second resource consumption and executions.\n\nInternet of Things (IoT)\nAzure IoT Hub lets you connect, monitor, and manage billions of IoT assets. On February 4, 2016, Microsoft announced the General Availability of the Azure IoT Hub service.\nAzure IoT Edge is a fully managed service built on IoT Hub that allows for cloud intelligence deployed locally on IoT edge devices.\nAzure IoT Central is a fully managed SaaS app that makes it easy to connect, monitor, and manage IoT assets at scale. On December 5, 2017, Microsoft announced the Public Preview of Azure IoT Central; its Azure IoT SaaS service.\nOn October 4, 2017, Microsoft began shipping GA versions of the official Microsoft Azure IoT Developer Kit (DevKit) board; manufactured by MXChip.\nOn April 16, 2018, Microsoft announced the launch of the Azure Sphere, an end-to-end IoT product that focuses on microcontroller-based devices and uses Linux.\nOn May 7, 2018, Microsoft announced the launch of Azure Maps, an enterprise mapping platform.\nOn June 27, 2018, Microsoft launched Azure IoT Edge, used to run Azure services and artificial intelligence on IoT devices.\nOn November 20, 2018, Microsoft launched the Open Enclave SDK for cross-platform systems such as ARM TrustZone and Intel SGX.\n\nAzure Orbital\nLaunched in September 2020, Azure Orbital lets private industries and government agencies process satellite data quickly by connecting directly to cloud computing networks. Mobile cloud computing ground stations are also available to provide connectivity to remote locations without ground infrastructure. Third-party satellite systems, like SpaceX's Starlink and SES' O3b constellation, can be employed.SES plans to use Microsoft's data centers to provide cloud connectivity to remote areas through its next-generation O3b mPOWER MEO satellites alongside Microsoft's data centers. The company will deploy satellite control and uplink ground stations to achieve this. SES launched the first two O3b mPOWER satellites in December 2022; nine more are scheduled between 2023 and 2024. The service should begin in Q3 2023.According to Microsoft, using satellites to connect to cloud data centers may provide faster speeds than complex fiber routes. For online media, entertainment, or gaming activities, connecting from home to the cloud can involve longer routes with multiple hops. Through their experiments with Xbox Cloud, Microsoft has discovered that satellite connection is faster than terrestrial networks in certain parts of the world (including specific locations in the USA).\n\nRegional expansion\nAs of 2018, Azure was available in 54 regions, and Microsoft was the first primary cloud provider to establish facilities in Africa, with two regions in South Africa. Azure geographies consist of multiple Azure Regions, like \"North Europe\" (located in Dublin, Ireland) and \"West Europe\" (located in Amsterdam, Netherlands). Each Azure Region is paired with another region within the same geography, forming a regional pair. For instance, the locations of Dublin and Amsterdam create a regional couple.\n\nMiddle East cloud data centers\nOn June 19, 2019, Microsoft announced the launch of two new cloud regions in the United Arab Emirates – Microsoft's first in the Middle East. Microsoft's management stated that these new data centers would empower customers and partners to embrace the benefits of the Fourth Industrial Revolution and achieve more using cloud technologies.\n\nResearch partnerships\nMicrosoft has partners that sell its products. In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide. The specific computer programs used in the process fall under the Azure Machine Learning and the Azure IoT Hub platforms.\n\nDesign\nMicrosoft Azure utilizes a specialized operating system with the same name to power its \"fabric layer\". This cluster is hosted at Microsoft's data centers and is responsible for managing computing and storage resources and allocating them to applications running on the Microsoft Azure platform. It's a \"cloud layer\" built upon various Windows Server systems, including the customized Microsoft Azure Hypervisor, which is based on Windows Server 2008 and enables the virtualization of services.The Microsoft Azure Fabric Controller maintains the scalability and dependability of services and environments in the data center. It prevents failure in server malfunction and manages users' web applications, including memory allocation and load balancing.Azure provides an API built on REST, HTTP, and XML that allows a developer to interact with the services offered by Microsoft Azure. Microsoft also provides a client-side managed class library that encapsulates the functions of interacting with the services. It also integrates with Microsoft Visual Studio, Git, and Eclipse.Users can manage Azure services in multiple ways, one of which is through the Web-based Azure Portal, which became generally available in December 2015. Apart from accessing services via API, users can browse active resources, adjust settings, launch new resources, and view primary monitoring data of functional virtual machines and services using the portal.\n\nDeployment models\nRegarding cloud resources, Microsoft Azure offers two deployment models: the \"classic\" model and the Azure Resource Manager. In the classic model, each resource, like a virtual machine or SQL database, had to be managed separately. But in 2014, Azure introduced the Azure Resource Manager, which allows users to group related services. This update makes it easier and more efficient to deploy, manage, and monitor resources that work closely together. The classic model will eventually be phased out.\n\nHistory and timeline\nIn 2005, Microsoft took over Groove Networks, and Bill Gates made Groove's founder Ray Ozzie one of his 5 direct reports as one of 3 chief technology officers. Ozzie met with Amitabh Srivastava, which let Srivastava change course. They convinced Dave Cutler to postpone his retirement and their teams developed a cloud operating system.\nOctober 2008 (PDC LA) – Announced the Windows Azure Platform.\nMarch 2009 – Announced SQL Azure Relational Database.\nNovember 2009 – Updated Windows Azure CTP, Enabled full trust, PHP, Java, CDN CTP and more.\nFebruary 1, 2010 – Windows Azure Platform commercially available.\n\nJune 2010 – Windows Azure Update, .NET Framework 4, OS Versioning, CDN, SQL Azure Update.\nOctober 2010 (PDC) – Platform enhancements, Windows Azure Connect, improved Dev / IT Pro Experience.\nDecember 2011 – Traffic manager, SQL Azure reporting, HPC scheduler.\nJune 2012 – Websites, Virtual machines for Windows and Linux, Python SDK, new portal, locally redundant storage.\nApril 2014 – Windows Azure renamed Microsoft Azure, ARM Portal introduced at Build 2014.\nJuly 2014 – Azure Machine Learning public preview.\nNovember 2014 – Outage affecting major websites including MSN.com.\nSeptember 2015 – Azure Cloud Switch introduced as a cross-platform Linux distribution. Currently known as SONiC\nDecember, 2015 – Azure ARM Portal (codename \"Ibiza\") released.\nMarch, 2016 – Azure Service Fabric is Generally Available (GA)\nSeptember 2017 – Microsoft Azure gets a new logo and a Manifesto\nMay 7, 2018 - Azure Maps is Generally Available (GA)\nJuly 16, 2018 – Azure Service Fabric Mesh public preview\nSeptember 24, 2018 – Microsoft Azure IoT Central is Generally Available (GA)\nOctober 10, 2018 – Microsoft joins the Linux-oriented group Open Invention Network.\nApril 17, 2019 – Azure Front Door Service is now available.\nMarch 2020 – Microsoft said that there was a 775% increase in Microsoft Teams usage in Italy due to the COVID-19 pandemic. The company estimates there are now 44 million daily active users of Teams worldwide.\n\nPrivacy\nAccording to the USA PATRIOT Act, Microsoft has acknowledged that the U.S. government can access data even if the hosting company is not American and the data is outside the U.S. To address concerns related to privacy and security, Microsoft has established the Microsoft Azure Trust Center. Microsoft Azure offers services that comply with multiple compliance programs, including ISO 27001:2005 and HIPAA. A comprehensive and up-to-date list of these services is available on the Microsoft Azure Trust Center Compliance page. It's worth noting that Microsoft Azure has received JAB Provisional Authority to Operate (P-ATO) from the U.S. government under the Federal Risk and Authorization Management Program (FedRAMP) guidelines. This program provides a standardized approach to security assessment, authorization, and continuous monitoring for cloud services used by the federal government.\n\nSignificant outages\nThe following is a list of Microsoft Azure outages and service disruptions.\n\nCertifications\nA large variety of Azure certifications can be attained, each requiring one or multiple successfully completed examinations.\nCertification levels range from beginner, intermediate to expert.\nExamples of common certifications include:\n\nAzure Fundamentals\nAzure Developer Associate\nAzure Administrator Associate\nAzure Data Engineers Associate\nAzure Security Engineer Associate\nAzure Solutions Architect Expert\nAzure DevOps Engineer Expert\n\nKey people\nDave Cutler, Lead Developer, Microsoft Azure\nMark Russinovich, CTO, Microsoft Azure\nScott Guthrie, Executive Vice President of the Cloud and AI group in Microsoft\nJason Zander, Executive Vice President, Microsoft Azure\nJulia White, Corporate Vice President, Microsoft Azure\n\nSee also\nCloud-computing comparison\nComparison of file hosting services\nMicrosoft Azure Dev Tools for Teaching\nCBL-Mariner\n\nReferences\nCitations\nSources\nFurther reading\nExternal links\nOfficial website",
    "BERT (language model)": "Bidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced in 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words).\n\nArchitecture\nBERT is based on the transformer architecture. Specifically, BERT is composed of Transformer encoder layers.\nBERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30,000. Any token not appearing in its vocabulary is replaced by [UNK] for \"unknown\".\nBERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction, and the training objective was to predict the selected token given its context. The selected token is \n\nreplaced with a [MASK] token with probability 80%,\nreplaced with a random word token with probability 10%,\nnot replaced with probability 10%.For example, the sentence \"my dog is cute\" may have the 4-th token selected for prediction. The model would have input text\n\n\"my dog is [MASK]\" with probability 80%,\n\"my dog is happy\" with probability 10%,\n\"my dog is cute\" with probability 10%.After processing the input text, the model's 4-th output vector is passed to a separate neural network, which outputs a probability distribution over its 30,000-large vocabulary.\nnext sentence prediction: Given two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for \"classify\"). The two spans are separated by a special token [SEP] (for \"separate\"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext].\n\nFor example, given \"[CLS] my dog is cute [SEP] he likes playing\" the model should output token [IsNext].\nGiven \"[CLS] my dog is cute [SEP] how do magnets work\" the model should output token [NotNext].As a result of this training process, BERT learns latent representations of words and sentences in context. After pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference, text classification) and sequence-to-sequence based language generation tasks (question-answering, conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning.\n\nPerformance\nWhen BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks:\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0\nSWAG (Situations With Adversarial Generations)\n\nAnalysis\nThe reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\nThe high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair).  BERT considers the words surrounding the target word fine from the left and right side.\nHowever it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt, with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data, BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example, if the task is a sentiment classification task on financial data, a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub.\n\nHistory\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\nOn October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.\n\nRecognition\nThe research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).\n\nReferences\nFurther reading\nRogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What we know about how BERT works\". arXiv:2002.12327 [cs.CL].\n\nExternal links\nOfficial GitHub repository\nBERT on Devopedia",
    "BIRCH": "BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets. With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\nIts inventors claim BIRCH to be the \"first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively\", beating DBSCAN by two months. The BIRCH algorithm received the SIGMOD 10 year test of time award in 2006.\n\nProblem with previous methods\nPrevious clustering algorithms performed less effectively over very large databases and did not adequately consider the case wherein a data-set was too large to fit in main memory. As a result, there was a lot of overhead maintaining high clustering quality while minimizing the cost of additional IO (input/output) operations. Furthermore, most of BIRCH's predecessors inspect all data points (or all currently existing clusters) equally for each 'clustering decision' and do not perform heuristic weighting based on the distance between these data points.\n\nAdvantages with BIRCH\nIt is local in that each clustering decision is made without scanning all data points and currently existing clusters.\nIt exploits the observation that the data space is not usually uniformly occupied and not every data point is equally important.\nIt makes full use of available memory to derive the finest possible sub-clusters while minimizing I/O costs.\nIt is also an incremental method that does not require the whole data set in advance.\n\nAlgorithm\nThe BIRCH algorithm takes as input a set of N data points, represented as real-valued vectors, and a desired number of clusters K. It operates in four phases, the second of which is optional.\nThe first phase builds a clustering feature (\n  \n    \n      C\n      F\n    \n    CF\n  ) tree out of the data points, a height-balanced tree data structure, defined as follows:\n\nGiven a set of N d-dimensional data points, the clustering feature \n  \n    \n      C\n      F\n    \n    CF\n   of the set is defined as the triple \n  \n    \n      \n        C\n        F\n        =\n        (\n        N\n        ,\n        \n          \n            \n              L\n              S\n            \n            →\n          \n        \n        ,\n        S\n        S\n        )\n      \n    \n    {\\displaystyle CF=(N,{\\overrightarrow {LS}},SS)}\n  , where\n\n  \n    \n      \n        \n          \n            \n              L\n              S\n            \n            →\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            \n              X\n              \n                i\n              \n            \n            →\n          \n        \n      \n    \n    {\\displaystyle {\\overrightarrow {LS}}=\\sum _{i=1}^{N}{\\overrightarrow {X_{i}}}}\n   is the linear sum.\n\n  \n    \n      \n        S\n        S\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        (\n        \n          \n            \n              X\n              \n                i\n              \n            \n            →\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle SS=\\sum _{i=1}^{N}({\\overrightarrow {X_{i}}})^{2}}\n   is the square sum of data points.\nClustering features are organized in a CF tree, a height-balanced tree with two parameters: branching factor \n  \n    B\n    B\n   and threshold \n  \n    T\n    T\n  . Each non-leaf node contains at most \n  \n    B\n    B\n   entries of the form \n  \n    \n      [\n      C\n      \n        F\n        \n          i\n        \n      \n      ,\n      c\n      h\n      i\n      l\n      \n        d\n        \n          i\n        \n      \n      ]\n    \n    [CF_{i},child_{i}]\n  , where \n  \n    \n      c\n      h\n      i\n      l\n      \n        d\n        \n          i\n        \n      \n    \n    child_{i}\n   is a pointer to its \n  \n    i\n    i\n  th child node and \n  \n    \n      C\n      \n        F\n        \n          i\n        \n      \n    \n    CF_{i}\n   the clustering feature representing the associated subcluster. A leaf node contains at most \n  \n    L\n    L\n   entries each of the form \n  \n    \n      [\n      C\n      \n        F\n        \n          i\n        \n      \n      ]\n    \n    [CF_{i}]\n   . It also has two pointers prev and next which are used to chain all leaf nodes together. The tree size depends on the parameter \n  \n    T\n    T\n  . A node is required to fit in a page of size \n  \n    P\n    P\n  . \n  \n    B\n    B\n   and \n  \n    L\n    L\n   are determined by \n  \n    P\n    P\n  . So \n  \n    P\n    P\n   can be varied for performance tuning. It is a very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster.In the second step, the algorithm scans all the leaf entries in the initial \n  \n    \n      C\n      F\n    \n    CF\n   tree to rebuild a smaller \n  \n    \n      C\n      F\n    \n    CF\n   tree, while removing outliers and grouping crowded subclusters into larger ones. This step is marked optional in the original presentation of BIRCH.\nIn step three an existing clustering algorithm is used to cluster all leaf entries. Here an agglomerative hierarchical clustering algorithm is applied directly to the subclusters represented by their \n  \n    \n      C\n      F\n    \n    CF\n   vectors. It also provides the flexibility of allowing the user to specify either the desired number of clusters or the desired diameter threshold for clusters. After this step a set of clusters is obtained that captures major distribution pattern in the data. However, there might exist minor and localized inaccuracies which can be handled by an optional step 4. In step 4 the centroids of the clusters produced in step 3 are used as seeds and redistribute the data points to its closest seeds to obtain a new set of clusters. Step 4 also provides us with an option of discarding outliers. That is a point which is too far from its closest seed can be treated as an outlier.\n\nCalculations with the clustering features\nGiven only the clustering feature \n  \n    \n      \n        C\n        F\n        =\n        [\n        N\n        ,\n        \n          \n            \n              L\n              S\n            \n            →\n          \n        \n        ,\n        S\n        S\n        ]\n      \n    \n    {\\displaystyle CF=[N,{\\overrightarrow {LS}},SS]}\n  , the same measures can be calculated without the knowledge of the underlying actual values.\n\nCentroid: \n  \n    \n      \n        \n          \n            C\n            →\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                \n                  \n                    X\n                    \n                      i\n                    \n                  \n                  →\n                \n              \n            \n            N\n          \n        \n        =\n        \n          \n            \n              \n                L\n                S\n              \n              →\n            \n            N\n          \n        \n      \n    \n    {\\displaystyle {\\overrightarrow {C}}={\\frac {\\sum _{i=1}^{N}{\\overrightarrow {X_{i}}}}{N}}={\\frac {\\overrightarrow {LS}}{N}}}\n  \nRadius: \n  \n    \n      \n        R\n        =\n        \n          \n            \n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                (\n                \n                  \n                    \n                      X\n                      \n                        i\n                      \n                    \n                    →\n                  \n                \n                −\n                \n                  \n                    C\n                    →\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              N\n            \n          \n        \n        =\n        \n          \n            \n              \n                N\n                ⋅\n                \n                  \n                    \n                      C\n                      →\n                    \n                  \n                  \n                    2\n                  \n                \n                +\n                S\n                S\n                −\n                2\n                ⋅\n                \n                  \n                    C\n                    →\n                  \n                \n                ⋅\n                \n                  \n                    \n                      L\n                      S\n                    \n                    →\n                  \n                \n              \n              N\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  S\n                  S\n                \n                N\n              \n            \n            −\n            (\n            \n              \n                \n                  \n                    L\n                    S\n                  \n                  →\n                \n                N\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle R={\\sqrt {\\frac {\\sum _{i=1}^{N}({\\overrightarrow {X_{i}}}-{\\overrightarrow {C}})^{2}}{N}}}={\\sqrt {\\frac {N\\cdot {\\overrightarrow {C}}^{2}+SS-2\\cdot {\\overrightarrow {C}}\\cdot {\\overrightarrow {LS}}}{N}}}={\\sqrt {{\\frac {SS}{N}}-({\\frac {\\overrightarrow {LS}}{N}})^{2}}}}\n  \nAverage Linkage Distance between clusters \n  \n    \n      \n        C\n        \n          F\n          \n            1\n          \n        \n        =\n        [\n        \n          N\n          \n            1\n          \n        \n        ,\n        \n          \n            \n              L\n              \n                S\n                \n                  1\n                \n              \n            \n            →\n          \n        \n        ,\n        S\n        \n          S\n          \n            1\n          \n        \n        ]\n      \n    \n    {\\displaystyle CF_{1}=[N_{1},{\\overrightarrow {LS_{1}}},SS_{1}]}\n   and \n  \n    \n      \n        C\n        \n          F\n          \n            2\n          \n        \n        =\n        [\n        \n          N\n          \n            2\n          \n        \n        ,\n        \n          \n            \n              L\n              \n                S\n                \n                  2\n                \n              \n            \n            →\n          \n        \n        ,\n        S\n        \n          S\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle CF_{2}=[N_{2},{\\overrightarrow {LS_{2}}},SS_{2}]}\n  :\n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n        =\n        \n          \n            \n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    \n                      N\n                      \n                        1\n                      \n                    \n                  \n                \n                \n                  ∑\n                  \n                    j\n                    =\n                    1\n                  \n                  \n                    \n                      N\n                      \n                        2\n                      \n                    \n                  \n                \n                (\n                \n                  \n                    \n                      X\n                      \n                        i\n                      \n                    \n                    →\n                  \n                \n                −\n                \n                  \n                    \n                      Y\n                      \n                        j\n                      \n                    \n                    →\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                \n                  N\n                  \n                    1\n                  \n                \n                ⋅\n                \n                  N\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  N\n                  \n                    1\n                  \n                \n                ⋅\n                S\n                \n                  S\n                  \n                    2\n                  \n                \n                +\n                \n                  N\n                  \n                    2\n                  \n                \n                ⋅\n                S\n                \n                  S\n                  \n                    1\n                  \n                \n                −\n                2\n                ⋅\n                \n                  \n                    \n                      L\n                      \n                        S\n                        \n                          1\n                        \n                      \n                    \n                    →\n                  \n                \n                ⋅\n                \n                  \n                    \n                      L\n                      \n                        S\n                        \n                          2\n                        \n                      \n                    \n                    →\n                  \n                \n              \n              \n                \n                  N\n                  \n                    1\n                  \n                \n                ⋅\n                \n                  N\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle D_{2}={\\sqrt {\\frac {\\sum _{i=1}^{N_{1}}\\sum _{j=1}^{N_{2}}({\\overrightarrow {X_{i}}}-{\\overrightarrow {Y_{j}}})^{2}}{N_{1}\\cdot N_{2}}}}={\\sqrt {\\frac {N_{1}\\cdot SS_{2}+N_{2}\\cdot SS_{1}-2\\cdot {\\overrightarrow {LS_{1}}}\\cdot {\\overrightarrow {LS_{2}}}}{N_{1}\\cdot N_{2}}}}}\n  In multidimensional cases the square root should be replaced with a suitable norm.\n\nNumerical issues in BIRCH clustering features\nUnfortunately, there are numerical issues associated with the use of the term \n  \n    \n      S\n      S\n    \n    SS\n   in BIRCH. When subtracting \n  \n    \n      \n        \n          \n            \n              S\n              S\n            \n            N\n          \n        \n        −\n        \n          \n            (\n          \n        \n        \n          \n            \n              \n                \n                  L\n                  S\n                \n                →\n              \n            \n            N\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {SS}{N}}-{\\big (}{\\frac {\\vec {LS}}{N}}{\\big )}^{2}}\n   or similar in the other distances such as \n  \n    \n      D\n      \n        2\n      \n    \n    D_{2}\n  , catastrophic cancellation can occur and yield a poor precision, and which can in some cases even cause the result to be negative (and the square root then become undefined). This can be resolved by using BETULA cluster features \n  \n    \n      \n        C\n        F\n        =\n        (\n        N\n        ,\n        μ\n        ,\n        S\n        )\n      \n    \n    {\\displaystyle CF=(N,\\mu ,S)}\n   instead, which store the count \n  \n    N\n    N\n  , mean \n  \n    μ\n    \\mu\n  , and sum of squared deviations instead based on numerically more reliable online algorithms to calculate variance. For these features, a similar additivity theorem holds. When storing a vector respectively a matrix for the squared deviations, the resulting BIRCH CF-tree can also be used to accelerate Gaussian Mixture Modeling with the expectation–maximization algorithm, besides k-means clustering and hierarchical agglomerative clustering.\nInstead of storing the linear sum and the sum of squares, we can instead store the mean and the squared deviation from the mean in each cluster feature \n  \n    \n      \n        C\n        \n          F\n          ′\n        \n        =\n        (\n        N\n        ,\n        μ\n        ,\n        S\n        )\n      \n    \n    {\\displaystyle CF'=(N,\\mu ,S)}\n  , where\n\n  \n    n\n    n\n   is the node weight (number of points)\n\n  \n    μ\n    \\mu\n   is the node center vector (arithmetic mean, centroid)\n\n  \n    S\n    S\n   is the sum of squared deviations from the mean (either a vector, or a sum to conserve memory, depending on the application)The main difference here is that S is computed relative to the center, instead of relative to the origin.\nA single point \n  \n    x\n    x\n   can be cast into a cluster feature \n  \n    \n      \n        C\n        \n          F\n          \n            x\n          \n        \n        =\n        (\n        1\n        ,\n        x\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle CF_{x}=(1,x,0)}\n  . In order to combine two cluster features \n  \n    \n      \n        C\n        \n          F\n          \n            A\n            B\n          \n        \n        =\n        C\n        \n          F\n          \n            A\n          \n        \n        +\n        C\n        \n          F\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle CF_{AB}=CF_{A}+CF_{B}}\n  , we use\n\n  \n    \n      \n        \n          N\n          \n            A\n            B\n          \n        \n        =\n        \n          N\n          \n            A\n          \n        \n        +\n        \n          N\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle N_{AB}=N_{A}+N_{B}}\n  \n\n  \n    \n      \n        \n          μ\n          \n            A\n            B\n          \n        \n        =\n        \n          μ\n          \n            A\n          \n        \n        +\n        \n          \n            \n              N\n              \n                B\n              \n            \n            \n              N\n              \n                A\n                B\n              \n            \n          \n        \n        (\n        \n          μ\n          \n            B\n          \n        \n        −\n        \n          μ\n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mu _{AB}=\\mu _{A}+{\\frac {N_{B}}{N_{AB}}}(\\mu _{B}-\\mu _{A})}\n   (incremental update of the mean)\n\n  \n    \n      \n        \n          S\n          \n            A\n            B\n          \n        \n        =\n        \n          S\n          \n            A\n          \n        \n        +\n        \n          S\n          \n            B\n          \n        \n        +\n        \n          N\n          \n            B\n          \n        \n        (\n        \n          μ\n          \n            B\n          \n        \n        −\n        \n          μ\n          \n            A\n          \n        \n        )\n        ∘\n        (\n        \n          μ\n          \n            B\n          \n        \n        −\n        \n          μ\n          \n            A\n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{AB}=S_{A}+S_{B}+N_{B}(\\mu _{B}-\\mu _{A})\\circ (\\mu _{B}-\\mu _{AB})}\n   in vector form using the element-wise product, respectively\n\n  \n    \n      \n        \n          S\n          \n            A\n            B\n          \n        \n        =\n        \n          S\n          \n            A\n          \n        \n        +\n        \n          S\n          \n            B\n          \n        \n        +\n        \n          N\n          \n            B\n          \n        \n        (\n        \n          μ\n          \n            B\n          \n        \n        −\n        \n          μ\n          \n            A\n          \n        \n        \n          )\n          \n            T\n          \n        \n        (\n        \n          μ\n          \n            B\n          \n        \n        −\n        \n          μ\n          \n            A\n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{AB}=S_{A}+S_{B}+N_{B}(\\mu _{B}-\\mu _{A})^{T}(\\mu _{B}-\\mu _{AB})}\n   to update a scalar sum of squared deviationsThese computations use numerically more reliable computations (c.f. online computation of the variance) that avoid the subtraction of two similar squared values. The centroid is simply the node center vector \n  \n    μ\n    \\mu\n  , and can directly be used for distance computations using, e.g., the Euclidean or Manhattan distances. The radius simplifies to \n  \n    \n      \n        R\n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            S\n          \n        \n      \n    \n    {\\displaystyle R={\\sqrt {{\\frac {1}{N}}S}}}\n   and the diameter to \n  \n    \n      \n        D\n        =\n        \n          \n            \n              \n                2\n                \n                  N\n                  −\n                  1\n                \n              \n            \n            S\n          \n        \n      \n    \n    {\\displaystyle D={\\sqrt {{\\frac {2}{N-1}}S}}}\n  .\nWe can now compute the different distances D0 to D4 used in the BIRCH algorithm as:\nEuclidean distance \n  \n    \n      \n        \n          D\n          \n            0\n          \n        \n        =\n        ‖\n        \n          μ\n          \n            A\n          \n        \n        −\n        \n          μ\n          \n            B\n          \n        \n        ‖\n      \n    \n    {\\displaystyle D_{0}=\\|\\mu _{A}-\\mu _{B}\\|}\n   and Manhattan distance \n  \n    \n      \n        \n          D\n          \n            1\n          \n        \n        =\n        ‖\n        \n          μ\n          \n            A\n          \n        \n        −\n        \n          μ\n          \n            B\n          \n        \n        \n          ‖\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle D_{1}=\\|\\mu _{A}-\\mu _{B}\\|_{1}}\n   are computed using the CF centers \n  \n    μ\n    \\mu\n  \nInter-cluster distance \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n        =\n        \n          \n            \n              \n                1\n                \n                  N\n                  \n                    A\n                  \n                \n              \n            \n            \n              S\n              \n                A\n              \n            \n            +\n            \n              \n                1\n                \n                  N\n                  \n                    B\n                  \n                \n              \n            \n            \n              S\n              \n                B\n              \n            \n            +\n            \n              \n                ‖\n              \n            \n            \n              μ\n              \n                A\n              \n            \n            −\n            \n              μ\n              \n                B\n              \n            \n            \n              \n                \n                  ‖\n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle D_{2}={\\sqrt {{\\frac {1}{N_{A}}}S_{A}+{\\frac {1}{N_{B}}}S_{B}+{\\big \\|}\\mu _{A}-\\mu _{B}{\\big \\|}^{2}}}}\n  \nIntra-cluster distance \n  \n    \n      \n        \n          D\n          \n            3\n          \n        \n        =\n        \n          \n            \n              \n                2\n                \n                  \n                    N\n                    \n                      A\n                      B\n                    \n                  \n                  (\n                  \n                    N\n                    \n                      A\n                      B\n                    \n                  \n                  −\n                  1\n                  )\n                \n              \n            \n            \n              (\n              \n                \n                  N\n                  \n                    A\n                    B\n                  \n                \n                (\n                \n                  S\n                  \n                    A\n                  \n                \n                +\n                \n                  S\n                  \n                    B\n                  \n                \n                )\n                +\n                \n                  N\n                  \n                    A\n                  \n                \n                \n                  N\n                  \n                    B\n                  \n                \n                \n                  \n                    ‖\n                  \n                \n                \n                  μ\n                  \n                    A\n                  \n                \n                −\n                \n                  μ\n                  \n                    B\n                  \n                \n                \n                  \n                    \n                      ‖\n                    \n                  \n                  \n                    2\n                  \n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle D_{3}={\\sqrt {{\\frac {2}{N_{AB}(N_{AB}-1)}}\\left(N_{AB}(S_{A}+S_{B})+N_{A}N_{B}{\\big \\|}\\mu _{A}-\\mu _{B}{\\big \\|}^{2}\\right)}}}\n  \nVariance-increase distance \n  \n    \n      \n        \n          D\n          \n            4\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    N\n                    \n                      A\n                    \n                  \n                  \n                    N\n                    \n                      B\n                    \n                  \n                \n                \n                  N\n                  \n                    A\n                    B\n                  \n                \n              \n            \n            \n              \n                ‖\n              \n            \n            \n              μ\n              \n                A\n              \n            \n            −\n            \n              μ\n              \n                B\n              \n            \n            \n              \n                \n                  ‖\n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle D_{4}={\\sqrt {{\\frac {N_{A}N_{B}}{N_{AB}}}{\\big \\|}\\mu _{A}-\\mu _{B}{\\big \\|}^{2}}}}\n  These distances can also be used to initialize the distance matrix for hierarchical clustering, depending on the chosen linkage. For accurate hierarchical clustering and k-means clustering, we also need to use the node weight \n  \n    N\n    N\n  .\n\n\n== Notes ==",
    "BLOOM (language model)": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based large language model. It was created by over 1000 AI researchers to provide a free large language model for everyone who wants to try.  Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.\nThe BLOOM project was started by a co-founder of Hugging Face. Six main groups of people were involved, including HuggingFace's BigScience team, the Microsoft DeepSpeed team, the NVIDIA Megatron-LM team,  the IDRIS/GENCI team, the PyTorch team, and the volunteers in the BigScience Engineering workgroup.\nBLOOM was trained using data of 46 natural languages and 13 programming languages. In total, 1.6 TeraByte pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.\n\n\n== References ==",
    "Backdoor (computing)": "A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\" —a tiny computer-within-a-computer such as that found in Intel's AMT technology). Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\nA backdoor may take the form of a hidden part of a program, a separate program (e.g. Back Orifice may subvert the system through a rootkit), code in the firmware of the hardware, or parts of an operating system such as Windows. Trojan horses can be used to create vulnerabilities in a device. A Trojan horse may appear to be an entirely legitimate program, but when executed, it triggers an activity that may install a backdoor. Although some are secretly installed, other backdoors are deliberate and widely known. These kinds of backdoors have \"legitimate\" uses such as providing the manufacturer with a way to restore user passwords.\nMany systems that store information within the cloud fail to create accurate security measures. If many systems are connected within the cloud, hackers can gain access to all other platforms through the most vulnerable system. Default passwords (or other default credentials) can function as backdoors if they are not changed by the user. Some debugging features can also act as backdoors if they are not removed in the release version. In 1993, the United States government attempted to deploy an encryption system, the Clipper chip, with an explicit backdoor for law enforcement and national security access. The chip was unsuccessful.\n\nOverview\nThe threat of backdoors surfaced when multiuser and networked operating systems became widely adopted. Petersen and Turn discussed computer subversion in a paper published in the proceedings of the 1967 AFIPS Conference. They noted a class of active infiltration attacks that use \"trapdoor\" entry points into the system to bypass security facilities and permit direct access to data. The use of the word trapdoor here clearly coincides with more recent definitions of a backdoor. However, since the advent of public key cryptography the term trapdoor has acquired a different meaning (see trapdoor function), and thus the term \"backdoor\" is now preferred, only after the term trapdoor went out of use. More generally, such security breaches were discussed at length in a RAND Corporation task force report published under ARPA sponsorship by J.P. Anderson and D.J. Edwards in 1970.A backdoor in a login system might take the form of a hard coded user and password combination which gives access to the system. An example of this sort of backdoor was used as a plot device in the 1983 film WarGames, in which the architect of the \"WOPR\" computer system had inserted a hardcoded password-less account which gave the user access to the system, and to undocumented parts of the system (in particular, a video game-like simulation mode and direct interaction with the artificial intelligence).\nAlthough the number of backdoors in systems using proprietary software (software whose source code is not publicly available) is not widely credited, they are nevertheless frequently exposed. Programmers have even succeeded in secretly installing large amounts of benign code as Easter eggs in programs, although such cases may involve official forbearance, if not actual permission.\n\nPolitics and attribution\nThere are a number of cloak and dagger considerations that come into play when apportioning responsibility.\nCovert backdoors sometimes masquerade as inadvertent defects (bugs) for reasons of plausible deniability. In some cases, these might begin life as an actual bug (inadvertent error), which, once discovered are then deliberately left unfixed and undisclosed, whether by a rogue employee for personal advantage, or with C-level executive awareness and oversight.\nIt is also possible for an entirely above-board corporation's technology base to be covertly and untraceably tainted by external agents (hackers), though this level of sophistication is thought to exist mainly at the level of nation state actors. For example, if a photomask obtained from a photomask supplier differs in a few gates from its photomask specification, a chip manufacturer would be hard-pressed to detect this if otherwise functionally silent; a covert rootkit running in the photomask etching equipment could enact this discrepancy unbeknown to the photomask manufacturer, either, and by such means, one backdoor potentially leads to another. (This hypothetical scenario is essentially a silicon version of the undetectable compiler backdoor, discussed below.)\nIn general terms, the long dependency-chains in the modern, highly specialized technological economy and innumerable human-elements process control-points make it difficult to conclusively pinpoint responsibility at such time as a covert backdoor becomes unveiled.\nEven direct admissions of responsibility must be scrutinized carefully if the confessing party is beholden to other powerful interests.\n\nExamples\nWorms\nMany computer worms, such as Sobig and Mydoom, install a backdoor on the affected computer (generally a PC on broadband running Microsoft Windows and Microsoft Outlook). Such backdoors appear to be installed so that spammers can send junk e-mail from the infected machines. Others, such as the Sony/BMG rootkit, placed secretly on millions of music CDs through late 2005, are intended as DRM measures—and, in that case, as data-gathering agents, since both surreptitious programs they installed routinely contacted central servers.\nA sophisticated attempt to plant a backdoor in the Linux kernel, exposed in November 2003, added a small and subtle code change by subverting the revision control system. In this case, a two-line change appeared to check root access permissions of a caller to the sys_wait4 function, but because it used assignment = instead of equality checking ==, it actually granted permissions to the system. This difference is easily overlooked, and could even be interpreted as an accidental typographical error, rather than an intentional attack.\nIn January 2014, a backdoor was discovered in certain Samsung Android products, like the Galaxy devices. The Samsung proprietary Android versions are fitted with a backdoor that provides remote access to the data stored on the device. In particular, the Samsung Android software that is in charge of handling the communications with the modem, using the Samsung IPC protocol, implements a class of requests known as remote file server (RFS) commands, that allows the backdoor operator to perform via modem remote I/O operations on the device hard disk or other storage. As the modem is running Samsung proprietary Android software, it is likely that it offers over-the-air remote control that could then be used to issue the RFS commands and thus to access the file system on the device.\n\nObject code backdoors\nHarder to detect backdoors involve modifying object code, rather than source code – object code is much harder to inspect, as it is designed to be machine-readable, not human-readable. These backdoors can be inserted either directly in the on-disk object code, or inserted at some point during compilation, assembly linking, or loading – in the latter case the backdoor never appears on disk, only in memory. Object code backdoors are difficult to detect by inspection of the object code, but are easily detected by simply checking for changes (differences), notably in length or in checksum, and in some cases can be detected or analyzed by disassembling the object code. Further, object code backdoors can be removed (assuming source code is available) by simply recompiling from source on a trusted system.\nThus for such backdoors to avoid detection, all extant copies of a binary must be subverted, and any validation checksums must also be compromised, and source must be unavailable, to prevent recompilation. Alternatively, these other tools (length checks, diff, checksumming, disassemblers) can themselves be compromised to conceal the backdoor, for example detecting that the subverted binary is being checksummed and returning the expected value, not the actual value. To conceal these further subversions, the tools must also conceal the changes in themselves – for example, a subverted checksummer must also detect if it is checksumming itself (or other subverted tools) and return false values. This leads to extensive changes in the system and tools being needed to conceal a single change.\nBecause object code can be regenerated by recompiling (reassembling, relinking) the original source code, making a persistent object code backdoor (without modifying source code) requires subverting the compiler itself – so that when it detects that it is compiling the program under attack it inserts the backdoor – or alternatively the assembler, linker, or loader. As this requires subverting the compiler, this in turn can be fixed by recompiling the compiler, removing the backdoor insertion code. This defense can in turn be subverted by putting a source meta-backdoor in the compiler, so that when it detects that it is compiling itself it then inserts this meta-backdoor generator, together with the original backdoor generator for the original program under attack. After this is done, the source meta-backdoor can be removed, and the compiler recompiled from original source with the compromised compiler executable: the backdoor has been bootstrapped. This attack dates to Karger & Schell (1974), and was popularized in Thompson's 1984 article, entitled \"Reflections on Trusting Trust\"; it is hence colloquially known as the \"Trusting Trust\" attack. See compiler backdoors, below, for details. Analogous attacks can target lower levels of the system,\nsuch as the operating system, and can be inserted during the system booting process; these are also mentioned in Karger & Schell (1974), and now exist in the form of boot sector viruses.\n\nAsymmetric backdoors\nA traditional backdoor is a symmetric backdoor: anyone that finds the backdoor can in turn use it. The notion of an asymmetric backdoor was introduced by Adam Young and Moti Yung in the Proceedings of Advances in Cryptology – Crypto '96. An asymmetric backdoor can only be used by the attacker who plants it, even if the full implementation of the backdoor becomes public (e.g., via publishing, being discovered and disclosed by reverse engineering, etc.). Also, it is computationally intractable to detect the presence of an asymmetric backdoor under black-box queries. This class of attacks have been termed kleptography; they can be carried out in software, hardware (for example, smartcards), or a combination of the two. The theory of asymmetric backdoors is part of a larger field now called cryptovirology. Notably, NSA inserted a kleptographic backdoor into the Dual EC DRBG standard.There exists an experimental asymmetric backdoor in RSA key generation. This OpenSSL RSA backdoor, designed by Young and Yung, utilizes a twisted pair of elliptic curves, and has been made available.\n\nCompiler backdoors\nA sophisticated form of black box backdoor is a compiler backdoor, where not only is a compiler subverted (to insert a backdoor in some other program, such as a login program), but it is further modified to detect when it is compiling itself and then inserts both the backdoor insertion code (targeting the other program) and the code-modifying self-compilation, like the mechanism through which retroviruses infect their host. This can be done by modifying the source code, and the resulting compromised compiler (object code) can compile the original (unmodified) source code and insert itself: the exploit has been boot-strapped.\nThis attack was originally presented in Karger & Schell (1974, p. 52, section 3.4.5: \"Trap Door Insertion\"), which was a United States Air Force security analysis of Multics, where they described such an attack on a PL/I compiler, and call it a \"compiler trap door\"; they also mention a variant where the system initialization code is modified to insert a backdoor during booting, as this is complex and poorly understood, and call it an \"initialization trapdoor\"; this is now known as a boot sector virus.This attack was then actually implemented by Ken Thompson, and popularized in his Turing Award acceptance speech in 1983 (published 1984), \"Reflections on Trusting Trust\", which points out that trust is relative, and the only software one can truly trust is code where every step of the bootstrapping has been inspected. This backdoor mechanism is based on the fact that people only review source (human-written) code, and not compiled machine code (object code). A program called a compiler is used to create the second from the first, and the compiler is usually trusted to do an honest job.\nThompson's paper describes a modified version of the Unix C compiler that would put an invisible backdoor in the Unix login command when it noticed that the login program was being compiled, and would also add this feature undetectably to future compiler versions upon their compilation as well.\nBecause the compiler itself was a compiled program, users would be extremely unlikely to notice the machine code instructions that performed these tasks. (Because of the second task, the compiler's source code would appear \"clean\".) What's worse, in Thompson's proof of concept implementation, the subverted compiler also subverted the analysis program (the disassembler), so that anyone who examined the binaries in the usual way would not actually see the real code that was running, but something else instead.\nAn updated analysis of the original exploit is given in Karger & Schell (2002, Section 3.2.4: Compiler trap doors), and a historical overview and survey of the literature is given in Wheeler (2009, Section 2: Background and related work).\n\nOccurrences\nThompson's version was, officially, never released into the wild. It is believed, however, that a version was distributed to BBN and at least one use of the backdoor was recorded. There are scattered anecdotal reports of such backdoors in subsequent years.\nIn August 2009, an attack of this kind was discovered by Sophos labs. The W32/Induc-A virus infected the program compiler for Delphi, a Windows programming language. The virus introduced its own code to the compilation of new Delphi programs, allowing it to infect and propagate to many systems, without the knowledge of the software programmer. The virus looks for a Delphi installation, modifies the SysConst.pas file, which is the source code of a part of the standard library and compiles it. After that, every program compiled by that Delphi installation will contain the virus. An attack that propagates by building its own Trojan horse can be especially hard to discover. It resulted in many software vendors releasing infected executables without realizing it, sometimes claiming false positives. After all, the executable was not tampered with, the compiler was. It is believed that the Induc-A virus had been propagating for at least a year before it was discovered.In 2015, a malicious copy of Xcode, XcodeGhost, also performed a similar attack and infected iOS apps from a dozen of software companies in China. Globally, 4000 apps were found to be affected. It was not a true Thompson Trojan, as it does not infect development tools themselves, but it did show toolchain poisoning can indeed cause substantial damages.\n\nCountermeasures\nOnce a system has been compromised with a backdoor or Trojan horse, such as the Trusting Trust compiler, it is very hard for the \"rightful\" user to regain control of the system – typically one should rebuild a clean system and transfer data (but not executables) over. However, several practical weaknesses in the Trusting Trust scheme have been suggested.  For example, a sufficiently motivated user could painstakingly review the machine code of the untrusted compiler before using it. As mentioned above, there are ways to hide the Trojan horse, such as subverting the disassembler; but there are ways to counter that defense, too, such as writing a disassembler from scratch.A generic method to counter trusting trust attacks is called diverse double-compiling. The method requires a different compiler and the source code of the compiler-under-test. That source, compiled with both compilers, results in two different stage-1 compilers, which however should have the same behavior. Thus the same source compiled with both stage-1 compilers must then result in two identical stage-2 compilers. A formal proof is given that the latter comparison guarantees that the purported source code and executable of the compiler-under-test correspond, under some assumptions. This method was applied by its author to verify that the C compiler of the GCC suite (v. 3.0.4) contained no trojan, using icc (v. 11.0) as the different compiler.In practice such verifications are not done by end users, except in extreme circumstances of intrusion detection and analysis, due to the rarity of such sophisticated attacks, and because programs are typically distributed in binary form. Removing backdoors (including compiler backdoors) is typically done by simply rebuilding a clean system. However, the sophisticated verifications are of interest to operating system vendors, to ensure that they are not distributing a compromised system, and in high-security settings, where such attacks are a realistic concern.\n\nList of known backdoors\nBack Orifice was created in 1998 by hackers from Cult of the Dead Cow group as a remote administration tool. It allowed Windows computers to be remotely controlled over a network and parodied the name of Microsoft's BackOffice.\nThe Dual EC DRBG cryptographically secure pseudorandom number generator was revealed in 2013 to possibly have a kleptographic backdoor deliberately inserted by NSA, who also had the private key to the backdoor.\nSeveral backdoors in the unlicensed copies of WordPress plug-ins were discovered in March 2014. They were inserted as obfuscated JavaScript code and silently created, for example, an admin account in the website database. A similar scheme was later exposed in the Joomla plugin.\nBorland Interbase versions 4.0 through 6.0 had a hard-coded backdoor, put there by the developers. The server code contains a compiled-in backdoor account (username: politically, password: correct), which could be accessed over a network connection; a user logging in with this backdoor account could take full control over all Interbase databases. The backdoor was detected in 2001 and a patch was released.\nJuniper Networks backdoor inserted in the year 2008 into the versions of firmware ScreenOS from 6.2.0r15 to 6.2.0r18 and from 6.3.0r12 to 6.3.0r20 that gives any user administrative access when using a special master password.\nSeveral backdoors were discovered in C-DATA Optical Line Termination (OLT) devices. Researchers released the findings without notifying C-DATA because they believe the backdoors were intentionally placed by the vendor.\n\nSee also\nBackdoor:Win32.Hupigon\nBackdoor.Win32.Seed\nHardware backdoor\nTitanium (malware)\n\nReferences\nFurther reading\nExternal links\nFinding and Removing Backdoors\nThree Archaic Backdoor Trojan Programs That Still Serve Great Pranks\nBackdoors removal — List of backdoors and their removal instructions.\nFAQ Farm's Backdoors FAQ: wiki question and answer forum\nList of backdoors and Removal —",
    "Backpropagation": "As a machine-learning algorithm, backpropagation performs a backward pass to adjust the model's parameters, aiming to minimize the mean squared error (MSE). In a single-layered network, backpropagation uses the following steps:\n\nTraverse through the network from the input to the output by computing the hidden layers' output and the output layer. (the feedforward step)\nIn the output layer, calculate the derivative of the cost function with respect to the input and the hidden layers.\nRepeatedly update the weights until they converge or the model has undergone enough iterations.It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).   The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\nStrictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.\n\nOverview\nBackpropagation computes the gradient in weight space of a feedforward neural network, with respect to a loss function. Denote:\n\n  \n    x\n    x\n  : input (vector of features)\n\n  \n    y\n    y\n  : target output\nFor classification, output will be a vector of class probabilities (e.g., \n  \n    \n      \n        (\n        0.1\n        ,\n        0.7\n        ,\n        0.2\n        )\n      \n    \n    {\\displaystyle (0.1,0.7,0.2)}\n  , and target output is a specific class, encoded by the one-hot/dummy variable (e.g., \n  \n    \n      (\n      0\n      ,\n      1\n      ,\n      0\n      )\n    \n    (0,1,0)\n  ).\n\n  \n    C\n    C\n  : loss function or \"cost function\"For classification, this is usually cross entropy (XC, log loss), while for regression it is usually squared error loss (SEL).\n\n  \n    L\n    L\n  : the number of layers\n\n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n        =\n        (\n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle W^{l}=(w_{jk}^{l})}\n  : the weights between layer \n  \n    \n      \n        l\n        −\n        1\n      \n    \n    {\\displaystyle l-1}\n   and \n  \n    l\n    l\n  , where \n  \n    \n      \n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle w_{jk}^{l}}\n   is the weight between the \n  \n    k\n    k\n  -th node in layer \n  \n    \n      \n        l\n        −\n        1\n      \n    \n    {\\displaystyle l-1}\n   and the \n  \n    j\n    j\n  -th node in layer \n  \n    l\n    l\n  \n\n  \n    \n      \n        \n          f\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle f^{l}}\n  : activation functions at layer \n  \n    l\n    l\n  \nFor classification the last layer is usually the logistic function for binary classification, and softmax (softargmax) for multi-class classification, while for the hidden layers this was traditionally a sigmoid function (logistic function or others) on each node (coordinate), but today is more varied, with rectifier (ramp, ReLU) being common.\n\n  \n    \n      \n        \n          a\n          \n            j\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle a_{j}^{l}}\n  : activation of the \n  \n    j\n    j\n  -th node in layer \n  \n    l\n    l\n  .In the derivation of backpropagation, other intermediate quantities are used by introducing them as needed below. Bias terms are not treated specially since they correspond to a weight with a fixed input of 1. For backpropagation the specific loss function and activation functions do not matter as long as they and their derivatives can be evaluated efficiently. Traditional activation functions include sigmoid, tanh, and ReLU. swish mish, and other activation functions have since been proposed as well.\nThe overall network is a combination of function composition and matrix multiplication:\n\n  \n    \n      \n        g\n        (\n        x\n        )\n        :=\n        \n          f\n          \n            L\n          \n        \n        (\n        \n          W\n          \n            L\n          \n        \n        \n          f\n          \n            L\n            −\n            1\n          \n        \n        (\n        \n          W\n          \n            L\n            −\n            1\n          \n        \n        ⋯\n        \n          f\n          \n            1\n          \n        \n        (\n        \n          W\n          \n            1\n          \n        \n        x\n        )\n        ⋯\n        )\n        )\n      \n    \n    {\\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\\cdots f^{1}(W^{1}x)\\cdots ))}\n  For a training set there will be a set of input–output pairs, \n  \n    \n      \n        \n          {\n          \n            (\n            \n              x\n              \n                i\n              \n            \n            ,\n            \n              y\n              \n                i\n              \n            \n            )\n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{(x_{i},y_{i})\\right\\}}\n  . For each input–output pair \n  \n    \n      (\n      \n        x\n        \n          i\n        \n      \n      ,\n      \n        y\n        \n          i\n        \n      \n      )\n    \n    (x_{i},y_{i})\n   in the training set, the loss of the model on that pair is the cost of the difference between the predicted output \n  \n    \n      \n        g\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle g(x_{i})}\n   and the target output \n  \n    \n      y\n      \n        i\n      \n    \n    y_{i}\n  :\n\n  \n    \n      \n        C\n        (\n        \n          y\n          \n            i\n          \n        \n        ,\n        g\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle C(y_{i},g(x_{i}))}\n  Note the distinction: during model evaluation the weights are fixed while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training the input–output pair is fixed while the weights vary, and the network ends with the loss function.\nBackpropagation computes the gradient for a fixed input–output pair \n  \n    \n      (\n      \n        x\n        \n          i\n        \n      \n      ,\n      \n        y\n        \n          i\n        \n      \n      )\n    \n    (x_{i},y_{i})\n  , where the weights \n  \n    \n      \n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle w_{jk}^{l}}\n   can vary. Each individual component of the gradient, \n  \n    \n      \n        ∂\n        C\n        \n          /\n        \n        ∂\n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\partial C/\\partial w_{jk}^{l},}\n   can be computed by the chain rule; but doing this separately for each weight is inefficient. Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer – specifically the gradient of the weighted input of each layer, denoted by \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   – from back to front.\nInformally, the key point is that since the only way a weight in \n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle W^{l}}\n   affects the loss is through its effect on the next layer, and it does so linearly, \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   are the only data you need to compute the gradients of the weights at layer \n  \n    l\n    l\n  , and then the previous layer can be computed \n  \n    \n      \n        \n          δ\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l-1}}\n   and repeated recursively. This avoids inefficiency in two ways. First, it avoids duplication because when computing the gradient at layer \n  \n    l\n    l\n   – it is unnecessary to recompute all derivatives on later layers \n  \n    \n      \n        l\n        +\n        1\n        ,\n        l\n        +\n        2\n        ,\n        …\n      \n    \n    {\\displaystyle l+1,l+2,\\ldots }\n   each time. Second, it avoids unnecessary intermediate calculations, because at each stage it directly computes the gradient of the weights with respect to the ultimate output (the loss), rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights \n  \n    \n      \n        ∂\n        \n          a\n          \n            \n              j\n              ′\n            \n          \n          \n            \n              l\n              ′\n            \n          \n        \n        \n          /\n        \n        ∂\n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\partial a_{j'}^{l'}/\\partial w_{jk}^{l}}\n  .\nBackpropagation can be expressed for simple feedforward networks in terms of matrix multiplication, or more generally in terms of the adjoint graph.\n\nMatrix multiplication\nFor the basic case of a feedforward network, where nodes in each layer are connected only to nodes in the immediate next layer (without skipping any layers), and there is a loss function that computes a scalar loss for the final output, backpropagation can be understood simply by matrix multiplication. Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from right to left – \"backwards\" – with the gradient of the weights between each layer being a simple modification of the partial products (the \"backwards propagated error\").\nGiven an input–output pair \n  \n    \n      (\n      x\n      ,\n      y\n      )\n    \n    (x,y)\n  , the loss is:\n\n  \n    \n      \n        C\n        (\n        y\n        ,\n        \n          f\n          \n            L\n          \n        \n        (\n        \n          W\n          \n            L\n          \n        \n        \n          f\n          \n            L\n            −\n            1\n          \n        \n        (\n        \n          W\n          \n            L\n            −\n            1\n          \n        \n        ⋯\n        \n          f\n          \n            2\n          \n        \n        (\n        \n          W\n          \n            2\n          \n        \n        \n          f\n          \n            1\n          \n        \n        (\n        \n          W\n          \n            1\n          \n        \n        x\n        )\n        )\n        ⋯\n        )\n        )\n        )\n      \n    \n    {\\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\\cdots f^{2}(W^{2}f^{1}(W^{1}x))\\cdots )))}\n  To compute this, one starts with the input \n  \n    x\n    x\n   and works forward; denote the weighted input of each hidden layer as \n  \n    \n      \n        \n          z\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle z^{l}}\n   and the output of hidden layer \n  \n    l\n    l\n   as the activation \n  \n    \n      \n        \n          a\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle a^{l}}\n  . For backpropagation, the activation \n  \n    \n      \n        \n          a\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle a^{l}}\n   as well as the derivatives \n  \n    \n      \n        (\n        \n          f\n          \n            l\n          \n        \n        \n          )\n          ′\n        \n      \n    \n    {\\displaystyle (f^{l})'}\n   (evaluated at \n  \n    \n      \n        \n          z\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle z^{l}}\n  ) must be cached for use during the backwards pass.\nThe derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input \n  \n    x\n    x\n  :\n\n  \n    \n      \n        \n          \n            \n              d\n              C\n            \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  L\n                \n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              d\n              \n                z\n                \n                  L\n                \n              \n            \n            \n              d\n              \n                a\n                \n                  L\n                  −\n                  1\n                \n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              d\n              \n                a\n                \n                  L\n                  −\n                  1\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  L\n                  −\n                  1\n                \n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              d\n              \n                z\n                \n                  L\n                  −\n                  1\n                \n              \n            \n            \n              d\n              \n                a\n                \n                  L\n                  −\n                  2\n                \n              \n            \n          \n        \n        ⋅\n        …\n        ⋅\n        \n          \n            \n              d\n              \n                a\n                \n                  1\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  1\n                \n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              ∂\n              \n                z\n                \n                  1\n                \n              \n            \n            \n              ∂\n              x\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dC}{da^{L}}}\\cdot {\\frac {da^{L}}{dz^{L}}}\\cdot {\\frac {dz^{L}}{da^{L-1}}}\\cdot {\\frac {da^{L-1}}{dz^{L-1}}}\\cdot {\\frac {dz^{L-1}}{da^{L-2}}}\\cdot \\ldots \\cdot {\\frac {da^{1}}{dz^{1}}}\\cdot {\\frac {\\partial z^{1}}{\\partial x}},}\n  where \n  \n    \n      \n        \n          \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  L\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {da^{L}}{dz^{L}}}}\n   is a diagonal matrix.\nThese terms are: the derivative of the loss function; the derivatives of the activation functions; and the matrices of weights:\n\n  \n    \n      \n        \n          \n            \n              d\n              C\n            \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n          \n        \n        ∘\n        (\n        \n          f\n          \n            L\n          \n        \n        \n          )\n          ′\n        \n        ⋅\n        \n          W\n          \n            L\n          \n        \n        ∘\n        (\n        \n          f\n          \n            L\n            −\n            1\n          \n        \n        \n          )\n          ′\n        \n        ⋅\n        \n          W\n          \n            L\n            −\n            1\n          \n        \n        ∘\n        ⋯\n        ∘\n        (\n        \n          f\n          \n            1\n          \n        \n        \n          )\n          ′\n        \n        ⋅\n        \n          W\n          \n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dC}{da^{L}}}\\circ (f^{L})'\\cdot W^{L}\\circ (f^{L-1})'\\cdot W^{L-1}\\circ \\cdots \\circ (f^{1})'\\cdot W^{1}.}\n  The gradient \n  \n    ∇\n    \\nabla\n   is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed, but the entries are the same:\n\n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        C\n        =\n        (\n        \n          W\n          \n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        (\n        \n          f\n          \n            1\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        …\n        ∘\n        (\n        \n          W\n          \n            L\n            −\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        (\n        \n          f\n          \n            L\n            −\n            1\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        (\n        \n          W\n          \n            L\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        (\n        \n          f\n          \n            L\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        \n          ∇\n          \n            \n              a\n              \n                L\n              \n            \n          \n        \n        C\n        .\n      \n    \n    {\\displaystyle \\nabla _{x}C=(W^{1})^{T}\\cdot (f^{1})'\\circ \\ldots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C.}\n  Backpropagation then consists essentially of evaluating this expression from right to left (equivalently, multiplying the previous expression for the derivative from left to right), computing the gradient at each layer on the way; there is an added step, because the gradient of the weights isn't just a subexpression: there's an extra multiplication.\nIntroducing the auxiliary quantity \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   for the partial products (multiplying from right to left), interpreted as the \"error at level \n  \n    l\n    l\n  \" and defined as the gradient of the input values at level \n  \n    l\n    l\n  :\n\n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n        :=\n        (\n        \n          f\n          \n            l\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        (\n        \n          W\n          \n            l\n            +\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        (\n        \n          f\n          \n            l\n            +\n            1\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        ⋯\n        ∘\n        (\n        \n          W\n          \n            L\n            −\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        (\n        \n          f\n          \n            L\n            −\n            1\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        (\n        \n          W\n          \n            L\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        (\n        \n          f\n          \n            L\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        \n          ∇\n          \n            \n              a\n              \n                L\n              \n            \n          \n        \n        C\n        .\n      \n    \n    {\\displaystyle \\delta ^{l}:=(f^{l})'\\circ (W^{l+1})^{T}\\cdot (f^{l+1})'\\circ \\cdots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C.}\n  Note that \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   is a vector, of length equal to the number of nodes in level \n  \n    l\n    l\n  ; each component is interpreted as the \"cost attributable to (the value of) that node\".\nThe gradient of the weights in layer \n  \n    l\n    l\n   is then:\n\n  \n    \n      \n        \n          ∇\n          \n            \n              W\n              \n                l\n              \n            \n          \n        \n        C\n        =\n        \n          δ\n          \n            l\n          \n        \n        (\n        \n          a\n          \n            l\n            −\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle \\nabla _{W^{l}}C=\\delta ^{l}(a^{l-1})^{T}.}\n  The factor of \n  \n    \n      \n        \n          a\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle a^{l-1}}\n   is because the weights \n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle W^{l}}\n   between level \n  \n    \n      \n        l\n        −\n        1\n      \n    \n    {\\displaystyle l-1}\n   and \n  \n    l\n    l\n   affect level \n  \n    l\n    l\n   proportionally to the inputs (activations): the inputs are fixed, the weights vary.\nThe \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   can easily be computed recursively, going from right to left, as:\n\n  \n    \n      \n        \n          δ\n          \n            l\n            −\n            1\n          \n        \n        :=\n        (\n        \n          f\n          \n            l\n            −\n            1\n          \n        \n        \n          )\n          ′\n        \n        ∘\n        (\n        \n          W\n          \n            l\n          \n        \n        \n          )\n          \n            T\n          \n        \n        ⋅\n        \n          δ\n          \n            l\n          \n        \n        .\n      \n    \n    {\\displaystyle \\delta ^{l-1}:=(f^{l-1})'\\circ (W^{l})^{T}\\cdot \\delta ^{l}.}\n  The gradients of the weights can thus be computed using a few matrix multiplications for each level; this is backpropagation.\nCompared with naively computing forwards (using the \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   for illustration):\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  δ\n                  \n                    1\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    1\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                (\n                \n                  W\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                ⋅\n                (\n                \n                  f\n                  \n                    2\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                ⋯\n                ∘\n                (\n                \n                  W\n                  \n                    L\n                    −\n                    1\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                ⋅\n                (\n                \n                  f\n                  \n                    L\n                    −\n                    1\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                (\n                \n                  W\n                  \n                    L\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                ⋅\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                \n                  ∇\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n              \n            \n            \n              \n                \n                  δ\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    2\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                ⋯\n                ∘\n                (\n                \n                  W\n                  \n                    L\n                    −\n                    1\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                ⋅\n                (\n                \n                  f\n                  \n                    L\n                    −\n                    1\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                (\n                \n                  W\n                  \n                    L\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                ⋅\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                \n                  ∇\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n              \n            \n            \n              \n              \n                \n                ⋮\n              \n            \n            \n              \n                \n                  δ\n                  \n                    L\n                    −\n                    1\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    L\n                    −\n                    1\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                (\n                \n                  W\n                  \n                    L\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                ⋅\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                \n                  ∇\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n              \n            \n            \n              \n                \n                  δ\n                  \n                    L\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  ′\n                \n                ∘\n                \n                  ∇\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\delta ^{1}&=(f^{1})'\\circ (W^{2})^{T}\\cdot (f^{2})'\\circ \\cdots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C\\\\\\delta ^{2}&=(f^{2})'\\circ \\cdots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C\\\\&\\vdots \\\\\\delta ^{L-1}&=(f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C\\\\\\delta ^{L}&=(f^{L})'\\circ \\nabla _{a^{L}}C,\\end{aligned}}}\n  there are two key differences with backpropagation:\n\nComputing \n  \n    \n      \n        \n          δ\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l-1}}\n   in terms of \n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   avoids the obvious duplicate multiplication of layers \n  \n    l\n    l\n   and beyond.\nMultiplying starting from \n  \n    \n      \n        \n          ∇\n          \n            \n              a\n              \n                L\n              \n            \n          \n        \n        C\n      \n    \n    {\\displaystyle \\nabla _{a^{L}}C}\n   – propagating the error backwards – means that each step simply multiplies a vector (\n  \n    \n      \n        \n          δ\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n  ) by the matrices of weights \n  \n    \n      \n        (\n        \n          W\n          \n            l\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle (W^{l})^{T}}\n   and derivatives of activations \n  \n    \n      \n        (\n        \n          f\n          \n            l\n            −\n            1\n          \n        \n        \n          )\n          ′\n        \n      \n    \n    {\\displaystyle (f^{l-1})'}\n  . By contrast, multiplying forwards, starting from the changes at an earlier layer, means that each multiplication multiplies a matrix by a matrix. This is much more expensive, and corresponds to tracking every possible path of a change in one layer \n  \n    l\n    l\n   forward to changes in the layer \n  \n    \n      \n        l\n        +\n        2\n      \n    \n    {\\displaystyle l+2}\n   (for multiplying \n  \n    \n      \n        \n          W\n          \n            l\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle W^{l+1}}\n   by \n  \n    \n      \n        \n          W\n          \n            l\n            +\n            2\n          \n        \n      \n    \n    {\\displaystyle W^{l+2}}\n  , with additional multiplications for the derivatives of the activations), which unnecessarily computes the intermediate quantities of how weight changes affect the values of hidden nodes.\n\nAdjoint graph\nFor more general graphs, and other advanced variations, backpropagation can be understood in terms of automatic differentiation, where backpropagation is a special case of reverse accumulation (or \"reverse mode\").\n\nIntuition\nMotivation\nThe goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n\nLearning as an optimization problem\nTo understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear) that is the weighted sum of its input. Initially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consist of a set of tuples \n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle (x_{1},x_{2},t)}\n   where \n  \n    \n      x\n      \n        1\n      \n    \n    x_{1}\n   and \n  \n    \n      x\n      \n        2\n      \n    \n    x_{2}\n   are the inputs to the network and t is the correct output (the output the network should produce given those inputs, when it has been trained). The initial network, given \n  \n    \n      x\n      \n        1\n      \n    \n    x_{1}\n   and \n  \n    \n      x\n      \n        2\n      \n    \n    x_{2}\n  , will compute an output y that likely differs from t (given random weights). A loss function \n  \n    \n      \n        L\n        (\n        t\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle L(t,y)}\n   is used for measuring the discrepancy between the target output t and the computed output y. For regression analysis problems the squared error can be used as a loss function, for classification the categorical crossentropy can be used.\nAs an example consider a regression problem using the square error as a loss:\n\n  \n    \n      \n        L\n        (\n        t\n        ,\n        y\n        )\n        =\n        (\n        t\n        −\n        y\n        \n          )\n          \n            2\n          \n        \n        =\n        E\n        ,\n      \n    \n    {\\displaystyle L(t,y)=(t-y)^{2}=E,}\n  where E is the discrepancy or error.\n\nConsider the network on a single training case: \n  \n    \n      (\n      1\n      ,\n      1\n      ,\n      0\n      )\n    \n    (1,1,0)\n  . Thus, the input \n  \n    \n      x\n      \n        1\n      \n    \n    x_{1}\n   and \n  \n    \n      x\n      \n        2\n      \n    \n    x_{2}\n   are 1 and 1 respectively and the correct output, t is 0. Now if the relation is plotted between the network's output y on the horizontal axis and the error E on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output y which minimizes the error E. For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output y that exactly matches the target output t. Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error. However, the output of a neuron depends on the weighted sum of all its inputs:\n\n  \n    \n      \n        y\n        =\n        \n          x\n          \n            1\n          \n        \n        \n          w\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        \n          w\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}\n  where \n  \n    \n      w\n      \n        1\n      \n    \n    w_{1}\n   and \n  \n    \n      w\n      \n        2\n      \n    \n    w_{2}\n   are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning.\nIn this example, upon injecting the training data \n  \n    \n      (\n      1\n      ,\n      1\n      ,\n      0\n      )\n    \n    (1,1,0)\n  , the loss function becomes\n\n  \n    \n      \n        E\n        =\n        (\n        t\n        −\n        y\n        \n          )\n          \n            2\n          \n        \n        =\n        \n          y\n          \n            2\n          \n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        \n          w\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        \n          w\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        (\n        \n          w\n          \n            1\n          \n        \n        +\n        \n          w\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}\n  \nThen, the loss function \n  \n    E\n    E\n   takes the form of a parabolic cylinder with its base directed along \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        =\n        −\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{1}=-w_{2}}\n  . Since all sets of weights that satisfy \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        =\n        −\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{1}=-w_{2}}\n   minimize the loss function, in this case additional constraints are required to converge to a unique solution. Additional constraints could either be generated by setting specific conditions to the weights, or by injecting additional training data.\nOne commonly used algorithm to find the set of weights that minimizes the error is gradient descent. By backpropagation, the steepest descent direction is calculated of the loss function versus the present synaptic weights. Then, the weights can be modified along the steepest descent direction, and the error is minimized in an efficient way.\n\nDerivation\nThe gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron, the squared error function is\n\n  \n    \n      \n        E\n        =\n        L\n        (\n        t\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle E=L(t,y)}\n  where\n\n  \n    L\n    L\n   is the loss for the output \n  \n    y\n    y\n   and target value \n  \n    t\n    t\n  ,\n\n  \n    t\n    t\n   is the target output for a training sample, and\n\n  \n    y\n    y\n   is the actual output of the output neuron.For each neuron \n  \n    j\n    j\n  , its output \n  \n    \n      o\n      \n        j\n      \n    \n    o_{j}\n   is defined as\n\n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n        =\n        φ\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        =\n        φ\n        \n          (\n          \n            \n              ∑\n              \n                k\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                k\n                j\n              \n            \n            \n              x\n              \n                k\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle o_{j}=\\varphi ({\\text{net}}_{j})=\\varphi \\left(\\sum _{k=1}^{n}w_{kj}x_{k}\\right),}\n  where the activation function \n  \n    φ\n    \\varphi\n   is non-linear and differentiable over the activation region (the ReLU is not differentiable at one point). A historically used activation function is the logistic function:\n\n  \n    \n      \n        φ\n        (\n        z\n        )\n        =\n        \n          \n            1\n            \n              1\n              +\n              \n                e\n                \n                  −\n                  z\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\varphi (z)={\\frac {1}{1+e^{-z}}}}\n  which has a convenient derivative of:\n\n  \n    \n      \n        \n          \n            \n              d\n              φ\n            \n            \n              d\n              z\n            \n          \n        \n        =\n        φ\n        (\n        z\n        )\n        (\n        1\n        −\n        φ\n        (\n        z\n        )\n        )\n      \n    \n    {\\displaystyle {\\frac {d\\varphi }{dz}}=\\varphi (z)(1-\\varphi (z))}\n  The input \n  \n    \n      \n        \n          \n            net\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\text{net}}_{j}}\n   to a neuron is the weighted sum of outputs \n  \n    \n      o\n      \n        k\n      \n    \n    o_{k}\n   of previous neurons. If the neuron is in the first layer after the input layer, the \n  \n    \n      o\n      \n        k\n      \n    \n    o_{k}\n   of the input layer are simply the inputs \n  \n    \n      x\n      \n        k\n      \n    \n    x_{k}\n   to the network. The number of input units to the neuron is \n  \n    n\n    n\n  . The variable \n  \n    \n      \n        \n          w\n          \n            k\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{kj}}\n   denotes the weight between neuron \n  \n    k\n    k\n   of the previous layer and neuron \n  \n    j\n    j\n   of the current layer.\n\nFinding the derivative of the error\nCalculating the partial derivative of the error with respect to a weight \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   is done using the chain rule twice:\n\nIn the last factor of the right-hand side of the above, only one term in the sum \n  \n    \n      \n        \n          \n            net\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\text{net}}_{j}}\n   depends on \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n  , so that\n\nIf the neuron is in the first layer after the input layer, \n  \n    \n      o\n      \n        i\n      \n    \n    o_{i}\n   is just \n  \n    \n      x\n      \n        i\n      \n    \n    x_{i}\n  .\nThe derivative of the output of neuron \n  \n    j\n    j\n   with respect to its input is simply the partial derivative of the activation function:\n\nwhich for the logistic activation function \n\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            ∂\n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        φ\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        =\n        φ\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        (\n        1\n        −\n        φ\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        )\n        =\n        \n          o\n          \n            j\n          \n        \n        (\n        1\n        −\n        \n          o\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}={\\frac {\\partial }{\\partial {\\text{net}}_{j}}}\\varphi ({\\text{net}}_{j})=\\varphi ({\\text{net}}_{j})(1-\\varphi ({\\text{net}}_{j}))=o_{j}(1-o_{j})}\n  This is the reason why backpropagation requires that the activation function be differentiable. (Nevertheless, the ReLU activation function, which is non-differentiable at 0, has become quite popular, e.g. in AlexNet)\nThe first factor is straightforward to evaluate if the neuron is in the output layer, because then \n  \n    \n      \n        o\n        \n          j\n        \n      \n      =\n      y\n    \n    o_{j}=y\n   and\n\nIf half of the square error is used as loss function we can rewrite it as\n\n  \n    \n      \n        \n          \n            ∂\n            E\n          \n          \n            ∂\n            \n              o\n              \n                j\n              \n            \n          \n        \n      \n      =\n      \n        \n          \n            ∂\n            E\n          \n          \n            ∂\n            y\n          \n        \n      \n      =\n      \n        \n          ∂\n          \n            ∂\n            y\n          \n        \n      \n      \n        \n          1\n          2\n        \n      \n      (\n      t\n      −\n      y\n      \n        )\n        \n          2\n        \n      \n      =\n      y\n      −\n      t\n    \n    {\\frac {\\partial E}{\\partial o_{j}}}={\\frac {\\partial E}{\\partial y}}={\\frac {\\partial }{\\partial y}}{\\frac {1}{2}}(t-y)^{2}=y-t\n  However, if \n  \n    j\n    j\n   is in an arbitrary inner layer of the network, finding the derivative \n  \n    E\n    E\n   with respect to \n  \n    \n      o\n      \n        j\n      \n    \n    o_{j}\n   is less obvious.\nConsidering \n  \n    E\n    E\n   as a function with the inputs being all neurons \n  \n    \n      \n        L\n        =\n        {\n        u\n        ,\n        v\n        ,\n        …\n        ,\n        w\n        }\n      \n    \n    {\\displaystyle L=\\{u,v,\\dots ,w\\}}\n   receiving input from neuron \n  \n    j\n    j\n  ,\n\n  \n    \n      \n        \n          \n            \n              ∂\n              E\n              (\n              \n                o\n                \n                  j\n                \n              \n              )\n            \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              E\n              (\n              \n                \n                  n\n                  e\n                  t\n                \n                \n                  u\n                \n              \n              ,\n              \n                \n                  net\n                \n                \n                  v\n                \n              \n              ,\n              …\n              ,\n              \n                \n                  n\n                  e\n                  t\n                \n                \n                  w\n                \n              \n              )\n            \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial E(o_{j})}{\\partial o_{j}}}={\\frac {\\partial E(\\mathrm {net} _{u},{\\text{net}}_{v},\\dots ,\\mathrm {net} _{w})}{\\partial o_{j}}}}\n  and taking the total derivative with respect to \n  \n    \n      o\n      \n        j\n      \n    \n    o_{j}\n  , a recursive expression for the derivative is obtained:\n\nTherefore, the derivative with respect to \n  \n    \n      o\n      \n        j\n      \n    \n    o_{j}\n   can be calculated if all the derivatives with respect to the outputs \n  \n    \n      \n        \n          o\n          \n            ℓ\n          \n        \n      \n    \n    {\\displaystyle o_{\\ell }}\n   of the next layer – the ones closer to the output neuron – are known. [Note, if any of the neurons in set \n  \n    L\n    L\n   were not connected to neuron \n  \n    j\n    j\n  , they would be independent of \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   and the corresponding partial derivative under the summation would vanish to 0.]\nSubstituting Eq. 2, Eq. 3 Eq.4 and Eq. 5 in Eq. 1  we obtain:\n\n  \n    \n      \n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        \n          o\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}{\\frac {\\partial {\\text{net}}_{j}}{\\partial w_{ij}}}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}o_{i}}\n  \n\n  \n    \n      \n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \n          o\n          \n            i\n          \n        \n        \n          δ\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}=o_{i}\\delta _{j}}\n  with\n\n  \n    \n      \n        \n          δ\n          \n            j\n          \n        \n        =\n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      \n                        ∂\n                        L\n                        (\n                        \n                          o\n                          \n                            j\n                          \n                        \n                        ,\n                        t\n                        )\n                      \n                      \n                        ∂\n                        \n                          o\n                          \n                            j\n                          \n                        \n                      \n                    \n                  \n                  \n                    \n                      \n                        d\n                        φ\n                        (\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                        )\n                      \n                      \n                        d\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an output neuron,\n                  \n                \n              \n              \n                \n                  (\n                  \n                    ∑\n                    \n                      ℓ\n                      ∈\n                      L\n                    \n                  \n                  \n                    w\n                    \n                      j\n                      ℓ\n                    \n                  \n                  \n                    δ\n                    \n                      ℓ\n                    \n                  \n                  )\n                  \n                    \n                      \n                        d\n                        φ\n                        (\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                        )\n                      \n                      \n                        d\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an inner neuron.\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\delta _{j}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}={\\begin{cases}{\\frac {\\partial L(o_{j},t)}{\\partial o_{j}}}{\\frac {d\\varphi ({\\text{net}}_{j})}{d{\\text{net}}_{j}}}&{\\text{if }}j{\\text{ is an output neuron,}}\\\\(\\sum _{\\ell \\in L}w_{j\\ell }\\delta _{\\ell }){\\frac {d\\varphi ({\\text{net}}_{j})}{d{\\text{net}}_{j}}}&{\\text{if }}j{\\text{ is an inner neuron.}}\\end{cases}}}\n  if \n  \n    φ\n    \\varphi\n   is the logistic function, and the error is the square error:\n\n  \n    \n      \n        \n          δ\n          \n            j\n          \n        \n        =\n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              ∂\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              ∂\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  (\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  −\n                  \n                    t\n                    \n                      j\n                    \n                  \n                  )\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  (\n                  1\n                  −\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  )\n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an output neuron,\n                  \n                \n              \n              \n                \n                  (\n                  \n                    ∑\n                    \n                      ℓ\n                      ∈\n                      L\n                    \n                  \n                  \n                    w\n                    \n                      j\n                      ℓ\n                    \n                  \n                  \n                    δ\n                    \n                      ℓ\n                    \n                  \n                  )\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  (\n                  1\n                  −\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  )\n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an inner neuron.\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\delta _{j}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}={\\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&{\\text{if }}j{\\text{ is an output neuron,}}\\\\(\\sum _{\\ell \\in L}w_{j\\ell }\\delta _{\\ell })o_{j}(1-o_{j})&{\\text{if }}j{\\text{ is an inner neuron.}}\\end{cases}}}\n  To update the weight \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   using gradient descent, one must choose a learning rate, \n  \n    \n      \n        η\n        >\n        0\n      \n    \n    {\\displaystyle \\eta >0}\n  . The change in weight needs to reflect the impact on \n  \n    E\n    E\n   of an increase or decrease in \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n  . If \n  \n    \n      \n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}>0}\n  , an increase in \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   increases \n  \n    E\n    E\n  ; conversely, if \n  \n    \n      \n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        <\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}<0}\n  , an increase in \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   decreases \n  \n    E\n    E\n  . The new \n  \n    \n      \n        Δ\n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle \\Delta w_{ij}}\n   is added to the old weight, and the product of the learning rate and the gradient, multiplied by \n  \n    \n      −\n      1\n    \n    -1\n   guarantees that \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   changes in a way that always decreases \n  \n    E\n    E\n  . In other words, in the equation immediately below, \n  \n    \n      \n        −\n        η\n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle -\\eta {\\frac {\\partial E}{\\partial w_{ij}}}}\n   always changes \n  \n    \n      w\n      \n        i\n        j\n      \n    \n    w_{ij}\n   in such a way that \n  \n    E\n    E\n   is decreased:\n\n  \n    \n      \n        Δ\n        \n          w\n          \n            i\n            j\n          \n        \n        =\n        −\n        η\n        \n          \n            \n              ∂\n              E\n            \n            \n              ∂\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        −\n        η\n        \n          o\n          \n            i\n          \n        \n        \n          δ\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Delta w_{ij}=-\\eta {\\frac {\\partial E}{\\partial w_{ij}}}=-\\eta o_{i}\\delta _{j}}\n\nSecond-order gradient descent\nUsing a Hessian matrix of second-order derivatives of the error function, the Levenberg-Marquardt algorithm often converges faster than first-order gradient descent, especially when the topology of the error function is complicated. It may also find solutions in smaller node counts for which other methods might not converge. The Hessian can be approximated by the Fisher information matrix.\n\nLoss function\nThe loss function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network.\n\nAssumptions\nThe mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation. The first is that it can be written as an average \n  \n    \n      \n        E\n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            x\n          \n        \n        \n          E\n          \n            x\n          \n        \n      \n    \n    {\\textstyle E={\\frac {1}{n}}\\sum _{x}E_{x}}\n   over error functions \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n      \n    \n    {\\textstyle E_{x}}\n  , for \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   individual training examples, \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  . The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function.  The second assumption is that it can be written as a function of the outputs from the neural network.\n\nExample loss function\nLet \n  \n    \n      \n        y\n        ,\n        \n          y\n          ′\n        \n      \n    \n    {\\displaystyle y,y'}\n   be vectors in \n  \n    \n      \n        R\n      \n      \n        n\n      \n    \n    \\mathbb {R} ^{n}\n  .\nSelect an error function \n  \n    \n      \n        E\n        (\n        y\n        ,\n        \n          y\n          ′\n        \n        )\n      \n    \n    {\\displaystyle E(y,y')}\n   measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors \n  \n    y\n    y\n   and \n  \n    \n      y\n      ′\n    \n    y'\n  :The error function over \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   training examples can then be written as an average of losses over individual examples:\n\nLimitations\nGradient descent with backpropagation is not guaranteed to find the global minimum of the error function, but only a local minimum; also, it has trouble crossing plateaus in the error function landscape. This issue, caused by the non-convexity of error functions in neural networks, was long thought to be a major drawback, but Yann LeCun et al. argue that in many practical problems, it is not.\nBackpropagation learning does not require normalization of input vectors; however, normalization could improve performance.\nBackpropagation requires the derivatives of activation functions to be known at network design time.\n\nHistory\nModern backpropagation is Seppo Linnainmaa's reverse mode of automatic differentiation (1970) for discrete connected networks of nested differentiable functions. It is an efficient application of the chain rule (derived by Gottfried Wilhelm Leibniz in 1673) to such networks. The terminology \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. The first deep learning multilayer perceptron (MLP) trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments, his five layer MLP with two modifiable layers learned  internal representations required to classify non-linearily separable pattern classes. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. \nIn 1985, David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.Kelley (1960) and Arthur E. Bryson (1961) used principles of dynamic programming to derive the above-mentioned continuous precursor of the method. In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. \nIn 1973, he adapted parameters of controllers in proportion to error gradients. Unlike Linnainmaa's 1970 method, these precursors used \"standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.\"In 1985, the method was also described by Parker. Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987. In 1993, Eric Wan won an international pattern recognition contest through backpropagation.During the 2000s it fell out of favour, but returned in the 2010s, benefitting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first and second language learning.).\nError backpropagation has been suggested to explain human brain ERP components like the N400 and P600.In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University.\n\nSee also\nArtificial neural network\nNeural circuit\nCatastrophic interference\nEnsemble learning\nAdaBoost\nOverfitting\nNeural backpropagation\nBackpropagation through time\n\nNotes\nReferences\nFurther reading\nGoodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). \"6.5 Back-Propagation and Other Differentiation Algorithms\". Deep Learning. MIT Press. pp. 200–220. ISBN 9780262035613.\nNielsen, Michael A. (2015). \"How the backpropagation algorithm works\". Neural Networks and Deep Learning. Determination Press.\nMcCaffrey, James (October 2012). \"Neural Network Back-Propagation for Programmers\". MSDN Magazine.\nRojas, Raúl (1996). \"The Backpropagation Algorithm\" (PDF). Neural Networks : A Systematic Introduction. Berlin: Springer. ISBN 3-540-60505-3.\n\nExternal links\nBackpropagation neural network tutorial at the Wikiversity\nBernacki, Mariusz; Włodarczyk, Przemysław (2004). \"Principles of training multi-layer neural network using backpropagation\".\nKarpathy, Andrej (2016). \"Lecture 4: Backpropagation, Neural Networks 1\". CS231n. Stanford University. Archived from the original on 2021-12-12 – via YouTube.\n\"What is Backpropagation Really Doing?\". 3Blue1Brown. November 3, 2017. Archived from the original on 2021-12-12 – via YouTube.\nPutta, Sudeep Raja (2022). \"Yet Another Derivation of Backpropagation in Matrix Form\".",
    "Bank fraud": "Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution. In many instances, bank fraud is a criminal offence. While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.\n\nTypes of bank fraud\nAccounting fraud\nIn order to hide serious financial problems, some businesses have been known to use fraudulent bookkeeping to overstate sales and income, inflate the worth of the company's assets, or state a profit when the company is operating at a loss. These tampered records are then used to seek investment in the company's bond or security issues or to make fraudulent loan applications in a final attempt to obtain more money to delay the inevitable collapse of an unprofitable or mismanaged firm. Examples of accounting frauds include the Enron scandal, World Com and Ocala Funding. These companies \"cooked the books\" in order to appear as though they had profits each quarter, when in fact they were deeply in debt.\n\nDemand draft fraud\nDemand draft (DD) fraud typically involves one or more corrupt bank employees. Firstly, such employees remove a few DD leaves or DD books from stock and write them like a regular DD. Since they are insiders, they know the coding and punching of a demand draft. Such fraudulent demand drafts are usually drawn payable at a distant city without debiting an account. The draft is cashed at the payable branch. The fraud is discovered only when the bank's head office does the branch-wide reconciliation, which normally take six months, by which time the money is gone.\n\nRemotely created check fraud\nRemotely created checks are orders of payment created by the payee and authorized by the customer remotely, using a telephone or the internet by providing the required information including the MICR code from a valid check. They do not bear the signatures of the customers like ordinary cheques. Instead, they bear a legend statement \"Authorized by Drawer\". This type of instrument is usually used by credit card companies, utility companies, or telemarketers. The lack of signature makes them susceptible to fraud. The fraud is considered Demand Draft fraud in the US.\n\nUninsured deposits\nA bank soliciting public deposits may be uninsured or not licensed to operate at all. The objective is usually to solicit for deposits to this uninsured \"bank,\" although some may also sell stock representing ownership of the \"bank.\" Sometimes the names appear very official or very similar to those of legitimate banks. For instance, the unlicensed \"Chase Trust Bank\" of Washington D.C. appeared in 2002, bearing no affiliation to its seemingly apparent namesake; the real Chase Manhattan Bank is based in New York.\nAccounting fraud has also been used to conceal other theft taking place within a company.\n\nBill discounting fraud\nEssentially a confidence trick, a fraudster uses a company at their disposal to gain the bank's confidence, by posing as a genuine, profitable customer.  To give the illusion of being a desired customer, the company regularly and repeatedly uses the bank to get payment from one or more of its customers. These payments are always made, as the customers in question are part of the fraud, actively paying any and all bills the bank attempts to collect.  After the fraudster has gained the bank's trust, the company requests that the bank begin paying the company up front for bills it will collect from the customers later. Many banks will agree but are not likely to go whole hog right away. So again, business continues as normal for the fraudulent company, its fraudulent customers, and the unwitting bank. As the bank grows more comfortable with the arrangement, it will trust the company more and more and be willing to give it larger and larger sums of money up front. Eventually, when the outstanding balance between the bank and the company is sufficiently large, the company and its customers disappear, taking the money the bank paid up front and leaving no one to pay the bills issued by the bank.\n\nDuplication or skimming of card information\nThis takes a number of forms, ranging from merchants copying clients' credit card numbers for use in later illegal activities or criminals using carbon copies from old mechanical card imprint machines to steal the info, to the use of tampered credit or debit card readers to copy the magnetic stripe from a payment card while a hidden camera captures the numbers on the face of the card.\nSome fraudsters have attached fraudulent card stripe readers to publicly accessible ATMs to gain unauthorised access to the contents of the magnetic stripe as well as hidden cameras to illegally record users' authorisation codes. The data recorded by the cameras and fraudulent card stripe readers are subsequently used to produce duplicate cards that could then be used to make ATM withdrawals from the victims' accounts.\n\nCheque kiting\nCheque kiting exploits a banking system known as \"the float\" wherein money is temporarily counted twice. When a cheque is deposited to an account at Bank X, the money is made available immediately in that account even though the corresponding amount of money is not immediately removed from the account at Bank Y at which the cheque is drawn. Thus both banks temporarily count the cheque amount as an asset until the cheque formally clears at Bank Y. The float serves a legitimate purpose in banking, but intentionally exploiting the float when funds at Bank Y are insufficient to cover the amount withdrawn from Bank X is a form of fraud.\n\nForged or fraudulent documents\nForged documents are often used to conceal other thefts; banks tend to count their money meticulously so every penny must be accounted for. A document claiming that a sum of money has been borrowed as a loan, withdrawn by an individual depositor or transferred or invested can therefore be valuable to someone who wishes to conceal the fact that the bank's money has in fact been stolen and is now gone.\n\nForgery and altered cheques\nFraudsters have altered cheques to change the name (in order to deposit cheques intended for payment to someone else) or the amount on the face of cheques, simple altering can change $100.00 into $100,000.00. (However, transactions for such large values are routinely investigated as a matter of policy to prevent fraud.)\nInstead of tampering with a real cheque, fraudsters may alternatively attempt to forge a depositor's signature on a blank cheque or even print their own cheques drawn on accounts owned by others, non-existent accounts, etc. They would subsequently cash the fraudulent cheque through another bank and withdraw the money before the banks realise that the cheque was a fraud.\n\nFraudulent loan applications\nThese take a number of forms varying from individuals using false information to hide a credit history filled with financial problems and unpaid loans to corporations using accounting fraud to overstate profits in order to make a risky loan appear to be a sound investment for the bank.\n\nFraudulent loans\nOne way to remove money from a bank is to take out a loan, which bankers are more than willing to encourage if they have good reason to believe that the money will be repaid in full with interest. A fraudulent loan, however, is one in which the borrower is a business entity controlled by a dishonest bank officer or an accomplice; the \"borrower\" then declares bankruptcy or vanishes and the money is gone. The borrower may even be a non-existent entity and the loan is merely an artifice to conceal a theft of a large sum of money from the bank. This can also be seen as a component within mortgage fraud (Bell, 2010).\n\nEmpty ATM envelope deposits\nA criminal overdraft can result due to the account holder making a worthless or misrepresented deposit at an automated teller machine in order to obtain more cash than present in the account or to prevent a check from being returned due to non-sufficient funds. United States banking law makes the first $100 immediately available and it may be possible for much more uncollected funds to be lost by the bank the following business day before this type of fraud is discovered. The crime could also be perpetrated against another person's account in an \"account takeover\" or with a counterfeit ATM card, or an account opened in another person's name as part of an identity theft scam. The emergence of ATM deposit technology that scans currency and checks without using an envelope may prevent this type of fraud in the future.\n\nIdentity theft or Impersonation\nIdentity theft operates by obtaining information about an individual, then using the information to apply for identity cards, accounts and credit in that person's name. Often little more than name, parents' name, date and place of birth are sufficient to obtain a birth certificate; each document obtained then is used as identification in order to obtain more identity documents. Government-issued standard identification numbers such as \"social security numbers\" are also valuable to the fraudster.\n\nMoney laundering\nThe term \"money laundering\" dates back to the days of Al Capone; Money laundering has since been used to describe any scheme by which the true origin of funds is hidden or concealed.\nMoney laundering is the process by which large amounts of illegally obtained money (from drug trafficking, terrorist activity or other serious crimes) is given the appearance of having originated from a legitimate source.\n\nPayment card fraud\nCredit card fraud is widespread as a means of stealing from banks, merchants and clients.\ncopy just the credit card numbers (instead of drawing attention by stealing the card itself) in order to use the numbers in online frauds.\n\nPhishing or Internet fraud\nPhishing, also known as Internet fraud, operates by sending forged e-mail, impersonating an online bank, auction or payment site; the e-mail directs the user to a forged web site which is designed to look like the login to the legitimate site but which claims that the user must update personal info. The information thus stolen is then used in other frauds, such as theft of identity or online auction fraud.\nA number of malicious \"Trojan horse\" programmes have also been used to snoop on Internet users while online, capturing keystrokes or confidential data in order to send it to outside sites.\nFake websites can trick a visitor into downloading computer viruses that steal personal information. A visitor encounter security messages claiming his machine has viruses and instructing him to download new software, which is actually a virus.\n\nPrime bank fraud\nThe \"prime bank\" operation which claims to offer an urgent, exclusive opportunity to cash in on the best-kept secret in the banking industry, guaranteed deposits in \"primebanks\",\"constitutional banks\", \"bank notes and bank-issued debentures from top 500 world banks\", \"bank guarantees and standby letters of credit\" which generate spectacular returns at no risk and are \"endorsed by the World Bank\" or various national governments and central bankers. However, these official-sounding phrases and more are the hallmark of the so-called \"prime bank\" fraud; they may sound great on paper, but the guaranteed offshore investment with the vague claims of an easy 100% monthly return are all fictitious financial instruments intended to defraud individuals.\n\nRogue traders\nA rogue trader is a trader at a financial institution who engages in unauthorized trading; at times to recoup the loss they incurred in earlier trades. In those instances, out of fear and desperation, they manipulate the internal controls to circumvent detection to buy more time.Unauthorized trading activities invariably produce more losses due to time constraints; most rogue traders are discovered at an early stage with losses ranging from $1 million to $100 million, but a very few working out of institutions with extremely lax controls were not discovered until the loss had reached well over a billion dollars. Rogue traders may not have criminal intent to defraud their employer to enrich themselves; they may be merely trying to recoup the loss to make their firm whole and salvage their employment.\nSome of the largest unauthorized trading losses were discovered at Barings Bank (Nick Leeson), Daiwa Bank (Toshihide Iguchi), Sumitomo Corporation (Yasuo Hamanaka), Allfirst Bank (John Rusnak), Société Générale (Jérôme Kerviel), UBS (Kweku Adoboli), and JPMorgan Chase (Bruno Iksil).\n\nWire transfer fraud\nWire transfer networks such as the international SWIFT interbank fund transfer system are tempting as targets as a transfer, once made, is difficult or impossible to reverse. As these networks are used by banks to settle accounts with each other, rapid or overnight wire transfer of large amounts of money are commonplace; while banks have put checks and balances in place, there is the risk that insiders may attempt to use fraudulent or forged documents which claim to request a bank depositor's money be wired to another bank, often an offshore account in some distant foreign country.\nThere is a very high risk of fraud when dealing with unknown or uninsured institutions.\nThe risk is greatest when dealing with offshore or Internet banks (as this allows selection of countries with lax banking regulations), but not by any means limited to these institutions. There is an annual list of unlicensed banks on the US Treasury Department web site which currently is fifteen pages in length.\nAlso, a person may send a wire transfer from country to country. Since this takes a few days for the transfer to \"clear\" and be available to withdraw, the other person may still be able to withdraw the money from the other bank. A new teller or corrupt officer may approve the withdrawal since it is in pending status which then the other person cancels the wire transfer and the bank institution takes a monetary loss.\n\nBanking fraud by country\nAustralia\nThe Commonwealth Fraud Control Framework outlines the preventions, detection, investigation and reporting obligations set by the Australian Government for fraud control. The framework includes three documents called The Fraud Rule, Fraud Policy and Fraud Guidance The Fraud Rule is a legislative instrument binding all Commonwealth entities setting out the key requirements of fraud control.\nThe Fraud Policy is a government policy binding non-corporate Commonwealth entities setting out the procedural requirements for specific areas of fraud control such as investigations and reporting.\nThe Fraud Guidance preventing, detecting and dealing with fraud, supports best practice guidance for the Fraud Rule and Fraud Policy setting out the government's expectations for fraud control arrangements within all Commonwealth entities.\nOther important acts and regulations in the Australian Government's fraud control framework include the:\n\nCrimesAct 1914, which sets out criminal offences against the Commonwealth, such as fraud\nCriminal Code 1995, which sets out criminal offences against the Commonwealth, such as fraudulent conduct\nPublic Service Act 1999 and the Public Service Regulations 1999, which provide for the establishment and management of the Australian Public Service and its employees\nProceeds of Crime Act 2002 and the Proceeds of Crime Regulations 2002, which provide for the confiscation of the proceeds of crime.\n\nUnited States\nUnder federal law, bank fraud in the United States is defined, and made illegal, primarily by the bank fraud statute in Title 18 of the U.S. Code. 18 U.S.C. § 1344 states:\nWhoever knowingly executes, or attempts to execute, a scheme or artifice—\n(1) to defraud a financial institution; or\n(2) to obtain any of the moneys, funds, credits, assets, securities, or other property owned by, or under the custody or control of, a financial institution, by means of false or fraudulent pretenses, representations, or promises;\nshall be fined not more than $1,000,000 or imprisoned not more than 30 years, or both.State law may also criminalize the same, or similar acts.\nThe bank fraud statute was enacted by Congress in response to the Supreme Court's decision in Williams v. United States, 458 U.S. 279 (1982), in which the Court held that check-kiting schemes did not constitute making false statements to financial institutions (18 U.S.C. § 1014). Section 1344 has subsequently been bolstered by the Financial Institutions Reform, Recovery and Enforcement Act of 1989 (FIRREA), Pub. L. No. 101-73, 103 Stat. 500.\nThe bank fraud statute federally criminalizes check-kiting, check forging, non-disclosure on loan applications, diversion of funds, unauthorized use of automated teller machines (ATMs), credit card fraud, and other similar offenses. Section 1344 does not cover certain forms of money laundering, bribery, and passing bad checks. Other provisions cover these offenses.\nThe Supreme Court has embraced a broad interpretation of both of the numbered clauses within section 1344.  The Supreme Court has held that the first clause only requires the prosecution to show that the crime involved accounts controlled by a bank; the prosecution need not show actual financial loss to the bank or intent to cause such loss.  The Supreme Court has also held that the second clause does not require a showing of intent to defraud a financial institution.In the United States, consumer liability for unauthorized electronic money transfers on debit cards is covered by Regulation E of the Federal Deposit Insurance Corporation. The extent of consumer liability, as detailed in section 205.6, is determined by the speed with which the consumer notifies the bank. If the bank is notified within 2 business days, the consumer is liable for $50. Over two business days the consumer is liable for $500, and over 60 business days, the consumer liability is unlimited. In contrast, all major credit card companies have a zero liability policy, effectively eliminating consumer liability in the case of fraud.\n\nNotable cases\n1873 Bank of England forgeries\nGone in 60 Seconds (bank fraud)\n2014 Moldovan bank fraud scandal\nRussian Laundromat\nThe Resistance Banker\n\nSee also\nReferences\nExternal links\n\"Bank Fraud\". Crimes of Persuasion.\n\"Bank Fraud Gallery\". Bankers Online. Archived from the original on 2004-10-15. Retrieved 2004-10-05.",
    "Banking": "A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans. Lending activities can be directly performed by the bank or indirectly through capital markets.\nWhereby banks play an important role in financial stability and the economy of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional-reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities. In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.\nBanking in its modern sense evolved in the fourteenth century in the prosperous cities of Renaissance Italy but in many ways functioned as a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties –  notably, the Medicis, the Fuggers, the Welsers, the Berenbergs, and the Rothschilds –  have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena (founded in 1472), while the oldest existing merchant bank is Berenberg Bank (founded in 1590).\n\nHistory\nMedieval\nThe present era of banking can be traced to medieval and early Renaissance Italy, to the rich cities in the centre and north like Florence, Lucca, Siena, Venice and Genoa. The Bardi and Peruzzi families dominated banking in 14th-century Florence, establishing branches in many other parts of Europe. Giovanni di Bicci de' Medici set up one of the most famous Italian banks, the Medici Bank, in 1397. The Republic of Genoa founded the earliest-known state deposit bank, Banco di San Giorgio (Bank of St. George), in 1407 at Genoa, Italy.\n\nEarly modern\nFractional reserve banking and the issue of banknotes emerged in the 17th and 18th centuries. Merchants started to store their gold with the goldsmiths of London, who possessed private vaults, and who charged a fee for that service. In exchange for each deposit of precious metal, the goldsmiths issued receipts certifying the quantity and purity of the metal they held as a bailee; these receipts could not be assigned, only the original depositor could collect the stored goods.\nGradually the goldsmiths began to lend money out on behalf of the depositor, and promissory notes (which evolved into banknotes) were issued for money deposited as a loan to the goldsmith. Thus by the 19th century we find in ordinary cases of deposits of money with banking corporations, or bankers, the transaction amounts to a mere loan or mutuum, and the bank is to restore, not the same money, but an equivalent sum, whenever it is demanded\nand money, when paid into a bank, ceases altogether to be the money of the principal (see Parker v. Marchant, 1 Phillips 360); it is then the money of the banker, who is bound to return an equivalent by paying a similar sum to that deposited with him when he is asked for it.\n\nThe goldsmith paid interest on deposits. Since the promissory notes were payable on demand, and the advances (loans) to the goldsmith's customers were repayable over a longer time-period, this was an early form of fractional reserve banking. The promissory notes developed into an assignable instrument which could circulate as a safe and convenient form of money\nbacked by the goldsmith's promise to pay,\nallowing goldsmiths to advance loans with little risk of default. Thus the goldsmiths of London became the forerunners of banking by creating new money based on credit.\n\nThe Bank of England originated the permanent issue of banknotes in 1695. The Royal Bank of Scotland established the first overdraft facility in 1728. By the beginning of the 19th century Lubbock's Bank had established a bankers' clearing house in London to allow multiple banks to clear transactions. The Rothschilds pioneered international finance on a large scale, financing the purchase of shares in the Suez canal for the British government in 1875.\n\nEtymology\nThe word bank was taken into Middle English from Middle French banque, from Old Italian banco, meaning \"table\", from Old High German banc, bank \"bench, counter\". Benches were used as makeshift desks or exchange counters during the Renaissance by Florentine bankers, who used to make their transactions atop desks covered by green tablecloths.\n\nDefinition\nThe definition of a bank varies from country to country. See the relevant country pages for more information.\nUnder English common law, a banker is defined as a person who carries on the business of banking by conducting current accounts for their customers, paying cheques drawn on them and also collecting cheques for their customers.\nIn most common law jurisdictions there is a Bills of Exchange Act that codifies the law in relation to negotiable instruments, including cheques, and this Act contains a statutory definition of the term banker: banker includes a body of persons, whether incorporated or not, who carry on the business of banking' (Section 2, Interpretation). Although this definition seems circular, it is actually functional, because it ensures that the legal basis for bank transactions such as cheques does not depend on how the bank is structured or regulated.\nThe business of banking is in many common law countries not defined by statute but by common law, the definition above. In other English common law jurisdictions there are statutory definitions of the business of banking or banking business. When looking at these definitions it is important to keep in mind that they are defining the business of banking for the purposes of the legislation, and not necessarily in general. In particular, most of the definitions are from legislation that has the purpose of regulating and supervising banks rather than regulating the actual business of banking. However, in many cases, the statutory definition closely mirrors the common law one. Examples of statutory definitions:\n\n\"banking business\" means the business of receiving money on current or deposit account, paying and collecting cheques drawn by or paid in by customers, the making of advances to customers, and includes such other business as the Authority may prescribe for the purposes of this Act; (Banking Act (Singapore), Section 2, Interpretation).\n\"banking business\" means the business of either or both of the following:receiving from the general public money on current, deposit, savings or other similar account repayable on demand or within less than [3 months] ... or with a period of call or notice of less than that period;\npaying or collecting cheques drawn by or paid in by customers.Since the advent of EFTPOS (Electronic Funds Transfer at Point Of Sale), direct credit, direct debit and internet banking, the cheque has lost its primacy in most banking systems as a payment instrument. This has led legal theorists to suggest that the cheque based definition should be broadened to include financial institutions that conduct current accounts for customers and enable customers to pay and be paid by third parties, even if they do not pay and collect cheques .\n\nStandard business\nBanks act as payment agents by conducting checking or current accounts for customers, paying cheques drawn by customers in the bank, and collecting cheques deposited to customers' current accounts. Banks also enable customer payments via other payment methods such as Automated Clearing House (ACH), Wire transfers or telegraphic transfer, EFTPOS, and automated teller machines (ATMs).\nBanks borrow money by accepting funds deposited on current accounts, by accepting term deposits, and by issuing debt securities such as banknotes and bonds. Banks lend money by making advances to customers on current accounts, by making installment loans, and by investing in marketable debt securities and other forms of money lending.\nBanks provide different payment services, and a bank account is considered indispensable by most businesses and individuals. Non-banks that provide payment services such as remittance companies are normally not considered as an adequate substitute for a bank account.\nBanks issue new money when they make loans. In contemporary banking systems, regulators set a minimum level of reserve funds that banks must hold against the deposit liabilities created by the funding of these loans, in order to ensure that the banks can meet demands for payment of such deposits. These reserves can be acquired through the acceptance of new deposits, sale of other assets, or borrowing from other banks including the central bank.\n\nRange of activities\nActivities undertaken by banks include personal banking, corporate banking, investment banking, private banking, transaction banking, insurance, consumer finance, trade finance and other related.\n\nChannels\nBanks offer many different channels to access their banking and other services:\n\nBranch, in-person banking in a retail location\nAutomated teller machine banking adjacent to or remote from the bank\nBank by mail: Most banks accept cheque deposits via mail and use mail to communicate to their customers\nOnline banking over the Internet to perform multiple types of transactions\nMobile banking is using one's mobile phone to conduct banking transactions\nTelephone banking allows customers to conduct transactions over the telephone with an automated attendant, or when requested, with a telephone operator\nVideo banking performs banking transactions or professional banking consultations via a remote video and audio connection. Video banking can be performed via purpose built banking transaction machines (similar to an Automated teller machine) or via a video conference enabled bank branch clarification\nRelationship manager, mostly for private banking or business banking, who visits customers at their homes or businesses\nDirect Selling Agent, who works for the bank based on a contract, whose main job is to increase the customer base for the bank\n\nBusiness models\nA bank can generate revenue in a variety of different ways including interest, transaction fees and financial advice. Traditionally, the most significant method is via charging interest on the capital it lends out to customers. The bank profits from the difference between the level of interest it pays for deposits and other sources of funds, and the level of interest it charges in its lending activities.\nThis difference is referred to as the spread between the cost of funds and the loan interest rate. Historically, profitability from lending activities has been cyclical and dependent on the needs and strengths of loan customers and the stage of the economic cycle. Fees and financial advice constitute a more stable revenue stream and banks have therefore placed more emphasis on these revenue lines to smooth their financial performance.\nIn the past 20 years, American banks have taken many measures to ensure that they remain profitable while responding to increasingly changing market conditions.\n\nFirst, this includes the Gramm–Leach–Bliley Act, which allows banks again to merge with investment and insurance houses. Merging banking, investment, and insurance functions allows traditional banks to respond to increasing consumer demands for \"one-stop shopping\" by enabling cross-selling of products (which, the banks hope, will also increase profitability).\nSecond, they have expanded the use of risk-based pricing from business lending to consumer lending, which means charging higher interest rates to those customers that are considered to be a higher credit risk and thus increased chance of default on loans. This helps to offset the losses from bad loans, lowers the price of loans to those who have better credit histories, and offers credit products to high risk customers who would otherwise be denied credit.\nThird, they have sought to increase the methods of payment processing available to the general public and business clients. These products include debit cards, prepaid cards, smart cards, and credit cards. They make it easier for consumers to conveniently make transactions and smooth their consumption over time (in some countries with underdeveloped financial systems, it is still common to deal strictly in cash, including carrying suitcases filled with cash to purchase a home).However, with the convenience of easy credit, there is also an increased risk that consumers will mismanage their financial resources and accumulate excessive debt. Banks make money from card products through interest charges and fees charged to cardholders, and transaction fees to retailers who accept the bank's credit and debit cards for payments.This helps in making a profit and facilitates economic development as a whole.Recently, as banks have been faced with pressure from fintechs, new and additional business models have been suggested such as freemium, monetisation of data, white-labeling of banking and payment applications, or the cross-selling of complementary products.\n\nProducts\nRetail\nSavings account\nRecurring deposit account\nFixed deposit account\nMoney market account\nCertificate of deposit (CD)\nIndividual retirement account (IRA)\nCredit card\nDebit card\nMortgage\nMutual fund\nPersonal loan (Secured and Unsecured Personal loan)\nTime deposits\nATM card\nCurrent accounts\nCheque books\nAutomated teller machine (ATM)\nNational Electronic Fund Transfer (NEFT)\nReal-time gross settlement (RTGS)\n\nBusiness (or commercial/investment) banking\nBusiness loan\nCapital raising (equity / debt / hybrids)\nRevolving credit\nRisk management (foreign exchange (FX), interest rates, commodities, derivatives)\nTerm loan\nCash management services (lock box, remote deposit capture, merchant processing)\nCredit services\nSecurities Services\n\nCapital and risk\nBanks face a number of risks in order to conduct their business, and how well these risks are managed and understood is a key driver behind profitability, and how much capital a bank is required to hold. Bank capital consists principally of equity, retained earnings and subordinated debt.\nSome of the main risks faced by banks include:\n\nCredit risk: risk of loss arising from a borrower who does not make payments as promised.\nLiquidity risk: risk that a given security or asset cannot be traded quickly enough in the market to prevent a loss (or make the required profit).\nMarket risk: risk that the value of a portfolio, either an investment portfolio or a trading portfolio, will decrease due to the change in value of the market risk factors.\nOperational risk: risk arising from the execution of a company's business functions.\nReputational risk: a type of risk related to the trustworthiness of the business.\nMacroeconomic risk: risks related to the aggregate economy the bank is operating in.The capital requirement is a bank regulation, which sets a framework within which a bank or depository institution must manage its balance sheet. The categorisation of assets and capital is highly standardised so that it can be risk weighted.\nAfter the financial crisis of 2007–2008, regulators force banks to issue Contingent convertible bonds (CoCos). These are hybrid capital securities that absorb losses in accordance with their contractual terms when the capital of the issuing bank falls below a certain level. Then debt is reduced and bank capitalisation gets a boost. Owing to their capacity to absorb losses, CoCos have the potential to satisfy regulatory capital requirement.\n\nBanks in the economy\nEconomic functions\nThe economic functions of banks include:\n\nIssue of money, in the form of banknotes and current accounts subject to cheque or payment at the customer's order. These claims on banks can act as money because they are negotiable or repayable on demand, and hence valued at par. They are effectively transferable by mere delivery, in the case of banknotes, or by drawing a cheque that the payee may bank or cash.\nNetting and settlement of payments – banks act as both collection and paying agents for customers, participating in interbank clearing and settlement systems to collect, present, be presented with, and pay payment instruments. This enables banks to economise on reserves held for settlement of payments since inward and outward payments offset each other. It also enables the offsetting of payment flows between geographical areas, reducing the cost of settlement between them.\nCredit quality improvement – banks lend money to ordinary commercial and personal borrowers (ordinary credit quality), but are high quality borrowers. The improvement comes from diversification of the bank's assets and capital which provides a buffer to absorb losses without defaulting on its obligations. However, banknotes and deposits are generally unsecured; if the bank gets into difficulty and pledges assets as security, to raise the funding it needs to continue to operate, this puts the note holders and depositors in an economically subordinated position.\nAsset liability mismatch/Maturity transformation – banks borrow more on demand debt and short term debt, but provide more long-term loans. In other words, they borrow short and lend long. With a stronger credit quality than most other borrowers, banks can do this by aggregating issues (e.g. accepting deposits and issuing banknotes) and redemptions (e.g. withdrawals and redemption of banknotes), maintaining reserves of cash, investing in marketable securities that can be readily converted to cash if needed, and raising replacement funding as needed from various sources (e.g. wholesale cash markets and securities markets).\nMoney creation/destruction – whenever a bank gives out a loan in a fractional-reserve banking system, a new sum of money is created and conversely, whenever the principal on that loan is repaid money is destroyed.\n\nBank crisis\nBanks are susceptible to many forms of risk which have triggered occasional systemic crises. These include liquidity risk (where many depositors may request withdrawals in excess of available funds), credit risk (the chance that those who owe money to the bank will not repay it), and interest rate risk (the possibility that the bank will become unprofitable, if rising interest rates force it to pay relatively more on its deposits than it receives on its loans).\nBanking crises have developed many times throughout history when one or more risks have emerged for the banking sector as a whole. Prominent examples include the bank run that occurred during the Great Depression, the U.S. Savings and Loan crisis in the 1980s and early 1990s, the Japanese banking crisis during the 1990s, and the sub-prime mortgage crisis in the 2000s.\nThe 2023 global banking crisis is the latest of these crises: In March 2023,  liquidity shortages and bank insolvencies led to three bank failures in the United States, and within two weeks, several of the world's largest banks failed or were shut down by regulators\n\nSize of global banking industry\nAssets of the largest 1,000 banks in the world grew by 6.8% in the 2008–2009 financial year to a record US$96.4 trillion while profits declined by 85% to US$115 billion. Growth in assets in adverse market conditions was largely a result of recapitalisation. EU banks held the largest share of the total, 56% in 2008–2009, down from 61% in the previous year. Asian banks' share increased from 12% to 14% during the year, while the share of US banks increased from 11% to 13%. Fee revenue generated by global investment in banking totalled US$66.3 billion in 2009, up 12% on the previous year.The United States has the most banks in the world in terms of institutions (5,330 as of 2015) and possibly branches (81,607 as of 2015). This is an indicator of the geography and regulatory structure of the US, resulting in a large number of small to medium-sized institutions in its banking system. As of November 2009, China's top four banks have in excess of 67,000 branches (ICBC:18000+, BOC:12000+, CCB:13000+, ABC:24000+) with an additional 140 smaller banks with an undetermined number of branches.\nJapan had 129 banks and 12,000 branches. In 2004, Germany, France, and Italy each had more than 30,000 branches – more than double the 15,000 branches in the United Kingdom.\n\nMergers and acquisitions\nBetween 1985 and 2018 banks engaged in around 28,798 mergers or acquisitions, either as the acquirer or the target company. The overall known value of these deals cumulates to around 5,169 bil. USD. In terms of value, there have been two major waves (1999 and 2007) which both peaked at around 460 bil. USD followed by a steep decline (-82% from 2007 until 2018).\nHere is a list of the largest deals in history in terms of value with participation from at least one bank:\n\nRegulation\nCurrently, commercial banks are regulated in most jurisdictions by government entities and require a special bank license to operate.\n\nUsually, the definition of the business of banking for the purposes of regulation is extended to include acceptance of deposits, even if they are not repayable to the customer's order – although money lending, by itself, is generally not included in the definition.\nUnlike most other regulated industries, the regulator is typically also a participant in the market, being either publicly or privately governed central bank. Central banks also typically have a monopoly on the business of issuing banknotes. However, in some countries, this is not the case. In the UK, for example, the Financial Services Authority licenses banks, and some commercial banks (such as the Bank of Scotland) issue their own banknotes in addition to those issued by the Bank of England, the UK government's central bank.\n\nBanking law is based on a contractual analysis of the relationship between the bank (defined above) and the customer – defined as any entity for which the bank agrees to conduct an account.\nThe law implies rights and obligations into this relationship as follows:\n\nThe bank account balance is the financial position between the bank and the customer: when the account is in credit, the bank owes the balance to the customer; when the account is overdrawn, the customer owes the balance to the bank.\n\nThe bank agrees to pay the customer's checks up to the amount standing to the credit of the customer's account, plus any agreed overdraft limit.\n\nThe bank may not pay from the customer's account without a mandate from the customer, e.g. a cheque drawn by the customer.\n\nThe bank agrees to promptly collect the cheques deposited to the customer's account as the customer's agent and to credit the proceeds to the customer's account.\n\nAnd, the bank has a right to combine the customer's accounts since each account is just an aspect of the same credit relationship.\n\nThe bank has a lien on cheques deposited to the customer's account, to the extent that the customer is indebted to the bank.\n\nThe bank must not disclose details of transactions through the customer's account – unless the customer consents, there is a public duty to disclose, the bank's interests require it, or the law demands it.\n\nThe bank must not close a customer's account without reasonable notice, since cheques are outstanding in the ordinary course of business for several days.These implied contractual terms may be modified by express agreement between the customer and the bank. The statutes and regulations in force within a particular jurisdiction may also modify the above terms or create new rights, obligations, or limitations relevant to the bank-customer relationship.\nSome types of financial institutions, such as building societies and credit unions, may be partly or wholly exempt from bank license requirements, and therefore regulated under separate rules.\nThe requirements for the issue of a bank license vary between jurisdictions but typically include:\n\nMinimum capital\nMinimum capital ratio\n'Fit and Proper' requirements for the bank's controllers, owners, directors, or senior officers\nApproval of the bank's business plan as being sufficiently prudent and plausible.\n\nDifferent types of banking\nBanks' activities can be divided into:\n\nretail banking, dealing directly with individuals and small businesses;\nbusiness banking, providing services to mid-market business;\ncorporate banking, directed at large business entities;\nprivate banking, providing wealth management services to high-net-worth individuals and families;\ninvestment banking, relating to activities on the financial markets.Most banks are profit-making, private enterprises. However, some are owned by the government, or are non-profit organisations.\n\nTypes of banks\nCommercial banks: the term used for a normal bank to distinguish it from an investment bank. After the Great Depression, the U.S. Congress required that banks only engage in banking activities, whereas investment banks were limited to capital market activities. Since the two no longer have to be under separate ownership, some use the term \"commercial bank\" to refer to a bank or a division of a bank that mostly deals with deposits and loans from corporations or large businesses.\nCommunity banks: locally operated financial institutions that empower employees to make local decisions to serve their customers and partners.\nCommunity development banks: regulated banks that provide financial services and credit to under-served markets or populations.\nLand development banks: The special banks providing long-term loans are called land development banks (LDB). The history of LDB is quite old. The first LDB was started at Jhang in Punjab in 1920. The main objective of the LDBs is to promote the development of land, agriculture and increase the agricultural production. The LDBs provide long-term finance to members  directly through their branches.\nCredit unions or co-operative banks: not-for-profit cooperatives owned by the depositors and often offering rates more favourable than for-profit banks. Typically, membership is restricted to employees of a particular company, residents of a defined area, members of a certain union or religious organisations, and their immediate families.\nPostal savings banks: savings banks associated with national postal systems.\nPrivate banks: banks that manage the assets of high-net-worth individuals. Historically a minimum of US$1 million was required to open an account, however, over the last years, many private banks have lowered their entry hurdles to US$350,000 for private investors.\nOffshore banks: banks located in jurisdictions with low taxation and regulation. Many offshore banks are essentially private banks.\nSavings banks: in Europe, savings banks took their roots in the 19th or sometimes even in the 18th century. Their original objective was to provide easily accessible savings products to all strata of the population. In some countries, savings banks were created on public initiative; in others, socially committed individuals created foundations to put in place the necessary infrastructure. Nowadays, European savings banks have kept their focus on retail banking: payments, savings products, credits, and insurances for individuals or small and medium-sized enterprises. Apart from this retail focus, they also differ from commercial banks by their broadly decentralised distribution network, providing local and regional outreach – and by their socially responsible approach to business and society.\nBuilding societies and Landesbanks: institutions that conduct retail banking.\nEthical banks: banks that prioritize the transparency of all operations and make only what they consider to be socially responsible investments.\nA direct or internet-only bank is a banking operation without any physical bank branches. Transactions are usually accomplished using ATMs and electronic transfers and direct deposits through an online interface.\n\nTypes of investment banks\nInvestment banks \"underwrite\" (guarantee the sale of) stock and bond issues, provide investment management, and advise corporations on capital market activities such as M&A, trade for their own accounts, make markets, provide securities services to institutional clients.\nMerchant banks were traditionally banks which engaged in trade finance. The modern definition, however, refers to banks which provide capital to firms in the form of shares rather than loans. Unlike venture caps, they tend not to invest in new companies.\n\nCombination banks\nUniversal banks, more commonly known as financial services companies, engage in several of these activities. These big banks are very diversified groups that, among other services, also distribute insurance –  hence the term bancassurance, a portmanteau word combining \"banque or bank\" and \"assurance\", signifying that both banking and insurance are provided by the same corporate entity.\n\nOther types of banks\nCentral banks are normally government-owned and charged with quasi-regulatory responsibilities, such as supervising commercial banks, or controlling the cash interest rate. They generally provide liquidity to the banking system and act as the lender of last resort in event of a crisis.\nIslamic banks adhere to the concepts of Islamic law. This form of banking revolves around several well-established principles based on Islamic laws. All banking activities must avoid interest, a concept that is forbidden in Islam. Instead, the bank earns profit (markup) and fees on the financing facilities that it extends to customers.\n\nChallenges within the banking industry\nUnited States\nThe United States banking industry is one of the most heavily regulated and guarded in the world, with multiple specialised and focused regulators. All banks with FDIC-insured deposits have the Federal Deposit Insurance Corporation (FDIC) as a regulator. However, for soundness examinations (i.e., whether a bank is operating in a sound manner), the Federal Reserve is the primary federal regulator for Fed-member state banks; the Office of the Comptroller of the Currency (OCC) is the primary federal regulator for national banks. State non-member banks are examined by the state agencies as well as the FDIC.: 236  National banks have one primary regulator – the OCC.\nEach regulatory agency has its own set of rules and regulations to which banks and thrifts must adhere.\nThe Federal Financial Institutions Examination Council (FFIEC) was established in 1979 as a formal inter-agency body empowered to prescribe uniform principles, standards, and report forms for the federal examination of financial institutions. Although the FFIEC has resulted in a greater degree of regulatory consistency between the agencies, the rules and regulations are constantly changing.\nIn addition to changing regulations, changes in the industry have led to consolidations within the Federal Reserve, FDIC, OTS, and OCC. Offices have been closed, supervisory regions have been merged, staff levels have been reduced and budgets have been cut. The remaining regulators face an increased burden with an increased workload and more banks per regulator. While banks struggle to keep up with the changes in the regulatory environment, regulators struggle to manage their workload and effectively regulate their banks. The impact of these changes is that banks are receiving less hands-on assessment by the regulators, less time spent with each institution, and the potential for more problems slipping through the cracks, potentially resulting in an overall increase in bank failures across the United States.\nThe changing economic environment has a significant impact on banks and thrifts as they struggle to effectively manage their interest rate spread in the face of low rates on loans, rate competition for deposits and the general market changes, industry trends and economic fluctuations. It has been a challenge for banks to effectively set their growth strategies with the recent economic market. A rising interest rate environment may seem to help financial institutions, but the effect of the changes on consumers and businesses is not predictable and the challenge remains for banks to grow and effectively manage the spread to generate a return to their shareholders.\nThe management of the banks’ asset portfolios also remains a challenge in today's economic environment. Loans are a bank's primary asset category and when loan quality becomes suspect, the foundation of a bank is shaken to the core. While always an issue for banks, declining asset quality has become a big problem for financial institutions.\n\nThere are several reasons for this, one of which is the lax attitude some banks have adopted because of the years of \"good times.\" The potential for this is exacerbated by the reduction in the regulatory oversight of banks and in some cases depth of management. Problems are more likely to go undetected, resulting in a significant impact on the bank when they are discovered. In addition, banks, like any business, struggle to cut costs and have consequently eliminated certain expenses, such as adequate employee training programs.\nBanks also face a host of other challenges such as ageing ownership groups. Across the country, many banks’ management teams and boards of directors are ageing. Banks also face ongoing pressure from shareholders, both public and private, to achieve earnings and growth projections. Regulators place added pressure on banks to manage the various categories of risk. Banking is also an extremely competitive industry. Competing in the financial services industry has become tougher with the entrance of such players as insurance agencies, credit unions, cheque cashing services, credit card companies, etc.\nAs a reaction, banks have developed their activities in financial instruments, through financial market operations such as brokerage and have become big players in such activities.\nAnother major challenge is the ageing infrastructure, also called legacy IT. Backend systems were built decades ago and are incompatible with new applications. Fixing bugs and creating interfaces costs huge sums, as knowledgeable programmers become scarce.\n\nLoan activities of banks\nTo be able to provide home buyers and builders with the funds needed, banks must compete for deposits. The phenomenon of disintermediation had to dollars moving from savings accounts and into direct market instruments such as U.S. Department of Treasury obligations, agency securities, and corporate debt. One of the greatest factors in recent years in the movement of deposits was the tremendous growth of money market funds whose higher interest rates attracted consumer deposits.To compete for deposits, US savings institutions offer many different types of plans:\nPassbook or ordinary deposit accounts  –  permit any amount to be added to or withdrawn from the account at any time.\nNOW and Super NOW accounts  –  function like checking accounts but earn interest. A minimum balance may be required on Super NOW accounts.\nMoney market accounts  –  carry a monthly limit of preauthorised transfers to other accounts or persons and may require a minimum or average balance.\nCertificate accounts  –  subject to loss of some or all interest on withdrawals before maturity.\nNotice accounts  –  the equivalent of certificate accounts with an indefinite term. Savers agree to notify the institution a specified time before withdrawal.\nIndividual retirement accounts (IRAs) and Keogh plans  –  a form of retirement savings in which the funds deposited and interest earned are exempt from income tax until after withdrawal.\nChecking accounts  –  offered by some institutions under definite restrictions.\nAll withdrawals and deposits are completely the sole decision and responsibility of the account owner unless the parent or guardian is required to do otherwise for legal reasons.\nClub accounts and other savings accounts  –  designed to help people save regularly to meet certain goals.\n\nTypes of accounts\nBank statements are accounting records produced by banks under the various accounting standards of the world. Under GAAP there are two kinds of accounts: debit and credit. Credit accounts are Revenue, Equity and Liabilities. Debit Accounts are Assets and Expenses. The bank credits a credit account to increase its balance, and debits a credit account to decrease its balance.The customer debits his or her savings/bank (asset) account in his ledger when making a deposit (and the account is normally in debit), while the customer credits a credit card (liability) account in his ledger every time he spends money (and the account is normally in credit). When the customer reads his bank statement, the statement will show a credit to the account for deposits, and debits for withdrawals of funds. The customer with a positive balance will see this balance reflected as a credit balance on the bank statement. If the customer is overdrawn, he will have a negative balance, reflected as a debit balance on the bank statement.\n\nBrokered deposits\nOne source of deposits for banks is deposit brokers who deposit large sums of money on behalf of investors through trust corporations. This money will generally go to the banks which offer the most favourable terms, often better than those offered local depositors. It is possible for a bank to engage in business with no local deposits at all, all funds being brokered deposits. Accepting a significant quantity of such deposits, or \"hot money\" as it is sometimes called, puts a bank in a difficult and sometimes risky position, as the funds must be lent or invested in a way that yields a return sufficient to pay the high interest being paid on the brokered deposits. This may result in risky decisions and even in eventual failure of the bank. Banks which failed during 2008 and 2009 in the United States during the global financial crisis had, on average, four times more brokered deposits as a percent of their deposits than the average bank. Such deposits, combined with risky real estate investments, factored into the savings and loan crisis of the 1980s. Regulation of brokered deposits is opposed by banks on the grounds that the practice can be a source of external funding to growing communities with insufficient local deposits. There are different types of accounts: saving, recurring and current accounts.\n\nCustodial accounts\nCustodial accounts are accounts in which assets are held for a third party. For example, businesses that accept custody of funds for clients prior to their conversion, return, or transfer may have a custodial account at a bank for these purposes.\n\nGlobalisation in the banking industry\nIn modern times there have been huge reductions to the barriers of global competition in the banking industry. Increases in telecommunications and other financial technologies, such as Bloomberg, have allowed banks to extend their reach all over the world since they no longer have to be near customers to manage both their finances and their risk. The growth in cross-border activities has also increased the demand for banks that can provide various services across borders to different nationalities. Despite these reductions in barriers and growth in cross-border activities, the banking industry is nowhere near as globalised as some other industries. In the US, for instance, very few banks even worry about the Riegle–Neal Act, which promotes more efficient interstate banking. In the vast majority of nations around the globe, the market share for foreign owned banks is currently less than a tenth of all market shares for banks in a particular nation. One reason the banking industry has not been fully globalised is that it is more convenient to have local banks provide loans to small businesses and individuals. On the other hand, for large corporations, it is not as important in what nation the bank is in since the corporation's financial information is available around the globe.\n\nSee also\nReferences\nFurther reading\nBorn, Karl Erich. International Banking in the 19th and 20th Centuries (St Martin's, 1983) online\n\nExternal links\n\nGuardian Datablog – World's Biggest Banks\nBanking, Banks, and Credit Unions from UCB Libraries GovPubs (archived 11January 2012)\nA Guide to the National Banking System (PDF). Office of the Comptroller of the Currency (OCC), Washington, D.C. Provides an overview of the national banking system of the US, its regulation, and the OCC.",
    "Bard (chatbot)": "Bard is a conversational generative artificial intelligence chatbot developed by Google, based initially on the LaMDA family of large language models (LLMs) and later the PaLM LLM. It was developed as a direct response to the rise of OpenAI's ChatGPT, and was released in a limited capacity in March 2023 to lukewarm responses, before expanding to other countries in May.\n\nBackground\nIn November 2022, OpenAI launched ChatGPT, a chatbot based on the GPT-3 family of large language models (LLM). ChatGPT gained worldwide attention following its release, becoming a viral Internet sensation. Alarmed by ChatGPT's potential threat to Google Search, Google executives issued a \"code red\" alert, reassigning several teams to assist in the company's artificial intelligence (AI) efforts. Sundar Pichai, the CEO of Google and parent company Alphabet, was widely reported to have issued the alert, but Pichai later denied this to The New York Times. In a rare and unprecedented move, Google co-founders Larry Page and Sergey Brin, who had stepped down from their roles as co-CEOs of Alphabet in 2019, were summoned to emergency meetings with company executives to discuss Google's response to ChatGPT. Earlier that year, the company had unveiled LaMDA, a prototype LLM, but did not release it to the public. When asked by employees at an all-hands meeting whether LaMDA was a missed opportunity for Google to compete with ChatGPT, Pichai and Google AI chief Jeff Dean stated that while the company had similar capabilities to ChatGPT, moving too quickly in that arena would represent a major \"reputational risk\" due to Google being substantially larger than OpenAI. In January 2023, DeepMind CEO Demis Hassabis hinted at plans for a ChatGPT rival, and Google employees were instructed to accelerate progress on a ChatGPT competitor, intensively testing \"Apprentice Bard\" and other chatbots. Pichai assured investors during Google's quarterly earnings investor call in February that the company had plans to expand LaMDA's availability and applications.\n\nHistory\nAnnouncement\nOn February 6, 2023, Google announced Bard, a conversational generative artificial intelligence chatbot powered by LaMDA. Bard was first rolled out to a select group of 10,000 \"trusted testers\", before a wide release scheduled at the end of the month. Bard is overseen by product lead Jack Krawczyk, who described the product as a \"collaborative AI service\" rather than a search engine, while Pichai detailed how Bard would be integrated into Google Search. Reuters calculated that adding ChatGPT-like features to Google Search could cost the company $6 billion in additional expenses by 2024, while research and consulting firm SemiAnalysis calculated that it would cost Google $3 billion. The technology was developed under the codename \"Atlas\", with the name \"Bard\" in reference to the Celtic term for a storyteller and chosen to \"reflect the creative nature of the algorithm underneath\". Multiple media outlets and financial analysts described Google as \"rushing\" Bard's announcement to preempt rival Microsoft's planned February 7 event unveiling its partnership with OpenAI to integrate ChatGPT into its Bing search engine, as well as playing \"catch-up\" to Microsoft. Tom Warren of The Verge and Davey Alba of Bloomberg News noted that this marked the beginning of another clash between the two Big Tech companies over \"the future of search\", after their six-year \"truce\" expired in 2021; Chris Stokel-Walker of The Guardian, Sara Morrison of Recode, and analyst Dan Ives of investment firm Wedbush Securities labeled this an AI arms race between the two.After an \"underwhelming\" February 8 live stream in Paris showcasing Bard, Google's stock fell eight  percent, equivalent to a $100  billion loss in market value, and the YouTube video of the live stream was made private. Many viewers also pointed out an error during the demo in which Bard gives inaccurate information about the James Webb Space Telescope in response to a query. Google employees criticized Pichai's \"rushed\" and \"botched\" announcement of Bard on Memgen, the company's internal forum, while Maggie Harrison of Futurism called the rollout \"chaos\". Pichai defended his actions by saying that Google had been \"deeply working on AI for a long time\", rejecting the notion that Bard's launch was a knee-jerk reaction. Alphabet chairman John Hennessy acknowledged that Bard was not fully product-ready, but expressed excitement at the technology's potential.A week after the James Webb debacle, Pichai asked employees to dedicate two to four hours to dogfood testing Bard, while Google executive Prabhakar Raghavan encouraged employees to correct any errors Bard makes. 80,000 employees responded to Pichai's call to action. In the following weeks, Google employees roundly criticized Bard in internal messages, citing a variety of safety and ethical concerns and calling on company leaders not to launch the service. Seeking to prioritize keeping up with competitors, Google executives decided to proceed with the launch anyway, overruling an unsympathetic risk assessment report conducted by its AI ethics team. After Pichai suddenly laid off 12,000 employees later that month due to slowing revenue growth, remaining workers shared memes and snippets of their humorous exchanges with Bard soliciting its \"opinion\" on the layoffs. Google employees began testing a more sophisticated version of Bard with larger parameters, dubbed \"Big Bard\", in mid-March.\n\nLaunch\nGoogle opened up early access for Bard on March 21, 2023, in a limited capacity, allowing users in the U.S. and UK to join a waitlist. Unlike Microsoft's approach with Bing, Bard was launched as a standalone web application featuring a text box and a disclaimer that the chatbot \"may display inaccurate or offensive information that doesn't represent Google's views\". Three responses are then provided to each question, with users prompted to submit feedback on the usefulness of each answer. Google vice presidents Sissie Hsiao and Eli Collins framed Bard as a complement to Google Search and stated that the company had not determined how to make the service profitable. Among those granted early access were those enrolled in Google's \"Pixel Superfans\" loyalty program, users of its Pixel and Nest devices, and Google One subscribers.Bard is trained by third-party contractors hired by Google, including Appen and Accenture workers, whom Business Insider and Bloomberg News reported were placed under extreme pressure, overworked, and underpaid. Bard is also trained on data from publicly available sources, which Google disclosed by amending its privacy policy. Shortly after Bard's initial launch, Google reorganized the team behind Google Assistant, the company's virtual assistant, to focus on Bard instead. Google researcher Jacob Devlin resigned from the company after claiming that Bard had surreptitiously leveraged data from ChatGPT; Google denied the allegations. Pichai revealed on March 31 that the company intended to \"upgrade\" Bard by basing it on PaLM, a newer and more powerful LLM from Google, rather than LaMDA. The same day, Krawczyk announced that Google had added \"math and logic capabilities\" to Bard. Bard gained the ability to assist in coding in April, being compatible with more than 20 programming languages at launch. Microsoft also began running advertisements in the address bar of a developer build of the Edge browser, urging users to try Bing whenever they visit the Bard web app. Google is working to integrate Bard into its ChromeOS operating system and Pixel devices.During the annual Google I/O keynote in May 2023, Pichai and Hsiao announced a series of updates to Bard, including the adoption of PaLM 2, integration with other Google products and third-party services, expansion to 180 countries, support for additional languages, and new features. The expanded rollout did not include any nations in the European Union (EU), possibly reflecting concerns about compliance with the General Data Protection Regulation. Those with Google Workspace accounts also gained access to the service. Google attempted to launch Bard in the EU in June, but was blocked by the Irish Data Protection Commission, who requested for a \"data protection impact assessment\" from the company. In July, Google launched Bard in the EU and Brazil, added support for dozens of new languages, and introduced multiple new personalization and productivity features.\n\nReception\nBard received mixed reviews upon its initial release. James Vincent of The Verge found Bard faster than ChatGPT and Bing, but noted that the lack of Bing-esque footnotes was \"both a blessing and a curse\", encouraging Google to be bolder when experimenting with AI. His colleague David Pierce was unimpressed by its uninteresting and sometimes inaccurate responses, adding that despite Google's insistence that Bard was not a search engine, its user interface resembled that of one, which could cause problems for Google. Cade Metz of The New York Times described Bard as \"more cautious\" than ChatGPT, while Shirin Ghaffary of Vox called it \"dry and uncontroversial\" due to the reserved nature of its responses. In a 60 Minutes conversation with Hsiao, Google senior vice president James Manyika, and Pichai, CBS News correspondent Scott Pelley found Bard \"unsettling\". Associate professor Ethan Mollick of the Wharton School of the University of Pennsylvania was underwhelmed by Bard's artistic ineptitude. The Times later conducted a test with ChatGPT and Bard regarding their ability to handle tasks expected of human assistants, and concluded that ChatGPT's performance was vastly superior to that of Bard. NewsGuard, a tool which rates the credibility of news articles, found that Bard was more skilled at debunking known conspiracy theories than ChatGPT.\n\nSee also\nTay (chatbot)\nPoe (chatbot)\nErnie Bot\n\nReferences\nExternal links\nOfficial website \nWhite paper",
    "Basic Books": "Basic Books is a book publisher founded in 1950 and located in New York City, now an imprint of Hachette Book Group. It publishes books in the fields of psychology, philosophy, economics, science, politics, sociology, current affairs, and history.\n\nHistory\nBasic Books originated as a small Greenwich Village-based book club marketed to psychoanalysts. Arthur Rosenthal took over the book club in 1950, and under his ownership it soon began producing original books, mostly in the behavioral sciences. Early successes included Ernest Jones's The Life and Work of Sigmund Freud, as well as works by Claude Lévi-Strauss, Jean Piaget and Erik Erikson. Irving Kristol joined Basic Books in 1960, and helped Basic to expand into the social sciences. Harper & Row purchased the company in 1969.In 1997, HarperCollins announced that it would merge Basic Books into its trade publishing program, effectively closing the imprint and ending its publishing of serious academic books.  That same year, Basic was purchased by the newly created Perseus Books Group. Perseus's publishing business was acquired by Hachette Book Group in 2016. In 2018, Seal Press became an imprint of Basic.\n\nAuthors\nBasic's list of authors includes:\n\nReferences\nExternal links\nBasic Books website\nPerseus Group website (archived February 25, 2017)\nSamuel G. Freedman on the formation of Perseus Books",
    "Basis function": "In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\nIn numerical analysis and approximation theory, basis functions are also called blending functions, because of their use in interpolation: In this application, a mixture of the basis functions provides an interpolating function (with the \"blend\" depending on the evaluation of the basis functions at the data points).\n\nExamples\nMonomial basis for Cω\nThe monomial basis for the vector space of analytic functions is given by \n\nThis basis is used in Taylor series, amongst others.\n\nMonomial basis for polynomials\nThe monomial basis also forms a basis for the vector space of polynomials. After all, every polynomial can be written as \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n        +\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a_{0}+a_{1}x^{1}+a_{2}x^{2}+\\cdots +a_{n}x^{n}}\n   for some \n  \n    \n      \n        n\n        ∈\n        \n          N\n        \n      \n    \n    {\\displaystyle n\\in \\mathbb {N} }\n  , which is a linear combination of monomials.\n\nFourier basis for L2[0,1]\nSines and cosines form an (orthonormal) Schauder basis for square-integrable functions on a bounded domain. As a particular example, the collection\n\nforms a basis for L2[0,1].\n\nSee also\nReferences\nItô, Kiyosi (1993). Encyclopedic Dictionary of Mathematics (2nd ed.). MIT Press. p. 1141. ISBN 0-262-59020-4.",
    "Batch learning": "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction.\nOnline learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n\nIntroduction\nIn the setting of supervised learning, a function of \n  \n    \n      f\n      :\n      X\n      →\n      Y\n    \n    f:X\\to Y\n   is to be learned, where \n  \n    X\n    X\n   is thought of as a space of inputs and \n  \n    Y\n    Y\n   as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution \n  \n    \n      p\n      (\n      x\n      ,\n      y\n      )\n    \n    p(x,y)\n   on \n  \n    \n      X\n      ×\n      Y\n    \n    X\\times Y\n  . In reality, the learner never knows the true distribution \n  \n    \n      p\n      (\n      x\n      ,\n      y\n      )\n    \n    p(x,y)\n   over instances. Instead, the learner usually has access to a training set of examples \n  \n    \n      (\n      \n        x\n        \n          1\n        \n      \n      ,\n      \n        y\n        \n          1\n        \n      \n      )\n      ,\n      …\n      ,\n      (\n      \n        x\n        \n          n\n        \n      \n      ,\n      \n        y\n        \n          n\n        \n      \n      )\n    \n    (x_{1},y_{1}),\\ldots ,(x_{n},y_{n})\n  . In this setting, the loss function is given as \n  \n    \n      V\n      :\n      Y\n      ×\n      Y\n      →\n      \n        R\n      \n    \n    V:Y\\times Y\\to \\mathbb {R}\n  , such that \n  \n    \n      V\n      (\n      f\n      (\n      x\n      )\n      ,\n      y\n      )\n    \n    V(f(x),y)\n   measures the difference between the predicted value \n  \n    \n      f\n      (\n      x\n      )\n    \n    f(x)\n   and the true value \n  \n    y\n    y\n  . The ideal goal is to select a function \n  \n    \n      f\n      ∈\n      \n        \n          H\n        \n      \n    \n    f\\in {\\mathcal {H}}\n  , where \n  \n    \n      \n        H\n      \n    \n    {\\mathcal {H}}\n   is a space of functions called a hypothesis space, so that some notion of total loss is minimised. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.\n\nStatistical view of online learning\nIn statistical learning models, the training sample \n  \n    \n      \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{i},y_{i})}\n   are assumed to have been drawn from the true distribution \n  \n    \n      p\n      (\n      x\n      ,\n      y\n      )\n    \n    p(x,y)\n   and the objective is to minimize the expected \"risk\"\n\n  \n    \n      I\n      [\n      f\n      ]\n      =\n      \n        E\n      \n      [\n      V\n      (\n      f\n      (\n      x\n      )\n      ,\n      y\n      )\n      ]\n      =\n      ∫\n      V\n      (\n      f\n      (\n      x\n      )\n      ,\n      y\n      )\n      \n      d\n      p\n      (\n      x\n      ,\n      y\n      )\n       \n      .\n    \n    I[f]=\\mathbb {E} [V(f(x),y)]=\\int V(f(x),y)\\,dp(x,y)\\ .\n  A common paradigm in this situation is to estimate a function \n  \n    \n      \n        \n          f\n          ^\n        \n      \n    \n    {\\hat {f}}\n   through empirical risk minimization or regularized empirical risk minimization (usually Tikhonov regularization). The choice of loss function here gives rise to several well-known learning algorithms such as regularized least squares and support vector machines.\nA purely online model in this category would learn based on just the new input \n  \n    \n      \n        (\n        \n          x\n          \n            t\n            +\n            1\n          \n        \n        ,\n        \n          y\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{t+1},y_{t+1})}\n  , the current best predictor \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle f_{t}}\n   and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where \n  \n    \n      f\n      \n        t\n        +\n        1\n      \n    \n    f_{t+1}\n   is permitted to depend on \n  \n    \n      f\n      \n        t\n      \n    \n    f_{t}\n   and all previous data points \n  \n    \n      (\n      \n        x\n        \n          1\n        \n      \n      ,\n      \n        y\n        \n          1\n        \n      \n      )\n      ,\n      …\n      ,\n      (\n      \n        x\n        \n          t\n        \n      \n      ,\n      \n        y\n        \n          t\n        \n      \n      )\n    \n    (x_{1},y_{1}),\\ldots ,(x_{t},y_{t})\n  . In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.\nA common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of \n  \n    \n      \n        b\n        ≥\n        1\n      \n    \n    {\\displaystyle b\\geq 1}\n   data points at a time, this can be considered as pseudo-online learning for \n  \n    b\n    b\n   much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core versions of machine learning algorithms, for example, stochastic gradient descent. When combined with backpropagation, this is currently the de facto training method for training artificial neural networks.\n\nExample: linear least squares\nThe simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.\n\nBatch learning\nConsider the setting of supervised learning with \n  \n    f\n    f\n   being a linear function to be learned:\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            j\n          \n        \n        )\n        =\n        ⟨\n        w\n        ,\n        \n          x\n          \n            j\n          \n        \n        ⟩\n        =\n        w\n        ⋅\n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle f(x_{j})=\\langle w,x_{j}\\rangle =w\\cdot x_{j}}\n  where \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle x_{j}\\in \\mathbb {R} ^{d}}\n   is a vector of inputs (data points) and \n  \n    \n      \n        w\n        ∈\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle w\\in \\mathbb {R} ^{d}}\n   is a linear filter vector.\nThe goal is to compute the filter vector \n  \n    w\n    w\n  .\nTo this end, a square loss function \n\n  \n    \n      \n        V\n        (\n        f\n        (\n        \n          x\n          \n            j\n          \n        \n        )\n        ,\n        \n          y\n          \n            j\n          \n        \n        )\n        =\n        (\n        f\n        (\n        \n          x\n          \n            j\n          \n        \n        )\n        −\n        \n          y\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        (\n        ⟨\n        w\n        ,\n        \n          x\n          \n            j\n          \n        \n        ⟩\n        −\n        \n          y\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\\langle w,x_{j}\\rangle -y_{j})^{2}}\n  is used to compute the vector \n  \n    w\n    w\n   that minimizes the empirical loss\n\n  \n    \n      \n        \n          I\n          \n            n\n          \n        \n        [\n        w\n        ]\n        =\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        V\n        (\n        ⟨\n        w\n        ,\n        \n          x\n          \n            j\n          \n        \n        ⟩\n        ,\n        \n          y\n          \n            j\n          \n        \n        )\n        =\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            j\n          \n          \n            T\n          \n        \n        w\n        −\n        \n          y\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle I_{n}[w]=\\sum _{j=1}^{n}V(\\langle w,x_{j}\\rangle ,y_{j})=\\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}}\n  where\n\n  \n    \n      \n        \n          y\n          \n            j\n          \n        \n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle y_{j}\\in \\mathbb {R} }\n  .Let  \n  \n    X\n    X\n   be the \n  \n    \n      \n        i\n        ×\n        d\n      \n    \n    {\\displaystyle i\\times d}\n   data matrix and \n  \n    \n      \n        y\n        ∈\n        \n          \n            R\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y\\in \\mathbb {R} ^{i}}\n   is the column vector of target values after the arrival of the first \n  \n    i\n    i\n   data points.\nAssuming that the covariance matrix \n  \n    \n      \n        \n          Σ\n          \n            i\n          \n        \n        =\n        \n          X\n          \n            T\n          \n        \n        X\n      \n    \n    {\\displaystyle \\Sigma _{i}=X^{T}X}\n   is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution \n  \n    \n      \n        \n          f\n          \n            ∗\n          \n        \n        (\n        x\n        )\n        =\n        ⟨\n        \n          w\n          \n            ∗\n          \n        \n        ,\n        x\n        ⟩\n      \n    \n    {\\displaystyle f^{*}(x)=\\langle w^{*},x\\rangle }\n   to the linear least squares problem is given by\n\n  \n    \n      \n        \n          w\n          \n            ∗\n          \n        \n        =\n        (\n        \n          X\n          \n            T\n          \n        \n        X\n        \n          )\n          \n            −\n            1\n          \n        \n        \n          X\n          \n            T\n          \n        \n        y\n        =\n        \n          Σ\n          \n            i\n          \n          \n            −\n            1\n          \n        \n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            i\n          \n        \n        \n          x\n          \n            j\n          \n        \n        \n          y\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle w^{*}=(X^{T}X)^{-1}X^{T}y=\\Sigma _{i}^{-1}\\sum _{j=1}^{i}x_{j}y_{j}}\n  .Now, calculating the covariance matrix \n  \n    \n      \n        \n          Σ\n          \n            i\n          \n        \n        =\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            i\n          \n        \n        \n          x\n          \n            j\n          \n        \n        \n          x\n          \n            j\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{i}=\\sum _{j=1}^{i}x_{j}x_{j}^{T}}\n   takes time \n  \n    \n      \n        O\n        (\n        i\n        \n          d\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(id^{2})}\n  , inverting the \n  \n    \n      d\n      ×\n      d\n    \n    d\\times d\n   matrix takes time \n  \n    \n      \n        O\n        (\n        \n          d\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(d^{3})}\n  , while the rest of the multiplication takes time \n  \n    \n      O\n      (\n      \n        d\n        \n          2\n        \n      \n      )\n    \n    O(d^{2})\n  , giving a total time of \n  \n    \n      \n        O\n        (\n        i\n        \n          d\n          \n            2\n          \n        \n        +\n        \n          d\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(id^{2}+d^{3})}\n  . When there are \n  \n    n\n    n\n   total points in the dataset, to recompute the solution after the arrival of every datapoint \n  \n    \n      i\n      =\n      1\n      ,\n      …\n      ,\n      n\n    \n    i=1,\\ldots ,n\n  , the naive approach will have a total complexity \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        \n          d\n          \n            2\n          \n        \n        +\n        n\n        \n          d\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2}d^{2}+nd^{3})}\n  . Note that when storing the matrix \n  \n    \n      \n        \n          Σ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{i}}\n  , then updating it at each step needs only adding \n  \n    \n      \n        \n          x\n          \n            i\n            +\n            1\n          \n        \n        \n          x\n          \n            i\n            +\n            1\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle x_{i+1}x_{i+1}^{T}}\n  , which takes \n  \n    \n      \n        O\n        (\n        \n          d\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(d^{2})}\n   time, reducing the total time to \n  \n    \n      \n        O\n        (\n        n\n        \n          d\n          \n            2\n          \n        \n        +\n        n\n        \n          d\n          \n            3\n          \n        \n        )\n        =\n        O\n        (\n        n\n        \n          d\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}\n  , but with an additional storage space of \n  \n    \n      \n        O\n        (\n        \n          d\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(d^{2})}\n   to store \n  \n    \n      \n        \n          Σ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{i}}\n  .\n\nOnline learning: recursive least squares\nThe recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising \n  \n    \n      \n        \n          \n            w\n            \n              0\n            \n          \n          =\n          0\n          ∈\n          \n            \n              R\n            \n            \n              d\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle w_{0}=0\\in \\mathbb {R} ^{d}}\n   and \n  \n    \n      \n        \n          \n            Γ\n            \n              0\n            \n          \n          =\n          I\n          ∈\n          \n            \n              R\n            \n            \n              d\n              ×\n              d\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle \\Gamma _{0}=I\\in \\mathbb {R} ^{d\\times d}}\n  , the solution of the linear least squares problem given in the previous section can be computed by the following iteration:\n\n  \n    \n      \n        Γ\n        \n          i\n        \n      \n      =\n      \n        Γ\n        \n          i\n          −\n          1\n        \n      \n      −\n      \n        \n          \n            \n              Γ\n              \n                i\n                −\n                1\n              \n            \n            \n              x\n              \n                i\n              \n            \n            \n              x\n              \n                i\n              \n              \n                T\n              \n            \n            \n              Γ\n              \n                i\n                −\n                1\n              \n            \n          \n          \n            1\n            +\n            \n              x\n              \n                i\n              \n              \n                T\n              \n            \n            \n              Γ\n              \n                i\n                −\n                1\n              \n            \n            \n              x\n              \n                i\n              \n            \n          \n        \n      \n    \n    \\Gamma _{i}=\\Gamma _{i-1}-{\\frac {\\Gamma _{i-1}x_{i}x_{i}^{T}\\Gamma _{i-1}}{1+x_{i}^{T}\\Gamma _{i-1}x_{i}}}\n  \n\n  \n    \n      \n        w\n        \n          i\n        \n      \n      =\n      \n        w\n        \n          i\n          −\n          1\n        \n      \n      −\n      \n        Γ\n        \n          i\n        \n      \n      \n        x\n        \n          i\n        \n      \n      (\n      \n        x\n        \n          i\n        \n        \n          T\n        \n      \n      \n        w\n        \n          i\n          −\n          1\n        \n      \n      −\n      \n        y\n        \n          i\n        \n      \n      )\n    \n    w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})\n  The above iteration algorithm can be proved using induction on \n  \n    i\n    i\n  . The proof also shows that \n  \n    \n      \n        \n          Γ\n          \n            i\n          \n        \n        =\n        \n          Σ\n          \n            i\n          \n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\Gamma _{i}=\\Sigma _{i}^{-1}}\n  . \nOne can look at RLS also in the context of adaptive filters (see RLS).\nThe complexity for \n  \n    n\n    n\n   steps of this algorithm is \n  \n    \n      O\n      (\n      n\n      \n        d\n        \n          2\n        \n      \n      )\n    \n    O(nd^{2})\n  , which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step \n  \n    i\n    i\n   here are to store the matrix \n  \n    \n      Γ\n      \n        i\n      \n    \n    \\Gamma _{i}\n  , which is constant at \n  \n    \n      O\n      (\n      \n        d\n        \n          2\n        \n      \n      )\n    \n    O(d^{2})\n  . For the case when \n  \n    \n      \n        \n          Σ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{i}}\n   is not invertible, consider the regularised version of the problem \nloss function \n  \n    \n      \n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            j\n          \n          \n            T\n          \n        \n        w\n        −\n        \n          y\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        λ\n        \n          |\n        \n        \n          |\n        \n        w\n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}+\\lambda ||w||_{2}^{2}}\n  . Then, it's easy to show that the same algorithm works with \n  \n    \n      \n        \n          Γ\n          \n            0\n          \n        \n        =\n        (\n        I\n        +\n        λ\n        I\n        \n          )\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\Gamma _{0}=(I+\\lambda I)^{-1}}\n  , and the iterations proceed to give \n  \n    \n      \n        \n          Γ\n          \n            i\n          \n        \n        =\n        (\n        \n          Σ\n          \n            i\n          \n        \n        +\n        λ\n        I\n        \n          )\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\Gamma _{i}=(\\Sigma _{i}+\\lambda I)^{-1}}\n  .\n\nStochastic gradient descent\nWhen this  \n\n  \n    \n      \n        \n          \n            w\n            \n              i\n            \n          \n          =\n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          −\n          \n            Γ\n            \n              i\n            \n          \n          \n            x\n            \n              i\n            \n          \n          (\n          \n            x\n            \n              i\n            \n            \n              T\n            \n          \n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          −\n          \n            y\n            \n              i\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}\n  is replaced by\n\n  \n    \n      \n        \n          \n            w\n            \n              i\n            \n          \n          =\n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          −\n          \n            γ\n            \n              i\n            \n          \n          \n            x\n            \n              i\n            \n          \n          (\n          \n            x\n            \n              i\n            \n            \n              T\n            \n          \n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          −\n          \n            y\n            \n              i\n            \n          \n          )\n          =\n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          −\n          \n            γ\n            \n              i\n            \n          \n          ∇\n          V\n          (\n          ⟨\n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          ,\n          \n            x\n            \n              i\n            \n          \n          ⟩\n          ,\n          \n            y\n            \n              i\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{i}\\rangle ,y_{i})}\n  or \n  \n    \n      \n        Γ\n        \n          i\n        \n      \n      ∈\n      \n        \n          R\n        \n        \n          d\n          ×\n          d\n        \n      \n    \n    \\Gamma _{i}\\in \\mathbb {R} ^{d\\times d}\n   by \n  \n    \n      \n        γ\n        \n          i\n        \n      \n      ∈\n      \n        R\n      \n    \n    \\gamma _{i}\\in \\mathbb {R}\n  , this becomes the stochastic gradient descent algorithm. In this case, the complexity for \n  \n    n\n    n\n   steps of this algorithm reduces to \n  \n    \n      O\n      (\n      n\n      d\n      )\n    \n    O(nd)\n  . The storage requirements at every step \n  \n    i\n    i\n   are constant at \n  \n    \n      O\n      (\n      d\n      )\n    \n    O(d)\n  .\nHowever, the stepsize \n  \n    \n      γ\n      \n        i\n      \n    \n    \\gamma _{i}\n   needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size \n  \n    \n      \n        \n          γ\n          \n            i\n          \n        \n        ≈\n        \n          \n            1\n            \n              i\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\gamma _{i}\\approx {\\frac {1}{\\sqrt {i}}},}\n   one can prove the convergence of the average iterate \n  \n    \n      \n        \n          \n            \n              w\n              ¯\n            \n          \n          \n            n\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\overline {w}}_{n}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}}\n  . This setting is a special case of stochastic optimization, a well known problem in optimization.\n\nIncremental stochastic gradient descent\nIn practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is\ncalled incremental gradient method and corresponds to an iteration\n\n  \n    \n      \n        \n          \n            w\n            \n              i\n            \n          \n          =\n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          −\n          \n            γ\n            \n              i\n            \n          \n          ∇\n          V\n          (\n          ⟨\n          \n            w\n            \n              i\n              −\n              1\n            \n          \n          ,\n          \n            x\n            \n              \n                t\n                \n                  i\n                \n              \n            \n          \n          ⟩\n          ,\n          \n            y\n            \n              \n                t\n                \n                  i\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{t_{i}}\\rangle ,y_{t_{i}})}\n  The main difference with the stochastic gradient method is that here a sequence \n  \n    \n      t\n      \n        i\n      \n    \n    t_{i}\n   is chosen to decide which training point is visited in the \n  \n    i\n    i\n  -th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk. Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.\n\nKernel methods\nKernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method.\nThis discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction  that if \n  \n    \n      X\n      \n        i\n      \n    \n    X_{i}\n   is the data matrix and \n  \n    \n      w\n      \n        i\n      \n    \n    w_{i}\n   is the output after \n  \n    i\n    i\n   steps of the SGD algorithm, then,\n\n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n        =\n        \n          X\n          \n            i\n          \n          \n            T\n          \n        \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i}=X_{i}^{T}c_{i}}\n  where \n  \n    \n      \n        \n          \n            c\n            \n              i\n            \n          \n          =\n          (\n          (\n          \n            c\n            \n              i\n            \n          \n          \n            )\n            \n              1\n            \n          \n          ,\n          (\n          \n            c\n            \n              i\n            \n          \n          \n            )\n            \n              2\n            \n          \n          ,\n          .\n          .\n          .\n          ,\n          (\n          \n            c\n            \n              i\n            \n          \n          \n            )\n            \n              i\n            \n          \n          )\n          ∈\n          \n            \n              R\n            \n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\\in \\mathbb {R} ^{i}}\n   and the sequence \n  \n    \n      c\n      \n        i\n      \n    \n    c_{i}\n   satisfies the recursion:\n\n  \n    \n      \n        \n          c\n          \n            0\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle c_{0}=0}\n  \n\n  \n    \n      \n        (\n        \n          c\n          \n            i\n          \n        \n        \n          )\n          \n            j\n          \n        \n        =\n        (\n        \n          c\n          \n            i\n            −\n            1\n          \n        \n        \n          )\n          \n            j\n          \n        \n        ,\n        j\n        =\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        i\n        −\n        1\n      \n    \n    {\\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}\n   and\n\n  \n    \n      \n        (\n        \n          c\n          \n            i\n          \n        \n        \n          )\n          \n            i\n          \n        \n        =\n        \n          γ\n          \n            i\n          \n        \n        \n          \n            (\n          \n        \n        \n          y\n          \n            i\n          \n        \n        −\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            i\n            −\n            1\n          \n        \n        (\n        \n          c\n          \n            i\n            −\n            1\n          \n        \n        \n          )\n          \n            j\n          \n        \n        ⟨\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            i\n          \n        \n        ⟩\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x_{i}\\rangle {\\Big )}}\n  Notice that here \n  \n    \n      \n        ⟨\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            i\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle x_{j},x_{i}\\rangle }\n   is just the standard Kernel on \n  \n    \n      \n        R\n      \n      \n        d\n      \n    \n    \\mathbb {R} ^{d}\n  , and the predictor is of the form \n\n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n        (\n        x\n        )\n        =\n        ⟨\n        \n          w\n          \n            i\n            −\n            1\n          \n        \n        ,\n        x\n        ⟩\n        =\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            i\n            −\n            1\n          \n        \n        (\n        \n          c\n          \n            i\n            −\n            1\n          \n        \n        \n          )\n          \n            j\n          \n        \n        ⟨\n        \n          x\n          \n            j\n          \n        \n        ,\n        x\n        ⟩\n      \n    \n    {\\displaystyle f_{i}(x)=\\langle w_{i-1},x\\rangle =\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x\\rangle }\n  .Now, if  a general kernel \n  \n    K\n    K\n   is introduced instead and let the predictor be \n\n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n        (\n        x\n        )\n        =\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            i\n            −\n            1\n          \n        \n        (\n        \n          c\n          \n            i\n            −\n            1\n          \n        \n        \n          )\n          \n            j\n          \n        \n        K\n        (\n        \n          x\n          \n            j\n          \n        \n        ,\n        x\n        )\n      \n    \n    {\\displaystyle f_{i}(x)=\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}\n  then the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to\n\n  \n    \n      \n        (\n        \n          c\n          \n            i\n          \n        \n        \n          )\n          \n            i\n          \n        \n        =\n        \n          γ\n          \n            i\n          \n        \n        \n          \n            (\n          \n        \n        \n          y\n          \n            i\n          \n        \n        −\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            i\n            −\n            1\n          \n        \n        (\n        \n          c\n          \n            i\n            −\n            1\n          \n        \n        \n          )\n          \n            j\n          \n        \n        K\n        (\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            i\n          \n        \n        )\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\\Big )}}\n  The above expression requires storing all the data for updating \n  \n    \n      c\n      \n        i\n      \n    \n    c_{i}\n  . The total time complexity for the recursion when evaluating for the \n  \n    n\n    n\n  -th datapoint is \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        d\n        k\n        )\n      \n    \n    {\\displaystyle O(n^{2}dk)}\n  , where \n  \n    k\n    k\n   is the cost of evaluating the kernel on a single pair of points.\nThus, the use of the kernel has allowed the movement from a finite dimensional parameter space \n  \n    \n      \n        \n          \n            w\n            \n              i\n            \n          \n          ∈\n          \n            \n              R\n            \n            \n              d\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle w_{i}\\in \\mathbb {R} ^{d}}\n   to a possibly infinite dimensional feature represented by a kernel \n  \n    K\n    K\n   by instead performing the recursion on the space of parameters \n  \n    \n      \n        \n          \n            c\n            \n              i\n            \n          \n          ∈\n          \n            \n              R\n            \n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle c_{i}\\in \\mathbb {R} ^{i}}\n  , whose dimension is the same as the size of the training dataset. In general, this is a consequence of the representer theorem.\n\nOnline convex optimization\nOnline convex optimization (OCO)  is a general framework for decision making which leverages convex optimization to allow for efficient algorithms. The framework is that of repeated game playing as follows:\nFor \n  \n    \n      \n        t\n        =\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        T\n      \n    \n    {\\displaystyle t=1,2,...,T}\n  \n\nLearner receives input \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \nLearner outputs \n  \n    \n      \n        \n          w\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle w_{t}}\n   from a fixed convex set \n  \n    S\n    S\n  \nNature sends back a convex loss function \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        :\n        S\n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\n  .\nLearner suffers loss \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        \n          w\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle v_{t}(w_{t})}\n   and updates its modelThe goal is to minimize regret, or the difference between cumulative loss and the loss of the best fixed point  \n  \n    \n      \n        u\n        ∈\n        S\n      \n    \n    {\\displaystyle u\\in S}\n   in hindsight.\nAs an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set \n  \n    \n      \n        S\n        =\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle S=\\mathbb {R} ^{d}}\n  , and nature sends back the convex loss function \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        w\n        )\n        =\n        (\n        ⟨\n        w\n        ,\n        \n          x\n          \n            t\n          \n        \n        ⟩\n        −\n        \n          y\n          \n            t\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle v_{t}(w)=(\\langle w,x_{t}\\rangle -y_{t})^{2}}\n  . Note here that \n  \n    \n      \n        \n          y\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle y_{t}}\n   is implicitly sent with \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle v_{t}}\n  .\nSome online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions.\nSome simple online convex optimisation algorithms are:\n\nFollow the leader (FTL)\nThe simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and is simply given round \n  \n    t\n    t\n   by:\n\n  \n    \n      \n        \n          w\n          \n            t\n          \n        \n        =\n        \n          \n            a\n            r\n            g\n            \n            m\n            i\n            n\n          \n          \n            w\n            ∈\n            S\n          \n        \n        ⁡\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            t\n            −\n            1\n          \n        \n        \n          v\n          \n            i\n          \n        \n        (\n        w\n        )\n      \n    \n    {\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)}\n  This method can thus be looked as a greedy algorithm. For the case of online quadratic optimization (where the loss function is \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        w\n        )\n        =\n        \n          |\n        \n        \n          |\n        \n        w\n        −\n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle v_{t}(w)=||w-x_{t}||_{2}^{2}}\n  ), one can show a regret bound that grows as \n  \n    \n      \n        log\n        ⁡\n        (\n        T\n        )\n      \n    \n    {\\displaystyle \\log(T)}\n  . However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.\n\nFollow the regularised leader (FTRL)\nThis is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function \n  \n    \n      \n        R\n        :\n        S\n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle R:S\\rightarrow \\mathbb {R} }\n    is chosen and learning performed in round t as follows:\n\n  \n    \n      \n        \n          w\n          \n            t\n          \n        \n        =\n        \n          \n            a\n            r\n            g\n            \n            m\n            i\n            n\n          \n          \n            w\n            ∈\n            S\n          \n        \n        ⁡\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            t\n            −\n            1\n          \n        \n        \n          v\n          \n            i\n          \n        \n        (\n        w\n        )\n        +\n        R\n        (\n        w\n        )\n      \n    \n    {\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)+R(w)}\n  As a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        w\n        )\n        =\n        ⟨\n        w\n        ,\n        \n          z\n          \n            t\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }\n  . Also, let \n  \n    \n      \n        S\n        =\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle S=\\mathbb {R} ^{d}}\n  . Suppose the regularisation function \n  \n    \n      \n        R\n        (\n        w\n        )\n        =\n        \n          \n            1\n            \n              2\n              η\n            \n          \n        \n        \n          |\n        \n        \n          |\n        \n        w\n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle R(w)={\\frac {1}{2\\eta }}||w||_{2}^{2}}\n   is chosen for some positive number \n  \n    η\n    \\eta\n  . Then, one can show that the regret minimising iteration becomes \n\n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        −\n        η\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            t\n          \n        \n        \n          z\n          \n            i\n          \n        \n        =\n        \n          w\n          \n            t\n          \n        \n        −\n        η\n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle w_{t+1}=-\\eta \\sum _{i=1}^{t}z_{i}=w_{t}-\\eta z_{t}}\n  Note that this can be rewritten as \n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          w\n          \n            t\n          \n        \n        −\n        η\n        ∇\n        \n          v\n          \n            t\n          \n        \n        (\n        \n          w\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle w_{t+1}=w_{t}-\\eta \\nabla v_{t}(w_{t})}\n  , which looks exactly like online gradient descent.\nIf S is instead some convex subspace of \n  \n    \n      \n        R\n      \n      \n        d\n      \n    \n    \\mathbb {R} ^{d}\n  , S would need to be projected onto, leading to the modified update rule\n\n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          Π\n          \n            S\n          \n        \n        (\n        −\n        η\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            t\n          \n        \n        \n          z\n          \n            i\n          \n        \n        )\n        =\n        \n          Π\n          \n            S\n          \n        \n        (\n        η\n        \n          θ\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle w_{t+1}=\\Pi _{S}(-\\eta \\sum _{i=1}^{t}z_{i})=\\Pi _{S}(\\eta \\theta _{t+1})}\n  This algorithm is known as lazy projection, as the vector \n  \n    \n      \n        \n          θ\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{t+1}}\n   accumulates the gradients. It is also known as Nesterov's dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by \n  \n    \n      \n        O\n        (\n        \n          \n            T\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {T}})}\n  , and thus the average regret goes to 0 as desired.\n\nOnline subgradient descent (OSD)\nThe above proved a regret bound for linear loss functions \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        w\n        )\n        =\n        ⟨\n        w\n        ,\n        \n          z\n          \n            t\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }\n  . To generalise the algorithm to any convex loss function, the subgradient \n  \n    \n      \n        ∂\n        \n          v\n          \n            t\n          \n        \n        (\n        \n          w\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\partial v_{t}(w_{t})}\n   of \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle v_{t}}\n   is used as a linear approximation to \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle v_{t}}\n   near \n  \n    \n      \n        \n          w\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle w_{t}}\n  , leading to the online subgradient descent algorithm:\nInitialise parameter \n  \n    \n      \n        η\n        ,\n        \n          w\n          \n            1\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\eta ,w_{1}=0}\n  \nFor \n  \n    \n      \n        t\n        =\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        T\n      \n    \n    {\\displaystyle t=1,2,...,T}\n  \n\nPredict using \n  \n    \n      \n        \n          w\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle w_{t}}\n  , receive \n  \n    \n      f\n      \n        t\n      \n    \n    f_{t}\n   from nature.\nChoose \n  \n    \n      \n        \n          z\n          \n            t\n          \n        \n        ∈\n        ∂\n        \n          v\n          \n            t\n          \n        \n        (\n        \n          w\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle z_{t}\\in \\partial v_{t}(w_{t})}\n  \nIf \n  \n    \n      \n        S\n        =\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle S=\\mathbb {R} ^{d}}\n  , update as \n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          w\n          \n            t\n          \n        \n        −\n        η\n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle w_{t+1}=w_{t}-\\eta z_{t}}\n  \nIf \n  \n    \n      \n        S\n        ⊂\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle S\\subset \\mathbb {R} ^{d}}\n  , project cumulative gradients onto \n  \n    S\n    S\n   i.e. \n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          Π\n          \n            S\n          \n        \n        (\n        η\n        \n          θ\n          \n            t\n            +\n            1\n          \n        \n        )\n        ,\n        \n          θ\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          θ\n          \n            t\n          \n        \n        +\n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle w_{t+1}=\\Pi _{S}(\\eta \\theta _{t+1}),\\theta _{t+1}=\\theta _{t}+z_{t}}\n  One can use the OSD algorithm to derive \n  \n    \n      \n        O\n        (\n        \n          \n            T\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {T}})}\n   regret bounds for the online version of SVM's for classification, which use the hinge loss\n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        w\n        )\n        =\n        max\n        {\n        0\n        ,\n        1\n        −\n        \n          y\n          \n            t\n          \n        \n        (\n        w\n        ⋅\n        \n          x\n          \n            t\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle v_{t}(w)=\\max\\{0,1-y_{t}(w\\cdot x_{t})\\}}\n\nOther algorithms\nQuadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent.  The optimal regularization in hindsight can be derived for linear loss functions, this leads to the AdaGrad algorithm.\nFor the Euclidean regularisation, one can show a regret bound of \n  \n    \n      \n        O\n        (\n        \n          \n            T\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {T}})}\n  , which can be improved further to a \n  \n    \n      \n        O\n        (\n        log\n        ⁡\n        T\n        )\n      \n    \n    {\\displaystyle O(\\log T)}\n   for strongly convex and exp-concave loss functions.\n\nContinual learning\nContinual learning means constantly improving the learned model by  processing continuous\nstreams of information.\nContinual learning capabilities are essential for software systems and autonomous agents interacting in an ever changing real world.\nHowever, continual learning is a challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions\ngenerally leads to catastrophic forgetting.\n\nInterpretations of online learning\nThe paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions \n  \n    \n      \n        \n          f\n          \n            1\n          \n        \n        ,\n        \n          f\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          f\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle f_{1},f_{2},\\ldots ,f_{n}}\n  . The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by\n\n  \n    \n      \n        \n          \n            w\n            \n              t\n            \n          \n          =\n          \n            w\n            \n              t\n              −\n              1\n            \n          \n          −\n          \n            γ\n            \n              t\n            \n          \n          ∇\n          V\n          (\n          ⟨\n          \n            w\n            \n              t\n              −\n              1\n            \n          \n          ,\n          \n            x\n            \n              t\n            \n          \n          ⟩\n          ,\n          \n            y\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle w_{t}=w_{t-1}-\\gamma _{t}\\nabla V(\\langle w_{t-1},x_{t}\\rangle ,y_{t})}\n  The first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk \n  \n    \n      I\n      [\n      w\n      ]\n    \n    I[w]\n   defined above. Indeed, in the case of an infinite stream of data, since the examples \n  \n    \n      (\n      \n        x\n        \n          1\n        \n      \n      ,\n      \n        y\n        \n          1\n        \n      \n      )\n      ,\n      (\n      \n        x\n        \n          2\n        \n      \n      ,\n      \n        y\n        \n          2\n        \n      \n      )\n      ,\n      …\n    \n    (x_{1},y_{1}),(x_{2},y_{2}),\\ldots\n   are assumed to be drawn i.i.d. from the distribution \n  \n    \n      p\n      (\n      x\n      ,\n      y\n      )\n    \n    p(x,y)\n  , the sequence of gradients of \n  \n    \n      V\n      (\n      ⋅\n      ,\n      ⋅\n      )\n    \n    V(\\cdot ,\\cdot )\n   in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk \n  \n    \n      I\n      [\n      w\n      ]\n    \n    I[w]\n   and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation \n  \n    \n      I\n      [\n      \n        w\n        \n          t\n        \n      \n      ]\n      −\n      I\n      [\n      \n        w\n        \n          ∗\n        \n      \n      ]\n    \n    I[w_{t}]-I[w^{\\ast }]\n  , where \n  \n    \n      w\n      \n        ∗\n      \n    \n    w^{\\ast }\n   is the minimizer of \n  \n    \n      I\n      [\n      w\n      ]\n    \n    I[w]\n  . This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.\nThe second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method. In this case, one instead looks at the empirical risk:\n\n  \n    \n      \n        I\n        \n          n\n        \n      \n      [\n      w\n      ]\n      =\n      \n        \n          1\n          n\n        \n      \n      \n        ∑\n        \n          i\n          =\n          1\n        \n        \n          n\n        \n      \n      V\n      (\n      ⟨\n      w\n      ,\n      \n        x\n        \n          i\n        \n      \n      ⟩\n      ,\n      \n        y\n        \n          i\n        \n      \n      )\n       \n      .\n    \n    I_{n}[w]={\\frac {1}{n}}\\sum _{i=1}^{n}V(\\langle w,x_{i}\\rangle ,y_{i})\\ .\n  Since the gradients of \n  \n    \n      V\n      (\n      ⋅\n      ,\n      ⋅\n      )\n    \n    V(\\cdot ,\\cdot )\n   in the incremental gradient descent iterations are also stochastic estimates of the gradient of \n  \n    \n      \n        I\n        \n          n\n        \n      \n      [\n      w\n      ]\n    \n    I_{n}[w]\n  , this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations \n  \n    \n      \n        I\n        \n          n\n        \n      \n      [\n      \n        w\n        \n          t\n        \n      \n      ]\n      −\n      \n        I\n        \n          n\n        \n      \n      [\n      \n        w\n        \n          n\n        \n        \n          ∗\n        \n      \n      ]\n    \n    I_{n}[w_{t}]-I_{n}[w_{n}^{\\ast }]\n  , where \n  \n    \n      w\n      \n        n\n      \n      \n        ∗\n      \n    \n    w_{n}^{\\ast }\n   is the minimizer of \n  \n    \n      \n        I\n        \n          n\n        \n      \n      [\n      w\n      ]\n    \n    I_{n}[w]\n  .\n\nImplementations\nVowpal Wabbit: Open-source fast out-of-core online learning system which is notable for supporting a number of machine learning reductions, importance weighting and a selection of different loss functions and optimisation algorithms. It uses the hashing trick for bounding the size of the set of features independent of the amount of training data.\nscikit-learn: Provides out-of-core implementations of algorithms for\nClassification: Perceptron, SGD classifier, Naive bayes classifier.\nRegression: SGD Regressor, Passive Aggressive regressor.\nClustering: Mini-batch k-means.\nFeature extraction: Mini-batch dictionary learning, Incremental PCA.\n\nSee also\nLearning paradigms\n\nIncremental learning\nLazy learning\nOffline learning, the opposite model\nReinforcement learning\nMulti-armed bandit\nSupervised learningGeneral algorithms\n\nOnline algorithm\nOnline optimization\nStreaming algorithm\nStochastic gradient descentLearning models\n\nAdaptive Resonance Theory\nHierarchical temporal memory\nk-nearest neighbor algorithm\nLearning vector quantization\nPerceptron\n\nReferences\nExternal links\n6.883: Online Methods in Machine Learning: Theory and Applications. Alexander Rakhlin. MIT",
    "Batch normalization": "Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others maintain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks.\n\nInternal covariate shift\nEach layer of a neural network has inputs with a corresponding distribution, which is affected during the training process by the randomness in the parameter initialization and the randomness in the input data. The effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift. Although a clear-cut precise definition seems to be missing, the phenomenon observed in experiments is the change on means and variances of the inputs to internal layers during training.\nBatch normalization was initially proposed to mitigate internal covariate shift. During the training stage of networks, as the parameters of the preceding layers change, the distribution of inputs to the current layer changes accordingly, such that the current layer needs to constantly readjust to new distributions. This problem is especially severe for deep networks, because small changes in shallower hidden layers will be amplified as they propagate within the network, resulting in significant shift in deeper hidden layers. Therefore, the method of batch normalization is proposed to reduce these unwanted shifts to speed up training and to produce more reliable models.\nBesides reducing internal covariate shift, batch normalization is believed to introduce many other benefits. With this additional operation, the network can use higher learning rate without vanishing or exploding gradients. Furthermore, batch normalization seems to have a regularizing effect such that the network improves its generalization properties, and it is thus unnecessary to use dropout to mitigate overfitting. It has been observed also that with batch norm the network becomes more robust to different initialization schemes and learning rates.\n\nProcedures\nTransformation\nIn a neural network, batch normalization is achieved through a normalization step that fixes the means and variances of each layer's inputs. Ideally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini-batch in the training process.\nLet us use B to denote a mini-batch of size m of the entire training set. The empirical mean and variance of B could thus be denoted as\n\n  \n    \n      \n        \n          μ\n          \n            B\n          \n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mu _{B}={\\frac {1}{m}}\\sum _{i=1}^{m}x_{i}}\n   and \n  \n    \n      \n        \n          σ\n          \n            B\n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        −\n        \n          μ\n          \n            B\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{B}^{2}={\\frac {1}{m}}\\sum _{i=1}^{m}(x_{i}-\\mu _{B})^{2}}\n  .\nFor a layer of the network with d-dimensional input, \n  \n    \n      \n        x\n        =\n        (\n        \n          x\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            (\n            d\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle x=(x^{(1)},...,x^{(d)})}\n  , each dimension of its input is then normalized (i.e. re-centered and re-scaled) separately,\n\n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n        =\n        \n          \n            \n              \n                x\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n              −\n              \n                μ\n                \n                  B\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n            \n              \n                \n                  (\n                  \n                    σ\n                    \n                      B\n                    \n                    \n                      (\n                      k\n                      )\n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n              +\n              ϵ\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {x}}_{i}^{(k)}={\\frac {x_{i}^{(k)}-\\mu _{B}^{(k)}}{\\sqrt {\\left(\\sigma _{B}^{(k)}\\right)^{2}+\\epsilon }}}}\n  , where \n  \n    \n      \n        k\n        ∈\n        [\n        1\n        ,\n        d\n        ]\n      \n    \n    {\\displaystyle k\\in [1,d]}\n   and  \n  \n    \n      \n        i\n        ∈\n        [\n        1\n        ,\n        m\n        ]\n      \n    \n    {\\displaystyle i\\in [1,m]}\n  ; \n  \n    \n      \n        \n          μ\n          \n            B\n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mu _{B}^{(k)}}\n   and \n  \n    \n      \n        \n          σ\n          \n            B\n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{B}^{(k)}}\n   are the per-dimension mean and standard deviation, respectively.\n\n  \n    ϵ\n    \\epsilon\n   is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized activation \n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\hat {x}}^{(k)}}\n  have zero mean and unit variance, if \n  \n    ϵ\n    \\epsilon\n   is not taken into account. To restore the representation power of the network, a transformation step then follows as\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n        =\n        \n          γ\n          \n            (\n            k\n            )\n          \n        \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n        +\n        \n          β\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{(k)}=\\gamma ^{(k)}{\\hat {x}}_{i}^{(k)}+\\beta ^{(k)}}\n  ,\nwhere the parameters \n  \n    \n      \n        \n          γ\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle \\gamma ^{(k)}}\n   and \n  \n    \n      \n        \n          β\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle \\beta ^{(k)}}\n   are subsequently learned in the optimization process.\nFormally, the operation that implements batch normalization is a transform \n  \n    \n      \n        B\n        \n          N\n          \n            \n              γ\n              \n                (\n                k\n                )\n              \n            \n            ,\n            \n              β\n              \n                (\n                k\n                )\n              \n            \n          \n        \n        :\n        \n          x\n          \n            1...\n            m\n          \n          \n            (\n            k\n            )\n          \n        \n        →\n        \n          y\n          \n            1...\n            m\n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle BN_{\\gamma ^{(k)},\\beta ^{(k)}}:x_{1...m}^{(k)}\\rightarrow y_{1...m}^{(k)}}\n   called the Batch Normalizing transform. The output of the BN transform \n  \n    \n      \n        \n          y\n          \n            (\n            k\n            )\n          \n        \n        =\n        B\n        \n          N\n          \n            \n              γ\n              \n                (\n                k\n                )\n              \n            \n            ,\n            \n              β\n              \n                (\n                k\n                )\n              \n            \n          \n        \n        (\n        \n          x\n          \n            (\n            k\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle y^{(k)}=BN_{\\gamma ^{(k)},\\beta ^{(k)}}(x^{(k)})}\n   is then passed to other network layers, while the normalized output  \n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\hat {x}}_{i}^{(k)}}\n   remains internal to the current layer.\n\nBackpropagation\nThe described BN transform is a differentiable operation, and the gradient of the loss l  with respect to the different parameters can be computed directly with the chain rule.\nSpecifically, \n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial y_{i}^{(k)}}}}\n   depends on the choice of activation function, and the gradient against other parameters could be expressed as a function of \n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial y_{i}^{(k)}}}}\n  :\n\n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                \n                  \n                    \n                      x\n                      ^\n                    \n                  \n                \n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        \n          γ\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial {\\hat {x}}_{i}^{(k)}}}={\\frac {\\partial l}{\\partial y_{i}^{(k)}}}\\gamma ^{(k)}}\n  ,\n\n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                γ\n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial \\gamma ^{(k)}}}=\\sum _{i=1}^{m}{\\frac {\\partial l}{\\partial y_{i}^{(k)}}}{\\hat {x}}_{i}^{(k)}}\n  , \n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                β\n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial \\beta ^{(k)}}}=\\sum _{i=1}^{m}{\\frac {\\partial l}{\\partial y_{i}^{(k)}}}}\n  ,\n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                σ\n                \n                  B\n                \n                \n                  (\n                  k\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        (\n        \n          x\n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n        −\n        \n          μ\n          \n            B\n          \n          \n            (\n            k\n            )\n          \n        \n        )\n        \n          (\n          \n            −\n            \n              \n                \n                  γ\n                  \n                    (\n                    k\n                    )\n                  \n                \n                2\n              \n            \n            (\n            \n              σ\n              \n                B\n              \n              \n                (\n                k\n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            +\n            ϵ\n            \n              )\n              \n                −\n                3\n                \n                  /\n                \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial \\sigma _{B}^{(k)^{2}}}}=\\sum _{i=1}^{m}{\\frac {\\partial l}{\\partial y_{i}^{(k)}}}(x_{i}^{(k)}-\\mu _{B}^{(k)})\\left(-{\\frac {\\gamma ^{(k)}}{2}}(\\sigma _{B}^{(k)^{2}}+\\epsilon )^{-3/2}\\right)}\n  , \n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                μ\n                \n                  B\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                y\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        \n          \n            \n              −\n              \n                γ\n                \n                  (\n                  k\n                  )\n                \n              \n            \n            \n              \n                σ\n                \n                  B\n                \n                \n                  (\n                  k\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n              +\n              ϵ\n            \n          \n        \n        +\n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                σ\n                \n                  B\n                \n                \n                  (\n                  k\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n          \n            1\n            m\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        (\n        −\n        2\n        )\n        ⋅\n        (\n        \n          x\n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n        −\n        \n          μ\n          \n            B\n          \n          \n            (\n            k\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial \\mu _{B}^{(k)}}}=\\sum _{i=1}^{m}{\\frac {\\partial l}{\\partial y_{i}^{(k)}}}{\\frac {-\\gamma ^{(k)}}{\\sqrt {\\sigma _{B}^{(k)^{2}}+\\epsilon }}}+{\\frac {\\partial l}{\\partial \\sigma _{B}^{(k)^{2}}}}{\\frac {1}{m}}\\sum _{i=1}^{m}(-2)\\cdot (x_{i}^{(k)}-\\mu _{B}^{(k)})}\n  ,\nand \n  \n    \n      \n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                x\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                \n                  \n                    \n                      x\n                      ^\n                    \n                  \n                \n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                σ\n                \n                  B\n                \n                \n                  (\n                  k\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n              +\n              ϵ\n            \n          \n        \n        +\n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                σ\n                \n                  B\n                \n                \n                  (\n                  k\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n              2\n              (\n              \n                x\n                \n                  i\n                \n                \n                  (\n                  k\n                  )\n                \n              \n              −\n              \n                μ\n                \n                  B\n                \n                \n                  (\n                  k\n                  )\n                \n              \n              )\n            \n            m\n          \n        \n        +\n        \n          \n            \n              ∂\n              l\n            \n            \n              ∂\n              \n                μ\n                \n                  B\n                \n                \n                  (\n                  k\n                  )\n                \n              \n            \n          \n        \n        \n          \n            1\n            m\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial l}{\\partial x_{i}^{(k)}}}={\\frac {\\partial l}{\\partial {\\hat {x}}_{i}^{(k)}}}{\\frac {1}{\\sqrt {\\sigma _{B}^{(k)^{2}}+\\epsilon }}}+{\\frac {\\partial l}{\\partial \\sigma _{B}^{(k)^{2}}}}{\\frac {2(x_{i}^{(k)}-\\mu _{B}^{(k)})}{m}}+{\\frac {\\partial l}{\\partial \\mu _{B}^{(k)}}}{\\frac {1}{m}}}\n  .\n\nInference\nDuring the training stage, the normalization steps depend on the mini-batches to ensure efficient and reliable training. However, in the inference stage, this dependence is not useful any more. Instead, the normalization step in this stage is computed with the population statistics such that the output could depend on the input in a deterministic manner. The population mean, \n  \n    \n      \n        E\n        [\n        \n          x\n          \n            (\n            k\n            )\n          \n        \n        ]\n      \n    \n    {\\displaystyle E[x^{(k)}]}\n  , and variance, \n  \n    \n      \n        Var\n        ⁡\n        [\n        \n          x\n          \n            (\n            k\n            )\n          \n        \n        ]\n      \n    \n    {\\displaystyle \\operatorname {Var} [x^{(k)}]}\n  , are computed as:\n\n  \n    \n      \n        E\n        [\n        \n          x\n          \n            (\n            k\n            )\n          \n        \n        ]\n        =\n        \n          E\n          \n            B\n          \n        \n        [\n        \n          μ\n          \n            B\n          \n          \n            (\n            k\n            )\n          \n        \n        ]\n      \n    \n    {\\displaystyle E[x^{(k)}]=E_{B}[\\mu _{B}^{(k)}]}\n  , and \n  \n    \n      \n        Var\n        ⁡\n        [\n        \n          x\n          \n            (\n            k\n            )\n          \n        \n        ]\n        =\n        \n          \n            m\n            \n              m\n              −\n              1\n            \n          \n        \n        \n          E\n          \n            B\n          \n        \n        [\n        \n          \n            (\n            \n              σ\n              \n                B\n              \n              \n                (\n                k\n                )\n              \n            \n            )\n          \n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle \\operatorname {Var} [x^{(k)}]={\\frac {m}{m-1}}E_{B}[\\left(\\sigma _{B}^{(k)}\\right)^{2}]}\n  .\nThe population statistics thus is a complete representation of the mini-batches.\nThe BN transform in the inference step thus becomes\n\n  \n    \n      \n        \n          y\n          \n            (\n            k\n            )\n          \n        \n        =\n        B\n        \n          N\n          \n            \n              γ\n              \n                (\n                k\n                )\n              \n            \n            ,\n            \n              β\n              \n                (\n                k\n                )\n              \n            \n          \n          \n            inf\n          \n        \n        (\n        \n          x\n          \n            (\n            k\n            )\n          \n        \n        )\n        =\n        \n          γ\n          \n            (\n            k\n            )\n          \n        \n        \n          \n            \n              \n                x\n                \n                  (\n                  k\n                  )\n                \n              \n              −\n              E\n              [\n              \n                x\n                \n                  (\n                  k\n                  )\n                \n              \n              ]\n            \n            \n              Var\n              ⁡\n              [\n              \n                x\n                \n                  (\n                  k\n                  )\n                \n              \n              ]\n              +\n              ϵ\n            \n          \n        \n        +\n        \n          β\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle y^{(k)}=BN_{\\gamma ^{(k)},\\beta ^{(k)}}^{\\text{inf}}(x^{(k)})=\\gamma ^{(k)}{\\frac {x^{(k)}-E[x^{(k)}]}{\\sqrt {\\operatorname {Var} [x^{(k)}]+\\epsilon }}}+\\beta ^{(k)}}\n  ,\nwhere \n  \n    \n      \n        \n          y\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle y^{(k)}}\n   is passed on to future layers instead of \n  \n    \n      x\n      \n        (\n        k\n        )\n      \n    \n    x^{(k)}\n  . Since the parameters are fixed in this transformation, the batch normalization procedure is essentially applying a linear transform to the activation.\n\nTheoretical Understanding\nAlthough batch normalization has become popular due to its strong empirical performance, the working mechanism of the method is not yet well-understood. The explanation made in the original paper was that batch norm works by reducing internal covariate shift, but this has been challenged by more recent work. One experiment trained a VGG-16 network under 3 different training regimes: standard (no batch norm), batch norm, and batch norm with noise added to each layer during training. In the third model, the noise has non-zero mean and non-unit variance, i.e. it explicitly introduces covariate shift. Despite this, it showed similar accuracy to the second model, and both performed better than the first, suggesting that covariate shift is not the reason that batch norm improves performance.\n\nSmoothness\nOne alternative explanation, is that the improvement with batch normalization is instead due to it producing a smoother parameter space and smoother gradients, as formalized by a smaller Lipschitz constant. \nConsider two identical networks, one contains batch normalization layers and the other doesn't, the behaviors of these two networks are then compared. Denote the loss functions as \n  \n    \n      \n        \n          L\n          ^\n        \n      \n    \n    {\\hat {L}}\n   and \n  \n    L\n    L\n  , respectively. Let the input to both networks be \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , and the output be \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  , for which \n  \n    \n      \n        y\n        =\n        W\n        x\n      \n    \n    {\\displaystyle y=Wx}\n  , where \n  \n    W\n    W\n   is the layer weights. For the second network, \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   additionally goes through a batch normalization layer. Denote the normalized activation as \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}}\n  , which has zero mean and unit variance. Let the transformed activation be \n  \n    \n      \n        z\n        =\n        γ\n        \n          \n            \n              y\n              ^\n            \n          \n        \n        +\n        β\n      \n    \n    {\\displaystyle z=\\gamma {\\hat {y}}+\\beta }\n  , and suppose \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n   and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n   are constants. Finally, denote the standard deviation over a mini-batch \n  \n    \n      \n        \n          \n            \n              \n                y\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\hat {y_{j}}}\\in \\mathbb {R} ^{m}}\n   as \n  \n    \n      \n        \n          σ\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{j}}\n  .\nFirst, it can be shown that the gradient magnitude of a batch normalized network, \n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            \n              y\n              \n                i\n              \n            \n          \n        \n        \n          \n            \n              L\n              ^\n            \n          \n        \n        \n          |\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle ||\\triangledown _{y_{i}}{\\hat {L}}||}\n  , is bounded, with the bound expressed as\n\n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            \n              y\n              \n                i\n              \n            \n          \n        \n        \n          \n            \n              L\n              ^\n            \n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        ≤\n        \n          \n            \n              γ\n              \n                2\n              \n            \n            \n              σ\n              \n                j\n              \n              \n                2\n              \n            \n          \n        \n        \n          \n            (\n          \n        \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            \n              y\n              \n                i\n              \n            \n          \n        \n        L\n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        −\n        \n          \n            1\n            m\n          \n        \n        ⟨\n        1\n        ,\n        \n          ▽\n          \n            \n              y\n              \n                i\n              \n            \n          \n        \n        L\n        \n          ⟩\n          \n            2\n          \n        \n        −\n        \n          \n            1\n            m\n          \n        \n        ⟨\n        \n          ▽\n          \n            \n              y\n              \n                i\n              \n            \n          \n        \n        L\n        ,\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        \n          ⟩\n          \n            2\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle ||\\triangledown _{y_{i}}{\\hat {L}}||^{2}\\leq {\\frac {\\gamma ^{2}}{\\sigma _{j}^{2}}}{\\Bigg (}||\\triangledown _{y_{i}}L||^{2}-{\\frac {1}{m}}\\langle 1,\\triangledown _{y_{i}}L\\rangle ^{2}-{\\frac {1}{m}}\\langle \\triangledown _{y_{i}}L,{\\hat {y}}_{j}\\rangle ^{2}{\\bigg )}}\n  .\nSince the gradient magnitude represents the Lipschitzness of the loss, this relationship indicates that a batch normalized network could achieve greater Lipschitzness comparatively. Notice that the bound gets tighter when the gradient \n  \n    \n      \n        \n          ▽\n          \n            \n              y\n              \n                i\n              \n            \n          \n        \n        \n          \n            \n              L\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\triangledown _{y_{i}}{\\hat {L}}}\n   correlates with the activation \n  \n    \n      \n        \n          \n            \n              \n                y\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {y_{i}}}}\n  , which is a common phenomena. The scaling of \n  \n    \n      \n        \n          \n            \n              γ\n              \n                2\n              \n            \n            \n              σ\n              \n                j\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\gamma ^{2}}{\\sigma _{j}^{2}}}}\n   is also significant, since the variance is often large.\nSecondly, the quadratic form of the loss Hessian with respect to activation in the gradient direction can be bounded as\n\n  \n    \n      \n        (\n        \n          ▽\n          \n            \n              y\n              \n                j\n              \n            \n          \n        \n        \n          \n            \n              L\n              ^\n            \n          \n        \n        \n          )\n          \n            T\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  \n                    L\n                    ^\n                  \n                \n              \n            \n            \n              ∂\n              \n                y\n                \n                  j\n                \n              \n              ∂\n              \n                y\n                \n                  j\n                \n              \n            \n          \n        \n        (\n        \n          ▽\n          \n            \n              y\n              \n                j\n              \n            \n          \n        \n        \n          \n            \n              L\n              ^\n            \n          \n        \n        )\n        ≤\n        \n          \n            \n              γ\n              \n                2\n              \n            \n            \n              σ\n              \n                2\n              \n            \n          \n        \n        \n          \n            (\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  \n                    L\n                    ^\n                  \n                \n              \n            \n            \n              ∂\n              \n                y\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          \n            (\n          \n        \n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                y\n                \n                  j\n                \n              \n              ∂\n              \n                y\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        \n          \n            (\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  \n                    L\n                    ^\n                  \n                \n              \n            \n            \n              ∂\n              \n                y\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        −\n        \n          \n            γ\n            \n              m\n              \n                σ\n                \n                  2\n                \n              \n            \n          \n        \n        ⟨\n        \n          ▽\n          \n            \n              y\n              \n                j\n              \n            \n          \n        \n        L\n        ,\n        \n          \n            \n              \n                y\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        ⟩\n        \n          \n            |\n          \n        \n        \n          \n            |\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  \n                    L\n                    ^\n                  \n                \n              \n            \n            \n              ∂\n              \n                y\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            |\n          \n        \n        \n          \n            \n              |\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (\\triangledown _{y_{j}}{\\hat {L}})^{T}{\\frac {\\partial {\\hat {L}}}{\\partial y_{j}\\partial y_{j}}}(\\triangledown _{y_{j}}{\\hat {L}})\\leq {\\frac {\\gamma ^{2}}{\\sigma ^{2}}}{\\bigg (}{\\frac {\\partial {\\hat {L}}}{\\partial y_{j}}}{\\bigg )}^{T}{\\bigg (}{\\frac {\\partial L}{\\partial y_{j}\\partial y_{j}}}{\\bigg )}{\\bigg (}{\\frac {\\partial {\\hat {L}}}{\\partial y_{j}}}{\\bigg )}-{\\frac {\\gamma }{m\\sigma ^{2}}}\\langle \\triangledown _{y_{j}}L,{\\hat {y_{j}}}\\rangle {\\bigg |}{\\bigg |}{\\frac {\\partial {\\hat {L}}}{\\partial y_{j}}}{\\bigg |}{\\bigg |}^{2}}\n  .\nThe scaling of \n  \n    \n      \n        \n          \n            \n              γ\n              \n                2\n              \n            \n            \n              σ\n              \n                j\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\gamma ^{2}}{\\sigma _{j}^{2}}}}\n   indicates that the loss Hessian is resilient to the mini-batch variance, whereas the second term on the right hand side suggests that it becomes smoother when the Hessian and the inner product are non-negative. If the loss is locally convex, then the Hessian is positive semi-definite, while the inner product is positive if \n  \n    \n      \n        \n          \n            \n              \n                g\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {g_{j}}}}\n   is in the direction towards the minimum of the loss. It could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer.\nIt then follows to translate the bounds related to the loss with respect to the normalized activation to a bound on the loss with respect to the network weights:\n\n  \n    \n      \n        \n          \n            \n              \n                g\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        ≤\n        \n          \n            \n              γ\n              \n                2\n              \n            \n            \n              σ\n              \n                j\n              \n              \n                2\n              \n            \n          \n        \n        (\n        \n          g\n          \n            j\n          \n          \n            2\n          \n        \n        −\n        m\n        \n          μ\n          \n            \n              g\n              \n                j\n              \n            \n          \n          \n            2\n          \n        \n        −\n        \n          λ\n          \n            2\n          \n        \n        ⟨\n        \n          ▽\n          \n            \n              y\n              \n                j\n              \n            \n          \n        \n        L\n        ,\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        \n          ⟩\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\hat {g_{j}}}\\leq {\\frac {\\gamma ^{2}}{\\sigma _{j}^{2}}}(g_{j}^{2}-m\\mu _{g_{j}}^{2}-\\lambda ^{2}\\langle \\triangledown _{y_{j}}L,{\\hat {y}}_{j}\\rangle ^{2})}\n  , where \n  \n    \n      \n        \n          g\n          \n            j\n          \n        \n        =\n        m\n        a\n        \n          x\n          \n            \n              |\n            \n            \n              |\n            \n            X\n            \n              |\n            \n            \n              |\n            \n            ≤\n            λ\n          \n        \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            W\n          \n        \n        L\n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle g_{j}=max_{||X||\\leq \\lambda }||\\triangledown _{W}L||^{2}}\n   and \n  \n    \n      \n        \n          \n            \n              \n                g\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        =\n        m\n        a\n        \n          x\n          \n            \n              |\n            \n            \n              |\n            \n            X\n            \n              |\n            \n            \n              |\n            \n            ≤\n            λ\n          \n        \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            W\n          \n        \n        \n          \n            \n              L\n              ^\n            \n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\hat {g}}_{j}=max_{||X||\\leq \\lambda }||\\triangledown _{W}{\\hat {L}}||^{2}}\n  .\nIn addition to the smoother landscape, it is further shown that batch normalization could result in a better initialization with the following inequality:\n\n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        \n          W\n          \n            0\n          \n        \n        −\n        \n          \n            \n              \n                W\n                ^\n              \n            \n          \n          \n            ∗\n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        ≤\n        \n          |\n        \n        \n          |\n        \n        \n          W\n          \n            0\n          \n        \n        −\n        \n          W\n          \n            ∗\n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        −\n        \n          \n            1\n            \n              \n                |\n              \n              \n                |\n              \n              \n                W\n                \n                  ∗\n                \n              \n              \n                |\n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n        (\n        \n          |\n        \n        \n          |\n        \n        \n          W\n          \n            ∗\n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        −\n        ⟨\n        \n          W\n          \n            ∗\n          \n        \n        ,\n        \n          W\n          \n            0\n          \n        \n        ⟩\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle ||W_{0}-{\\hat {W}}^{*}||^{2}\\leq ||W_{0}-W^{*}||^{2}-{\\frac {1}{||W^{*}||^{2}}}(||W^{*}||^{2}-\\langle W^{*},W_{0}\\rangle )^{2}}\n  , where \n  \n    \n      \n        \n          W\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle W^{*}}\n   and \n  \n    \n      \n        \n          \n            \n              \n                W\n                ^\n              \n            \n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle {\\hat {W}}^{*}}\n   are the local optimal weights for the two networks, respectively.\nSome scholars argue that the above analysis cannot fully capture the performance of batch normalization, because the proof only concerns the largest eigenvalue, or equivalently, one direction in the landscape at all points. It is suggested that the complete eigenspectrum needs to be taken into account to make a conclusive analysis.\n\nMeasure\nSince it is hypothesized that batch normalization layers could reduce internal covariate shift, an experiment is set up to measure quantitatively how much covariate shift is reduced. First, the notion of internal covariate shift needs to be defined mathematically. Specifically, to quantify the adjustment that a layer's parameters make in response to updates in previous layers, the correlation between the gradients of the loss before and after all previous layers are updated is measured, since gradients could capture the shifts from the first-order training method. If the shift introduced by the changes in previous layers is small, then the correlation between the gradients would be close to 1.\nThe correlation between the gradients are computed for four models: a standard VGG network, a VGG network with batch normalization layers, a 25-layer deep linear network (DLN) trained with full-batch gradient descent, and a DLN network with batch normalization layers. Interestingly, it is shown that the standard VGG and DLN models both have higher correlations of gradients compared with their counterparts, indicating that the additional batch normalization layers are not reducing internal covariate shift.\n\nVanishing/exploding gradients\nEven though batchnorm was originally introduced to alleviate gradient vanishing or explosion problems, a deep batchnorm network in fact suffers from gradient explosion at initialization time, no matter what it uses for nonlinearity. Thus the optimization landscape is very far from smooth for a randomly initialized, deep batchnorm network.\nMore precisely, if the network has \n  \n    L\n    L\n   layers, then the gradient of the first layer weights has norm \n  \n    \n      \n        >\n        c\n        \n          λ\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle >c\\lambda ^{L}}\n   for some \n  \n    \n      \n        λ\n        >\n        1\n        ,\n        c\n        >\n        0\n      \n    \n    {\\displaystyle \\lambda >1,c>0}\n   depending only on the nonlinearity.\nFor any fixed nonlinearity, \n  \n    λ\n    \\lambda\n   decreases as the batch size increases. For example, for ReLU, \n  \n    λ\n    \\lambda\n   decreases to \n  \n    \n      \n        π\n        \n          /\n        \n        (\n        π\n        −\n        1\n        )\n        ≈\n        1.467\n      \n    \n    {\\displaystyle \\pi /(\\pi -1)\\approx 1.467}\n   as the batch size tends to infinity.\nPractically, this means deep batchnorm networks are untrainable.\nThis is only relieved by skip connections in the fashion of residual networks.This gradient explosion on the surface contradicts the smoothness property explained in the previous section, but in fact they are consistent. The previous section studies the effect of inserting a single batchnorm in a network, while the gradient explosion depends on stacking batchnorms typical of modern deep neural networks.\n\nDecoupling\nAnother possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training.\nBy interpreting batch norm as a reparametrization of weight space, it can be shown that the length and the direction of the weights are separated and can thus be trained separately. For a particular neural network unit with input \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and weight vector \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  , denote its output as \n  \n    \n      \n        f\n        (\n        w\n        )\n        =\n        \n          E\n          \n            x\n          \n        \n        [\n        ϕ\n        (\n        \n          x\n          \n            T\n          \n        \n        w\n        )\n        ]\n      \n    \n    {\\displaystyle f(w)=E_{x}[\\phi (x^{T}w)]}\n  , where \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n   is the activation function, and denote \n  \n    \n      \n        S\n        =\n        E\n        [\n        x\n        \n          x\n          \n            T\n          \n        \n        ]\n      \n    \n    {\\displaystyle S=E[xx^{T}]}\n  . Assume that \n  \n    \n      \n        E\n        [\n        x\n        ]\n        =\n        0\n      \n    \n    {\\displaystyle E[x]=0}\n  , and that the spectrum of the matrix \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   is bounded as \n  \n    \n      \n        0\n        <\n        μ\n        =\n        \n          λ\n          \n            m\n            i\n            n\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle 0<\\mu =\\lambda _{min}(S)}\n  , \n  \n    \n      \n        L\n        =\n        \n          λ\n          \n            m\n            a\n            x\n          \n        \n        (\n        S\n        )\n        <\n        ∞\n      \n    \n    {\\displaystyle L=\\lambda _{max}(S)<\\infty }\n  , such that \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   is symmetric positive definite. Adding batch normalization to this unit thus results in\n\n  \n    \n      \n        \n          f\n          \n            B\n            N\n          \n        \n        (\n        w\n        ,\n        γ\n        ,\n        β\n        )\n        =\n        \n          E\n          \n            x\n          \n        \n        [\n        ϕ\n        (\n        B\n        N\n        (\n        \n          x\n          \n            T\n          \n        \n        w\n        )\n        )\n        ]\n        =\n        \n          E\n          \n            x\n          \n        \n        \n          \n            [\n          \n        \n        ϕ\n        \n          \n            (\n          \n        \n        γ\n        (\n        \n          \n            \n              \n                x\n                \n                  T\n                \n              \n              w\n              −\n              \n                E\n                \n                  x\n                \n              \n              [\n              \n                x\n                \n                  T\n                \n              \n              w\n              ]\n            \n            \n              v\n              a\n              \n                r\n                \n                  x\n                \n              \n              [\n              \n                x\n                \n                  T\n                \n              \n              w\n              \n                ]\n                \n                  1\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        )\n        +\n        β\n        \n          \n            )\n          \n        \n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle f_{BN}(w,\\gamma ,\\beta )=E_{x}[\\phi (BN(x^{T}w))]=E_{x}{\\bigg [}\\phi {\\bigg (}\\gamma ({\\frac {x^{T}w-E_{x}[x^{T}w]}{var_{x}[x^{T}w]^{1/2}}})+\\beta {\\bigg )}{\\bigg ]}}\n  , by definition.\nThe variance term can be simplified such that \n  \n    \n      \n        v\n        a\n        \n          r\n          \n            x\n          \n        \n        [\n        \n          x\n          \n            T\n          \n        \n        w\n        ]\n        =\n        \n          w\n          \n            T\n          \n        \n        S\n        w\n      \n    \n    {\\displaystyle var_{x}[x^{T}w]=w^{T}Sw}\n  . Assume that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   has zero mean and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n   can be omitted, then it follows that\n\n  \n    \n      \n        \n          f\n          \n            B\n            N\n          \n        \n        (\n        w\n        ,\n        γ\n        )\n        =\n        \n          E\n          \n            x\n          \n        \n        \n          \n            [\n          \n        \n        ϕ\n        \n          \n            (\n          \n        \n        γ\n        \n          \n            \n              \n                x\n                \n                  T\n                \n              \n              w\n            \n            \n              (\n              \n                w\n                \n                  T\n                \n              \n              S\n              w\n              \n                )\n                \n                  1\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle f_{BN}(w,\\gamma )=E_{x}{\\bigg [}\\phi {\\bigg (}\\gamma {\\frac {x^{T}w}{(w^{T}Sw)^{1/2}}}{\\bigg )}{\\bigg ]}}\n  , where \n  \n    \n      \n        (\n        \n          w\n          \n            T\n          \n        \n        S\n        w\n        \n          )\n          \n            \n              1\n              2\n            \n          \n        \n      \n    \n    {\\displaystyle (w^{T}Sw)^{\\frac {1}{2}}}\n   is the induced norm of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  , \n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        w\n        \n          |\n        \n        \n          \n            |\n          \n          \n            s\n          \n        \n      \n    \n    {\\displaystyle ||w||_{s}}\n  .\nHence, it could be concluded that \n  \n    \n      \n        \n          f\n          \n            B\n            N\n          \n        \n        (\n        w\n        ,\n        γ\n        )\n        =\n        \n          E\n          \n            x\n          \n        \n        [\n        ϕ\n        (\n        \n          x\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle f_{BN}(w,\\gamma )=E_{x}[\\phi (x^{T}{\\tilde {w}})]}\n  , where \n  \n    \n      \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        =\n        γ\n        \n          \n            w\n            \n              \n                |\n              \n              \n                |\n              \n              w\n              \n                |\n              \n              \n                \n                  |\n                \n                \n                  s\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}=\\gamma {\\frac {w}{||w||_{s}}}}\n  , and \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n   and \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   accounts for its length and direction separately. This property could then be used to prove the faster convergence of problems with batch normalization.\n\nLinear convergence\nLeast-square problem\nWith the reparametrization interpretation, it could then be proved that applying batch normalization to the ordinary least squares problem achieves a linear convergence rate in gradient descent, which is faster than the regular gradient descent with only sub-linear convergence.\nDenote the objective of minimizing an ordinary least squares problem as\n\n  \n    \n      \n        m\n        i\n        \n          n\n          \n            \n              \n                \n                  w\n                  ~\n                \n              \n            \n            ∈\n            \n              R\n              \n                d\n              \n            \n          \n        \n        \n          f\n          \n            O\n            L\n            S\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        =\n        m\n        i\n        \n          n\n          \n            \n              \n                \n                  w\n                  ~\n                \n              \n            \n            ∈\n            \n              R\n              \n                d\n              \n            \n          \n        \n        (\n        \n          E\n          \n            x\n            ,\n            y\n          \n        \n        [\n        (\n        y\n        −\n        \n          x\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        ]\n        )\n        =\n        m\n        i\n        \n          n\n          \n            \n              \n                \n                  w\n                  ~\n                \n              \n            \n            ∈\n            \n              R\n              \n                d\n              \n            \n          \n        \n        (\n        2\n        \n          u\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        +\n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            T\n          \n        \n        S\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle min_{{\\tilde {w}}\\in R^{d}}f_{OLS}({\\tilde {w}})=min_{{\\tilde {w}}\\in R^{d}}(E_{x,y}[(y-x^{T}{\\tilde {w}})^{2}])=min_{{\\tilde {w}}\\in R^{d}}(2u^{T}{\\tilde {w}}+{\\tilde {w}}^{T}S{\\tilde {w}})}\n  , where \n  \n    \n      \n        u\n        =\n        E\n        [\n        −\n        y\n        x\n        ]\n      \n    \n    {\\displaystyle u=E[-yx]}\n  .\nSince \n  \n    \n      \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        =\n        γ\n        \n          \n            w\n            \n              \n                |\n              \n              \n                |\n              \n              w\n              \n                |\n              \n              \n                \n                  |\n                \n                \n                  s\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}=\\gamma {\\frac {w}{||w||_{s}}}}\n  , the objective thus becomes\n\n  \n    \n      \n        m\n        i\n        \n          n\n          \n            w\n            ∈\n            \n              R\n              \n                d\n              \n            \n            ∖\n            {\n            0\n            }\n            ,\n            γ\n            ∈\n            R\n          \n        \n        \n          f\n          \n            O\n            L\n            S\n          \n        \n        (\n        w\n        ,\n        γ\n        )\n        =\n        m\n        i\n        \n          n\n          \n            w\n            ∈\n            \n              R\n              \n                d\n              \n            \n            ∖\n            {\n            0\n            }\n            ,\n            γ\n            ∈\n            R\n          \n        \n        \n          \n            (\n          \n        \n        2\n        γ\n        \n          \n            \n              \n                u\n                \n                  T\n                \n              \n              w\n            \n            \n              \n                |\n              \n              \n                |\n              \n              w\n              \n                |\n              \n              \n                \n                  |\n                \n                \n                  S\n                \n              \n              +\n              \n                γ\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle min_{w\\in R^{d}\\backslash \\{0\\},\\gamma \\in R}f_{OLS}(w,\\gamma )=min_{w\\in R^{d}\\backslash \\{0\\},\\gamma \\in R}{\\bigg (}2\\gamma {\\frac {u^{T}w}{||w||_{S}+\\gamma ^{2}}}{\\bigg )}}\n  , where 0 is excluded to avoid 0 in the denominator.\nSince the objective is convex with respect to \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  , its optimal value could be calculated by setting the partial derivative of the objective against \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n   to 0. The objective could be further simplified to be\n\n  \n    \n      \n        m\n        i\n        \n          n\n          \n            w\n            ∈\n            \n              R\n              \n                d\n              \n            \n            ∖\n            {\n            0\n            }\n          \n        \n        ρ\n        (\n        w\n        )\n        =\n        m\n        i\n        \n          n\n          \n            w\n            ∈\n            \n              R\n              \n                d\n              \n            \n            ∖\n            {\n            0\n            }\n          \n        \n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              \n                w\n                \n                  T\n                \n              \n              u\n              \n                u\n                \n                  T\n                \n              \n              w\n            \n            \n              \n                w\n                \n                  T\n                \n              \n              S\n              w\n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle min_{w\\in R^{d}\\backslash \\{0\\}}\\rho (w)=min_{w\\in R^{d}\\backslash \\{0\\}}{\\bigg (}-{\\frac {w^{T}uu^{T}w}{w^{T}Sw}}{\\bigg )}}\n  .\nNote that this objective is a form of the generalized Rayleigh quotient\n\n  \n    \n      \n        \n          \n            \n              ρ\n              ~\n            \n          \n        \n        (\n        w\n        )\n        =\n        \n          \n            \n              \n                w\n                \n                  T\n                \n              \n              B\n              w\n            \n            \n              \n                w\n                \n                  T\n                \n              \n              A\n              w\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {\\rho }}(w)={\\frac {w^{T}Bw}{w^{T}Aw}}}\n  , where \n  \n    \n      \n        B\n        ∈\n        \n          R\n          \n            d\n            ×\n            d\n          \n        \n      \n    \n    {\\displaystyle B\\in R^{d\\times d}}\n   is a symmetric matrix and \n  \n    \n      \n        A\n        ∈\n        \n          R\n          \n            d\n            ×\n            d\n          \n        \n      \n    \n    {\\displaystyle A\\in R^{d\\times d}}\n   is a symmetric positive definite matrix.\nIt is proven that the gradient descent convergence rate of the generalized Rayleigh quotient is\n\n  \n    \n      \n        \n          \n            \n              \n                λ\n                \n                  1\n                \n              \n              −\n              ρ\n              (\n              \n                w\n                \n                  t\n                  +\n                  1\n                \n              \n              )\n            \n            \n              ρ\n              (\n              \n                w\n                \n                  t\n                  +\n                  1\n                \n              \n              −\n              \n                λ\n                \n                  2\n                \n              \n              )\n            \n          \n        \n        ≤\n        \n          \n            (\n          \n        \n        1\n        −\n        \n          \n            \n              \n                λ\n                \n                  1\n                \n              \n              −\n              \n                λ\n                \n                  2\n                \n              \n            \n            \n              \n                λ\n                \n                  1\n                \n              \n              −\n              \n                λ\n                \n                  m\n                  i\n                  n\n                \n              \n            \n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n            t\n          \n        \n        \n          \n            \n              \n                λ\n                \n                  1\n                \n              \n              −\n              ρ\n              (\n              \n                w\n                \n                  t\n                \n              \n              )\n            \n            \n              ρ\n              (\n              \n                w\n                \n                  t\n                \n              \n              )\n              −\n              \n                λ\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\lambda _{1}-\\rho (w_{t+1})}{\\rho (w_{t+1}-\\lambda _{2})}}\\leq {\\bigg (}1-{\\frac {\\lambda _{1}-\\lambda _{2}}{\\lambda _{1}-\\lambda _{min}}}{\\bigg )}^{2t}{\\frac {\\lambda _{1}-\\rho (w_{t})}{\\rho (w_{t})-\\lambda _{2}}}}\n  , where \n  \n    \n      \n        \n          λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{1}}\n   is the largest eigenvalue of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , \n  \n    \n      \n        \n          λ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{2}}\n   is the second largest eigenvalue of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , and \n  \n    \n      \n        \n          λ\n          \n            m\n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{min}}\n   is the smallest eigenvalue of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  .In our case, \n  \n    \n      \n        B\n        =\n        u\n        \n          u\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle B=uu^{T}}\n  is a rank one matrix, and the convergence result can be simplified accordingly. Specifically, consider gradient descent steps of the form \n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          w\n          \n            t\n          \n        \n        −\n        \n          η\n          \n            t\n          \n        \n        ▽\n        ρ\n        (\n        \n          w\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle w_{t+1}=w_{t}-\\eta _{t}\\triangledown \\rho (w_{t})}\n   with step size \n  \n    \n      \n        \n          η\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                w\n                \n                  t\n                \n                \n                  T\n                \n              \n              S\n              \n                w\n                \n                  t\n                \n              \n            \n            \n              2\n              L\n              \n                |\n              \n              ρ\n              (\n              \n                w\n                \n                  t\n                \n              \n              )\n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\eta _{t}={\\frac {w_{t}^{T}Sw_{t}}{2L|\\rho (w_{t})|}}}\n  , and starting from \n  \n    \n      \n        ρ\n        (\n        \n          w\n          \n            0\n          \n        \n        )\n        ≠\n        0\n      \n    \n    {\\displaystyle \\rho (w_{0})\\neq 0}\n  , then\n\n  \n    \n      \n        ρ\n        (\n        \n          w\n          \n            t\n          \n        \n        )\n        −\n        ρ\n        (\n        \n          w\n          \n            ∗\n          \n        \n        )\n        ≤\n        \n          \n            (\n          \n        \n        1\n        −\n        \n          \n            μ\n            L\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n            t\n          \n        \n        (\n        ρ\n        (\n        \n          w\n          \n            0\n          \n        \n        )\n        −\n        ρ\n        (\n        \n          w\n          \n            ∗\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\rho (w_{t})-\\rho (w^{*})\\leq {\\bigg (}1-{\\frac {\\mu }{L}}{\\bigg )}^{2t}(\\rho (w_{0})-\\rho (w^{*}))}\n  .\n\nLearning halfspace problem\nThe problem of learning halfspaces refers to the training of the Perceptron, which is the simplest form of neural network. The optimization problem in this case is\n\n  \n    \n      \n        m\n        i\n        \n          n\n          \n            \n              \n                \n                  w\n                  ~\n                \n              \n            \n            ∈\n            \n              R\n              \n                d\n              \n            \n          \n        \n        \n          f\n          \n            L\n            H\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        =\n        \n          E\n          \n            y\n            ,\n            x\n          \n        \n        [\n        ϕ\n        (\n        \n          z\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle min_{{\\tilde {w}}\\in R^{d}}f_{LH}({\\tilde {w}})=E_{y,x}[\\phi (z^{T}{\\tilde {w}})]}\n  , where \n  \n    \n      \n        z\n        =\n        −\n        y\n        x\n      \n    \n    {\\displaystyle z=-yx}\n   and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n   is an arbitrary loss function.\nSuppose that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n   is infinitely differentiable and has a bounded derivative. Assume that the objective function \n  \n    \n      \n        \n          f\n          \n            L\n            H\n          \n        \n      \n    \n    {\\displaystyle f_{LH}}\n   is \n  \n    \n      \n        ζ\n      \n    \n    {\\displaystyle \\zeta }\n  -smooth, and that a solution \n  \n    \n      \n        \n          α\n          \n            ∗\n          \n        \n        =\n        a\n        r\n        g\n        m\n        i\n        \n          n\n          \n            α\n          \n        \n        \n          |\n        \n        \n          |\n        \n        ▽\n        f\n        (\n        α\n        w\n        )\n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\alpha ^{*}=argmin_{\\alpha }||\\triangledown f(\\alpha w)||^{2}}\n   exists and is bounded such that \n  \n    \n      \n        −\n        ∞\n        <\n        \n          α\n          \n            ∗\n          \n        \n        <\n        ∞\n      \n    \n    {\\displaystyle -\\infty <\\alpha ^{*}<\\infty }\n  . Also assume \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n   is a multivariate normal random variable. With the Gaussian assumption, it can be shown that all critical points lie on the same line, for any choice of loss function \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  . Specifically, the gradient of \n  \n    \n      \n        \n          f\n          \n            L\n            H\n          \n        \n      \n    \n    {\\displaystyle f_{LH}}\n   could be represented as\n\n  \n    \n      \n        \n          ▽\n          \n            \n              \n                w\n                ~\n              \n            \n          \n        \n        \n          f\n          \n            L\n            H\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        =\n        \n          c\n          \n            1\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        u\n        +\n        \n          c\n          \n            2\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        S\n        \n          \n            \n              w\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle \\triangledown _{\\tilde {w}}f_{LH}({\\tilde {w}})=c_{1}({\\tilde {w}})u+c_{2}({\\tilde {w}})S{\\tilde {w}}}\n  , where  \n  \n    \n      \n        \n          c\n          \n            1\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        =\n        \n          E\n          \n            z\n          \n        \n        [\n        \n          ϕ\n          \n            (\n            1\n            )\n          \n        \n        (\n        \n          z\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        ]\n        −\n        \n          E\n          \n            z\n          \n        \n        [\n        \n          ϕ\n          \n            (\n            2\n            )\n          \n        \n        (\n        \n          z\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        ]\n        (\n        \n          u\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle c_{1}({\\tilde {w}})=E_{z}[\\phi ^{(1)}(z^{T}{\\tilde {w}})]-E_{z}[\\phi ^{(2)}(z^{T}{\\tilde {w}})](u^{T}{\\tilde {w}})}\n  , \n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        (\n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        =\n        \n          E\n          \n            z\n          \n        \n        [\n        \n          ϕ\n          \n            (\n            2\n            )\n          \n        \n        (\n        \n          z\n          \n            T\n          \n        \n        \n          \n            \n              w\n              ~\n            \n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle c_{2}({\\tilde {w}})=E_{z}[\\phi ^{(2)}(z^{T}{\\tilde {w}})]}\n  , and \n  \n    \n      \n        \n          ϕ\n          \n            (\n            i\n            )\n          \n        \n      \n    \n    {\\displaystyle \\phi ^{(i)}}\n   is the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th derivative of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  .\nBy setting the gradient to 0, it thus follows that the bounded critical points \n  \n    \n      \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}_{*}}\n   can be expressed as \n  \n    \n      \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            ∗\n          \n        \n        =\n        \n          g\n          \n            ∗\n          \n        \n        \n          S\n          \n            −\n            1\n          \n        \n        u\n      \n    \n    {\\displaystyle {\\tilde {w}}_{*}=g_{*}S^{-1}u}\n  , where \n  \n    \n      \n        \n          g\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle g_{*}}\n   depends on \n  \n    \n      \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}_{*}}\n   and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  . Combining this global property with length-direction decoupling, it could thus be proved that this optimization problem converges linearly.\nFirst, a variation of gradient descent with batch normalization, Gradient Descent in Normalized Parameterization (GDNP), is designed for the objective function \n  \n    \n      \n        m\n        i\n        \n          n\n          \n            w\n            ∈\n            \n              R\n              \n                d\n              \n            \n            ∖\n            {\n            0\n            }\n            ,\n            γ\n            ∈\n            R\n          \n        \n        \n          f\n          \n            L\n            H\n          \n        \n        (\n        w\n        ,\n        γ\n        )\n      \n    \n    {\\displaystyle min_{w\\in R^{d}\\backslash \\{0\\},\\gamma \\in R}f_{LH}(w,\\gamma )}\n  , such that the direction and length of the weights are updated separately. Denote the stopping criterion of GDNP as\n\n  \n    \n      \n        h\n        (\n        \n          w\n          \n            t\n          \n        \n        ,\n        \n          γ\n          \n            t\n          \n        \n        )\n        =\n        \n          E\n          \n            z\n          \n        \n        [\n        \n          ϕ\n          ′\n        \n        (\n        \n          z\n          \n            T\n          \n        \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        )\n        ]\n        (\n        \n          u\n          \n            T\n          \n        \n        \n          w\n          \n            t\n          \n        \n        )\n        −\n        \n          E\n          \n            z\n          \n        \n        [\n        \n          ϕ\n          ″\n        \n        (\n        \n          z\n          \n            T\n          \n        \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        )\n        ]\n        (\n        \n          u\n          \n            T\n          \n        \n        \n          w\n          \n            t\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle h(w_{t},\\gamma _{t})=E_{z}[\\phi '(z^{T}{\\tilde {w}}_{t})](u^{T}w_{t})-E_{z}[\\phi ''(z^{T}{\\tilde {w}}_{t})](u^{T}w_{t})^{2}}\n  .\nLet the step size be\n\n  \n    \n      \n        \n          s\n          \n            t\n          \n        \n        =\n        s\n        (\n        \n          w\n          \n            t\n          \n        \n        ,\n        \n          γ\n          \n            t\n          \n        \n        )\n        =\n        −\n        \n          \n            \n              \n                |\n              \n              \n                |\n              \n              \n                w\n                \n                  t\n                \n              \n              \n                |\n              \n              \n                \n                  |\n                \n                \n                  S\n                \n                \n                  3\n                \n              \n            \n            \n              L\n              \n                g\n                \n                  t\n                \n              \n              h\n              (\n              \n                w\n                \n                  t\n                \n              \n              ,\n              \n                γ\n                \n                  t\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle s_{t}=s(w_{t},\\gamma _{t})=-{\\frac {||w_{t}||_{S}^{3}}{Lg_{t}h(w_{t},\\gamma _{t})}}}\n  .\nFor each step, if \n  \n    \n      \n        h\n        (\n        \n          w\n          \n            t\n          \n        \n        ,\n        \n          γ\n          \n            t\n          \n        \n        )\n        ≠\n        0\n      \n    \n    {\\displaystyle h(w_{t},\\gamma _{t})\\neq 0}\n  , then update the direction as\n\n  \n    \n      \n        \n          w\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          w\n          \n            t\n          \n        \n        −\n        \n          s\n          \n            t\n          \n        \n        \n          ▽\n          \n            w\n          \n        \n        f\n        (\n        \n          w\n          \n            t\n          \n        \n        ,\n        \n          γ\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle w_{t+1}=w_{t}-s_{t}\\triangledown _{w}f(w_{t},\\gamma _{t})}\n  .\nThen update the length according to\n\n  \n    \n      \n        \n          γ\n          \n            t\n          \n        \n        =\n        B\n        i\n        s\n        e\n        c\n        t\n        i\n        o\n        n\n        (\n        \n          T\n          \n            s\n          \n        \n        ,\n        f\n        ,\n        \n          w\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\gamma _{t}=Bisection(T_{s},f,w_{t})}\n  , where \n  \n    \n      \n        B\n        i\n        s\n        e\n        c\n        t\n        i\n        o\n        n\n        (\n        )\n      \n    \n    {\\displaystyle Bisection()}\n   is the classical bisection algorithm, and \n  \n    \n      \n        \n          T\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle T_{s}}\n   is the total iterations ran in the bisection step.\nDenote the total number of iterations as \n  \n    \n      \n        \n          T\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle T_{d}}\n  , then the final output of GDNP is\n\n  \n    \n      \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            \n              T\n              \n                d\n              \n            \n          \n        \n        =\n        \n          γ\n          \n            \n              T\n              \n                d\n              \n            \n          \n        \n        \n          \n            \n              w\n              \n                \n                  T\n                  \n                    d\n                  \n                \n              \n            \n            \n              \n                |\n              \n              \n                |\n              \n              \n                w\n                \n                  \n                    T\n                    \n                      d\n                    \n                  \n                \n              \n              \n                |\n              \n              \n                \n                  |\n                \n                \n                  S\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}_{T_{d}}=\\gamma _{T_{d}}{\\frac {w_{T_{d}}}{||w_{T_{d}}||_{S}}}}\n  .\nThe GDNP algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis.\nIt can be shown that in GDNP, the partial derivative of \n  \n    \n      \n        \n          f\n          \n            L\n            H\n          \n        \n      \n    \n    {\\displaystyle f_{LH}}\n  against the length component converges to zero at a linear rate, such that\n\n  \n    \n      \n        (\n        \n          ∂\n          \n            γ\n          \n        \n        \n          f\n          \n            L\n            H\n          \n        \n        (\n        \n          w\n          \n            t\n          \n        \n        ,\n        \n          a\n          \n            t\n          \n          \n            (\n            \n              T\n              \n                s\n              \n            \n            )\n          \n        \n        \n          )\n          \n            2\n          \n        \n        ≤\n        \n          \n            \n              \n                2\n                \n                  −\n                  \n                    T\n                    \n                      s\n                    \n                  \n                \n              \n              ζ\n              \n                |\n              \n              \n                b\n                \n                  t\n                \n                \n                  (\n                  0\n                  )\n                \n              \n              −\n              \n                a\n                \n                  t\n                \n                \n                  (\n                  0\n                  )\n                \n              \n              \n                |\n              \n            \n            \n              μ\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle (\\partial _{\\gamma }f_{LH}(w_{t},a_{t}^{(T_{s})})^{2}\\leq {\\frac {2^{-T_{s}}\\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\\mu ^{2}}}}\n  , where \n  \n    \n      \n        \n          a\n          \n            t\n          \n          \n            (\n            0\n            )\n          \n        \n      \n    \n    {\\displaystyle a_{t}^{(0)}}\n   and \n  \n    \n      \n        \n          b\n          \n            t\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle b_{t}^{0}}\n   are the two starting points of the bisection algorithm on the left and on the right, correspondingly.\nFurther, for each iteration, the norm of the gradient of \n  \n    \n      \n        \n          f\n          \n            L\n            H\n          \n        \n      \n    \n    {\\displaystyle f_{LH}}\n   with respect to \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   converges linearly, such that\n\n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        \n          w\n          \n            t\n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            S\n          \n          \n            2\n          \n        \n        \n          |\n        \n        \n          |\n        \n        ▽\n        \n          f\n          \n            L\n            H\n          \n        \n        (\n        \n          w\n          \n            t\n          \n        \n        ,\n        \n          g\n          \n            t\n          \n        \n        )\n        \n          |\n        \n        \n          \n            |\n          \n          \n            \n              S\n              \n                −\n                1\n              \n            \n          \n          \n            2\n          \n        \n        ≤\n        \n          \n            (\n          \n        \n        1\n        −\n        \n          \n            μ\n            L\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n            t\n          \n        \n        \n          Φ\n          \n            2\n          \n        \n        \n          γ\n          \n            t\n          \n          \n            2\n          \n        \n        (\n        ρ\n        (\n        \n          w\n          \n            0\n          \n        \n        )\n        −\n        \n          ρ\n          \n            ∗\n          \n        \n        )\n      \n    \n    {\\displaystyle ||w_{t}||_{S}^{2}||\\triangledown f_{LH}(w_{t},g_{t})||_{S^{-1}}^{2}\\leq {\\bigg (}1-{\\frac {\\mu }{L}}{\\bigg )}^{2t}\\Phi ^{2}\\gamma _{t}^{2}(\\rho (w_{0})-\\rho ^{*})}\n  .\nCombining these two inequalities, a bound could thus be obtained for the gradient with respect to \n  \n    \n      \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            \n              T\n              \n                d\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}_{T_{d}}}\n  :\n\n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            \n              \n                w\n                ~\n              \n            \n          \n        \n        f\n        (\n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            \n              T\n              \n                d\n              \n            \n          \n        \n        )\n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        ≤\n        \n          \n            (\n          \n        \n        1\n        −\n        \n          \n            μ\n            L\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n            \n              T\n              \n                d\n              \n            \n          \n        \n        \n          Φ\n          \n            2\n          \n        \n        (\n        ρ\n        (\n        \n          w\n          \n            0\n          \n        \n        )\n        −\n        \n          ρ\n          \n            ∗\n          \n        \n        )\n        +\n        \n          \n            \n              \n                2\n                \n                  −\n                  \n                    T\n                    \n                      s\n                    \n                  \n                \n              \n              ζ\n              \n                |\n              \n              \n                b\n                \n                  t\n                \n                \n                  (\n                  0\n                  )\n                \n              \n              −\n              \n                a\n                \n                  t\n                \n                \n                  (\n                  0\n                  )\n                \n              \n              \n                |\n              \n            \n            \n              μ\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle ||\\triangledown _{\\tilde {w}}f({\\tilde {w}}_{T_{d}})||^{2}\\leq {\\bigg (}1-{\\frac {\\mu }{L}}{\\bigg )}^{2T_{d}}\\Phi ^{2}(\\rho (w_{0})-\\rho ^{*})+{\\frac {2^{-T_{s}}\\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\\mu ^{2}}}}\n  , such that the algorithm is guaranteed to converge linearly.\nAlthough the proof stands on the assumption of Gaussian input, it is also shown in experiments that GDNP could accelerate optimization without this constraint.\n\nNeural networks\nConsider a multilayer perceptron (MLP) with one hidden layer and \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   hidden units with mapping from input \n  \n    \n      \n        x\n        ∈\n        \n          R\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle x\\in R^{d}}\n   to a scalar output described as\n\n  \n    \n      \n        \n          F\n          \n            x\n          \n        \n        (\n        \n          \n            \n              W\n              ~\n            \n          \n        \n        ,\n        Θ\n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          θ\n          \n            i\n          \n        \n        ϕ\n        (\n        \n          x\n          \n            T\n          \n        \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle F_{x}({\\tilde {W}},\\Theta )=\\sum _{i=1}^{m}\\theta _{i}\\phi (x^{T}{\\tilde {w}}^{(i)})}\n  , where \n  \n    \n      \n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {w}}^{(i)}}\n   and \n  \n    \n      \n        \n          θ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n   are the input and output weights of unit \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   correspondingly, and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n   is the activation function and is assumed to be a tanh function.\nThe input and output weights could then be optimized with\n\n  \n    \n      \n        m\n        i\n        \n          n\n          \n            \n              \n                \n                  W\n                  ~\n                \n              \n            \n            ,\n            Θ\n          \n        \n        (\n        \n          f\n          \n            N\n            N\n          \n        \n        (\n        \n          \n            \n              W\n              ~\n            \n          \n        \n        ,\n        Θ\n        )\n        =\n        \n          E\n          \n            y\n            ,\n            x\n          \n        \n        [\n        l\n        (\n        −\n        y\n        \n          F\n          \n            x\n          \n        \n        (\n        \n          \n            \n              W\n              ~\n            \n          \n        \n        ,\n        Θ\n        )\n        )\n        ]\n        )\n      \n    \n    {\\displaystyle min_{{\\tilde {W}},\\Theta }(f_{NN}({\\tilde {W}},\\Theta )=E_{y,x}[l(-yF_{x}({\\tilde {W}},\\Theta ))])}\n  , where \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   is a loss function, \n  \n    \n      \n        \n          \n            \n              W\n              ~\n            \n          \n        \n        =\n        {\n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            (\n            m\n            )\n          \n        \n        }\n      \n    \n    {\\displaystyle {\\tilde {W}}=\\{{\\tilde {w}}^{(1)},...,{\\tilde {w}}^{(m)}\\}}\n  , and \n  \n    \n      \n        Θ\n        =\n        {\n        \n          θ\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          θ\n          \n            (\n            m\n            )\n          \n        \n        }\n      \n    \n    {\\displaystyle \\Theta =\\{\\theta ^{(1)},...,\\theta ^{(m)}\\}}\n  .\nConsider fixed \n  \n    \n      \n        Θ\n      \n    \n    {\\displaystyle \\Theta }\n   and optimizing only \n  \n    \n      \n        \n          \n            \n              W\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {W}}}\n  , it can be shown that the critical points of \n  \n    \n      \n        \n          f\n          \n            N\n            N\n          \n        \n        (\n        \n          \n            \n              W\n              ~\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle f_{NN}({\\tilde {W}})}\n   of a particular hidden unit \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  , \n  \n    \n      \n        \n          \n            \n              \n                w\n                ^\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\hat {w}}^{(i)}}\n  , all align along one line depending on incoming information into the hidden layer, such that\n\n  \n    \n      \n        \n          \n            \n              \n                w\n                ^\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n        =\n        \n          \n            \n              \n                c\n                ^\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n        \n          S\n          \n            −\n            1\n          \n        \n        u\n      \n    \n    {\\displaystyle {\\hat {w}}^{(i)}={\\hat {c}}^{(i)}S^{-1}u}\n  , where \n  \n    \n      \n        \n          \n            \n              \n                c\n                ^\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n        ∈\n        R\n      \n    \n    {\\displaystyle {\\hat {c}}^{(i)}\\in R}\n   is a scalar, \n  \n    \n      \n        i\n        =\n        1\n        ,\n        .\n        .\n        .\n        ,\n        m\n      \n    \n    {\\displaystyle i=1,...,m}\n  .\nThis result could be proved by setting the gradient of \n  \n    \n      \n        \n          f\n          \n            N\n            N\n          \n        \n      \n    \n    {\\displaystyle f_{NN}}\n   to zero and solving the system of equations.\nApply the GDNP algorithm to this optimization problem by alternating optimization over the different hidden units. Specifically, for each hidden unit, run GDNP to find the optimal \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n   and \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  . With the same choice of stopping criterion and stepsize, it follows that\n\n  \n    \n      \n        \n          |\n        \n        \n          |\n        \n        \n          ▽\n          \n            \n              \n                \n                  \n                    w\n                    ~\n                  \n                \n              \n              \n                (\n                i\n                )\n              \n            \n          \n        \n        f\n        (\n        \n          \n            \n              \n                w\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            (\n            i\n            )\n          \n        \n        )\n        \n          |\n        \n        \n          \n            |\n          \n          \n            \n              S\n              \n                −\n                1\n              \n            \n          \n          \n            2\n          \n        \n        ≤\n        \n          \n            (\n          \n        \n        1\n        −\n        \n          \n            μ\n            L\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n            t\n          \n        \n        C\n        (\n        ρ\n        (\n        \n          w\n          \n            0\n          \n        \n        )\n        −\n        \n          ρ\n          \n            ∗\n          \n        \n        )\n        +\n        \n          \n            \n              \n                2\n                \n                  −\n                  \n                    T\n                    \n                      s\n                    \n                    \n                      (\n                      i\n                      )\n                    \n                  \n                \n              \n              ζ\n              \n                |\n              \n              \n                b\n                \n                  t\n                \n                \n                  (\n                  0\n                  )\n                \n              \n              −\n              \n                a\n                \n                  t\n                \n                \n                  (\n                  0\n                  )\n                \n              \n              \n                |\n              \n            \n            \n              μ\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle ||\\triangledown _{{\\tilde {w}}^{(i)}}f({\\tilde {w}}_{t}^{(i)})||_{S^{-1}}^{2}\\leq {\\bigg (}1-{\\frac {\\mu }{L}}{\\bigg )}^{2t}C(\\rho (w_{0})-\\rho ^{*})+{\\frac {2^{-T_{s}^{(i)}}\\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\\mu ^{2}}}}\n  .\nSince the parameters of each hidden unit converge linearly, the whole optimization problem has a linear rate of convergence.\n\nReferences\n\nIoffe, Sergey; Szegedy, Christian (2015). \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", ICML'15: Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, July 2015 Pages 448–456\nSimonyan, Karen; Zisserman, Andrew (2014). \"Very Deep Convolutional Networks for Large-Scale Image Recognition\". arXiv:1409.1556 [cs.CV].",
    "Bayesian network": "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\nEfficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\nGraphical model\nFormally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (no path connects one node to another) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if \n  \n    m\n    m\n   parent nodes represent \n  \n    m\n    m\n   Boolean variables, then the probability function could be represented by a table of \n  \n    \n      2\n      \n        m\n      \n    \n    2^{m}\n   entries, one entry for each of the \n  \n    \n      2\n      \n        m\n      \n    \n    2^{m}\n   possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.\n\nExample\nLet us use an illustration to enforce the concepts of a Bayesian network. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).\nThe joint probability function is, by the chain rule of probability,\n\n  \n    \n      \n        Pr\n        (\n        G\n        ,\n        S\n        ,\n        R\n        )\n        =\n        Pr\n        (\n        G\n        ∣\n        S\n        ,\n        R\n        )\n        Pr\n        (\n        S\n        ∣\n        R\n        )\n        Pr\n        (\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(G,S,R)=\\Pr(G\\mid S,R)\\Pr(S\\mid R)\\Pr(R)}\n  where G = \"Grass wet (true/false)\", S = \"Sprinkler turned on (true/false)\", and R = \"Raining (true/false)\".\nThe model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like \"What is the probability that it is raining, given the grass is wet?\" by using the conditional probability formula and summing over all nuisance variables:\n\n  \n    \n      \n        Pr\n        (\n        R\n        =\n        T\n        ∣\n        G\n        =\n        T\n        )\n        =\n        \n          \n            \n              Pr\n              (\n              G\n              =\n              T\n              ,\n              R\n              =\n              T\n              )\n            \n            \n              Pr\n              (\n              G\n              =\n              T\n              )\n            \n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  x\n                  ∈\n                  {\n                  T\n                  ,\n                  F\n                  }\n                \n              \n              Pr\n              (\n              G\n              =\n              T\n              ,\n              S\n              =\n              x\n              ,\n              R\n              =\n              T\n              )\n            \n            \n              \n                ∑\n                \n                  x\n                  ,\n                  y\n                  ∈\n                  {\n                  T\n                  ,\n                  F\n                  }\n                \n              \n              Pr\n              (\n              G\n              =\n              T\n              ,\n              S\n              =\n              x\n              ,\n              R\n              =\n              y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr(R=T\\mid G=T)={\\frac {\\Pr(G=T,R=T)}{\\Pr(G=T)}}={\\frac {\\sum _{x\\in \\{T,F\\}}\\Pr(G=T,S=x,R=T)}{\\sum _{x,y\\in \\{T,F\\}}\\Pr(G=T,S=x,R=y)}}}\n  Using the expansion for the joint probability function \n  \n    \n      \n        Pr\n        (\n        G\n        ,\n        S\n        ,\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(G,S,R)}\n   and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,\n\n  \n    \n      \n        \n          \n            \n              \n                Pr\n                (\n                G\n                =\n                T\n                ,\n                S\n                =\n                T\n                ,\n                R\n                =\n                T\n                )\n              \n              \n                \n                =\n                Pr\n                (\n                G\n                =\n                T\n                ∣\n                S\n                =\n                T\n                ,\n                R\n                =\n                T\n                )\n                Pr\n                (\n                S\n                =\n                T\n                ∣\n                R\n                =\n                T\n                )\n                Pr\n                (\n                R\n                =\n                T\n                )\n              \n            \n            \n              \n              \n                \n                =\n                0.99\n                ×\n                0.01\n                ×\n                0.2\n              \n            \n            \n              \n              \n                \n                =\n                0.00198.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Pr(G=T,S=T,R=T)&=\\Pr(G=T\\mid S=T,R=T)\\Pr(S=T\\mid R=T)\\Pr(R=T)\\\\&=0.99\\times 0.01\\times 0.2\\\\&=0.00198.\\end{aligned}}}\n  Then the numerical results (subscripted by the associated variable values) are\n\n  \n    \n      \n        Pr\n        (\n        R\n        =\n        T\n        ∣\n        G\n        =\n        T\n        )\n        =\n        \n          \n            \n              \n                0.00198\n                \n                  T\n                  T\n                  T\n                \n              \n              +\n              \n                0.1584\n                \n                  T\n                  F\n                  T\n                \n              \n            \n            \n              \n                0.00198\n                \n                  T\n                  T\n                  T\n                \n              \n              +\n              \n                0.288\n                \n                  T\n                  T\n                  F\n                \n              \n              +\n              \n                0.1584\n                \n                  T\n                  F\n                  T\n                \n              \n              +\n              \n                0.0\n                \n                  T\n                  F\n                  F\n                \n              \n            \n          \n        \n        =\n        \n          \n            891\n            2491\n          \n        \n        ≈\n        35.77\n        %\n        .\n      \n    \n    {\\displaystyle \\Pr(R=T\\mid G=T)={\\frac {0.00198_{TTT}+0.1584_{TFT}}{0.00198_{TTT}+0.288_{TTF}+0.1584_{TFT}+0.0_{TFF}}}={\\frac {891}{2491}}\\approx 35.77\\%.}\n  To answer an interventional question, such as \"What is the probability that it would rain, given that we wet the grass?\" the answer is governed by the post-intervention joint distribution function\n\n  \n    \n      \n        Pr\n        (\n        S\n        ,\n        R\n        ∣\n        \n          do\n        \n        (\n        G\n        =\n        T\n        )\n        )\n        =\n        Pr\n        (\n        S\n        ∣\n        R\n        )\n        Pr\n        (\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(S,R\\mid {\\text{do}}(G=T))=\\Pr(S\\mid R)\\Pr(R)}\n  obtained by removing the factor \n  \n    \n      \n        Pr\n        (\n        G\n        ∣\n        S\n        ,\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(G\\mid S,R)}\n   from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:\n\n  \n    \n      \n        Pr\n        (\n        R\n        ∣\n        \n          do\n        \n        (\n        G\n        =\n        T\n        )\n        )\n        =\n        Pr\n        (\n        R\n        )\n        .\n      \n    \n    {\\displaystyle \\Pr(R\\mid {\\text{do}}(G=T))=\\Pr(R).}\n  To predict the impact of turning the sprinkler on:\n\n  \n    \n      \n        Pr\n        (\n        R\n        ,\n        G\n        ∣\n        \n          do\n        \n        (\n        S\n        =\n        T\n        )\n        )\n        =\n        Pr\n        (\n        R\n        )\n        Pr\n        (\n        G\n        ∣\n        R\n        ,\n        S\n        =\n        T\n        )\n      \n    \n    {\\displaystyle \\Pr(R,G\\mid {\\text{do}}(S=T))=\\Pr(R)\\Pr(G\\mid R,S=T)}\n  with the term \n  \n    \n      \n        Pr\n        (\n        S\n        =\n        T\n        ∣\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(S=T\\mid R)}\n   removed, showing that the action affects the grass but not the rain.\nThese predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action \n  \n    \n      \n        do\n      \n      (\n      x\n      )\n    \n    {\\text{do}}(x)\n   can still be predicted, however, whenever the back-door criterion is satisfied. It states that, if a set Z of nodes can be observed that d-separates (or blocks) all back-door paths from X to Y then\n\n  \n    \n      \n        Pr\n        (\n        Y\n        ,\n        Z\n        ∣\n        \n          do\n        \n        (\n        x\n        )\n        )\n        =\n        \n          \n            \n              Pr\n              (\n              Y\n              ,\n              Z\n              ,\n              X\n              =\n              x\n              )\n            \n            \n              Pr\n              (\n              X\n              =\n              x\n              ∣\n              Z\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\Pr(Y,Z\\mid {\\text{do}}(x))={\\frac {\\Pr(Y,Z,X=x)}{\\Pr(X=x\\mid Z)}}.}\n  A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called \"sufficient\" or \"admissible.\" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not \"identified\". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious\n(apparent dependence arising from a common cause, R). (see Simpson's paradox)\nTo determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of \"do-calculus\" and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for \n  \n    \n      \n        2\n        \n          10\n        \n      \n      =\n      1024\n    \n    2^{10}=1024\n   values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most \n  \n    \n      10\n      ⋅\n      \n        2\n        \n          3\n        \n      \n      =\n      80\n    \n    10\\cdot 2^{3}=80\n   values.\nOne advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.\n\nInference and learning\nBayesian networks perform three main inference tasks:\n\nInferring unobserved variables\nBecause a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.\nThe most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.\n\nParameter learning\nIn order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)\nOften these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.\nA more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.\n\nStructure learning\nIn the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.\nAutomatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl and rests on the distinction between the three possible patterns allowed in a 3-node DAG:\n\nThe first 2 represent the same dependencies (\n  \n    X\n    X\n   and \n  \n    Z\n    Z\n   are independent given \n  \n    Y\n    Y\n  ) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since \n  \n    X\n    X\n   and \n  \n    Z\n    Z\n   are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when \n  \n    X\n    X\n   and \n  \n    Z\n    Z\n   have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.\nA particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Such method can handle problems with up to 100 variables.\nIn order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.\n\nStatistical introduction\nGiven data \n  \n    \n      x\n      \n      \n    \n    x\\,\\!\n   and parameter \n  \n    θ\n    \\theta\n  , a simple Bayesian analysis starts with a prior probability (prior) \n  \n    \n      p\n      (\n      θ\n      )\n    \n    p(\\theta )\n   and likelihood \n  \n    \n      p\n      (\n      x\n      ∣\n      θ\n      )\n    \n    p(x\\mid \\theta )\n   to compute a posterior probability \n  \n    \n      p\n      (\n      θ\n      ∣\n      x\n      )\n      ∝\n      p\n      (\n      x\n      ∣\n      θ\n      )\n      p\n      (\n      θ\n      )\n    \n    p(\\theta \\mid x)\\propto p(x\\mid \\theta )p(\\theta )\n  .\nOften the prior on \n  \n    θ\n    \\theta\n   depends in turn on other parameters \n  \n    φ\n    \\varphi\n   that are not mentioned in the likelihood. So, the prior \n  \n    \n      p\n      (\n      θ\n      )\n    \n    p(\\theta )\n   must be replaced by a likelihood \n  \n    \n      p\n      (\n      θ\n      ∣\n      φ\n      )\n    \n    p(\\theta \\mid \\varphi )\n  , and a prior \n  \n    \n      p\n      (\n      φ\n      )\n    \n    p(\\varphi )\n   on the newly introduced parameters \n  \n    φ\n    \\varphi\n   is required, resulting in a posterior probability\n\n  \n    \n      \n        p\n        (\n        θ\n        ,\n        φ\n        ∣\n        x\n        )\n        ∝\n        p\n        (\n        x\n        ∣\n        θ\n        )\n        p\n        (\n        θ\n        ∣\n        φ\n        )\n        p\n        (\n        φ\n        )\n        .\n      \n    \n    {\\displaystyle p(\\theta ,\\varphi \\mid x)\\propto p(x\\mid \\theta )p(\\theta \\mid \\varphi )p(\\varphi ).}\n  This is the simplest example of a hierarchical Bayes model.\nThe process may be repeated; for example, the parameters \n  \n    φ\n    \\varphi\n   may depend in turn on additional parameters \n  \n    \n      ψ\n      \n      \n    \n    \\psi \\,\\!\n  , which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.\n\nIntroductory examples\nGiven the measured quantities \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n      \n      \n    \n    x_{1},\\dots ,x_{n}\\,\\!\n  each with normally distributed errors of known standard deviation \n  \n    \n      σ\n      \n      \n    \n    \\sigma \\,\\!\n  ,\n\n  \n    \n      \n        x\n        \n          i\n        \n      \n      ∼\n      N\n      (\n      \n        θ\n        \n          i\n        \n      \n      ,\n      \n        σ\n        \n          2\n        \n      \n      )\n    \n    x_{i}\\sim N(\\theta _{i},\\sigma ^{2})\n  Suppose we are interested in estimating the \n  \n    \n      θ\n      \n        i\n      \n    \n    \\theta _{i}\n  . An approach would be to estimate the \n  \n    \n      θ\n      \n        i\n      \n    \n    \\theta _{i}\n   using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply\n\n  \n    \n      \n        \n          θ\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle \\theta _{i}=x_{i}.}\n  However, if the quantities are related, so that for example the individual \n  \n    \n      θ\n      \n        i\n      \n    \n    \\theta _{i}\n  have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,\n\n  \n    \n      \n        x\n        \n          i\n        \n      \n      ∼\n      N\n      (\n      \n        θ\n        \n          i\n        \n      \n      ,\n      \n        σ\n        \n          2\n        \n      \n      )\n      ,\n    \n    x_{i}\\sim N(\\theta _{i},\\sigma ^{2}),\n  \n\n  \n    \n      \n        \n          θ\n          \n            i\n          \n        \n        ∼\n        N\n        (\n        φ\n        ,\n        \n          τ\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\theta _{i}\\sim N(\\varphi ,\\tau ^{2}),}\n  with improper priors \n  \n    \n      \n        φ\n        ∼\n        \n          flat\n        \n      \n    \n    {\\displaystyle \\varphi \\sim {\\text{flat}}}\n  , \n  \n    \n      \n        τ\n        ∼\n        \n          flat\n        \n        ∈\n        (\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle \\tau \\sim {\\text{flat}}\\in (0,\\infty )}\n  . When \n  \n    \n      n\n      ≥\n      3\n    \n    n\\geq 3\n  , this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual \n  \n    \n      θ\n      \n        i\n      \n    \n    \\theta _{i}\n   will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.\n\nRestrictions on priors\nSome care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable \n  \n    \n      τ\n      \n      \n    \n    \\tau \\,\\!\n   in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.\n\nDefinitions and concepts\nSeveral equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.\n\nFactorization definition\nX is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:\n\n  \n    \n      p\n      (\n      x\n      )\n      =\n      \n        ∏\n        \n          v\n          ∈\n          V\n        \n      \n      p\n      \n        (\n        \n          \n            x\n            \n              v\n            \n          \n          \n          \n            \n              |\n            \n          \n          \n          \n            x\n            \n              pa\n              ⁡\n              (\n              v\n              )\n            \n          \n        \n        )\n      \n    \n    p(x)=\\prod _{v\\in V}p\\left(x_{v}\\,{\\big |}\\,x_{\\operatorname {pa} (v)}\\right)\n  where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).\nFor any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:\n\n  \n    \n      \n        P\n        ⁡\n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          ∏\n          \n            v\n            =\n            1\n          \n          \n            n\n          \n        \n        P\n        ⁡\n        \n          (\n          \n            \n              X\n              \n                v\n              \n            \n            =\n            \n              x\n              \n                v\n              \n            \n            ∣\n            \n              X\n              \n                v\n                +\n                1\n              \n            \n            =\n            \n              x\n              \n                v\n                +\n                1\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                n\n              \n            \n            =\n            \n              x\n              \n                n\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\operatorname {P} (X_{1}=x_{1},\\ldots ,X_{n}=x_{n})=\\prod _{v=1}^{n}\\operatorname {P} \\left(X_{v}=x_{v}\\mid X_{v+1}=x_{v+1},\\ldots ,X_{n}=x_{n}\\right)}\n  Using the definition above, this can be written as:\n\n  \n    \n      \n        P\n        ⁡\n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          ∏\n          \n            v\n            =\n            1\n          \n          \n            n\n          \n        \n        P\n        ⁡\n        (\n        \n          X\n          \n            v\n          \n        \n        =\n        \n          x\n          \n            v\n          \n        \n        ∣\n        \n          X\n          \n            j\n          \n        \n        =\n        \n          x\n          \n            j\n          \n        \n        \n           for each \n        \n        \n          X\n          \n            j\n          \n        \n        \n        \n           that is a parent of \n        \n        \n          X\n          \n            v\n          \n        \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {P} (X_{1}=x_{1},\\ldots ,X_{n}=x_{n})=\\prod _{v=1}^{n}\\operatorname {P} (X_{v}=x_{v}\\mid X_{j}=x_{j}{\\text{ for each }}X_{j}\\,{\\text{ that is a parent of }}X_{v}\\,)}\n  The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.\n\nLocal Markov property\nX is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:\n\n  \n    \n      \n        \n          X\n          \n            v\n          \n        \n        ⊥\n        \n        \n        \n        ⊥\n        \n          X\n          \n            V\n            \n            ∖\n            \n            de\n            ⁡\n            (\n            v\n            )\n          \n        \n        ∣\n        \n          X\n          \n            pa\n            ⁡\n            (\n            v\n            )\n          \n        \n        \n        \n          for all \n        \n        v\n        ∈\n        V\n      \n    \n    {\\displaystyle X_{v}\\perp \\!\\!\\!\\perp X_{V\\,\\smallsetminus \\,\\operatorname {de} (v)}\\mid X_{\\operatorname {pa} (v)}\\quad {\\text{for all }}v\\in V}\n  where de(v) is the set of descendants and V \\ de(v) is the set of non-descendants of v.\nThis can be expressed in terms similar to the first definition, as\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                ⁡\n                (\n                \n                  X\n                  \n                    v\n                  \n                \n                =\n                \n                  x\n                  \n                    v\n                  \n                \n                ∣\n                \n                  X\n                  \n                    i\n                  \n                \n                =\n                \n                  x\n                  \n                    i\n                  \n                \n                \n                   for each \n                \n                \n                  X\n                  \n                    i\n                  \n                \n                \n                   that is not a descendant of \n                \n                \n                  X\n                  \n                    v\n                  \n                \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                (\n                \n                  X\n                  \n                    v\n                  \n                \n                =\n                \n                  x\n                  \n                    v\n                  \n                \n                ∣\n                \n                  X\n                  \n                    j\n                  \n                \n                =\n                \n                  x\n                  \n                    j\n                  \n                \n                \n                   for each \n                \n                \n                  X\n                  \n                    j\n                  \n                \n                \n                   that is a parent of \n                \n                \n                  X\n                  \n                    v\n                  \n                \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\operatorname {P} (X_{v}=x_{v}\\mid X_{i}=x_{i}{\\text{ for each }}X_{i}{\\text{ that is not a descendant of }}X_{v}\\,)\\\\[6pt]={}&P(X_{v}=x_{v}\\mid X_{j}=x_{j}{\\text{ for each }}X_{j}{\\text{ that is a parent of }}X_{v}\\,)\\end{aligned}}}\n  The set of parents is a subset of the set of non-descendants because the graph is acyclic.\n\nDeveloping Bayesian networks\nDeveloping a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.\n\nMarkov blanket\nThe Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.\n\nd-separation\nThis definition can be made more general by defining the \"d\"-separation of two nodes, where d stands for directional. We first define the \"d\"-separation of a trail and then we will define the \"d\"-separation of two nodes in terms of that.\nLet P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:\n\nP contains (but does not need to be entirely) a directed chain, \n  \n    \n      \n        u\n        ⋯\n        ←\n        m\n        ←\n        ⋯\n        v\n      \n    \n    {\\displaystyle u\\cdots \\leftarrow m\\leftarrow \\cdots v}\n   or \n  \n    \n      \n        u\n        ⋯\n        →\n        m\n        →\n        ⋯\n        v\n      \n    \n    {\\displaystyle u\\cdots \\rightarrow m\\rightarrow \\cdots v}\n  , such that the middle node m is in Z,\nP contains a fork, \n  \n    \n      \n        u\n        ⋯\n        ←\n        m\n        →\n        ⋯\n        v\n      \n    \n    {\\displaystyle u\\cdots \\leftarrow m\\rightarrow \\cdots v}\n  , such that the middle node m is in Z, or\nP contains an inverted fork (or collider), \n  \n    \n      \n        u\n        ⋯\n        →\n        m\n        ←\n        ⋯\n        v\n      \n    \n    {\\displaystyle u\\cdots \\rightarrow m\\leftarrow \\cdots v}\n  , such that the middle node m is not in Z and no descendant of m is in Z.The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.\nX is a Bayesian network with respect to G if, for any two nodes u, v:\n\n  \n    \n      \n        X\n        \n          u\n        \n      \n      ⊥\n      \n      \n      \n      ⊥\n      \n        X\n        \n          v\n        \n      \n      ∣\n      \n        X\n        \n          Z\n        \n      \n    \n    X_{u}\\perp \\!\\!\\!\\perp X_{v}\\mid X_{Z}\n  where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)\n\nCausal networks\nAlthough Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:\n\n  \n    \n      \n        a\n        →\n        b\n        →\n        c\n        \n        \n          and\n        \n        \n        a\n        ←\n        b\n        ←\n        c\n      \n    \n    {\\displaystyle a\\rightarrow b\\rightarrow c\\qquad {\\text{and}}\\qquad a\\leftarrow b\\leftarrow c}\n  are equivalent: that is they impose exactly the same conditional independence requirements.\nA causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x. Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.\n\nInference complexity and approximation algorithms\nIn 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks. First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.\nAt about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by \n  \n    \n      \n        1\n        \n          /\n        \n        p\n        (\n        n\n        )\n      \n    \n    {\\displaystyle 1/p(n)}\n   where \n  \n    \n      p\n      (\n      n\n      )\n    \n    p(n)\n   was any polynomial of the number of nodes in the network, \n  \n    n\n    n\n  .\n\nSoftware\nNotable software for Bayesian networks include:\n\nJust another Gibbs sampler (JAGS) – Open-source alternative to WinBUGS. Uses Gibbs sampling.\nOpenBUGS – Open-source development of WinBUGS.\nSPSS Modeler – Commercial software that includes an implementation for Bayesian networks.\nStan (software) – Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo.\nPyMC3 – A Python library implementing an embedded domain specific language to represent bayesian networks, and a variety of samplers (including NUTS)\nWinBUGS – One of the first computational implementations of MCMC samplers. No longer maintained.\n\nHistory\nThe term Bayesian network was coined by Judea Pearl in 1985 to emphasize:\nthe often subjective nature of the input information\nthe reliance on Bayes' conditioning as the basis for updating information\nthe distinction between causal and evidential modes of reasoningIn the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems and Neapolitan's Probabilistic Reasoning in Expert Systems summarized their properties and established them as a field of study.\n\nSee also\nNotes\nReferences\nFurther reading\nConrady S, Jouffe L (2015-07-01). Bayesian Networks and BayesiaLab – A practical introduction for researchers. Franklin, Tennessee: Bayesian USA. ISBN 978-0-9965333-0-0.\nCharniak E (Winter 1991). \"Bayesian networks without tears\" (PDF). AI Magazine.\nKruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P (2013). Computational Intelligence A Methodological Introduction. London: Springer-Verlag. ISBN 978-1-4471-5012-1.\nBorgelt C, Steinbrecher M, Kruse R (2009). Graphical Models – Representations for Learning, Reasoning and Data Mining (Second ed.). Chichester: Wiley. ISBN 978-0-470-74956-2.\n\nExternal links\nAn Introduction to Bayesian Networks and their Contemporary Applications\nOn-line Tutorial on Bayesian nets and probability\nWeb-App to create Bayesian nets and run it with a Monte Carlo method\nContinuous Time Bayesian Networks\nBayesian Networks: Explanation and Analogy\nA live tutorial on learning Bayesian networks\nA hierarchical Bayes Model for handling sample heterogeneity in classification problems, provides a classification model taking into consideration the uncertainty associated with measuring replicate samples.\nHierarchical Naive Bayes Model for handling sample uncertainty Archived 2007-09-28 at the Wayback Machine, shows how to perform classification and learning with continuous and discrete variables with replicated measurements.",
    "Bayesian optimization": "Bayesian optimization is a sequential design strategy for global optimization of black-box functions  that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\n\nHistory\nThe term is generally attributed to Jonas Mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.\n\nStrategy\nBayesian optimization is typically used on problems of the form \n  \n    \n      \n        \n          max\n          \n            x\n            ∈\n            A\n          \n        \n        f\n        (\n        x\n        )\n      \n    \n    {\\textstyle \\max _{x\\in A}f(x)}\n  , where \n  \n    \n      \n        A\n      \n    \n    {\\textstyle A}\n   is a set of points, \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  , which rely upon less than 20 dimensions (\n  \n    \n      \n        \n          \n            R\n          \n          \n            d\n          \n        \n        ,\n        d\n        ≤\n        20\n      \n    \n    {\\textstyle \\mathbb {R} ^{d},d\\leq 20}\n  ), and whose membership can easily be evaluated. Bayesian optimization is particularly advantageous for problems where \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\textstyle f(x)}\n   is difficult to evaluate due to its computational cost. The objective function, \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n  , is continuous and takes the form of some unknown structure, referred to as a \"black box\". Upon its evaluation, only \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\textstyle f(x)}\n   is observed and its derivatives are not evaluated.Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point.\nThere are several methods used to define the prior/posterior distribution over the objective function. The most common two methods use Gaussian processes in a method called kriging. Another less expensive method uses the Parzen-Tree Estimator to construct two distributions for 'high' and 'low' points, and then finds the location that maximizes the expected improvement.Standard Bayesian optimization relies upon each \n  \n    \n      x\n      ∈\n      A\n    \n    x\\in A\n   being easy to evaluate, and problems that deviate from this assumption are known as exotic Bayesian optimization problems. Optimization problems can become exotic if it is known that there is noise, the evaluations are being done in parallel, the quality of evaluations relies upon a tradeoff between difficulty and accuracy, the presence of random environmental conditions, or if the evaluation involves derivatives.\n\nAcquisition functions\nExamples of acquisition functions include \n\nprobability of improvement\nexpected improvement\nBayesian expected losses\nupper confidence bounds (UCB) or lower confidence bounds\nThompson samplingand hybrids of these. They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are expensive to evaluate.\n\nSolution methods\nThe maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer. Acquisition functions are typically well-behaved and are \nmaximized using a numerical optimization technique, such as Newton's Method or quasi-Newton methods like the Broyden–Fletcher–Goldfarb–Shanno algorithm.\n\nApplications\nThe approach has been applied to solve a wide range of problems , including learning to rank , computer graphics and visual design , robotics , sensor networks , automatic algorithm configuration , automatic machine learning toolboxes , reinforcement learning , planning, visual attention, architecture configuration in deep learning, static program analysis, experimental particle physics , chemistry, material design, and drug development .\n\nSee also\nMulti-armed bandit\nKriging\nThompson sampling\nGlobal optimization\nBayesian experimental design\nProbabilistic numerics\nPareto optimum\n\n\n== References ==",
    "Behaviorism": "Behaviorism (also spelled behaviourism) is a systematic approach to understanding the behavior of humans and other animals. It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.\nBehaviorism emerged in the early 1900s as a reaction to depth psychology and other traditional forms of psychology, which often had difficulty making predictions that could be tested experimentally, but derived from earlier research in the late nineteenth century, such as when Edward Thorndike pioneered the law of effect, a procedure that involved the use of consequences to strengthen or weaken behavior.\nWith a 1924 publication, John B. Watson devised methodological behaviorism, which rejected introspective methods and sought to understand behavior by only measuring observable behaviors and events. It was not until the 1930s that B. F. Skinner suggested that covert behavior—including cognition and emotions—is subject to the same controlling variables as observable behavior, which became the basis for his philosophy called radical behaviorism. While Watson and Ivan Pavlov investigated how (conditioned) neutral stimuli elicit reflexes in respondent conditioning, Skinner assessed the reinforcement histories of the discriminative (antecedent) stimuli that emits behavior; the technique became known as operant conditioning.\nThe application of radical behaviorism—known as applied behavior analysis—is used in a variety of contexts, including, for example, applied animal behavior and organizational behavior management to treatment of mental disorders, such as autism and substance abuse. In addition, while behaviorism and cognitive schools of psychological thought do not agree theoretically, they have complemented each other in the cognitive-behavior therapies, which have demonstrated utility in treating certain pathologies, including simple phobias, PTSD, and mood disorders.\n\nBranches of behaviorism\nThe titles given to the various branches of behaviorism include:\n\nBehavioral genetics: Proposed in 1869 by Francis Galton, a relative of Charles Darwin.\nInterbehaviorism: Proposed by Jacob Robert Kantor before B. F. Skinner's writings.\nMethodological behaviorism: John B. Watson's behaviorism states that only public events (motor behaviors of an individual) can be objectively observed. Although it was still acknowledged that thoughts and feelings exist, they were not considered part of the science of behavior. It also laid the theoretical foundation for the early approach behavior modification in the 1970s and 1980s.\nPsychological behaviorism: As proposed by Arthur W. Staats, unlike the previous behaviorisms of Skinner, Hull, and Tolman, was based upon a program of human research involving various types of human behavior. Psychological behaviorism introduces new principles of human learning. Humans learn not only by animal learning principles but also by special human learning principles. Those principles involve humans' uniquely huge learning ability. Humans learn repertoires that enable them to learn other things. Human learning is thus cumulative. No other animal demonstrates that ability, making the human species unique.\nRadical behaviorism: Skinner's philosophy is an extension of Watson's form of behaviorism by theorizing that processes within the organism—particularly, private events, such as thoughts and feelings—are also part of the science of behavior, and suggests that environmental variables control these internal events just as they control observable behaviors. Although private events cannot be directly seen by others, they are later determined through the species' overt behavior. Radical behaviorism forms the core philosophy behind behavior analysis. Willard Van Orman Quine used many of radical behaviorism's ideas in his study of knowledge and language.\nTeleological behaviorism: Proposed by Howard Rachlin, post-Skinnerian, purposive, close to microeconomics. Focuses on objective observation as opposed to cognitive processes.\nTheoretical behaviorism: Proposed by J. E. R. Staddon,  adds a concept of internal state to allow for the effects of context. According to theoretical behaviorism, a state is a set of equivalent histories, i.e., past histories in which members of the same stimulus class produce members of the same response class (i.e., B. F. Skinner's concept of the operant). Conditioned stimuli are thus seen to control neither stimulus nor response but state. Theoretical behaviorism is a logical extension of Skinner's class-based (generic) definition of the operant.Two subtypes of theoretical behaviorism are:\n\nHullian and post-Hullian: theoretical, group data, not dynamic, physiological\nPurposive: Tolman's behavioristic anticipation of cognitive psychology\n\nModern-day theory: radical behaviorism\nB. F. Skinner proposed radical behaviorism as the conceptual underpinning of the experimental analysis of behavior. This viewpoint differs from other approaches to behavioral research in various ways, but, most notably here, it contrasts with methodological behaviorism in accepting feelings, states of mind and introspection as behaviors also subject to scientific investigation. Like methodological behaviorism, it rejects the reflex as a model of all behavior, and it defends the science of behavior as complementary to but independent of physiology. Radical behaviorism overlaps considerably with other western philosophical positions, such as American pragmatism.Although John B. Watson mainly emphasized his position of methodological behaviorism throughout his career, Watson and Rosalie Rayner conducted the infamous Little Albert experiment (1920), a study in which Ivan Pavlov's theory to respondent conditioning was first applied to eliciting a fearful reflex of crying in a human infant, and this became the launching point for understanding covert behavior (or private events) in radical behaviorism. However, Skinner felt that aversive stimuli should only be experimented on with animals and spoke out against Watson for testing something so controversial on a human.\nIn 1959, Skinner observed the emotions of two pigeons by noting that they appeared angry because their feathers ruffled. The pigeons were placed together in an operant chamber, where they were aggressive as a consequence of previous reinforcement in the environment. Through stimulus control and subsequent discrimination training, whenever Skinner turned off the green light, the pigeons came to notice that the food reinforcer is discontinued following each peck and responded without aggression. Skinner concluded that humans also learn aggression and possess such emotions (as well as other private events) no differently than do nonhuman animals.\n\nExperimental and conceptual innovations\nAs experimental behavioural psychology is related to behavioral neuroscience, we can date the first researches in the area were done in the beginning of 19th century. Later, this essentially philosophical position gained strength from the success of Skinner's early experimental work with rats and pigeons, summarized in his books The Behavior of Organisms and Schedules of Reinforcement. Of particular importance was his concept of the operant response, of which the canonical example was the rat's lever-press. In contrast with the idea of a physiological or reflex response, an operant is a class of structurally distinct but functionally equivalent responses. For example, while a rat might press a lever with its left paw or its right paw or its tail, all of these responses operate on the world in the same way and have a common consequence. Operants are often thought of as species of responses, where the individuals differ but the class coheres in its function-shared consequences with operants and reproductive success with species. This is a clear distinction between Skinner's theory and S–R theory.\nSkinner's empirical work expanded on earlier research on trial-and-error learning by researchers such as Thorndike and Guthrie with both conceptual reformulations—Thorndike's notion of a stimulus-response \"association\" or \"connection\" was abandoned; and methodological ones—the use of the \"free operant\", so-called because the animal was now permitted to respond at its own rate rather than in a series of trials determined by the experimenter procedures. With this method, Skinner carried out substantial experimental work on the effects of different schedules and rates of reinforcement on the rates of operant responses made by rats and pigeons. He achieved remarkable success in training animals to perform unexpected responses, to emit large numbers of responses, and to demonstrate many empirical regularities at the purely behavioral level. This lent some credibility to his conceptual analysis. It is largely his conceptual analysis that made his work much more rigorous than his peers, a point which can be seen clearly in his seminal work Are Theories of Learning Necessary? in which he criticizes what he viewed to be theoretical weaknesses then common in the study of psychology. An important descendant of the experimental analysis of behavior is the Society for Quantitative Analysis of Behavior.\n\nRelation to language\nAs Skinner turned from experimental work to concentrate on the philosophical underpinnings of a science of behavior, his attention turned to human language with his 1957 book Verbal Behavior and other language-related publications; Verbal Behavior laid out a vocabulary and theory for functional analysis of verbal behavior, and was strongly criticized in a review by Noam Chomsky.Skinner did not respond in detail but claimed that Chomsky failed to understand his ideas, and the disagreements between the two and the theories involved have been further discussed. Innateness theory, which has been heavily critiqued, is opposed to behaviorist theory which claims that language is a set of habits that can be acquired by means of conditioning. According to some, the behaviorist account is a process which would be too slow to explain a phenomenon as complicated as language learning. What was important for a behaviorist's analysis of human behavior was not language acquisition so much as the interaction between language and overt behavior. In an essay republished in his 1969 book Contingencies of Reinforcement, Skinner took the view that humans could construct linguistic stimuli that would then acquire control over their behavior in the same way that external stimuli could. The possibility of such \"instructional control\" over behavior meant that contingencies of reinforcement would not always produce the same effects on human behavior as they reliably do in other animals. The focus of a radical behaviorist analysis of human behavior therefore shifted to an attempt to understand the interaction between instructional control and contingency control, and also to understand the behavioral processes that determine what instructions are constructed and what control they acquire over behavior. Recently, a new line of behavioral research on language was started under the name of relational frame theory.\n\nEducation\nBehaviourism focuses on one particular view of learning: a change in external behaviour achieved through using reinforcement and repetition (Rote learning) to shape behavior of learners. Skinner found that behaviors could be shaped when the use of reinforcement was implemented. Desired behavior is rewarded, while the undesired behavior is not rewarded. Incorporating behaviorism into the classroom allowed educators to assist their students in excelling both academically and personally. In the field of language learning, this type of teaching was called the audio-lingual method, characterised by the whole class using choral chanting of key phrases, dialogues and immediate correction.\nWithin the behaviourist view of learning, the \"teacher\" is the dominant person in the classroom and takes complete control, evaluation of learning comes from the teacher who decides what is right or wrong. The learner does not have any opportunity for evaluation or reflection within the learning process, they are simply told what is right or wrong.\nThe conceptualization of learning using this approach could be considered \"superficial,\" as the focus is on external changes in behaviour, i.e., not interested in the internal processes of learning leading to behaviour change and has no place for the emotions involved in the process.\n\nOperant conditioning\nOperant conditioning was developed by B.F. Skinner in 1937 and deals with the management of environmental contingencies to change behavior. In other words, behavior is controlled by historical consequential contingencies, particularly reinforcement—a stimulus that increases the probability of performing behaviors, and punishment—a stimulus that decreases such probability. The core tools of consequences are either positive (presenting stimuli following a response), or negative (withdrawn stimuli following a response).The following descriptions explains the concepts of four common types of consequences in operant conditioning:\nPositive reinforcement: Providing a stimulus that an individual enjoys, seeks, or craves, in order to reinforce desired behaviors. For example, when a person is teaching a dog to sit, they pair the command \"sit\" with a treat. The treat is the positive reinforcement to the behavior of sitting. The key to making positive reinforcement effect is to reward the behavior immediately.\nNegative reinforcement: Removing a stimulus that an individual does not desire to reinforce desired behaviors. For example, a child hates being nagged to clean his room. His mother reinforces his room cleaning by removing the undesired stimulus of nagging after he has cleaned. Another example would be putting on sunscreen before going outside. The negative effect is getting a sunburn, so by putting on sunscreen, the behavior in this case, you avoid the stimulus of getting a sunburn.\nPositive punishment: Providing a stimulus that an individual does not desire to decrease undesired behaviors. An example of this would be spanking. If a child is doing something they have been warned not to do, the parent might spank them. The undesired stimulus would be the spanking, and by adding this stimulus, the goal is to have that behavior avoided. The key to this technique is that even though the title says positive, the meaning of positive here is \"to add to.\" So, in order to stop the behavior, the parent adds the adverse stimulus (spanking). The biggest problem with this type of training though is that the trainee does not usually learn the desired behavior, rather it teaches the trainee to avoid the punisher.\nNegative punishment: Removing a stimulus that an individual desires in order to decrease undesired behaviors. An example of this would be grounding a child for failing a test. Grounding in this example is taking away the child's ability to play video games. As long as it is clear that the ability to play video games was taken away because they failed a test, this is negative punishment. The key here is the connection to the behavior and the result of the behavior.Classical experiment in operant conditioning, for example, the Skinner Box, \"puzzle box\" or operant conditioning chamber to test the effects of operant conditioning principles on rats, cats and other species. From the study of Skinner box, he discovered that the rats learned very effectively if they were rewarded frequently with food. Skinner also found that he could shape the rats' behavior through the use of rewards, which could, in turn, be applied to human learning as well.\nSkinner's model was based on the premise that reinforcement is used for the desired actions or responses while punishment was used to stop the responses of the undesired actions that are not. This theory proved that humans or animals will repeat any action that leads to a positive outcome, and avoiding any action that leads to a negative outcome. The experiment with the pigeons showed that a positive outcome leads to learned behavior since the pigeon learned to peck the disc in return for the reward of food.\nThese historical consequential contingencies subsequently lead to (antecedent) stimulus control, but in contrast to respondent conditioning where antecedent stimuli elicit reflexive behavior, operant behavior is only emitted and therefore does not force its occurrence. It includes the following controlling stimuli:\nDiscriminative stimulus (Sd): An antecedent stimulus that increases the chance of the organism engaging in a behavior. One example of this occurred in Skinner's laboratory. Whenever the green light (Sd) appeared, it signaled the pigeon to perform the behavior of pecking because it learned in the past that each time it pecked, food was presented (the positive reinforcing stimulus).\nStimulus delta (S-delta): An antecedent stimulus that signals the organism not to perform a behavior since it was extinguished or punished in the past. One notable instance of this occurs when a person stops their car immediately after the traffic light turns red (S-delta). However, the person could decide to drive through the red light, but subsequently receive a speeding ticket (the positive punishing stimulus), so this behavior will potentially not reoccur following the presence of the S-delta.\n\nRespondent conditioning\nAlthough operant conditioning plays the largest role in discussions of behavioral mechanisms, respondent conditioning (also called Pavlovian or classical conditioning) is also an important behavior-analytic process that needs not refer to mental or other internal processes. Pavlov's experiments with dogs provide the most familiar example of the classical conditioning procedure. In the beginning, the dog was provided meat (unconditioned stimulus, UCS, naturally elicit a response that is not controlled) to eat, resulting in increased salivation (unconditioned response, UCR, which means that a response is naturally caused by UCS). Afterward, a bell ring was presented together with food to the dog. Although bell ring was a neutral stimulus (NS, meaning that the stimulus did not have any effect), dog would start to salivate when only hearing a bell ring after a number of pairings. Eventually, the neutral stimulus (bell ring) became conditioned. Therefore, salivation was elicited as a conditioned response (the response same as the unconditioned response), pairing up with meat—the conditioned stimulus)   Although Pavlov proposed some tentative physiological processes that might be involved in classical conditioning, these have not been confirmed. The idea of classical conditioning helped behaviorist John Watson discover the key mechanism behind how humans acquire the behaviors that they do, which was to find a natural reflex that produces the response being considered.\nWatson's \"Behaviourist Manifesto\" has three aspects that deserve special recognition: one is that psychology should be purely objective, with any interpretation of conscious experience being removed, thus leading to psychology as the \"science of behaviour\"; the second one is that the goals of psychology should be to predict and control behaviour (as opposed to describe and explain conscious mental states); the third one is that there is no notable distinction between human and non-human behaviour. Following Darwin's theory of evolution, this would simply mean that human behaviour is just a more complex version in respect to behaviour displayed by other species.\n\nIn philosophy\nBehaviorism is a psychological movement that can be contrasted with philosophy of mind. The basic premise of behaviorism is that the study of behavior should be a natural science, such as chemistry or physics.  Initially behaviorism rejected any reference to hypothetical inner states of organisms as causes for their behavior, but B.F. Skinner's radical behaviorism reintroduced reference to inner states and also advocated for the study of thoughts and feelings as behaviors subject to the same mechanisms as external behavior.  Behaviorism takes a functional view of behavior. According to Edmund Fantino and colleagues: \"Behavior analysis has much to offer the study of phenomena normally dominated by cognitive and social psychologists. We hope that successful application of behavioral theory and methodology will not only shed light on central problems in judgment and choice but will also generate greater appreciation of the behavioral approach.\"Behaviorist sentiments are not uncommon within philosophy of language and analytic philosophy. It is sometimes argued that Ludwig Wittgenstein defended a logical behaviorist position (e.g., the beetle in a box argument). In logical positivism (as held, e.g., by Rudolf Carnap and Carl Hempel),  the meaning of psychological statements are their verification conditions, which consist of performed overt behavior. W. V. O. Quine made use of a type of behaviorism, influenced by some of Skinner's ideas, in his own work on language. Quine's work in semantics differed substantially from the empiricist semantics of Carnap which he attempted to create an alternative to, couching his semantic theory in references to physical objects rather than sensations. Gilbert Ryle defended a distinct strain of philosophical behaviorism, sketched in his book The Concept of Mind.  Ryle's central claim was that instances of dualism frequently represented \"category mistakes\", and hence that they were really misunderstandings of the use of ordinary language. Daniel Dennett likewise acknowledges himself to be a type of behaviorist, though he offers extensive criticism of radical behaviorism and refutes Skinner's rejection of the value of intentional idioms and the possibility of free will.\nThis is Dennett's main point in \"Skinner Skinned.\" Dennett argues that there is a crucial difference between explaining and explaining away… If our explanation of apparently rational behavior turns out to be extremely simple, we may want to say that the behavior was not really rational after all. But if the explanation is very complex and intricate, we may want to say not that the behavior is not rational, but that we now have a better understanding of what rationality consists in. (Compare: if we find out how a computer program solves problems in linear algebra, we don't say it's not really solving them, we just say we know how it does it. On the other hand, in cases like Weizenbaum's ELIZA program, the explanation of how the computer carries on a conversation is so simple that the right thing to say seems to be that the machine isn't really carrying on a conversation, it's just a trick.)\n\nLaw of effect and trace conditioning\nLaw of effect: Although Edward Thorndike's methodology mainly dealt with reinforcing observable behavior, it viewed cognitive antecedents as the causes of behavior, and was theoretically much more similar to the cognitive-behavior therapies than classical (methodological) or modern-day (radical) behaviorism. Nevertheless, Skinner's operant conditioning was heavily influenced by the Law of Effect's principle of reinforcement.\nTrace conditioning: Akin to B.F. Skinner's radical behaviorism, it is a respondent conditioning technique based on Ivan Pavlov's concept of a \"memory trace\" in which the observer recalls the conditioned stimulus (CS), with the memory or recall being the unconditioned response (UR). There is also a time delay between the CS and unconditioned stimulus (US), causing the conditioned response (CR)—particularly the reflex—to be faded over time.\n\nMolecular versus molar behaviorism\nSkinner's view of behavior is most often characterized as a \"molecular\" view of behavior; that is, behavior can be decomposed into atomistic parts or molecules. This view is inconsistent with Skinner's complete description of behavior as delineated in other works, including his 1981 article \"Selection by Consequences\". Skinner proposed that a complete account of behavior requires understanding of selection history at three levels: biology (the natural selection or phylogeny of the animal); behavior (the reinforcement history or ontogeny of the behavioral repertoire of the animal); and for some species, culture (the cultural practices of the social group to which the animal belongs). This whole organism then interacts with its environment. Molecular behaviorists use notions from melioration theory, negative power function discounting or additive versions of negative power function discounting.Molar behaviorists, such as Howard Rachlin, Richard Herrnstein, and William Baum, argue that behavior cannot be understood by focusing on events in the moment. That is, they argue that behavior is best understood as the ultimate product of an organism's history and that molecular behaviorists are committing a fallacy by inventing fictitious proximal causes for behavior. Molar behaviorists argue that standard molecular constructs, such as \"associative strength\", are better replaced by molar variables such as rate of reinforcement. Thus, a molar behaviorist would describe \"loving someone\" as a pattern of loving behavior over time; there is no isolated, proximal cause of loving behavior, only a history of behaviors (of which the current behavior might be an example) that can be summarized as \"love\".\n\nTheoretical behaviorism\nSkinner's radical behaviorism has been highly successful experimentally, revealing new phenomena with new methods, but Skinner's dismissal of theory limited its development. Theoretical behaviorism recognized that a historical system, an organism, has a state as well as sensitivity to stimuli and the ability to emit responses.  Indeed, Skinner himself acknowledged the possibility of what he called \"latent\" responses in humans, even though he neglected to extend this idea to rats and pigeons.  Latent responses constitute a repertoire, from which operant reinforcement can select.  Theoretical behaviorism links between the brain and the behavior that provides a real understanding of the behavior, rather than a mental presumption of how brain-behavior relates.\n\nBehavior analysis and culture\nCultural analysis has always been at the philosophical core of radical behaviorism from the early days (as seen in Skinner's Walden Two, Science & Human Behavior, Beyond Freedom & Dignity, and About Behaviorism).\nDuring the 1980s, behavior analysts, most notably Sigrid Glenn, had a productive interchange with cultural anthropologist Marvin Harris (the most notable proponent of \"cultural materialism\") regarding interdisciplinary work. Very recently, behavior analysts have produced a set of basic exploratory experiments in an effort toward this end. Behaviorism is also frequently used in game development, although this application is controversial.\n\nBehavior informatics and behavior computing\nWith the fast growth of big behavioral data and applications, behavior analysis is ubiquitous. Understanding behavior from the informatics and computing perspective becomes increasingly critical for in-depth understanding of what, why and how behaviors are formed, interact, evolve, change and affect business and decision. Behavior informatics and behavior computing deeply explore behavior intelligence and behavior insights from the informatics and computing perspectives.\n\nCriticisms and limitations\nIn the second half of the 20th century, behaviorism was largely eclipsed as a result of the cognitive revolution. This shift was due to radical behaviorism being highly criticized for not examining mental processes, and this led to the development of the cognitive therapy movement.\nIn the mid-20th century, three main influences arose that would inspire and shape cognitive psychology as a formal school of thought:\n\nNoam Chomsky's 1959 critique of behaviorism, and empiricism more generally, initiated what would come to be known as the \"cognitive revolution\".\nDevelopments in computer science would lead to parallels being drawn between human thought and the computational functionality of computers, opening entirely new areas of psychological thought. Allen Newell and Herbert Simon spent years developing the concept of artificial intelligence (AI) and later worked with cognitive psychologists regarding the implications of AI. The effective result was more of a framework conceptualization of mental functions with their counterparts in computers (memory, storage, retrieval, etc.)\nFormal recognition of the field involved the establishment of research institutions such as George Mandler's Center for Human Information Processing in 1964. Mandler described the origins of cognitive psychology in a 2002 article in the Journal of the History of the Behavioral SciencesIn the early years of cognitive psychology, behaviorist critics held that the empiricism it pursued was incompatible with the concept of internal mental states. Cognitive neuroscience, however, continues to gather evidence of direct correlations between physiological brain activity and putative mental states, endorsing the basis for cognitive psychology.\n\nBehavior therapy\nBehavior therapy is a term referring to different types of therapies that treat mental health disorders. It identifies and helps change people's unhealthy behaviors or destructive behaviors through learning theory and conditioning. Ivan Pavlov's classical conditioning, as well as counterconditioning are the basis for much of clinical behavior therapy, but also includes other techniques, including operant conditioning—or contingency management, and modeling (sometimes called observational learning). A frequently noted behavior therapy is systematic desensitization (graduated exposure therapy), which was first demonstrated by Joseph Wolpe and Arnold Lazarus.\n\n21st-century behaviorism (behavior analysis)\nApplied behavior analysis (ABA)—also called behavioral engineering—is a scientific discipline that applies the principles of behavior analysis to change behavior. ABA derived from much earlier research in the Journal of the Experimental Analysis of Behavior, which was founded by B.F. Skinner and his colleagues at Harvard University. Nearly a decade after the study \"The psychiatric nurse as a behavioral engineer\" (1959) was published in that journal, which demonstrated how effective the token economy was in reinforcing more adaptive behavior for hospitalized patients with schizophrenia and intellectual disability, it led to researchers at the University of Kansas to start the Journal of Applied Behavior Analysis in 1968.\nAlthough ABA and behavior modification are similar behavior-change technologies in that the learning environment is modified through respondent and operant conditioning, behavior modification did not initially address the causes of the behavior (particularly, the environmental stimuli that occurred in the past), or investigate solutions that would otherwise prevent the behavior from reoccurring. As the evolution of ABA began to unfold in the mid-1980s, functional behavior assessments (FBAs) were developed to clarify the function of that behavior, so that it is accurately determined which differential reinforcement contingencies will be most effective and less likely for aversive punishments to be administered. In addition, methodological behaviorism was the theory underpinning behavior modification since private events were not conceptualized during the 1970s and early 1980s, which contrasted from the radical behaviorism of behavior analysis. ABA—the term that replaced behavior modification—has emerged into a thriving field.The independent development of behaviour analysis outside the United States also continues to develop. In the US, the American Psychological Association (APA) features a subdivision for Behavior Analysis, titled APA Division 25: Behavior Analysis, which has been in existence since 1964, and the interests among behavior analysts today are wide-ranging, as indicated in a review of the 30 Special Interest Groups (SIGs) within the Association for Behavior Analysis International (ABAI). Such interests include everything from animal behavior and environmental conservation, to classroom instruction (such as direct instruction and precision teaching), verbal behavior, developmental disabilities and autism, clinical psychology (i.e., forensic behavior analysis), behavioral medicine (i.e., behavioral gerontology, AIDS prevention, and fitness training), and consumer behavior analysis.\nThe field of applied animal behavior—a sub-discipline of ABA that involves training animals—is regulated by the Animal Behavior Society, and those who practice this technique are called applied animal behaviorists. Research on applied animal behavior has been frequently conducted in the Applied Animal Behaviour Science journal since its founding in 1974.\nABA has also been particularly well-established in the area of developmental disabilities since the 1960s, but it was not until the late 1980s that individuals diagnosed with autism spectrum disorders were beginning to grow so rapidly and groundbreaking research was being published that parent advocacy groups started demanding for services throughout the 1990s, which encouraged the formation of the Behavior Analyst Certification Board, a credentialing program that certifies professionally trained behavior analysts on the national level to deliver such services. Nevertheless, the certification is applicable to all human services related to the rather broad field of behavior analysis (other than the treatment for autism), and the ABAI currently has 14 accredited MA and Ph.D. programs for comprehensive study in that field.\nEarly behavioral interventions (EBIs) based on ABA are empirically validated for teaching children with autism and has been proven as such for over the past five decades. Since the late 1990s and throughout the twenty-first century, early ABA interventions have also been identified as the treatment of choice by the US Surgeon General, American Academy of Pediatrics, and US National Research Council.\nDiscrete trial training—also called early intensive behavioral intervention—is the traditional EBI technique implemented for thirty to forty hours per week that instructs a child to sit in a chair, imitate fine and gross motor behaviors, as well as learn eye contact and speech, which are taught through shaping, modeling, and prompting, with such prompting being phased out as the child begins mastering each skill. When the child becomes more verbal from discrete trials, the table-based instructions are later discontinued, and another EBI procedure known as incidental teaching is introduced in the natural environment by having the child ask for desired items kept out of their direct access, as well as allowing the child to choose the play activities that will motivate them to engage with their facilitators before teaching the child how to interact with other children their own age.\nA related term for incidental teaching, called pivotal response treatment (PRT), refers to EBI procedures that exclusively entail twenty-five hours per week of naturalistic teaching (without initially using discrete trials). Current research is showing that there is a wide array of learning styles and that is the children with receptive language delays who initially require discrete trials to acquire speech.\nOrganizational behavior management, which applies contingency management procedures to model and reinforce appropriate work behavior for employees in organizations, has developed a particularly strong following within ABA, as evidenced by the formation of the OBM Network and Journal of Organizational Behavior Management, which was rated the third-highest impact journal in applied psychology by ISI JOBM rating.\nModern-day clinical behavior analysis has also witnessed a massive resurgence in research, with the development of relational frame theory (RFT), which is described as an extension of verbal behavior and a \"post-Skinnerian account of language and cognition.\" RFT also forms the empirical basis for acceptance and commitment therapy, a therapeutic approach to counseling often used to manage such conditions as anxiety and obesity that consists of acceptance and commitment, value-based living, cognitive defusion, counterconditioning (mindfulness), and contingency management (positive reinforcement). Another evidence-based counseling technique derived from RFT is the functional analytic psychotherapy known as behavioral activation that relies on the ACL model—awareness, courage, and love—to reinforce more positive moods for those struggling with depression.\nIncentive-based contingency management (CM) is the standard of care for adults with substance-use disorders; it has also been shown to be highly effective for other addictions (i.e., obesity and gambling). Although it does not directly address the underlying causes of behavior, incentive-based CM is highly behavior analytic as it targets the function of the client's motivational behavior by relying on a preference assessment, which is an assessment procedure that allows the individual to select the preferred reinforcer (in this case, the monetary value of the voucher, or the use of other incentives, such as prizes). Another evidence-based CM intervention for substance abuse is community reinforcement approach and family training that uses FBAs and counterconditioning techniques—such as behavioral skills training and relapse prevention—to model and reinforce healthier lifestyle choices which promote self-management of abstinence from drugs, alcohol, or cigarette smoking during high-risk exposure when engaging with family members, friends, and co-workers.\nWhile schoolwide positive behavior support consists of conducting assessments and a task analysis plan to differentially reinforce curricular supports that replace students' disruptive behavior in the classroom, pediatric feeding therapy incorporates a liquid chaser and chin feeder to shape proper eating behavior for children with feeding disorders. Habit reversal training, an approach firmly grounded in counterconditioning which uses contingency management procedures to reinforce alternative behavior, is currently the only empirically validated approach for managing tic disorders.\nSome studies on exposure (desensitization) therapies—which refer to an array of interventions based on the respondent conditioning procedure known as habituation and typically infuses counterconditioning procedures, such as meditation and breathing exercises—have recently been published in behavior analytic journals since the 1990s, as most other research are conducted from a cognitive-behavior therapy framework. When based on a behavior analytic research standpoint, FBAs are implemented to precisely outline how to employ the flooding form of desensitization (also called direct exposure therapy) for those who are unsuccessful in overcoming their specific phobia through systematic desensitization (also known as graduated exposure therapy). These studies also reveal that systematic desensitization is more effective for children if used in conjunction with shaping, which is further termed contact desensitization, but this comparison has yet to be substantiated with adults.\nOther widely published behavior analytic journals include Behavior Modification, The Behavior Analyst, Journal of Positive Behavior Interventions, Journal of Contextual Behavioral Science, The Analysis of Verbal Behavior, Behavior and Philosophy, Behavior and Social Issues, and The Psychological Record.\n\nCognitive-behavior therapy\nCognitive-behavior therapy (CBT) is a behavior therapy discipline that often overlaps considerably with the clinical behavior analysis subfield of ABA, but differs in that it initially incorporates cognitive restructuring and emotional regulation to alter a person's cognition and emotions.\nA popularly noted counseling intervention known as dialectical behavior therapy (DBT) includes the use of a chain analysis, as well as cognitive restructuring, emotional regulation, distress tolerance, counterconditioning (mindfulness), and contingency management (positive reinforcement). DBT is quite similar to acceptance and commitment therapy, but contrasts in that it derives from a CBT framework. Although DBT is most widely researched for and empirically validated to reduce the risk of suicide in psychiatric patients with borderline personality disorder, it can often be applied effectively to other mental health conditions, such as substance abuse, as well as mood and eating disorders.\nMost research on exposure therapies (also called desensitization)—ranging from eye movement desensitization and reprocessing therapy to exposure and response prevention—are conducted through a CBT framework in non-behavior analytic journals, and these enhanced exposure therapies are well-established in the research literature for treating phobic, post-traumatic stress, and other anxiety disorders (such as obsessive-compulsive disorder, or OCD).\nCognitive-based behavioral activation (BA)—the psychotherapeutic approach used for depression—is shown to be highly effective and is widely used in clinical practice. Some large randomized control trials have indicated that cognitive-based BA is as beneficial as antidepressant medications but more efficacious than traditional cognitive therapy. Other commonly used clinical treatments derived from behavioral learning principles that are often implemented through a CBT model include community reinforcement approach and family training, and habit reversal training for substance abuse and tics, respectively.\n\nRelated therapies\nList of notable behaviorists\nSee also\nReference in APA 7th edition format\nFurther reading\nExternal links\n\nGraham, George. \"Behaviorism\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\n\"Behaviorism\". Internet Encyclopedia of Philosophy.",
    "Bias–variance decomposition": "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\nThe bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\nThe bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\nThe variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\n\nMotivation\nThe bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that may fail to capture important regularities (i.e. underfit) in the data.\nIt is an often made fallacy to assume that complex models must have high variance; High variance models are 'complex' in some sense, but the reverse needs not be true.\nIn addition, one has to be careful how to define complexity: In particular, the number of parameters used to describe the model is a poor measure of complexity. This is illustrated by an example adapted from: The model \n  \n    \n      \n        \n          f\n          \n            a\n            ,\n            b\n          \n        \n        (\n        x\n        )\n        =\n        a\n        sin\n        ⁡\n        (\n        b\n        x\n        )\n      \n    \n    {\\displaystyle f_{a,b}(x)=a\\sin(bx)}\n   has only two parameters (\n  \n    \n      a\n      ,\n      b\n    \n    a,b\n  ) but it can interpolate any number of points by oscillating with a high enough frequency, resulting in both a high bias and high variance.\nAn analogy can be made to the relationship between accuracy and precision. Accuracy is a description of bias and can intuitively be improved by selecting from only local information. Consequently, a sample will appear accurate (i.e. have low bias) under the aforementioned selection conditions, but may result in underfitting. In other words, test data may not agree as closely with training data, which would indicate imprecision and therefore inflated variance. A graphical example would be a straight line fit to data exhibiting quadratic behavior overall. Precision is a description of variance and generally can only be improved by selecting information from a comparatively larger space. The option to select many data points over a broad sample space is the ideal condition for any analysis. However, intrinsic constraints (whether physical, theoretical, computational, etc.) will always play a limiting role. The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting). This means that test data would also not agree as closely with the training data, but in this case the reason is due to inaccuracy or high bias. To borrow from the previous example, the graphical representation would appear as a high-order polynomial fit to the same data exhibiting quadratic behavior. Note that error in each case is measured the same way, but the reason ascribed to the error is different depending on the balance between bias and variance. To mitigate how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage.\n\nBias–variance decomposition of mean squared error\nSuppose that we have a training set consisting of a set of points \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n    \n    x_{1},\\dots ,x_{n}\n   and real values \n  \n    \n      y\n      \n        i\n      \n    \n    y_{i}\n   associated with each point \n  \n    \n      x\n      \n        i\n      \n    \n    x_{i}\n  . We assume that there is a function f(x) such as  \n  \n    \n      \n        y\n        =\n        f\n        (\n        x\n        )\n        +\n        ε\n      \n    \n    {\\displaystyle y=f(x)+\\varepsilon }\n  , where the noise, \n  \n    ε\n    \\varepsilon\n  , has zero mean and variance \n  \n    \n      σ\n      \n        2\n      \n    \n    \\sigma ^{2}\n  .\nWe want to find a function \n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n      \n    \n    {\\displaystyle {\\hat {f}}(x;D)}\n  , that approximates the true function \n  \n    \n      f\n      (\n      x\n      )\n    \n    f(x)\n   as well as possible, by means of some learning algorithm based on a training dataset (sample) \n  \n    \n      \n        D\n        =\n        {\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        …\n        ,\n        (\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle D=\\{(x_{1},y_{1})\\dots ,(x_{n},y_{n})\\}}\n  . We make \"as well as possible\" precise by measuring the mean squared error between \n  \n    y\n    y\n   and \n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n      \n    \n    {\\displaystyle {\\hat {f}}(x;D)}\n  : we want \n  \n    \n      \n        (\n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (y-{\\hat {f}}(x;D))^{2}}\n   to be minimal, both for \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n    \n    x_{1},\\dots ,x_{n}\n   and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the \n  \n    \n      y\n      \n        i\n      \n    \n    y_{i}\n   contain noise \n  \n    ε\n    \\varepsilon\n  ; this means we must be prepared to accept an irreducible error in any function we come up with.\nFinding an \n  \n    \n      \n        \n          f\n          ^\n        \n      \n    \n    {\\hat {f}}\n   that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function \n  \n    \n      \n        \n          f\n          ^\n        \n      \n    \n    {\\hat {f}}\n   we select, we can decompose its expected error on an unseen sample \n  \n    x\n    x\n   (i.e. conditional to x) as follows:: 34 : 223 \n\n  \n    \n      \n        \n          E\n          \n            D\n            ,\n            ε\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            (\n          \n        \n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        =\n        \n          \n            (\n          \n        \n        \n          Bias\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n        +\n        \n          Var\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} _{D,\\varepsilon }{\\Big [}{\\big (}y-{\\hat {f}}(x;D){\\big )}^{2}{\\Big ]}={\\Big (}\\operatorname {Bias} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}{\\Big )}^{2}+\\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}+\\sigma ^{2}}\n  where\n\n  \n    \n      \n        \n          Bias\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        =\n        \n          E\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        −\n        f\n        (\n        x\n        )\n        \n          \n            ]\n          \n        \n        =\n        \n          E\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        −\n        \n          E\n          \n            y\n            \n              |\n            \n            x\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        y\n        (\n        x\n        )\n        \n          \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\operatorname {Bias} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}=\\operatorname {E} _{D}{\\big [}{\\hat {f}}(x;D)-f(x){\\big ]}=\\operatorname {E} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}-\\operatorname {E} _{y|x}{\\big [}y(x){\\big ]},}\n  \n  \n    \n      \n        \n          Var\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        =\n        \n          E\n          \n            D\n          \n        \n        ⁡\n        [\n        \n          \n            (\n          \n        \n        \n          E\n          \n            D\n          \n        \n        ⁡\n        [\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        ]\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n        ]\n        .\n      \n    \n    {\\displaystyle \\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}=\\operatorname {E} _{D}[{\\big (}\\operatorname {E} _{D}[{\\hat {f}}(x;D)]-{\\hat {f}}(x;D){\\big )}^{2}].}\n  and \n\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n        =\n        \n          E\n          \n            y\n          \n        \n        ⁡\n        [\n        (\n        y\n        −\n        \n          \n            \n              \n                f\n                (\n                x\n                )\n              \n              ⏟\n            \n          \n          \n            \n              E\n              \n                y\n                \n                  |\n                \n                x\n              \n            \n            [\n            y\n            ]\n          \n        \n        \n          )\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle \\sigma ^{2}=\\operatorname {E} _{y}[(y-\\underbrace {f(x)} _{E_{y|x}[y]})^{2}]}\n  The expectation ranges over different choices of the training set \n  \n    \n      \n        D\n        =\n        {\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        …\n        ,\n        (\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle D=\\{(x_{1},y_{1})\\dots ,(x_{n},y_{n})\\}}\n  , all sampled from the same joint distribution \n  \n    \n      P\n      (\n      x\n      ,\n      y\n      )\n    \n    P(x,y)\n   which can for example be done via bootstrapping.\nThe three terms represent:\n\nthe square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function \n  \n    \n      f\n      (\n      x\n      )\n    \n    f(x)\n   using a learning method for linear models, there will be error in the estimates \n  \n    \n      \n        \n          \n            f\n            ^\n          \n        \n      \n      (\n      x\n      )\n    \n    {\\hat {f}}(x)\n   due to this assumption;\nthe variance of the learning method, or, intuitively, how much the learning method \n  \n    \n      \n        \n          \n            f\n            ^\n          \n        \n      \n      (\n      x\n      )\n    \n    {\\hat {f}}(x)\n   will move around its mean;\nthe irreducible error \n  \n    \n      σ\n      \n        2\n      \n    \n    \\sigma ^{2}\n  .Since all three terms are non-negative, the irreducible error forms a lower bound on the expected error on unseen samples.: 34 The more complex the model \n  \n    \n      \n        \n          \n            f\n            ^\n          \n        \n      \n      (\n      x\n      )\n    \n    {\\hat {f}}(x)\n   is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model \"move\" more to capture the data points, and hence its variance will be larger.\n\nDerivation\nThe derivation of the bias–variance decomposition for squared error proceeds as follows. For notational convenience, we abbreviate \n  \n    \n      f\n      =\n      f\n      (\n      x\n      )\n    \n    f=f(x)\n  , \n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n      \n    \n    {\\displaystyle {\\hat {f}}={\\hat {f}}(x;D)}\n   and we drop the \n  \n    D\n    D\n   subscript on our expectation operators.\nLet us write the mean-squared error of our model:\n\n  \n    \n      \n        \n          MSE\n        \n        ≜\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        (\n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        =\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        \n          y\n          \n            2\n          \n        \n        −\n        2\n        y\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        +\n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        =\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        \n          y\n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        −\n        2\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        y\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        \n          \n            ]\n          \n        \n        +\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\text{MSE}}\\triangleq \\operatorname {E} {\\big [}(y-{\\hat {f}})^{2}{\\big ]}=\\operatorname {E} {\\big [}y^{2}-2y{\\hat {f}}+{\\hat {f}}^{2}{\\big ]}=\\operatorname {E} {\\big [}y^{2}{\\big ]}-2\\operatorname {E} {\\big [}y{\\hat {f}}{\\big ]}+\\operatorname {E} {\\big [}{\\hat {f}}^{2}{\\big ]}}\n  First,\n\n  \n    \n      \n        \n          \n            \n              \n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  \n                    \n                      \n                        f\n                        ^\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                Var\n                ⁡\n                (\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                )\n                +\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  ]\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                  since \n                \n                Var\n                ⁡\n                [\n                X\n                ]\n                ≜\n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n                =\n                E\n                ⁡\n                [\n                \n                  X\n                  \n                    2\n                  \n                \n                ]\n                −\n                E\n                ⁡\n                [\n                X\n                \n                  ]\n                  \n                    2\n                  \n                \n                \n                   for any random variable \n                \n                X\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {E} {\\big [}{\\hat {f}}^{2}{\\big ]}&=\\operatorname {Var} ({\\hat {f}})+\\operatorname {E} [{\\hat {f}}]^{2}&&{\\text{since }}\\operatorname {Var} [X]\\triangleq \\operatorname {E} {\\Big [}(X-\\operatorname {E} [X])^{2}{\\Big ]}=\\operatorname {E} [X^{2}]-\\operatorname {E} [X]^{2}{\\text{ for any random variable }}X\\end{aligned}}}\n  Secondly, since we model \n  \n    \n      \n        y\n        =\n        f\n        +\n        ε\n      \n    \n    {\\displaystyle y=f+\\varepsilon }\n  , we show that\n\n  \n    \n      \n        \n          \n            \n              \n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  y\n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                (\n                f\n                +\n                ε\n                \n                  )\n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                \n                  f\n                  \n                    2\n                  \n                \n                ]\n                +\n                2\n                E\n                ⁡\n                [\n                f\n                ε\n                ]\n                +\n                E\n                ⁡\n                [\n                \n                  ε\n                  \n                    2\n                  \n                \n                ]\n              \n              \n              \n                \n                  by linearity of \n                \n                E\n              \n            \n            \n              \n              \n                \n                =\n                \n                  f\n                  \n                    2\n                  \n                \n                +\n                2\n                f\n                E\n                ⁡\n                [\n                ε\n                ]\n                +\n                E\n                ⁡\n                [\n                \n                  ε\n                  \n                    2\n                  \n                \n                ]\n              \n              \n              \n                \n                  since \n                \n                f\n                \n                   does not depend on the data\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  f\n                  \n                    2\n                  \n                \n                +\n                2\n                f\n                ⋅\n                0\n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                  since \n                \n                ε\n                \n                   has zero mean and variance \n                \n                \n                  σ\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {E} {\\big [}y^{2}{\\big ]}&=\\operatorname {E} {\\big [}(f+\\varepsilon )^{2}{\\big ]}\\\\&=\\operatorname {E} [f^{2}]+2\\operatorname {E} [f\\varepsilon ]+\\operatorname {E} [\\varepsilon ^{2}]&&{\\text{by linearity of }}\\operatorname {E} \\\\&=f^{2}+2f\\operatorname {E} [\\varepsilon ]+\\operatorname {E} [\\varepsilon ^{2}]&&{\\text{since }}f{\\text{ does not depend on the data}}\\\\&=f^{2}+2f\\cdot 0+\\sigma ^{2}&&{\\text{since }}\\varepsilon {\\text{ has zero mean and variance }}\\sigma ^{2}\\end{aligned}}}\n  Lastly,\n\n  \n    \n      \n        \n          \n            \n              \n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                y\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                (\n                f\n                +\n                ε\n                )\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                f\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                E\n                ⁡\n                [\n                ε\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n              \n              \n              \n                \n                  by linearity of \n                \n                E\n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                f\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                E\n                ⁡\n                [\n                ε\n                ]\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n              \n              \n              \n                \n                  since \n                \n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                   and \n                \n                ε\n                \n                   are independent\n                \n              \n            \n            \n              \n              \n                \n                =\n                f\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n              \n              \n              \n                \n                  since \n                \n                E\n                ⁡\n                [\n                ε\n                ]\n                =\n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {E} {\\big [}y{\\hat {f}}{\\big ]}&=\\operatorname {E} {\\big [}(f+\\varepsilon ){\\hat {f}}{\\big ]}\\\\&=\\operatorname {E} [f{\\hat {f}}]+\\operatorname {E} [\\varepsilon {\\hat {f}}]&&{\\text{by linearity of }}\\operatorname {E} \\\\&=\\operatorname {E} [f{\\hat {f}}]+\\operatorname {E} [\\varepsilon ]\\operatorname {E} [{\\hat {f}}]&&{\\text{since }}{\\hat {f}}{\\text{ and }}\\varepsilon {\\text{ are independent}}\\\\&=f\\operatorname {E} [{\\hat {f}}]&&{\\text{since }}\\operatorname {E} [\\varepsilon ]=0\\end{aligned}}}\n  Eventually, we plug these 3 formulas in our previous derivation of \n  \n    \n      \n        \n          MSE\n        \n      \n    \n    {\\displaystyle {\\text{MSE}}}\n   and thus show that:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  MSE\n                \n              \n              \n                \n                =\n                \n                  f\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                −\n                2\n                f\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                Var\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  ]\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                (\n                f\n                −\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                Var\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                Bias\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  ]\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                Var\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MSE}}&=f^{2}+\\sigma ^{2}-2f\\operatorname {E} [{\\hat {f}}]+\\operatorname {Var} [{\\hat {f}}]+\\operatorname {E} [{\\hat {f}}]^{2}\\\\&=(f-\\operatorname {E} [{\\hat {f}}])^{2}+\\sigma ^{2}+\\operatorname {Var} {\\big [}{\\hat {f}}{\\big ]}\\\\[5pt]&=\\operatorname {Bias} [{\\hat {f}}]^{2}+\\sigma ^{2}+\\operatorname {Var} {\\big [}{\\hat {f}}{\\big ]}\\end{aligned}}}\n  Finally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over \n  \n    \n      \n        x\n        ∼\n        P\n      \n    \n    {\\displaystyle x\\sim P}\n  :\n\n  \n    \n      \n        \n          MSE\n        \n        =\n        \n          E\n          \n            x\n          \n        \n        ⁡\n        \n          \n            {\n          \n        \n        \n          Bias\n          \n            D\n          \n        \n        ⁡\n        [\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          ]\n          \n            2\n          \n        \n        +\n        \n          Var\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        \n          \n            }\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\text{MSE}}=\\operatorname {E} _{x}{\\bigg \\{}\\operatorname {Bias} _{D}[{\\hat {f}}(x;D)]^{2}+\\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}{\\bigg \\}}+\\sigma ^{2}.}\n\nApproaches\nDimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,\n\nlinear  and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.\nIn artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied.\nIn k-nearest neighbor models, a high value of k leads to high bias and low variance (see below).\nIn instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.\nIn decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.: 307 One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many \"weak\" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines \"strong\" learners in a way that reduces their variance.\nModel validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off.\n\nk-nearest neighbors\nIn the case of k-nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias–variance decomposition to the parameter k:: 37, 223 \n\n  \n    \n      \n        E\n        ⁡\n        [\n        (\n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        )\n        \n          )\n          \n            2\n          \n        \n        ∣\n        X\n        =\n        x\n        ]\n        =\n        \n          \n            (\n            \n              f\n              (\n              x\n              )\n              −\n              \n                \n                  1\n                  k\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  k\n                \n              \n              f\n              (\n              \n                N\n                \n                  i\n                \n              \n              (\n              x\n              )\n              )\n            \n            )\n          \n          \n            2\n          \n        \n        +\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            k\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} [(y-{\\hat {f}}(x))^{2}\\mid X=x]=\\left(f(x)-{\\frac {1}{k}}\\sum _{i=1}^{k}f(N_{i}(x))\\right)^{2}+{\\frac {\\sigma ^{2}}{k}}+\\sigma ^{2}}\n  where \n  \n    \n      \n        N\n        \n          1\n        \n      \n      (\n      x\n      )\n      ,\n      …\n      ,\n      \n        N\n        \n          k\n        \n      \n      (\n      x\n      )\n    \n    N_{1}(x),\\dots ,N_{k}(x)\n   are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under \"reasonable assumptions\" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.\n\nApplications\nIn regression\nThe bias–variance decomposition forms the conceptual basis for regression regularization methods such as Lasso and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution.  Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.\n\nIn classification\nThe bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it is possible to find a similar decomposition. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.It has been argued that as training data increases, the variance of learned models will tend to decrease, and hence that as training data quantity increases, error is minimized by methods that learn models with lesser bias, and that conversely, for smaller training data quantities it is ever more important to minimize variance.\n\nIn reinforcement learning\nEven though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.\n\nIn human learning\nWhile widely discussed in the context of machine learning, the bias–variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.Geman et al. argue that the bias–variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of \"hard wiring\"   that is later tuned by experience.  This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.\n\nSee also\nReferences\nExternal links\nMLU-Explain: The Bias Variance Tradeoff — An interactive visualization of the bias-variance tradeoff in LOESS Regression and K-Nearest Neighbors.\n\nLiterature\nHarry L. Van Trees; Kristine L. Bell, \"Exploring Estimator BiasVariance Tradeoffs Using the Uniform CR Bound,\" in Bayesian Bounds for Parameter Estimation and Nonlinear Filtering/Tracking , IEEE, 2007, pp.451-466, doi: 10.1109/9780470544198.ch40.",
    "Bias–variance tradeoff": "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\nThe bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\nThe bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\nThe variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\n\nMotivation\nThe bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that may fail to capture important regularities (i.e. underfit) in the data.\nIt is an often made fallacy to assume that complex models must have high variance; High variance models are 'complex' in some sense, but the reverse needs not be true.\nIn addition, one has to be careful how to define complexity: In particular, the number of parameters used to describe the model is a poor measure of complexity. This is illustrated by an example adapted from: The model \n  \n    \n      \n        \n          f\n          \n            a\n            ,\n            b\n          \n        \n        (\n        x\n        )\n        =\n        a\n        sin\n        ⁡\n        (\n        b\n        x\n        )\n      \n    \n    {\\displaystyle f_{a,b}(x)=a\\sin(bx)}\n   has only two parameters (\n  \n    \n      a\n      ,\n      b\n    \n    a,b\n  ) but it can interpolate any number of points by oscillating with a high enough frequency, resulting in both a high bias and high variance.\nAn analogy can be made to the relationship between accuracy and precision. Accuracy is a description of bias and can intuitively be improved by selecting from only local information. Consequently, a sample will appear accurate (i.e. have low bias) under the aforementioned selection conditions, but may result in underfitting. In other words, test data may not agree as closely with training data, which would indicate imprecision and therefore inflated variance. A graphical example would be a straight line fit to data exhibiting quadratic behavior overall. Precision is a description of variance and generally can only be improved by selecting information from a comparatively larger space. The option to select many data points over a broad sample space is the ideal condition for any analysis. However, intrinsic constraints (whether physical, theoretical, computational, etc.) will always play a limiting role. The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting). This means that test data would also not agree as closely with the training data, but in this case the reason is due to inaccuracy or high bias. To borrow from the previous example, the graphical representation would appear as a high-order polynomial fit to the same data exhibiting quadratic behavior. Note that error in each case is measured the same way, but the reason ascribed to the error is different depending on the balance between bias and variance. To mitigate how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage.\n\nBias–variance decomposition of mean squared error\nSuppose that we have a training set consisting of a set of points \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n    \n    x_{1},\\dots ,x_{n}\n   and real values \n  \n    \n      y\n      \n        i\n      \n    \n    y_{i}\n   associated with each point \n  \n    \n      x\n      \n        i\n      \n    \n    x_{i}\n  . We assume that there is a function f(x) such as  \n  \n    \n      \n        y\n        =\n        f\n        (\n        x\n        )\n        +\n        ε\n      \n    \n    {\\displaystyle y=f(x)+\\varepsilon }\n  , where the noise, \n  \n    ε\n    \\varepsilon\n  , has zero mean and variance \n  \n    \n      σ\n      \n        2\n      \n    \n    \\sigma ^{2}\n  .\nWe want to find a function \n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n      \n    \n    {\\displaystyle {\\hat {f}}(x;D)}\n  , that approximates the true function \n  \n    \n      f\n      (\n      x\n      )\n    \n    f(x)\n   as well as possible, by means of some learning algorithm based on a training dataset (sample) \n  \n    \n      \n        D\n        =\n        {\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        …\n        ,\n        (\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle D=\\{(x_{1},y_{1})\\dots ,(x_{n},y_{n})\\}}\n  . We make \"as well as possible\" precise by measuring the mean squared error between \n  \n    y\n    y\n   and \n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n      \n    \n    {\\displaystyle {\\hat {f}}(x;D)}\n  : we want \n  \n    \n      \n        (\n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (y-{\\hat {f}}(x;D))^{2}}\n   to be minimal, both for \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n    \n    x_{1},\\dots ,x_{n}\n   and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the \n  \n    \n      y\n      \n        i\n      \n    \n    y_{i}\n   contain noise \n  \n    ε\n    \\varepsilon\n  ; this means we must be prepared to accept an irreducible error in any function we come up with.\nFinding an \n  \n    \n      \n        \n          f\n          ^\n        \n      \n    \n    {\\hat {f}}\n   that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function \n  \n    \n      \n        \n          f\n          ^\n        \n      \n    \n    {\\hat {f}}\n   we select, we can decompose its expected error on an unseen sample \n  \n    x\n    x\n   (i.e. conditional to x) as follows:: 34 : 223 \n\n  \n    \n      \n        \n          E\n          \n            D\n            ,\n            ε\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            (\n          \n        \n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        =\n        \n          \n            (\n          \n        \n        \n          Bias\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n        +\n        \n          Var\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} _{D,\\varepsilon }{\\Big [}{\\big (}y-{\\hat {f}}(x;D){\\big )}^{2}{\\Big ]}={\\Big (}\\operatorname {Bias} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}{\\Big )}^{2}+\\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}+\\sigma ^{2}}\n  where\n\n  \n    \n      \n        \n          Bias\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        =\n        \n          E\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        −\n        f\n        (\n        x\n        )\n        \n          \n            ]\n          \n        \n        =\n        \n          E\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        −\n        \n          E\n          \n            y\n            \n              |\n            \n            x\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        y\n        (\n        x\n        )\n        \n          \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\operatorname {Bias} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}=\\operatorname {E} _{D}{\\big [}{\\hat {f}}(x;D)-f(x){\\big ]}=\\operatorname {E} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}-\\operatorname {E} _{y|x}{\\big [}y(x){\\big ]},}\n  \n  \n    \n      \n        \n          Var\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        =\n        \n          E\n          \n            D\n          \n        \n        ⁡\n        [\n        \n          \n            (\n          \n        \n        \n          E\n          \n            D\n          \n        \n        ⁡\n        [\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        ]\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            \n              )\n            \n          \n          \n            2\n          \n        \n        ]\n        .\n      \n    \n    {\\displaystyle \\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}=\\operatorname {E} _{D}[{\\big (}\\operatorname {E} _{D}[{\\hat {f}}(x;D)]-{\\hat {f}}(x;D){\\big )}^{2}].}\n  and \n\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n        =\n        \n          E\n          \n            y\n          \n        \n        ⁡\n        [\n        (\n        y\n        −\n        \n          \n            \n              \n                f\n                (\n                x\n                )\n              \n              ⏟\n            \n          \n          \n            \n              E\n              \n                y\n                \n                  |\n                \n                x\n              \n            \n            [\n            y\n            ]\n          \n        \n        \n          )\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle \\sigma ^{2}=\\operatorname {E} _{y}[(y-\\underbrace {f(x)} _{E_{y|x}[y]})^{2}]}\n  The expectation ranges over different choices of the training set \n  \n    \n      \n        D\n        =\n        {\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        …\n        ,\n        (\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle D=\\{(x_{1},y_{1})\\dots ,(x_{n},y_{n})\\}}\n  , all sampled from the same joint distribution \n  \n    \n      P\n      (\n      x\n      ,\n      y\n      )\n    \n    P(x,y)\n   which can for example be done via bootstrapping.\nThe three terms represent:\n\nthe square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function \n  \n    \n      f\n      (\n      x\n      )\n    \n    f(x)\n   using a learning method for linear models, there will be error in the estimates \n  \n    \n      \n        \n          \n            f\n            ^\n          \n        \n      \n      (\n      x\n      )\n    \n    {\\hat {f}}(x)\n   due to this assumption;\nthe variance of the learning method, or, intuitively, how much the learning method \n  \n    \n      \n        \n          \n            f\n            ^\n          \n        \n      \n      (\n      x\n      )\n    \n    {\\hat {f}}(x)\n   will move around its mean;\nthe irreducible error \n  \n    \n      σ\n      \n        2\n      \n    \n    \\sigma ^{2}\n  .Since all three terms are non-negative, the irreducible error forms a lower bound on the expected error on unseen samples.: 34 The more complex the model \n  \n    \n      \n        \n          \n            f\n            ^\n          \n        \n      \n      (\n      x\n      )\n    \n    {\\hat {f}}(x)\n   is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model \"move\" more to capture the data points, and hence its variance will be larger.\n\nDerivation\nThe derivation of the bias–variance decomposition for squared error proceeds as follows. For notational convenience, we abbreviate \n  \n    \n      f\n      =\n      f\n      (\n      x\n      )\n    \n    f=f(x)\n  , \n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n      \n    \n    {\\displaystyle {\\hat {f}}={\\hat {f}}(x;D)}\n   and we drop the \n  \n    D\n    D\n   subscript on our expectation operators.\nLet us write the mean-squared error of our model:\n\n  \n    \n      \n        \n          MSE\n        \n        ≜\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        (\n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        =\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        \n          y\n          \n            2\n          \n        \n        −\n        2\n        y\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        +\n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        =\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        \n          y\n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n        −\n        2\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        y\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        \n          \n            ]\n          \n        \n        +\n        E\n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\text{MSE}}\\triangleq \\operatorname {E} {\\big [}(y-{\\hat {f}})^{2}{\\big ]}=\\operatorname {E} {\\big [}y^{2}-2y{\\hat {f}}+{\\hat {f}}^{2}{\\big ]}=\\operatorname {E} {\\big [}y^{2}{\\big ]}-2\\operatorname {E} {\\big [}y{\\hat {f}}{\\big ]}+\\operatorname {E} {\\big [}{\\hat {f}}^{2}{\\big ]}}\n  First,\n\n  \n    \n      \n        \n          \n            \n              \n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  \n                    \n                      \n                        f\n                        ^\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                Var\n                ⁡\n                (\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                )\n                +\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  ]\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                  since \n                \n                Var\n                ⁡\n                [\n                X\n                ]\n                ≜\n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n                =\n                E\n                ⁡\n                [\n                \n                  X\n                  \n                    2\n                  \n                \n                ]\n                −\n                E\n                ⁡\n                [\n                X\n                \n                  ]\n                  \n                    2\n                  \n                \n                \n                   for any random variable \n                \n                X\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {E} {\\big [}{\\hat {f}}^{2}{\\big ]}&=\\operatorname {Var} ({\\hat {f}})+\\operatorname {E} [{\\hat {f}}]^{2}&&{\\text{since }}\\operatorname {Var} [X]\\triangleq \\operatorname {E} {\\Big [}(X-\\operatorname {E} [X])^{2}{\\Big ]}=\\operatorname {E} [X^{2}]-\\operatorname {E} [X]^{2}{\\text{ for any random variable }}X\\end{aligned}}}\n  Secondly, since we model \n  \n    \n      \n        y\n        =\n        f\n        +\n        ε\n      \n    \n    {\\displaystyle y=f+\\varepsilon }\n  , we show that\n\n  \n    \n      \n        \n          \n            \n              \n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  y\n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                (\n                f\n                +\n                ε\n                \n                  )\n                  \n                    2\n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                \n                  f\n                  \n                    2\n                  \n                \n                ]\n                +\n                2\n                E\n                ⁡\n                [\n                f\n                ε\n                ]\n                +\n                E\n                ⁡\n                [\n                \n                  ε\n                  \n                    2\n                  \n                \n                ]\n              \n              \n              \n                \n                  by linearity of \n                \n                E\n              \n            \n            \n              \n              \n                \n                =\n                \n                  f\n                  \n                    2\n                  \n                \n                +\n                2\n                f\n                E\n                ⁡\n                [\n                ε\n                ]\n                +\n                E\n                ⁡\n                [\n                \n                  ε\n                  \n                    2\n                  \n                \n                ]\n              \n              \n              \n                \n                  since \n                \n                f\n                \n                   does not depend on the data\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  f\n                  \n                    2\n                  \n                \n                +\n                2\n                f\n                ⋅\n                0\n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                  since \n                \n                ε\n                \n                   has zero mean and variance \n                \n                \n                  σ\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {E} {\\big [}y^{2}{\\big ]}&=\\operatorname {E} {\\big [}(f+\\varepsilon )^{2}{\\big ]}\\\\&=\\operatorname {E} [f^{2}]+2\\operatorname {E} [f\\varepsilon ]+\\operatorname {E} [\\varepsilon ^{2}]&&{\\text{by linearity of }}\\operatorname {E} \\\\&=f^{2}+2f\\operatorname {E} [\\varepsilon ]+\\operatorname {E} [\\varepsilon ^{2}]&&{\\text{since }}f{\\text{ does not depend on the data}}\\\\&=f^{2}+2f\\cdot 0+\\sigma ^{2}&&{\\text{since }}\\varepsilon {\\text{ has zero mean and variance }}\\sigma ^{2}\\end{aligned}}}\n  Lastly,\n\n  \n    \n      \n        \n          \n            \n              \n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                y\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                E\n                ⁡\n                \n                  \n                    [\n                  \n                \n                (\n                f\n                +\n                ε\n                )\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                f\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                E\n                ⁡\n                [\n                ε\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n              \n              \n              \n                \n                  by linearity of \n                \n                E\n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                f\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                E\n                ⁡\n                [\n                ε\n                ]\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n              \n              \n              \n                \n                  since \n                \n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                   and \n                \n                ε\n                \n                   are independent\n                \n              \n            \n            \n              \n              \n                \n                =\n                f\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n              \n              \n              \n                \n                  since \n                \n                E\n                ⁡\n                [\n                ε\n                ]\n                =\n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {E} {\\big [}y{\\hat {f}}{\\big ]}&=\\operatorname {E} {\\big [}(f+\\varepsilon ){\\hat {f}}{\\big ]}\\\\&=\\operatorname {E} [f{\\hat {f}}]+\\operatorname {E} [\\varepsilon {\\hat {f}}]&&{\\text{by linearity of }}\\operatorname {E} \\\\&=\\operatorname {E} [f{\\hat {f}}]+\\operatorname {E} [\\varepsilon ]\\operatorname {E} [{\\hat {f}}]&&{\\text{since }}{\\hat {f}}{\\text{ and }}\\varepsilon {\\text{ are independent}}\\\\&=f\\operatorname {E} [{\\hat {f}}]&&{\\text{since }}\\operatorname {E} [\\varepsilon ]=0\\end{aligned}}}\n  Eventually, we plug these 3 formulas in our previous derivation of \n  \n    \n      \n        \n          MSE\n        \n      \n    \n    {\\displaystyle {\\text{MSE}}}\n   and thus show that:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  MSE\n                \n              \n              \n                \n                =\n                \n                  f\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                −\n                2\n                f\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                Var\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                +\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  ]\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                (\n                f\n                −\n                E\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                Var\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                Bias\n                ⁡\n                [\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  ]\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                Var\n                ⁡\n                \n                  \n                    [\n                  \n                \n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MSE}}&=f^{2}+\\sigma ^{2}-2f\\operatorname {E} [{\\hat {f}}]+\\operatorname {Var} [{\\hat {f}}]+\\operatorname {E} [{\\hat {f}}]^{2}\\\\&=(f-\\operatorname {E} [{\\hat {f}}])^{2}+\\sigma ^{2}+\\operatorname {Var} {\\big [}{\\hat {f}}{\\big ]}\\\\[5pt]&=\\operatorname {Bias} [{\\hat {f}}]^{2}+\\sigma ^{2}+\\operatorname {Var} {\\big [}{\\hat {f}}{\\big ]}\\end{aligned}}}\n  Finally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over \n  \n    \n      \n        x\n        ∼\n        P\n      \n    \n    {\\displaystyle x\\sim P}\n  :\n\n  \n    \n      \n        \n          MSE\n        \n        =\n        \n          E\n          \n            x\n          \n        \n        ⁡\n        \n          \n            {\n          \n        \n        \n          Bias\n          \n            D\n          \n        \n        ⁡\n        [\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          ]\n          \n            2\n          \n        \n        +\n        \n          Var\n          \n            D\n          \n        \n        ⁡\n        \n          \n            [\n          \n        \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        ;\n        D\n        )\n        \n          \n            ]\n          \n        \n        \n          \n            }\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\text{MSE}}=\\operatorname {E} _{x}{\\bigg \\{}\\operatorname {Bias} _{D}[{\\hat {f}}(x;D)]^{2}+\\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}{\\bigg \\}}+\\sigma ^{2}.}\n\nApproaches\nDimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,\n\nlinear  and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.\nIn artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied.\nIn k-nearest neighbor models, a high value of k leads to high bias and low variance (see below).\nIn instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.\nIn decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.: 307 One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many \"weak\" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines \"strong\" learners in a way that reduces their variance.\nModel validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off.\n\nk-nearest neighbors\nIn the case of k-nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias–variance decomposition to the parameter k:: 37, 223 \n\n  \n    \n      \n        E\n        ⁡\n        [\n        (\n        y\n        −\n        \n          \n            \n              f\n              ^\n            \n          \n        \n        (\n        x\n        )\n        \n          )\n          \n            2\n          \n        \n        ∣\n        X\n        =\n        x\n        ]\n        =\n        \n          \n            (\n            \n              f\n              (\n              x\n              )\n              −\n              \n                \n                  1\n                  k\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  k\n                \n              \n              f\n              (\n              \n                N\n                \n                  i\n                \n              \n              (\n              x\n              )\n              )\n            \n            )\n          \n          \n            2\n          \n        \n        +\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            k\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} [(y-{\\hat {f}}(x))^{2}\\mid X=x]=\\left(f(x)-{\\frac {1}{k}}\\sum _{i=1}^{k}f(N_{i}(x))\\right)^{2}+{\\frac {\\sigma ^{2}}{k}}+\\sigma ^{2}}\n  where \n  \n    \n      \n        N\n        \n          1\n        \n      \n      (\n      x\n      )\n      ,\n      …\n      ,\n      \n        N\n        \n          k\n        \n      \n      (\n      x\n      )\n    \n    N_{1}(x),\\dots ,N_{k}(x)\n   are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under \"reasonable assumptions\" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.\n\nApplications\nIn regression\nThe bias–variance decomposition forms the conceptual basis for regression regularization methods such as Lasso and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution.  Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.\n\nIn classification\nThe bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it is possible to find a similar decomposition. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.It has been argued that as training data increases, the variance of learned models will tend to decrease, and hence that as training data quantity increases, error is minimized by methods that learn models with lesser bias, and that conversely, for smaller training data quantities it is ever more important to minimize variance.\n\nIn reinforcement learning\nEven though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.\n\nIn human learning\nWhile widely discussed in the context of machine learning, the bias–variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.Geman et al. argue that the bias–variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of \"hard wiring\"   that is later tuned by experience.  This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.\n\nSee also\nReferences\nExternal links\nMLU-Explain: The Bias Variance Tradeoff — An interactive visualization of the bias-variance tradeoff in LOESS Regression and K-Nearest Neighbors.\n\nLiterature\nHarry L. Van Trees; Kristine L. Bell, \"Exploring Estimator BiasVariance Tradeoffs Using the Uniform CR Bound,\" in Bayesian Bounds for Parameter Estimation and Nonlinear Filtering/Tracking , IEEE, 2007, pp.451-466, doi: 10.1109/9780470544198.ch40.",
    "Bibcode (identifier)": "The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.\n\nAdoption\nThe Bibliographic Reference Code (refcode) was originally developed to be used in SIMBAD and the NASA/IPAC Extragalactic Database (NED), but it became a de facto standard and is now used more widely, for example, by the NASA Astrophysics Data System, which coined and prefers the term \"bibcode\".\n\nFormat\nThe code has a fixed length of 19 characters and has the form\n\nYYYYJJJJJVVVVMPPPPAwhere YYYY is the four-digit year of the reference and JJJJJ is a code indicating where the reference was published. In the case of a journal reference, VVVV is the volume number, M indicates the section of the journal where the reference was published (e.g., L for a letters section), PPPP gives the starting page number, and A is the first letter of the last name of the first author. Periods (.) are used to fill unused fields and to pad fields out to their fixed length if too short; padding is done on the right for the publication code and on the left for the volume number and page number. Page numbers greater than 9999 are continued in the M column. The 6-digit article ID numbers (in lieu of page numbers) used by the Physical Review publications since the late 1990s are treated as follows: The first two digits of the article ID, corresponding to the issue number, are converted to a lower-case letter (01 = a, etc.) and inserted into column M. The remaining four digits are used in the page field.\n\nExamples\nSome examples of bibcodes are:\n\nSee also\nDigital object identifier\n\n\n== References ==",
    "Big data": "Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly because of a lack of formal definition, the interpretation that seems to best describe big data is the one associated with a large body of information that we could not comprehend when used only in smaller amounts.Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.\"\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"\n\nDefinition\nThe term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.\nBig data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time. Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data. Big data \"size\" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.\nBig data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.\"Variety\", \"veracity\", and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the \"three Vs\", \"four Vs\", and \"five Vs\". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.\nA 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model.\"In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.\n\nBig data vs. business intelligence\nThe growing maturity of the concept more starkly delineates the difference between \"big data\" and \"business intelligence\":\nBusiness intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.\nBig data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.\n\nCharacteristics\nBig data can be described by the following characteristics:\n\nVolume\nThe quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.Variety\nThe type and nature of the data. Earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. Big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.Velocity\nThe speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.Veracity\nThe truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.Value\nThe worth in information that can be achieved by the processing and analysis of large datasets. Value also can be measured by an assessment of the other qualities of big data. Value may also represent the profitability of information that is retrieved from the analysis of big data.Variability\nThe characteristic of the changing formats, structure, or sources of big data. Big data can include structured, unstructured, or combinations of structured and unstructured data. Big data analysis may integrate raw data from multiple sources. The processing of raw data may also involve transformations of unstructured data to structured data.Other possible characteristics of big data are:\nExhaustive\nWhether the entire system (i.e., \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n  =all) is captured or recorded or not. Big data may or may not include all the available data from sources.Fine-grained and uniquely lexical\nRespectively, the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified.Relational\nIf the data collected contains common fields that would enable a conjoining, or meta-analysis, of different data sets.Extensional\nIf new fields in each element of the data collected can be added or changed easily.Scalability\nIf the size of the big data storage system can expand rapidly.\n\nArchitecture\nBig data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added unstructured data types including XML, JSON, and Avro.\nIn 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.\nCERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current \"big data\" movement.\nIn 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the \"map\" step). The results are then gathered and delivered (the \"reduce\" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named \"Hadoop\". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).\nMIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.\n\nTechnologies\nA 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:\nTechniques for analyzing data, such as A/B testing, machine learning, and natural language processing\nBig data technologies, like business intelligence, cloud computing, and databases\nVisualization, such as charts, graphs, and other displays of the dataMultidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.\nAdditional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called \"Ayasdi\".The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\nReal or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.\n\nApplications\nBig data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year, about twice as fast as the software business as a whole.Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).\nWhile many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.\n\nGovernment\nThe use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.\nCivil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.\n\nInternational development\nResearch on the effective usage of information and communication technologies for development (also known as \"ICT4D\") suggests that big data technology can make important contributions but also present unique challenges to international development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management. Additionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues. The challenge of \"big data for development\" is currently evolving toward the application of this data through machine learning, known as \"artificial intelligence for development (AI4D).\n\nBenefits\nA major practical application of big data for development has been \"fighting poverty with data\". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata  and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:\n\nThematic coverage: including areas that were previously difficult or impossible to measure\nGeographical coverage: our international sources provided sizable and comparable data for almost all countries, including many small countries that usually are not included in international inventories\nLevel of detail: providing fine-grained data with many interrelated variables, and new aspects, like network connections\nTimeliness and timeseries: graphs can be produced within days of being collected\n\nChallenges\nAt the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:\n\nRepresentativeness. While traditional development statistics is mainly concerned with the representativeness of random survey samples, digital trace data is never a random sample.\nGeneralizability. While observational data always represents this source very well, it only represents what it represents, and nothing more. While it is tempting to generalize from specific observations of one platform to broader settings, this is often very deceptive.\nHarmonization. Digital trace data still requires international harmonization of indicators. It adds the challenge of so-called \"data-fusion\", the harmonization of different sources.\nData overload. Analysts and institutions are not used to effectively deal with a large number of variables, which is efficiently done with interactive dashboards. Practitioners still lack a standard workflow that would allow researchers, users and policymakers to efficiently and effectively.\n\nFinance\nBig Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions.. The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large.\n\nHealthcare\nBig data analytics was used in healthcare by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries. Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. \"Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth.\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.\nA related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.\n For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily.\n Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.\n\nThese are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.\n\nEducation\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,\nproduct development, branding) that all use different types of data.\n\nMedia\nTo understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.\nTargeting of consumers (for advertising by marketers)\nData capture\nData journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics.Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.\n\nInsurance\nHealth insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.\n\nInternet of things (IoT)\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.\nKevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\"\n\nInformation technology\nEspecially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.\n\nCase studies\nGovernment\nChina\nThe Integrated Joint Operations Platform (IJOP, 一体化联合作战平台) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals.\nBy 2020, China plans to give all its citizens a personal \"social credit\" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology.\n\nIndia\nBig data analysis was tried out for the BJP to win the 2014 Indian General Election.\nThe Indian government uses numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.\n\nIsrael\nPersonalized diabetic treatments can be created through GlucoMe's big data solution.\n\nUnited Kingdom\nExamples of uses of big data in public services:\n\nData on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify and examine the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.\nJoining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as Meals on Wheels. The connection of data allowed the local authority to avoid any weather-related delay.\n\nUnited States\nIn 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments.\nBig data analysis played a large role in Barack Obama's successful 2012 re-election campaign.\nThe United States Federal Government owns five of the ten most powerful supercomputers in the world.\nThe Utah Data Center has been constructed by the United States National Security Agency. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected.\n\nRetail\nWalmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US Library of Congress.\nWindermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.\nFICO Card Detection System protects accounts worldwide.\n\nScience\nThe Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% of these streams, there are 1,000 collisions of interest per second.As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.\nIf all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5×1020) bytes per day, almost 200 times more than all the other sources combined in the world.\nThe Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.\nWhen the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.\nDecoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by Moore's law.\nThe NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.\nGoogle's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any \"friction points\", or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.\n23andme's DNA database contains the genetic information of over 1,000,000 people worldwide. The company explores selling the \"anonymous aggregated genetic data\" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.\nComputational fluid dynamics (CFD) and hydrodynamic turbulence research generate massive data sets. The Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using \"virtual sensors\" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients' platforms, to cut out services to download raw data. The data have been used in over 150 scientific publications.\n\nSports\nBig data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.\nFuture performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.\n\nTechnology\neBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.\nAmazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.\nFacebook handles 50 billion photos from its user base. As of June 2017, Facebook reached 2 billion monthly active users.\nGoogle was handling roughly 100 billion searches per month as of August 2012.\n\nCOVID-19\nDuring the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.Governments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.\n\nResearch activities\nEncrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.In March 2012, The White House announced a national \"Big Data Initiative\" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.The initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.\nThe U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the \"future orientation index\". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.\nTobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data. Regarding big data, such concepts of magnitude are relative. As it is stated \"If the past is of any guidance, then today's big data most likely will not be considered as such in the near future.\"\n\nSampling big data\nA research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.\nThere has been some work done in sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.\n\nCritique\nCritiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.\n\nCritiques of the big data paradigm\n\"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.\" In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment\", according to an article in the Harvard Business Review.Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggest to use \"abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge\".\nAdditionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.\nIn health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.\nA new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (\"Glory of Science and Philosophy scandal\", C. D. Broad, 1926) are to be considered.Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.\n\nCritiques of the \"V\" model\nThe \"V\" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:\nData completeness: understanding of the non-obvious from data\nData correlation, causation, and predictability: causality as not essential requirement to achieve predictability\nExplainability and interpretability: humans desire to understand and accept what they understand, where algorithms do not cope with this\nLevel of automated decision-making: algorithms that support automated decision making and algorithmic self-learning\n\nCritiques of novelty\nLarge data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.\n\nCritiques of big data execution\nUlf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a \"fad\" in scientific research. Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results that have a bias in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.\nIn the provocative article \"Critical Questions for Big Data\", the authors title big data a part of mythology: \"large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy\". Users of big data are often \"lost in the sheer volume of numbers\", and \"working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth\". Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlations either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers \"speak for themselves\" and revolutionize scientific method, is questioned. Catherine Tucker has pointed to \"hype\" around big data, writing \"By itself, big data is unlikely to be valuable.\" The article explains: \"The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm.\"Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.Big data is a buzzword and a \"vague term\", but at the same time an \"obsession\" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.\nBig data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.\nOn the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.\nIoannidis argued that \"most published research findings are false\" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast – even more so, when only positive results are published.\nFurthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election with varying degrees of success.\n\nCritiques of big data policing and surveillance\nBig data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:\n\nPlacing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm\nIncreasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system\nEncouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusionIf these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.\n\nSee also\nReferences\nFurther reading\nPeter Kinnaird; Inbal Talgam-Cohen, eds. (2012). \"Big Data\". XRDS: Crossroads, The ACM Magazine for Students. Vol. 19, no. 1. Association for Computing Machinery. ISSN 1528-4980. OCLC 779657714.\nJure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Mining of massive datasets. Cambridge University Press. ISBN 9781107077232. OCLC 888463433.\nViktor Mayer-Schönberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN 9781299903029. OCLC 828620988.\nPress, Gil (9 May 2013). \"A Very Short History of Big Data\". forbes.com. Jersey City, NJ. Retrieved 17 September 2016.\nStephens-Davidowitz, Seth (2017). Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are. Dey Street Books. ISBN 978-0062390851.\n\"Big Data: The Management Revolution\". Harvard Business Review. October 2012.\nO'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. ISBN 978-0553418835.\n\nExternal links\n Media related to Big data at Wikimedia Commons\n The dictionary definition of big data at Wiktionary",
    "Binary classifier": "Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\n\nMedical testing to determine if a patient has certain disease or not;\nQuality control in industry, deciding whether a specification has been met;\nIn information retrieval, deciding whether a page should be in the result set of a search or not.Binary classification is dichotomization applied to a practical situation. In many practical binary classification problems, the two groups are not symmetric, and rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative).\n\nStatistical binary classification\nStatistical classification is a problem studied in machine learning.  It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories.  When there are only two categories the problem is known as statistical binary classification.\nSome of the methods commonly used for binary classification are:\n\nDecision trees\nRandom forests\nBayesian networks\nSupport vector machines\nNeural networks\nLogistic regression\nProbit model\nGenetic Programming\nMulti expression programming\nLinear genetic programmingEach classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example, random forests perform better than SVM classifiers for 3D point clouds.\n\nEvaluation of binary classifiers\nThere are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. In medicine sensitivity and specificity are often used, while in information retrieval precision and recall are preferred. An important distinction is between metrics that are independent of how often each category occurs in the population (the prevalence), and metrics that depend on the prevalence – both types are useful, but they have very different properties.\nGiven a classification of a specific data set, there are four basic combinations of actual data category and assigned category: true positives TP (correct positive assignments), true negatives TN (correct negative assignments), false positives FP (incorrect positive assignments), and false negatives FN (incorrect negative assignments).\n\nThese can be arranged into a 2×2 contingency table, with columns corresponding to actual value – condition positive or condition negative – and rows corresponding to classification value – test outcome positive or test outcome negative.\n\nThe eight basic ratios\nThere are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form \"true positive row ratio\" or \"false negative column ratio\". \nThere are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements.\nThe row ratios are:\n\ntrue positive rate (TPR) = (TP/(TP+FN)), aka sensitivity or recall.  These are the proportion of the population with the condition for which the test is correct.\nwith complement the false negative rate (FNR) = (FN/(TP+FN))\ntrue negative rate (TNR) = (TN/(TN+FP), aka specificity (SPC),\nwith complement false positive rate (FPR) = (FP/(TN+FP)), also called independent of prevalenceThe column ratios are:\n\npositive predictive value (PPV, aka precision) (TP/(TP+FP)).  These are the proportion of the population with a given test result for which the test is correct.\nwith complement the false discovery rate (FDR) (FP/(TP+FP))\nnegative predictive value (NPV) (TN/(TN+FN))\nwith complement the false omission rate (FOR) (FN/(TN+FN)), also called dependence on prevalence.In diagnostic testing, the main ratios used are the true column ratios – true positive rate and true negative rate – where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) – positive predictive value and true positive rate – where they are known as precision and recall.\nOne can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an odds ratio – and is prevalence-independent.\nThere are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youden's J statistic, the uncertainty coefficient, the phi coefficient, and Cohen's kappa.\n\nConverting continuous values to binary\nTests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff.\nHowever, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as \"positive\" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as \"positive\" as the one of 52 mIU/ml.\n\nSee also\nExamples of Bayesian inference\nClassification rule\nConfusion matrix\nDetection theory\nKernel methods\nMulticlass classification\nMulti-label classification\nOne-class classification\nProsecutor's fallacy\nReceiver operating characteristic\nThresholding (image processing)\nUncertainty coefficient, aka proficiency\nQualitative property\nPrecision and recall (equivalent classification schema)\n\nReferences\nBibliography\nNello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([1] SVM Book)\nJohn Shawe-Taylor and Nello Cristianini.  Kernel Methods for Pattern Analysis.  Cambridge University Press, 2004.  ISBN 0-521-81397-2 (Website for the book)\nBernhard Schölkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, Massachusetts, 2002. ISBN 0-262-19475-9",
    "Bioinformatics": "Bioinformatics ( (listen)) is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. Bioinformatics uses biology, chemistry, physics, computer science, computer programming, information engineering, mathematics and statistics to analyze and interpret biological data. The subsequent process of analyzing and interpreting data is referred to as computational biology. \nComputational, statistical, and computer programming techniques have been used  for computer simulation analyses of biological queries. They include reused specific analysis \"pipelines\", particularly in the field of genomics, such as by the identification of genes and single nucleotide polymorphisms (SNPs). These pipelines are used to better understand the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. Bioinformatics also includes proteomics, which tries to understand the organizational principles within nucleic acid and protein sequences.Image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics, it aids in sequencing and annotating genomes and their observed mutations. Bioinformatics includes text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in comparing, analyzing and interpreting genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.\n\nHistory\nThe first definition of the term bioinformatics was coined by Paulien Hogeweg and Ben Hesper in 1970, to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biochemistry (the study of chemical processes in biological systems).Bioinformatics and computational biology involved the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.\nAnalyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.\n\nSequences\nThere has been a tremendous advance in speed and cost reduction since the completion of the Human Genome Project, with some labs able to sequence over 100,000 billion bases each year, and a full genome can be sequenced for $1,000 or less.Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. Margaret Oakley Dayhoff, a pioneer in the field, compiled one of the first protein sequence databases, initially published as books as well as methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released online with Tai Te Wu between 1980 and 1991.In the 1970s, new techniques for sequencing DNA were applied to bacteriophage MS2 and øX174, and the extended nucleotide sequences were then parsed with informational and statistical algorithms.  These studies illustrated that well known features, such as the coding segments and the triplet code, are revealed in straightforward statistical analyses and were the proof of the concept that bioinformatics would be insightful.\n\nGoals\nIn order to study how normal cellular activities are altered in different disease states, raw biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This also includes nucleotide and amino acid sequences, protein domains, and protein structures.Important sub-disciplines within bioinformatics and computational biology include:\n\nDevelopment and implementation of computer programs to efficiently access, manage, and use various types of information.\nDevelopment of new mathematical algorithms and statistical measures to assess relationships among members of large data sets. For example, there are methods to locate a gene within a sequence, to predict protein structure and/or function, and to cluster protein sequences into families of related sequences.The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.\nBioinformatics entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.\nOver the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.\nCommon activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.\n\nSequence analysis\nSince the bacteriophage Phage Φ-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Computer programs such as BLAST are used routinely to search sequences—as of 2008, from more than 260,000 organisms, containing over 190 billion nucleotides.\n\nDNA sequencing\nBefore sequences can be analyzed, they are obtained from a data storage bank, such as Genbank. DNA sequencing is still a non-trivial problem as the raw data may be noisy or affected by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.\n\nSequence assembly\nMost DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The shotgun sequencing technique (used by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae) generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced (rather than chain-termination or chemical degradation methods), and genome assembly algorithms are a critical area of bioinformatics research.\n\nGenome annotation\nIn genomics, annotation refers to the process of marking the stop and start regions of genes and other biological features in a sequenced DNA sequence. Many genomes are too large to be annotated by hand. As the rate of sequencing exceeds the rate of genome annotation, genome annotation has become the new bottleneck in bioinformatics. \nGenome annotation can be classified into three levels: the nucleotide, protein, and process levels.\nGene finding is a chief aspect of nucleotide-level annotation. For complex genomes, a combination of ab initio gene prediction and sequence comparison with expressed sequence databases and other organisms can be successful. Nucleotide-level annotation also allows the integration of genome sequence with other genetic and physical maps of the genome.\nThe principal aim of protein-level annotation is to assign function to the protein products of the genome. Databases of protein sequences and functional domains and motifs are used for this type of annotation. About half of the predicted proteins in a new genome sequence tend to have no obvious function.\nUnderstanding the function of genes and their products in the context of cellular and organismal physiology is the goal of process-level annotation. An obstacle of process-level annotation has been the inconsistency of terms used by different model systems. The Gene Ontology Consortium is helping to solve this problem.The first description of a comprehensive annotation system was published in 1995 by the The Institute for Genomic Research, which performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae. The system identifies the genes encoding all proteins, transfer RNAs, ribosomal RNAs, in order to make initial functional assignments. The GeneMark program trained to find protein-coding genes in Haemophilus influenzae is constantly changing and improving.\nFollowing the goals that the Human Genome Project left to achieve after its closure in 2003, the ENCODE project was developed by the National Human Genome Research Institute. This project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).\n\nGene function prediction\nWhile genome annotation is primarily based on sequence similarity (and thus homology), other properties of sequences can be used to predict the function of genes. In fact, most gene function prediction methods focus on protein sequences as they are more informative and more feature-rich. For instance, the distribution of hydrophobic amino acids predicts transmembrane segments in proteins. However, protein function prediction can also use external information such as gene (or protein) expression data, protein structure, or protein-protein interactions.\n\nComputational evolutionary biology\nEvolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:\n\ntrace the evolution of a large number of organisms by measuring changes in their DNA, rather than through physical taxonomy or physiological observations alone,\ncompare entire genomes, which permits the study of more complex evolutionary events, such as gene duplication, horizontal gene transfer, and the prediction of factors important in bacterial speciation,\nbuild complex computational population genetics models to predict the outcome of the system over time\ntrack and share information on an increasingly large number of species and organismsFuture work endeavours to reconstruct the now more complex tree of life.\n\nComparative genomics\nThe core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. Intergenomic maps are made to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Entire genomes are involved in processes of hybridization, polyploidization and endosymbiosis that lead to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.\nMany of these studies are based on the detection of sequence homology to assign sequences to protein families.\n\nPan genomics\nPan genomics is a concept introduced in 2005 by Tettelin and Medini. Pan genome is the complete gene repertoire of a particular monophyletic taxonomic group. Although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum, etc. It is divided in two parts: the Core genome, a set of genes common to all the genomes under study (often housekeeping genes vital for survival), and the Dispensable/Flexible genome: a set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.\n\nGenetics of disease\nAs of 2013, the existence of efficient high-throughput next-generation sequencing technology allows for the identification of cause many different human disorders. Simple Mendelian inheritance has been observed for over 3,000 disorders that have been identified at the Online Mendelian Inheritance in Man database, but complex diseases are more difficult. Association studies have found many individual genetic regions that individually are weakly associated with complex diseases (such as infertility, breast cancer and Alzheimer's disease), rather than a single cause. There are currently many challenges to using genes for diagnosis and treatment, such as how we don't know which genes are important, or how stable the choices an algorithm provides. Genome-wide association studies have successfully identified thousands of common genetic variants for complex diseases and traits; however, these common variants only explain a small fraction of heritability. Rare variants may account for some of the missing heritability. Large-scale whole genome sequencing studies have rapidly sequenced millions of whole genomes, and such studies have identified hundreds of millions of rare variants. Functional annotations predict the effect or function of a genetic variant and help to prioritize rare functional variants, and incorporating these annotations can effectively boost the power of genetic association of rare variants analysis of whole genome sequencing studies. Some tools have been developed to provide all-in-one rare variant association analysis for whole-genome sequencing data, including integration of genotype data and their functional annotations, association analysis, result summary and visualization. Meta-analysis of whole genome sequencing studies provides an attractive solution to the problem of collecting large sample sizes for discovering rare variants associated with complex phenotypes.\n\nAnalysis of mutations in cancer\nIn cancer, the genomes of affected cells are rearranged in complex or unpredictable ways. In addition to single-nucleotide polymorphism arrays identifying point mutations that cause cancer, oligonucleotide microarrays can be used to identify chromosomal gains and losses (called comparative genomic hybridization). These detection methods generate terabytes of data per experiment. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.Two important principles can be used to identify cancer by mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second, cancer contains driver mutations which need to be distinguished from passengers.Further improvements in bioinformatics could allow for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples. Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.\n\nGene and protein expression\nAnalysis of gene expression\nThe expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as \"Whole Transcriptome Shotgun Sequencing\" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.\n\nAnalysis of protein expression\nProtein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. The former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples when multiple incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.\n\nAnalysis of regulation\nGene regulation is a complex process where a signal, such as an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.\nFor example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the protein-coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.\nExpression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). Clustering algorithms can be then applied to expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.\n\nAnalysis of cellular organization\nSeveral approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. A gene ontology category, cellular component, has been devised to capture subcellular localization in many biological databases.\n\nMicroscopy and image analysis\nMicroscopic pictures allow for the location of organelles as well as molecules, which may be the source of abnormalities in diseases.\n\nProtein localization\nFinding the location of proteins allows us to predict what they do. This is called protein function prediction. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. There are well developed protein subcellular localization prediction resources available, including protein subcellular location databases, and prediction tools.\n\nNuclear organization of chromatin\nData from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.\n\nStructural bioinformatics\nFinding the structure of proteins important application of bioinformatics. The Critical Assessment of Protein Structure Prediction (CASP) is an open competition where worldwide research groups submit protein models for evaluating unknown protein models.\n\nAmino acid sequence\nThe linear amino acid sequence of a protein is called the primary structure, can be easily determined from the sequence of codons on the DNA gene that codes for it. In most proteins, the primary structure uniquely determines the 3-dimensional structure of a protein in its native environment. An exception is the misfolded protein involved in bovine spongiform encephalopathy. This structure is linked to the function of the protein. Additional structural information includes the secondary, tertiary and quaternary structure. A viable general solution to the prediction of the function of a protein remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.\n\nHomology\nIn the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In structural bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. Homology modeling is used to predict the structure of an unknown protein from existing homologous proteins. \nOne example of this is hemoglobin in humans and the hemoglobin in legumes (leghemoglobin), which are distant relatives from the same protein superfamily. Both serve the same purpose of transporting oxygen in the organism. Although both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes and shared ancestor.Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.\nAnother aspect of structural bioinformatics include the use of protein structures for Virtual Screening models such as Quantitative Structure-Activity Relationship models and proteochemometric models (PCM). Furthermore, a protein's crystal structure can be used in simulation of for example ligand-binding studies and in silico mutagenesis studies.\nA 2021 deep-learning algorithms-based software called AlphaFold, developed by Google's DeepMind, greatly outperforms all other prediction software methods, and has released predicted structures for hundreds of millions of proteins in the AlphaFold protein structure database.\n\nNetwork and systems biology\nNetwork analysis seeks to understand the relationships within biological networks such as metabolic or protein–protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.\nSystems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.\n\nMolecular interaction networks\nTens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without performing protein–protein interaction experiments. A variety of methods have been developed to tackle the protein–protein docking problem, though it seems that there is still much work to be done in this field.\nOther interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.\n\nOthers\nLiterature analysis\nThe enormous number of published literature makes it virtually impossible for individuals to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:\n\nAbbreviation recognition – identify the long-form and abbreviation of biological terms\nNamed-entity recognition – recognizing biological terms such as gene names\nProtein–protein interaction – identify which proteins interact with which proteins from textThe area of research draws from statistics and computational linguistics.\n\nHigh-throughput image analysis\nComputational technologies are used to automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems can improve an observer's accuracy, objectivity, or speed. Image analysis is important for both diagnostics and research. Some examples are:\n\nhigh-throughput and high-fidelity quantification and sub-cellular localization (high-content screening, cytohistopathology, Bioimage informatics)\nmorphometrics\nclinical image analysis and visualization\ndetermining the real-time air-flow patterns in breathing lungs of living animals\nquantifying occlusion size in real-time imagery from the development of and recovery during arterial injury\nmaking behavioral observations from extended video recordings of laboratory animals\ninfrared measurements for metabolic activity determination\ninferring clone overlaps in DNA mapping, e.g. the Sulston score\n\nHigh-throughput single cell data analysis\nComputational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.\n\nBiodiversity informatics\nBiodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.\n\nOntologies and data integration\nBiological ontologies are directed acyclic graphs of controlled vocabularies. They create categories for biological concepts and descriptions so they can be easily analyzed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.\n\nDatabases\nDatabases are essential for bioinformatics research and applications. Databases exist for many different information types, including DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases can contain both empirical data (obtained directly from experiments) and predicted data (obtained from analysis of existing data). They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. Databases can have different formats, access mechanisms, and be public or private.\nSome of the most commonly used databases are listed below:\n\nUsed in biological sequence analysis: Genbank, UniProt\nUsed in structure analysis: Protein Data Bank (PDB)\nUsed in finding Protein Families and Motif Finding: InterPro, Pfam\nUsed for Next Generation Sequencing: Sequence Read Archive\nUsed in Network Analysis: Metabolic Pathway Databases (KEGG, BioCyc), Interaction Analysis Databases, Functional Networks\nUsed in design of synthetic genetic circuits: GenoCAD\n\nSoftware and tools\nSoftware tools for bioinformatics include simple command-line tools, more complex graphical programs, and standalone web-services. They are made by bioinformatics companies or by public institutions.\n\nOpen-source bioinformatics software\nMany free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have created opportunities for research groups to contribute to both bioinformatics regardless of funding. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.\nOpen-source bioinformatics software includes Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. \nThe non-profit Open Bioinformatics Foundation and the annual Bioinformatics Open Source Conference promote open-source bioinformatics software.\n\nWeb services in bioinformatics\nSOAP- and REST-based interfaces have been developed to allow client computers to use algorithms, data and computing resources from servers in other parts of the world. The main advantage are that end users do not have to deal with software and database maintenance overheads.\nBasic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.\n\nBioinformatics workflow management systems\nA bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to\n\nprovide an easy-to-use environment for individual application scientists themselves to create their own workflows,\nprovide interactive tools for the scientists enabling them to execute their workflows and view their results in real-time,\nsimplify the process of sharing and reusing workflows between the scientists, and\nenable scientists to track the provenance of the workflow execution results and the workflow creation steps.Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.\n\nBioCompute and BioCompute Objects\nIn 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics. Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm. These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.\nIt was decided that the BioCompute paradigm would be in the form of digital 'lab notebooks' which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as both a \"standard trial use\" document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.\n\nEducation platforms\nBioinformatics is not only taught as in-person masters degree at many universities. The computational nature of bioinformatics lends it to computer-aided and online learning. Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273π project or 4273pi project also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils. 4283 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4283π operating system.MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard).\n\nConferences\nThere are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).\n\nSee also\nReferences\nFurther reading\nExternal links\n\n The dictionary definition of bioinformatics at Wiktionary\n Learning materials related to Bioinformatics at Wikiversity\n Media related to Bioinformatics at Wikimedia Commons\nBioinformatics Resource Portal (SIB)",
    "Biological neural network": "A neural circuit (also known as a biological neural network BNNs) is a population of neurons interconnected by synapses to carry out a specific function when activated. Multiple neural circuits interconnect with one another to form large scale brain networks.Neural circuits have inspired the design of artificial neural networks, though there are significant differences.\n\nEarly study\nEarly treatments of neural networks can be found in Herbert Spencer's Principles of Psychology, 3rd edition (1872), Theodor Meynert's Psychiatry (1884), William James' Principles of Psychology (1890), and Sigmund Freud's Project for a Scientific Psychology (composed 1895). The first rule of neuronal learning was described by Hebb in 1949, in the Hebbian theory. Thus, Hebbian pairing of pre-synaptic and post-synaptic activity can substantially alter the dynamic characteristics of the synaptic connection and therefore either facilitate or inhibit signal transmission. In 1959, the neuroscientists, Warren Sturgis McCulloch and Walter Pitts published the first works on the processing of neural networks. They showed theoretically that networks of artificial neurons could implement logical, arithmetic, and symbolic functions. Simplified models of biological neurons were set up, now usually called perceptrons or artificial neurons. These simple models accounted for neural summation (i.e., potentials at the post-synaptic membrane will summate in the cell body). Later models also provided for excitatory and inhibitory synaptic transmission.\n\nConnections between neurons\nThe connections between neurons in the brain are much more complex than those of the artificial neurons used in the connectionist neural computing models of artificial neural networks. The basic kinds of connections between neurons are synapses: both chemical and electrical synapses.\nThe establishment of synapses enables the connection of neurons into millions of overlapping, and interlinking neural circuits. Presynaptic proteins called neurexins are central to this process.One principle by which neurons work is neural summation – potentials at the postsynaptic membrane will sum up in the cell body. If the depolarization of the neuron at the axon hillock goes above threshold an action potential will occur that travels down the axon to the terminal endings to transmit a signal to other neurons. Excitatory and inhibitory synaptic transmission is realized mostly by excitatory postsynaptic potentials (EPSPs), and inhibitory postsynaptic potentials (IPSPs).\nOn the electrophysiological level, there are various phenomena which alter the response characteristics of individual synapses (called synaptic plasticity) and individual neurons (intrinsic plasticity). These are often divided into short-term plasticity and long-term plasticity. Long-term synaptic plasticity is often contended to be the most likely memory substrate. Usually, the term \"neuroplasticity\" refers to changes in the brain that are caused by activity or experience.\nConnections display temporal and spatial characteristics. Temporal characteristics refers to the continuously modified activity-dependent efficacy of synaptic transmission, called spike-timing-dependent plasticity. It has been observed in several studies that the synaptic efficacy of this transmission can undergo short-term increase (called facilitation) or decrease (depression) according to the activity of the presynaptic neuron. The induction of long-term changes in synaptic efficacy, by long-term potentiation (LTP) or depression (LTD), depends strongly on the relative timing of the onset of the excitatory postsynaptic potential and the postsynaptic action potential. LTP is induced by a series of action potentials which cause a variety of biochemical responses. Eventually, the reactions cause the expression of new receptors on the cellular membranes of the postsynaptic neurons or increase the efficacy of the existing receptors through phosphorylation.\nBackpropagating action potentials cannot occur because after an action potential travels down a given segment of the axon, the m gates on voltage-gated sodium channels close, thus blocking any transient opening of the h gate from causing a change in the intracellular sodium ion (Na+) concentration, and preventing the generation of an action potential back towards the cell body. In some cells, however, neural backpropagation does occur through the dendritic branching and may have important effects on synaptic plasticity and computation.\nA neuron in the brain requires a single signal to a neuromuscular junction to stimulate contraction of the postsynaptic muscle cell. In the spinal cord, however, at least 75 afferent neurons are required to produce firing. This picture is further complicated by variation in time constant between neurons, as some cells can experience their EPSPs over a wider period of time than others.\nWhile in synapses in the developing brain synaptic depression has been particularly widely observed it has been speculated that it changes to facilitation in adult brains.\n\nCircuitry\nAn example of a neural circuit is the trisynaptic circuit in the hippocampus. Another is the Papez circuit linking the hypothalamus to the limbic lobe. There are several neural circuits in the cortico-basal ganglia-thalamo-cortical loop. These circuits carry information between the cortex, basal ganglia, thalamus, and back to the cortex. The largest structure within the basal ganglia, the striatum, is seen as having its own internal microcircuitry.Neural circuits in the spinal cord called central pattern generators are responsible for controlling motor instructions involved in rhythmic behaviours. Rhythmic behaviours include walking, urination, and ejaculation. The central pattern generators are made up of different groups of spinal interneurons.There are four principal types of neural circuits that are responsible for a broad scope of neural functions. These circuits are a diverging circuit, a converging circuit, a reverberating circuit, and a parallel after-discharge circuit.In a diverging circuit, one neuron synapses with a number of postsynaptic cells. Each of these\nmay synapse with many more making it possible for one neuron to stimulate up to thousands of cells. This is exemplified in the way that thousands of muscle fibers can be stimulated from the initial input from a single motor neuron.In a converging circuit, inputs from many sources are converged into one output, affecting just one neuron or a neuron pool. This type of circuit is exemplified in the respiratory center of the brainstem, which responds to a number of inputs from different sources by giving out an appropriate breathing pattern.A reverberating circuit produces a repetitive output. In a signalling procedure from one neuron to another in a linear sequence, one of the neurons may send a signal back to initiating neuron.\nEach time that the first neuron fires, the other neuron further down the sequence fire again sending it back to the source. This restimulates the first neuron and also allows the path of transmission to continue to its output. A resulting repetitive pattern is the outcome that only stops if one or more of the synapses fail, or if an inhibitory feed from another source causes it to stop. This type of reverberating circuit is found in the respiratory center that sends signals to the respiratory muscles, causing inhalation. When the circuit is interrupted by an inhibitory signal the muscles relax causing exhalation. This type of circuit may play a part in epileptic seizures.In a parallel after-discharge circuit, a neuron inputs to several chains of neurons. Each chain is made up of a different number of neurons but their signals converge onto one output neuron. Each synapse in the circuit acts to delay the signal by about 0.5 msec, so that the more synapses there are, the longer is the delay to the output neuron. After the input has stopped, the output will go on firing for some time. This type of circuit does not have a feedback loop as does the reverberating circuit. Continued firing after the stimulus has stopped is called after-discharge. This circuit type is found in the reflex arcs of certain reflexes.\n\nStudy methods\nDifferent neuroimaging techniques have been developed to investigate the activity of neural circuits and networks. The use of \"brain scanners\" or functional neuroimaging to investigate the structure or function of the brain is common, either as simply a way of better assessing brain injury with high-resolution pictures, or by examining the relative activations of different brain areas. Such technologies may include functional magnetic resonance imaging (fMRI), brain positron emission tomography (brain PET), and computed axial tomography (CAT) scans. Functional neuroimaging uses specific brain imaging technologies to take scans from the brain, usually when a person is doing a particular task, in an attempt to understand how the activation of particular brain areas is related to the task. In functional neuroimaging, especially fMRI, which measures hemodynamic activity (using BOLD-contrast imaging) which is closely linked to neural activity, PET, and electroencephalography (EEG) is used.\nConnectionist models serve as a test platform for different hypotheses of representation, information processing, and signal transmission. Lesioning studies in such models, e.g. artificial neural networks, where parts of the nodes are deliberately destroyed to see how the network performs, can also yield important insights in the working of several cell assemblies. Similarly, simulations of dysfunctional neurotransmitters in neurological conditions (e.g., dopamine in the basal ganglia of Parkinson's patients) can yield insights into the underlying mechanisms for patterns of cognitive deficits observed in the particular patient group. Predictions from these models can be tested in patients or via pharmacological manipulations, and these studies can in turn be used to inform the models, making the process iterative.\nThe modern balance between the connectionist approach and the single-cell approach in neurobiology has been achieved through a lengthy discussion.\t\nIn 1972, Barlow announced the single neuron revolution: \"our perceptions are caused by the activity of a rather small number of neurons selected from a very large population of predominantly silent cells.\" This approach was stimulated by the idea of grandmother cell put forward two years earlier. Barlow formulated \"five dogmas\" of neuron doctrine.  Recent studies of 'grandmother cell' and sparse coding phenomena develop and modify these ideas. The single cell experiments  used intracranial electrodes in the medial temporal lobe (the hippocampus and surrounding cortex). Modern development of concentration of measure theory (stochastic separation theorems) with applications to artificial neural networks give mathematical background to unexpected effectiveness of small neural ensembles in high-dimensional brain.\n\nClinical significance\nSometimes neural circuitries can become pathological and cause problems such as in Parkinson's disease when the basal ganglia are involved. Problems in the Papez circuit can also give rise to a number of neurodegenerative disorders including Parkinson's.\n\nSee also\nFeedback\nList of regions in the human brain\nNetwork science\nNeural coding\nNeural engineering\nNeural oscillation\nPulse-coupled networks\nSystems neuroscience\nNerve tract\nNeural pathway\nNerve plexus\n\nReferences\nFurther reading\nIntrinsic plasticity Robert H. Cudmore, Niraj S. Desai Scholarpedia 3(2):1363. doi:10.4249/scholarpedia.1363\n\nExternal links\nComparison of Neural Networks in the Brain and Artificial Neural Networks\nLecture notes at MIT OpenCourseWare\nComputation in the Brain\nBiological Neural Network Toolbox - A free Matlab toolbox for simulating networks of several different types of neurons\nWormWeb.org: Interactive Visualization of the C. elegans Neural Network - C. elegans, a nematode with 302 neurons, is the only organism for whom the entire neural network has been uncovered.  Use this site to browse through the network and to search for paths between any 2 neurons.\nIntroduction to Neurons and Neuronal Networks, Neuroscience Online (electronic neuroscience textbook)\nDelaying Pulse Networks (Wave Interference Networks)",
    "Biology": "Biology is the scientific study of life. It is a natural science with a broad scope but has several unifying themes that tie it together as a single, coherent field. For instance, all organisms are made up of cells that process hereditary information encoded in genes, which can be transmitted to future generations. Another major theme is evolution, which explains the unity and diversity of life. Energy processing is also important to life as it allows organisms to move, grow, and reproduce. Finally, all organisms are able to regulate their own internal environments.Biologists are able to study life at multiple levels of organization, from the molecular biology of a cell to the anatomy and physiology of plants and animals, and evolution of populations. Hence, there are multiple subdisciplines within biology, each defined by the nature of their research questions and the tools that they use. Like other scientists, biologists use the scientific method to make observations, pose questions, generate hypotheses, perform experiments, and form conclusions about the world around them.Life on Earth, which emerged more than 3.7 billion years ago, is immensely diverse. Biologists have sought to study and classify the various forms of life, from prokaryotic organisms such as archaea and bacteria to eukaryotic organisms such as protists, fungi, plants, and animals. These various organisms contribute to the biodiversity of an ecosystem, where they play specialized roles in the cycling of nutrients and energy through their biophysical environment.\n\nHistory\nThe earliest of roots of science, which included medicine, can be traced to ancient Egypt and Mesopotamia in around 3000 to 1200 BCE. Their contributions shaped ancient Greek natural philosophy. Ancient Greek philosophers such as Aristotle (384–322 BCE) contributed extensively to the development of biological knowledge. He explored biological causation and the diversity of life. His successor, Theophrastus, began the scientific study of plants. Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought.\nBiology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining. Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory.Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735, and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent.\nSerious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution. The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions.The basis for modern genetics began with the work of Gregor Mendel in 1865. This outlined the principles of biological inheritance. However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics. In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome.\n\nChemical basis\nAtoms and molecules\nAll organisms are made up of chemical elements; oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life. Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions.\n\nWater\nLife arose from the Earth's first ocean, which formed some 3.8 billion years ago. Since then, water continues to be the most abundant molecule in every organism. Water is important to life because it is an effective solvent, capable of dissolving solutes such as sodium and chloride ions or other small molecules to form an aqueous solution. Once dissolved in water, these solutes are more likely to come in contact with one another and therefore take part in chemical reactions that sustain life. In terms of its molecular structure, water is a small polar molecule with a bent shape formed by the polar covalent bonds of two hydrogen (H) atoms to one oxygen (O) atom (H2O). Because the O–H bonds are polar, the oxygen atom has a slight negative charge and the two hydrogen atoms have a slight positive charge. This polar property of water allows it to attract other water molecules via hydrogen bonds, which makes water cohesive. Surface tension results from the cohesive force due to the attraction between molecules at the surface of the liquid. Water is also adhesive as it is able to adhere to the surface of any polar or charged non-water molecules. Water is denser as a liquid than it is as a solid (or ice). This unique property of water allows ice to float above liquid water such as ponds, lakes, and oceans, thereby insulating the liquid below from the cold air above. Water has the capacity to absorb energy, giving it a higher specific heat capacity than other solvents such as ethanol. Thus, a large amount of energy is needed to break the hydrogen bonds between water molecules to convert liquid water into water vapor. As a molecule, water is not completely stable as each water molecule continuously dissociates into hydrogen and hydroxyl ions before reforming into a water molecule again. In pure water, the number of hydrogen ions balances (or equals) the number of hydroxyl ions, resulting in a pH that is neutral.\n\nOrganic compounds\nOrganic compounds are molecules that contain carbon bonded to another element such as hydrogen. With the exception of water, nearly all the molecules that make up each organism contain carbon. Carbon can form covalent bonds with up to four other atoms, enabling it to form diverse, large, and complex molecules. For example, a single carbon atom can form four single covalent bonds such as in methane, two double covalent bonds such as in carbon dioxide (CO2), or a triple covalent bond such as in carbon monoxide (CO). Moreover, carbon can form very long chains of interconnecting carbon–carbon bonds such as octane or ring-like structures such as glucose.\nThe simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound. Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups. There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group.In 1953, the Miller-Urey experiment showed that organic compounds could be synthesized abiotically within a closed system mimicking the conditions of early Earth, thus suggesting that complex organic molecules could have arisen spontaneously in early Earth (see abiogenesis).\n\nMacromolecules\nMacromolecules are large molecules made up of smaller subunits or monomers. Monomers include  sugars, amino acids, and nucleotides. Carbohydrates include monomers and polymers of sugars.\nLipids are the only class of macromolecules that are not made up of polymers. They include steroids, phospholipids, and fats, largely nonpolar and hydrophobic (water-repelling) substances.\nProteins are the most diverse of the macromolecules. They include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid. Twenty amino acids are used in proteins.\nNucleic acids are polymers of nucleotides. Their function is to store, transmit, and express hereditary information.\n\nCells\nCell theory states that cells are the fundamental units of life, that all living things are composed of one or more cells, and that all cells arise from preexisting cells through cell division. Most cells are very small, with diameters ranging from 1 to 100 micrometers and are therefore only visible under a light or electron microscope. There are generally two types of cells: eukaryotic cells, which contain a nucleus, and prokaryotic cells, which do not. Prokaryotes are single-celled organisms such as bacteria, whereas eukaryotes can be single-celled or multicellular. In multicellular organisms, every cell in the organism's body is derived ultimately from a single cell in a fertilized egg.\n\nCell structure\nEvery cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space. A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions. Cell membranes also contains membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell. Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton.\n\nWithin the cytoplasm of a cell, there are many biomolecules such as proteins and nucleic acids. In addition to biomolecules, eukaryotic cells have specialized structures called organelles that have their own lipid bilayers or are spatially units. These organelles include the cell nucleus, which contains most of the cell's DNA, or mitochondria, which generates adenosine triphosphate (ATP) to power cellular processes. Other organelles such as endoplasmic reticulum and Golgi apparatus play a role in the synthesis and packaging of proteins, respectively. Biomolecules such as proteins can be engulfed by lysosomes, another specialized organelle. Plant cells have additional organelles that distinguish them from animal cells such as a cell wall that provides support for the plant cell, chloroplasts that harvest sunlight energy to produce sugar, and vacuoles that provide storage and structural support as well as being involved in reproduction and breakdown of plant seeds. Eukaryotic cells also have cytoskeleton that is made up of microtubules, intermediate filaments, and microfilaments, all of which provide support for the cell and are involved in the movement of the cell and its organelles. In terms of their structural composition, the microtubules are made up of tubulin (e.g., α-tubulin and β-tubulin whereas intermediate filaments are made up of fibrous proteins. Microfilaments are made up of actin molecules that interact with other strands of proteins.\n\nMetabolism\nAll cells require energy to sustain cellular processes. Metabolism is the set of chemical reactions in an organism. The three main purposes of metabolism are: the conversion of food to energy to run cellular processes; the conversion of food/fuel to monomer building blocks; and the elimination of metabolic wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. Metabolic reactions may be categorized as catabolic—the breaking down of compounds (for example, the breaking down of glucose to pyruvate by cellular respiration); or anabolic—the building up (synthesis) of compounds (such as proteins, carbohydrates, lipids, and nucleic acids). Usually, catabolism releases energy, and anabolism consumes energy. The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts—they allow a reaction to proceed more rapidly without being consumed by it—by reducing the amount of activation energy needed to convert reactants into products. Enzymes also allow the regulation of the rate of a metabolic reaction, for example in response to changes in the cell's environment or to signals from other cells.\n\nCellular respiration\nCellular respiration is a set of metabolic reactions and processes that take place in cells to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products. The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions.\nSugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation. Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time. Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-Coa enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force. Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor.\nIf oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis.  In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis.  This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen.\n\nPhotosynthesis\nPhotosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water. In most cases, oxygen is released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth.Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation. Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration.During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase. The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle.\n\nCell signaling\nCell signaling (or communication) is the ability of cells to receive, process, and transmit signals with its environment and with itself. Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell. There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones. In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affects them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an inotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction\n\nCell cycle\nThe cell cycle is a series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA and some of its organelles, and the subsequent partitioning of its cytoplasm into two daughter cells in a process called cell division. In eukaryotes (i.e., animal, plant, fungal, and protist cells), there are two distinct types of cell division: mitosis and meiosis. Mitosis is part of the cell cycle, in which replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components. The different stages of mitosis all together define the mitotic phase of an animal cell cycle—the division of the mother cell into two genetically identical daughter cells. The cell cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. In contrast to mitosis, meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions. Homologous chromosomes are separated in the first division (meiosis I), and sister chromatids are separated in the second division (meiosis II). Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor.\nProkaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission takes in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and \"Z-ring\" formation) The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids.\n\nGenetics\nInheritance\nGenetics is the scientific study of inheritance. Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring. It has several principles. The first is that genetic characteristics, alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on the law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele. During gamete formation, the alleles for each gene segregate, so that each gamete carries only one allele for each gene. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, the law of independent assortment, states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype. A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies, which established the sex linkage between eye color and sex in these insects.\n\nGenes and DNA\nA gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that controls form or function of an organism. DNA is composed of two polynucleotide chains that coil around each other to form a double helix. It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell is collectively known as its genome. In eukaryotes, DNA is mainly in the cell nucleus. In prokaryotes, the DNA is held within the nucleoid. The genetic information is held within genes, and the complete assemblage in an organism is called its genotype.DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA. Mutations are heritable changes in DNA. They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes). Mutations can lead to phenotypic effects such as loss-of-function, gain-of-function, and conditional mutations.\nSome mutations are beneficial, as they are a source of genetic variation for evolution. Others are harmful if they were to result in a loss of function of genes needed for survival. Mutagens such as carcinogens are typically avoided as a matter of public health policy goals.\n\nGene expression\nGene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in the proteins of an organism's body. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958. According to the Central Dogma, genetic information flows from DNA to RNA to protein. There are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein).\n\nGene regulation\nThe regulation of gene expression by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein. Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter. A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans). In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. Negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. Repressors can be inhibited by compounds called inducers (e.g., allolactose), thereby allowing transcription to occur. Specific genes that can be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active. In contrast to both, structural genes encode proteins that are not involved in gene regulation. In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells.\n\nGenes, development, and evolution\nDevelopment is the process by which a multicellular organism (plant or animal) goes through a series of changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle. There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells from less specialized cells such as stem cells. Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics.  With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself. Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or the development of body form, is the result of spatial differences in gene expression. A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva.\n\nEvolution\nEvolutionary processes\nEvolution is a central organizing concept in biology. It is the change in heritable characteristics of populations over successive generations. In artificial selection, animals were selectively bred for specific traits.\n Given that traits are inherited, populations contain a varied mix of traits, and reproduction is able to increase any population, Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits. Darwin inferred that individuals who possessed heritable traits better adapted to their environments are more likely to survive and produce more offspring than other individuals. He further inferred that this would lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment.\n\nSpeciation\nA species is a group of organisms that mate with one another and speciation is the process by which one lineage splits into two lineages as a result of having evolved independently from each other. For speciation to occur, there has to be reproductive isolation. Reproductive isolation can result from incompatibilities between genes as described by Bateson–Dobzhansky–Muller model. Reproductive isolation also tends to increase with genetic divergence. Speciation can occur when there are physical barriers that divide an ancestral species, a process known as allopatric speciation.\n\nPhylogeny\nA phylogeny is an evolutionary history of a specific group of organisms or their genes. It can be represented using a phylogenetic tree, a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a fork or split on the phylogenetic tree. Phylogenetic trees are the basis for comparing and grouping different species. Different species that share a feature inherited from a common ancestor are described as having homologous features (or synapomorphy). Phylogeny provides the basis of biological classification. This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order, family, genus, and species. All organisms can be classified as belonging to one of three domains: Archaea (originally Archaebacteria); bacteria (originally eubacteria), or eukarya (includes the protist, fungi, plant, and animal kingdoms).\n\nHistory of life\nThe history of life on Earth traces how organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago. Geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years. Each eon can be divided into eras, with the Phanerozoic eon that began 539 million years ago being subdivided into Paleozoic, Mesozoic, and Cenozoic eras. These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary).The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor. Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes. Microbal mats of coexisting bacteria and archaea were the dominant form of life in the early Archean epoch and many of the major steps in early evolution are thought to have taken place in this environment. The earliest evidence of eukaryotes dates from 1.85 billion years ago, and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions.Algae-like multicellular land plants are dated back even to about 1 billion years ago, although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago. Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event.Ediacara biota appear during the Ediacaran period, while vertebrates, along with most other modern phyla originated about 525 million years ago during the Cambrian explosion. During the Permian period, synapsids, including the ancestors of mammals, dominated the land, but most of this group became extinct in the Permian–Triassic extinction event 252 million years ago. During the recovery from this catastrophe, archosaurs became the most abundant land vertebrates; one archosaur group, the dinosaurs, dominated the Jurassic and Cretaceous periods. After the Cretaceous–Paleogene extinction event 66 million years ago killed off the non-avian dinosaurs, mammals increased rapidly in size and diversity. Such mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.\n\nDiversity\nBacteria and Archaea\nBacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste, and the deep biosphere of the earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory.\nArchaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use. Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi. Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes, including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores.\nThe first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet.\nArchaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin. Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example.\n\nEukaryotes\nEukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern-day eukaryotic cells. The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals. Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals. While it is likely that protists share a common ancestor (the last eukaryotic common ancestor), protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience. Most protists are unicellular; these are called microbial eukaryotes.Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. Plant cells were derived by endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts. The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related. Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae. Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts.Fungi are eukaryotes that digest foods outside their bodies, secreting digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes, feeding on dead organic matter, making them important decomposers in ecological systems.Animals are multicellular eukaryotes. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. They have complex interactions with each other and their environments, forming intricate food webs.\n\nViruses\nViruses are submicroscopic infectious agents that replicate inside the cells of organisms. Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea. More than 6,000 virus species have been described in detail. Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity.The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Because viruses possess some but not all characteristics of life, they have been described as \"organisms at the edge of life\", and as self-replicators.\n\nEcology\nEcology is the study of the distribution and abundance of life, the interaction between organisms and their environment.\n\nEcosystems\nThe community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem. These biotic and abiotic components are linked together through nutrient cycles and energy flows. Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals move matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.\n\nPopulations\nA population is the group of organisms of the same species that occupies an area and reproduce from generation to generation. Population size can be estimated by multiplying population density by the area or volume. The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available. The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability resources and the cost of maintaining them. In human populations, new technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the most famous of which was by Thomas Malthus in the 18th century.\n\nCommunities\nA community is a group of populations of species occupying the same geographical area at the same time. A biological interaction is the effect that a pair of organisms living together in a community have on each other. They can be either of the same species (intraspecific interactions), or of different species (interspecific interactions). These effects may be short-term, like pollination and predation, or long-term; both often strongly influence the evolution of the species involved. A long-term interaction is called a symbiosis. Symbioses range from mutualism, beneficial to both partners, to competition, harmful to both partners. Every species participates as a consumer, resource, or both in consumer–resource interactions, which form the core of food chains or food webs. There are different trophic levels within any food web, with the lowest level being the primary producers (or autotrophs) such as plants and algae that convert energy and inorganic material into organic compounds, which can then be used by the rest of the community. At the next level are the heterotrophs, which are the species that obtain energy by breaking apart organic compounds from other organisms. Heterotrophs that consume plants are primary consumers (or herbivores) whereas heterotrophs that consume herbivores are secondary consumers (or carnivores). And those that eat secondary consumers are tertiary consumers and so on. Omnivorous heterotrophs are able to consume at multiple levels. Finally, there are decomposers that feed on the waste products or dead bodies of organisms.\nOn average, the total amount of energy incorporated into the biomass of a trophic level per unit of time is about one-tenth of the energy of the trophic level that it consumes. Waste and dead material used by decomposers as well as heat lost from metabolism make up the other ninety percent of energy that is not consumed by the next trophic level.\n\nBiosphere\nIn the global ecosystem or biosphere, matter exists as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations. For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water.\n\nConservation\nConservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, which has contributed to poverty, starvation, and will reset the course of evolution on this planet. Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend. Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales.\n\nSee also\nReferences\nFurther reading\nExternal links\n\nBiology at Curlie\nOSU's Phylocode\nBiology Online – Wiki Dictionary\nMIT video lecture series on biology\nOneZoom Tree of LifeJournal links\n\nPLOS Biology A peer-reviewed, open-access journal published by the Public Library of Science\nCurrent Biology: General journal publishing original research from all areas of biology\nBiology Letters: A high-impact Royal Society journal publishing peer-reviewed biology papers of general interest\nScience: Internationally renowned AAAS science journal – see sections of the life sciences\nInternational Journal of Biological Sciences: A biological journal publishing significant peer-reviewed scientific papers\nPerspectives in Biology and Medicine: An interdisciplinary scholarly journal publishing essays of broad relevance",
    "Black swan theory": "The black swan theory or theory of black swan events is a metaphor that describes an event that comes as a surprise, has a major effect, and is often inappropriately rationalized after the fact with the benefit of hindsight. The term is based on an ancient saying that presumed black swans did not exist –  a saying that became reinterpreted to teach a different lesson after they were discovered in Australia.The theory was developed by Nassim Nicholas Taleb, starting in 2001, to explain:\n\nThe disproportionate role of high-profile, hard-to-predict, and rare events that are beyond the realm of normal expectations in history, science, finance, and technology.\nThe non-computability of the probability of consequential rare events using scientific methods (owing to the very nature of small probabilities).\nThe psychological biases that blind people, both individually and collectively, to uncertainty and the substantial role of rare events in historical affairs.Taleb's \"black swan theory\" refers only to unexpected events of large magnitude and consequence and their dominant role in history. Such events, considered extreme outliers, collectively play vastly larger roles than regular occurrences.: xxi  More technically, in the scientific monograph \"Silent Risk\", Taleb mathematically defines the black swan problem as \"stemming from the use of degenerate metaprobability\".\n\nBackground\nThe phrase \"black swan\" derives from a Latin expression; its oldest known occurrence is from the 2nd-century Roman poet Juvenal's characterization in his Satire VI of something being \"rara avis in terris nigroque simillima cygno\" (\"a rare bird in the lands and very much like a black swan\").: 165  When the phrase was coined, the black swan was presumed not to exist. The importance of the metaphor lies in its analogy to the fragility of any system of thought.  A set of conclusions is potentially undone once any of its fundamental postulates is disproved. In this case, the observation of a single black swan would be the undoing of the logic of any system of thought, as well as any reasoning that followed from that underlying logic.\nJuvenal's phrase was a common expression in 16th century London as a statement of impossibility. The London expression derives from the Old World presumption that all swans must be white because all historical records of swans reported that they had white feathers. In that context, a black swan was impossible or at least nonexistent.\nHowever, in 1697, Dutch explorers led by Willem de Vlamingh became the first Europeans to see black swans, in Western Australia. The term subsequently metamorphosed to connote the idea that a perceived impossibility might later be disproven. Taleb notes that in the 19th century, John Stuart Mill used the black swan logical fallacy as a new term to identify falsification.Black swan events were discussed by Nassim Nicholas Taleb in his 2001 book Fooled By Randomness, which concerned financial events. His 2007 book The Black Swan extended the metaphor to events outside of financial markets. Taleb regards almost all major scientific discoveries, historical events, and artistic accomplishments as \"black swans\"—undirected and unpredicted.  He gives the rise of the Internet, the personal computer, World War I, the dissolution of the Soviet Union, and the September 11, 2001 attacks as examples of black swan events.: prologue \nTaleb asserts:What we call here a Black Swan (and capitalize it) is an event with the following three attributes.\nFirst, it is an outlier, as it lies outside the realm of regular expectations, because nothing in the past can convincingly point to its possibility. Second, it carries an extreme 'impact'. Third, in spite of its outlier status, human nature makes us concoct explanations for its occurrence after the fact, making it explainable and predictable.\n\nI stop and summarize the triplet: rarity, extreme 'impact', and retrospective (though not prospective) predictability. A small number of Black Swans explains almost everything in our world, from the success of ideas and religions, to the dynamics of historical events, to elements of our own personal lives.\n\nIdentifying\nBased on the author's criteria:\n\nThe event is a surprise (to the observer).\nThe event has a major effect.\nAfter the first recorded instance of the event, it is rationalized by hindsight, as if it could have been expected; that is, the relevant data were available but unaccounted for in risk mitigation programs.  The same is true for the personal perception by individuals.According to Taleb, as it was expected with great certainty that a global pandemic would eventually take place, the COVID-19 pandemic is not a black swan, but is considered to be a white swan; such an event has a major effect, but is compatible with statistical properties.\n\nCoping with black swans\nThe practical aim of Taleb's book is not to attempt to predict events which are unpredictable, but to build robustness against negative events while still exploiting positive events. Taleb contends that banks and trading firms are very vulnerable to hazardous black swan events and are exposed to unpredictable losses. On the subject of business, and quantitative finance in particular, Taleb critiques the widespread use of the normal distribution model employed in financial engineering, calling it a Great Intellectual Fraud.  Taleb elaborates the robustness concept as a central topic of his later book, Antifragile: Things That Gain From Disorder.\nIn the second edition of The Black Swan, Taleb provides \"Ten Principles for a Black-Swan-Robust Society\".: 374–78 Taleb states that a black swan event depends on the observer.  For example, what may be a Black Swan surprise for a turkey is not a Black Swan surprise to its butcher; hence the objective should be to \"avoid being the turkey\" by identifying areas of vulnerability in order to \"turn the Black Swans white\".\n\nEpistemological approach\nTaleb's black swan is different from the earlier philosophical versions of the problem, specifically in epistemology, as it concerns a phenomenon with specific empirical and statistical properties which he calls, \"the fourth quadrant\".Taleb's problem is about epistemic limitations in some parts of the areas covered in decision making. These limitations are twofold: philosophical (mathematical) and empirical (human-known) epistemic biases. The philosophical problem is about the decrease in knowledge when it comes to rare events because these are not visible in past samples and therefore require a strong a priori (extrapolating) theory; accordingly, predictions of events depend more and more on theories when their probability is small. In the \"fourth quadrant\", knowledge is uncertain and consequences are large, requiring more robustness.According to Taleb, thinkers who came before him who dealt with the notion of the improbable (such as Hume, Mill, and Popper) focused on the problem of induction in logic, specifically, that of drawing general conclusions from specific observations. The central and unique attribute of Taleb's black swan event is that it is high-profile. His claim is that almost all consequential events in history come from the unexpected – yet humans later convince themselves that these events are explainable in hindsight.One problem, labeled the ludic fallacy by Taleb, is the belief that the unstructured randomness found in life resembles the structured randomness found in games. This stems from the assumption that the unexpected may be predicted by extrapolating from variations in statistics based on past observations, especially when these statistics are presumed to represent samples from a normal distribution. These concerns often are highly relevant in financial markets, where major players sometimes assume normal distributions when using value at risk models, although market returns typically have fat tail distributions.\nTaleb said:I don't particularly care about the usual.  If you want to get an idea of a friend's temperament, ethics, and personal elegance, you need to look at him under the tests of severe circumstances, not under the regular rosy glow of daily life.  Can you assess the danger a criminal poses by examining only what he does on an ordinary day?  Can we understand health without considering wild diseases and epidemics?  Indeed the normal is often irrelevant. Almost everything in social life is produced by rare but consequential shocks and jumps; all the while almost everything studied about social life focuses on the 'normal,' particularly with 'bell curve' methods of inference that tell you close to nothing. Why? Because the bell curve ignores large deviations, cannot handle them, yet makes us confident that we have tamed uncertainty.  Its nickname in this book is GIF, Great Intellectual Fraud.More generally, decision theory, which is based on a fixed universe or a model of possible outcomes, ignores and minimizes the effect of events that are \"outside the model\". For instance, a simple model of daily stock market returns may include extreme moves such as Black Monday (1987), but might not model the breakdown of markets following the September 11, 2001 attacks.  Consequently, the New York Stock Exchange and Nasdaq exchange remained closed till September 17, 2001, the most protracted shutdown since the Great Depression. A fixed model considers the \"known unknowns\", but ignores the \"unknown unknowns\", made famous by a statement of Donald Rumsfeld. The term \"unknown unknowns\" appeared in a 1982 New Yorker article on the aerospace industry, which cites the example of metal fatigue, the cause of crashes in Comet airliners in the 1950s.Deterministic chaotic dynamics reproducing the Black Swan Event have been researched in economics. That is in agreement with Taleb's comment regarding some distributions which are not usable with precision, but which are more descriptive, such as the fractal, power law, or scalable distributions and that awareness of these might help to temper expectations. Beyond this, Taleb emphasizes that many events simply are without precedent, undercutting the basis of this type of reasoning altogether.Taleb also argues for the use of counterfactual reasoning when considering risk.: p. xvii\n\nSee also\nReferences\nBibliography\nTaleb, Nassim Nicholas (2010) [2007], The Black Swan: The Impact of the Highly Improbable (2nd ed.), London: Penguin, ISBN 978-0-14103459-1, retrieved 26 February 2017.\nTaleb, Nassim Nicholas (September 2008), \"The Fourth Quadrant: A Map of the Limits of Statistics\", Third Culture, The Edge Foundation, retrieved 23 May 2012.\nThe U.S. response to NEOs- avoiding a black swan event\n\nExternal links\nMcGee, Suzanne (5 December 2012), Black Swan Stocks Could Make Your Portfolio a Turkey, Fiscal Times, CNBC, retrieved 20 January 2016.",
    "Boosting (machine learning)": "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\nRobert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\n\nBoosting algorithms\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\n\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation) and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize.\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.The main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\n\nObject categorization in computer vision\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\n\nProblem of object categorization\nObject categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There are many ways to represent a category of objects, e.g. from shape analysis, bag of words models, or local descriptors such as SIFT, etc.  Examples of supervised classifiers are Naive Bayes classifiers, support vector machines, mixtures of Gaussians, and neural networks.  However, research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well.\n\nStatus quo for object categorization\nThe recognition of object categories in images is a challenging problem in computer vision, especially when the number of categories is large.  This is due to high intra class variability and the need for generalization across variations of objects within the same category. Objects within one category may look quite different. Even the same object may appear unalike under different viewpoint, scale, and illumination. Background clutter and partial occlusion add difficulties to recognition as well.  Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few, e.g. human faces, cars, simple objects, etc.  Research has been very active on dealing with more categories and enabling incremental additions of new categories, and although the general problem remains unsolved, several multi-category objects detectors (for up to hundreds or thousands of categories) have been developed.  One means is by feature sharing and boosting.\n\nBoosting for binary categorization\nAdaBoost can be used for face detection as an example of binary categorization. The two categories are faces versus background. The general algorithm is as follows:\n\nForm a large set of simple features\nInitialize weights for training images\nFor T rounds\nNormalize the weights\nFor available features from the set, train a classifier using a single feature and evaluate the training error\nChoose the classifier with the lowest error\nUpdate the weights of the training images: increase if classified wrongly by this classifier, decrease if correctly\nForm the final strong classifier as the linear combination of the T classifiers (coefficient larger if training error is small)After boosting, a classifier constructed from 200 features could yield a 95% detection rate under a \n  \n    \n      10\n      \n        −\n        5\n      \n    \n    10^{-5}\n   false positive rate.Another application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance. This work is the first to combine both motion information and appearance information as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework.\n\nBoosting for multi-class categorization\nCompared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time.  They turn to be more generic edge like features. During learning, the detectors for each category can be trained jointly. Compared with training separately, it generalizes better, needs less training data, and requires fewer features to achieve the same performance.\nThe main flow of the algorithm is similar to the binary case. What is different is that a measure of the joint training error shall be defined in advance. During each iteration the algorithm chooses a classifier of a single feature (features that can be shared by more categories shall be encouraged). This can be done via converting multi-class classification into a binary one (a set of categories versus the rest), or by introducing a penalty error from the categories that do not have the feature of the classifier.In the paper \"Sharing visual features for multiclass and multiview object detection\", A. Torralba et al. used GentleBoost for boosting and showed that when training data is limited, learning via sharing features does a much better job than no sharing, given same boosting rounds. Also, for a given performance level, the total number of features required (and therefore the run time cost of the classifier) for the feature sharing detectors, is observed to scale approximately logarithmically with the number of class, i.e., slower than linear growth in the non-sharing case. Similar results are shown in the paper \"Incremental learning of object detectors using a visual shape alphabet\", yet the authors used AdaBoost for boosting.\n\nConvex vs. non-convex boosting algorithms\nBoosting algorithms can be based on convex or non-convex optimization algorithms.  Convex algorithms, such as AdaBoost and LogitBoost, can be \"defeated\" by random  noise such that they can't learn basic and learnable combinations of weak hypotheses. This limitation was pointed out by Long & Servedio in 2008.  However, by 2009, multiple authors demonstrated that  boosting algorithms based on non-convex optimization, such as BrownBoost, can learn from noisy datasets and can specifically learn the underlying classifier of the Long–Servedio dataset.\n\nSee also\nImplementations\nscikit-learn, an open source machine learning library for Python\nOrange, a free data mining software suite, module Orange.ensemble\nWeka is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost\nR package GBM (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.\njboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees\nR package adabag: Applies Multiclass AdaBoost.M1, AdaBoost-SAMME and Bagging\nR package xgboost: An implementation of gradient boosting for linear and tree-based models.\n\nNotes\nReferences\nFurther reading\nYoav Freund and Robert E. Schapire (1997); A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting, Journal of Computer and System Sciences, 55(1):119-139\nRobert E. Schapire and Yoram Singer (1999); Improved Boosting Algorithms Using Confidence-Rated Predictors, Machine Learning, 37(3):297-336\n\nExternal links\nRobert E. Schapire (2003); The Boosting Approach to Machine Learning: An Overview, MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification\nZhou Zhi-Hua (2014) Boosting 25 years, CCL 2014 Keynote.\nZhou, Zhihua (2008). \"On the margin explanation of boosting algorithm\" (PDF). In: Proceedings of the 21st Annual Conference on Learning Theory (COLT'08): 479–490.\nZhou, Zhihua (2013). \"On the doubt about margin explanation of boosting\" (PDF). Artificial Intelligence. 203: 1–18. arXiv:1009.3613. doi:10.1016/j.artint.2013.07.002. S2CID 2828847.",
    "Bootstrap aggregating": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach. Bootstrap aggregation can be related to the posterior predictive distribution\n\nDescription of the technique\nGiven a standard training set \n  \n    D\n    D\n   of size n, bagging generates m new training sets \n  \n    \n      D\n      \n        i\n      \n    \n    D_{i}\n  , each of size n′, by sampling from D uniformly and with replacement. By sampling with replacement, some observations may be repeated in each \n  \n    \n      D\n      \n        i\n      \n    \n    D_{i}\n  . If n′=n, then for large n the set \n  \n    \n      D\n      \n        i\n      \n    \n    D_{i}\n   is expected to have the fraction (1 - 1/e) (≈63.2%) of the unique examples of D, the rest being duplicates. This kind of sample is known as a bootstrap sample. Sampling with replacement ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling.   Then, m models  are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n\nBagging leads to \"improvements for unstable procedures\", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. Bagging was shown to improve preimage learning. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors.\n\nProcess of the algorithm\nKey Terms\nThere are three types of datasets in bootstrap aggregating. These are the original, bootstrap, and out-of-bag datasets. Each section below will explain how each dataset is made except for the original dataset. The original dataset is whatever information is given.\n\nCreating the bootstrap dataset\nThe bootstrap dataset is made by randomly picking objects from the original dataset. Also, it must be the same size as the original dataset. However, the difference is that the bootstrap dataset can have duplicate objects. Here is simple example to demonstrate how it works along with the illustration below:\n\nSuppose the original dataset is a group of 12 people. These guys are Emily, Jessie, George, Constantine, Lexi, Theodore, John, James, Rachel, Anthony, Ellie, and Jamal.\nBy randomly picking a group of names, let us say our bootstrap dataset had James, Ellie, Constantine, Lexi, John, Constantine, Theodore, Constantine, Anthony, Lexi, Constantine, and Theodore. In this case, the bootstrap sample contained four duplicates for Constantine, and two duplicates for Lexi, and Theodore.\n\nCreating the out-of-bag dataset\nThe out-of-bag dataset represents the remaining people who were not in the bootstrap dataset. It can be calculated by taking the difference between the original and the bootstrap datasets. In this case, the remaining samples who were not selected are Emily, Jessie, George, Rachel, and Jamal. Keep in mind that since both datasets are sets, when taking the difference the duplicate names are ignored in the bootstrap dataset. The illustration below shows how the math is done:\n\nImportance\nCreating the bootstrap and out-of-bag datasets is crucial since it is used to test the accuracy of a random forest algorithm. For example, a model that produces 50 trees using the bootstrap/out-of-bag datasets will have a better accuracy than if it produced 10 trees. Since the algorithm generates multiple trees and therefore multiple datasets the chance that an object is left out of the bootstrap dataset is low. The next few sections talk about how the random forest algorithm works in more detail.\n\nCreation of Decision Trees\nThe next step of the algorithm involves the generation of decision trees from the bootstrapped dataset. To achieve this, the process examines each gene/feature and determines for how many samples the feature's presence or absence yields a positive or negative result. This information is then used to compute a confusion matrix, which lists the true positives, false positives, true negatives, and false negatives of the feature when used as a classifier. These features are then ranked according to various classification metrics based on their confusion matrices. Some common metrics include estimate of positive correctness (calculated by subtracting false positives from true positives), measure of \"goodness\", and information gain. These features are then used to partition the samples into two sets: those who possess the top feature, and those who do not.\nThe diagram below shows a decision tree of depth two being used to classify data. For example, a data point that exhibits Feature 1, but not Feature 2, will be given a \"No\". Another point that does not exhibit Feature 1, but does exhibit Feature 3, will be given a \"Yes\".\n\nThis process is repeated recursively for successive levels of the tree until the desired depth is reached. At the very bottom of the tree, samples that test positive for the final feature are generally classified as positive, while those that lack the feature are classified as negative. These trees are then used as predictors to classify new data.\n\nRandom Forests\nThe next part of the algorithm involves introducing yet another element of variability amongst the bootstrapped trees. In addition to each tree only examining a bootstrapped set of samples, only a small but consistent number of unique features are considered when ranking them as classifiers. This means that each tree only knows about the data pertaining to a small constant number of features, and a variable number of samples that is less than or equal to that of the original dataset. Consequently, the trees are more likely to return a wider array of answers, derived from more diverse knowledge. This results in a random forest, which possesses numerous benefits over a single decision tree generated without randomness. In a random forest, each tree \"votes\" on whether or not to classify a sample as positive based on its features. The sample is then classified based on majority vote. An example of this is given in the diagram below, where the four trees in a random forest vote on whether or not a patient with mutations A, B, F, and G has cancer. Since three out of four trees vote yes, the patient is then classified as cancer positive.\n\nBecause of their properties, random forests are considered one of the most accurate data mining algorithms, are less likely to overfit their data, and run quickly and efficiently even for large datasets. They are primarily useful for classification as opposed to regression, which attempts to draw observed connections between statistical variables in a dataset. This makes random forests particularly useful in such fields as banking, healthcare, the stock market, and e-commerce where it is important to be able to predict future results based on past data. One of their applications would be as a useful tool for predicting cancer based on genetic factors, as seen in the above example.\nThere are several important factors to consider when designing a random forest. If the trees in the random forests are too deep, overfitting can still occur due to over-specificity. If the forest is too large, the algorithm may become less efficient due to an increased runtime. Random forests also do not generally perform well when given sparse data with little variability. However, they still have numerous advantages over similar data classification algorithms such as neural networks, as they are much easier to interpret and generally require less data for training. As an integral component of random forests, bootstrap aggregating is very important to classification algorithms, and provides a critical element of variability that allows for increased accuracy when analyzing new data, as discussed below.\n\nImproving Random Forests and Bagging\nWhile the techniques described above utilize random forests and bagging (otherwise known as bootstrapping), there are certain techniques that can be used in order to improve their execution and voting time, their prediction accuracy, and their overall performance. The following are key steps in creating an efficient random forest:\n\nSpecify the maximum depth of trees: Instead of allowing your random forest to continue until all nodes are pure, it is better to cut it off at a certain point in order to further decrease chances of overfitting.\nPrune the dataset: Using an extremely large dataset may prove to create results that is less indicative of the data provided than a smaller set that more accurately represents what is being focused on.\nContinue pruning the data at each node split rather than just in the original bagging process.\nDecide on accuracy or speed: Depending on the desired results, increasing or decreasing the number of trees within the forest can help. Increasing the number of trees generally provides more accurate results while decreasing the number of trees will provide quicker results.\n\nAlgorithm (classification)\nFor classification, use a training set \n  \n    D\n    D\n  , Inducer \n  \n    I\n    I\n   and the number of bootstrap samples \n  \n    m\n    m\n   as input. Generate a classifier \n  \n    \n      C\n      \n        ∗\n      \n    \n    C^{*}\n   as output\nCreate \n  \n    m\n    m\n    new training sets  \n  \n    \n      D\n      \n        i\n      \n    \n    D_{i}\n  , from \n  \n    D\n    D\n   with replacement\nClassifier \n  \n    \n      C\n      \n        i\n      \n    \n    C_{i}\n   is built from each set \n  \n    \n      D\n      \n        i\n      \n    \n    D_{i}\n   using \n  \n    I\n    I\n   to determine the classification of set \n  \n    \n      D\n      \n        i\n      \n    \n    D_{i}\n  \nFinally classifier \n  \n    \n      C\n      \n        ∗\n      \n    \n    C^{*}\n   is generated by using the previously created set of classifiers \n  \n    \n      C\n      \n        i\n      \n    \n    C_{i}\n   on the original data set \n  \n    D\n    D\n  , the classification predicted most often by the sub-classifiers \n  \n    \n      C\n      \n        i\n      \n    \n    C_{i}\n   is the final classificationfor i = 1 to m {\n    D' = bootstrap sample from D    (sample with replacement)\n    Ci = I(D')\n}\nC*(x) = argmax #{i:Ci(x)=y}         (most often predicted label y)\n         y∈Y\n\nExample: ozone data\nTo illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy (1986), analysis done in R).\nThe relationship between temperature and ozone appears to be nonlinear in this data set, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with bandwidth 0.5) are used. Rather than building a single smoother for the complete data set, 100 bootstrap samples were drawn. Each sample is composed of a random subset of the original data and maintains a semblance of the master set’s distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The black lines represent these initial predictions. The lines lack agreement in their predictions and tend to overfit their data points: evident by the wobbly flow of the lines.\n\nBy taking the average of 100 smoothers, each corresponding to a subset of the original data set, we arrive at one bagged predictor (red line). The red line's flow is stable and does not overly conform to any data point(s).\n\nAdvantages and disadvantages\nAdvantages:\n\nMany weak learners aggregated typically outperform a single learner over the entire set, and has less overfit\nRemoves variance in high-variance low-bias weak learner, which can improve efficiency (statistics)\nCan be performed in parallel, as each separate bootstrap can be processed on its own before combinationDisadvantages:\n\nFor weak learner with high bias, bagging will also carry high bias into its aggregate\nLoss of interpretability of a model.\nCan be computationally expensive depending on the data set\n\nHistory\nThe concept of bootstrap aggregating is derived from the concept of bootstrapping which was developed by Bradley Efron.\nBootstrap aggregating was proposed by Leo Breiman who also coined the abbreviated term \"bagging\" (bootstrap aggregating). Breiman developed the concept of bagging in 1994 to improve classification by combining classifications of randomly generated training sets. He argued, \"If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy\".\n\nSee also\nBoosting (meta-algorithm)\nBootstrapping (statistics)\nCross-validation (statistics)\nOut-of-bag error\nRandom forest\nRandom subspace method (attribute bagging)\nResampled efficient frontier\nPredictive analysis: Classification and regression trees\n\nReferences\nFurther reading\nBreiman, Leo (1996). \"Bagging predictors\". Machine Learning. 24 (2): 123–140. CiteSeerX 10.1.1.32.9399. doi:10.1007/BF00058655. S2CID 47328136.\nAlfaro, E., Gámez, M. and García, N. (2012). \"adabag: An R package for classification with AdaBoost.M1, AdaBoost-SAMME and Bagging\". {{cite journal}}: Cite journal requires |journal= (help)\nKotsiantis, Sotiris (2014). \"Bagging and boosting variants for handling classifications problems: a survey\". Knowledge Eng. Review. 29 (1): 78–100. doi:10.1017/S0269888913000313. S2CID 27301684.\nBoehmke, Bradley; Greenwell, Brandon (2019). \"Bagging\". Hands-On Machine Learning with R. Chapman & Hall. pp. 191–202. ISBN 978-1-138-49568-5.",
    "Bootstrapping (statistics)": "Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.Bootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of  resamples with replacement, of the observed data set (and of equal size to the observed data set).\nIt may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\nHistory\nThe bootstrap was published by Bradley Efron in \"Bootstrap methods: another look at the jackknife\" (1979), inspired by earlier work on the jackknife. Improved estimates of the variance were developed later. A Bayesian extension was developed in 1981. The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987, and the ABC procedure in 1992.\n\nApproach\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample). As the population is unknown, the true error in a sample statistic against its population value is unknown. In bootstrap-resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference of the 'true' sample from resampled data (resampled → sample) is measurable.\nMore formally, the bootstrap works by treating inference of the true probability distribution J, given the original data, as being analogous to an inference of the empirical distribution Ĵ, given the resampled data. The accuracy of inferences regarding Ĵ using the resampled data can be assessed because we know Ĵ. If Ĵ is a reasonable approximation to J, then the quality of inference on J can in turn be inferred.\nAs an example, assume we are interested in the average (or mean) height of people worldwide.  We cannot measure all the people in the global population, so instead, we sample only a tiny part of it, and measure that.  Assume the sample is of size N; that is, we measure the heights of N individuals.  From that single sample, only one estimate of the mean can be obtained.  In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N.  The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample.  This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples, we compute its mean (each of these is called a \"bootstrap estimate\").  We now can create a histogram of bootstrap means.  This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples.  (The method here, described for the mean, can be applied to almost any other statistic or estimator.)\n\nDiscussion\nAdvantages\nA great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients. However, despite its simplicity, bootstrapping can be applied to complex sampling designs (e.g. for population divided into s strata with ns observations per strata, bootstrapping can be applied for each stratum). Bootstrap is also an appropriate way to control and check the stability of the results.  Although for most problems it is impossible to know the true confidence interval, bootstrap is asymptotically more accurate than the standard intervals obtained using sample variance and assumptions of normality. Bootstrapping is also a convenient method that avoids the cost of repeating the experiment to get other groups of sample data.\n\nDisadvantages\nBootstrapping depends heavily on the estimator used and, though simple, naive use of bootstrapping will not always yield asymptotically valid results and can lead to inconsistency. Although bootstrapping is (under some conditions) asymptotically consistent, it does not provide general finite-sample guarantees. The result may depend on the representative sample. The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples or large enough of a sample size) where these would be more formally stated in other approaches. Also, bootstrapping can be time-consuming and there are not many available software for bootstrapping as it is difficult to automate using traditional statistical computer packages.\n\nRecommendations\nScholars have recommended more bootstrap samples as available computing power has increased. If the results may have substantial real-world consequences, then one should use as many samples as is reasonable, given available computing power and time. Increasing the number of samples cannot increase the amount of information in the original data; it can only reduce the effects of random sampling errors which can arise from a bootstrap procedure itself. Moreover, there is evidence that numbers of samples greater than 100 lead to negligible improvements in the estimation of standard errors. In fact, according to the original developer of the bootstrapping method, even setting the number of samples at 50 is likely to lead to fairly good standard error estimates.Adèr et al. recommend the bootstrap procedure for the following situations:\nWhen the theoretical distribution of a statistic of interest is complicated or unknown. Since the bootstrapping procedure is distribution-independent it provides an indirect method to assess the properties of the distribution underlying the sample and the parameters of interest that are derived from this distribution.When the sample size is insufficient for straightforward statistical inference. If the underlying distribution is well-known, bootstrapping provides a way to account for the distortions caused by the specific sample that may not be fully representative of the population.When power calculations have to be performed, and a small pilot sample is available. Most power and sample size calculations are heavily dependent on the standard deviation of the statistic of interest. If the estimate used is incorrect, the required sample size will also be wrong. One method to get an impression of the variation of the statistic is to use a small pilot sample and perform bootstrapping on it to get impression of the variance.However, Athreya has shown that if one performs a naive bootstrap on the sample mean when the underlying population lacks a finite variance (for example, a power law distribution), then the bootstrap distribution will not converge to the same limit as the sample mean. As a result, confidence intervals on the basis of a Monte Carlo simulation of the bootstrap could be misleading. Athreya states that \"Unless one is reasonably sure that the underlying distribution is not heavy tailed, one should hesitate to use the naive bootstrap\".\n\nTypes of bootstrap scheme\nIn univariate problems, it is usually acceptable to resample the individual observations with replacement (\"case resampling\" below) unlike subsampling, in which resampling is without replacement and is valid under much weaker conditions compared to the bootstrap. In small samples, a parametric bootstrap approach might be preferred. For other problems, a smooth bootstrap will likely be preferred.\nFor regression problems, various other alternatives are available.\n\nCase resampling\nThe bootstrap is generally useful for estimating the distribution of a statistic (e.g. mean, variance) without using normality assumptions (as required, e.g., for a z-statistic or a t-statistic). In particular, the bootstrap is useful when there is no analytical form or an asymptotic theory (e.g., an applicable central limit theorem) to help estimate the distribution of the statistics of interest. This is because bootstrap methods can apply to most random quantities, e.g., the ratio of variance and mean. There are at least two ways of performing case resampling.\n\nThe Monte Carlo algorithm for case resampling is quite simple. First, we resample the data with replacement, and the size of the resample must be equal to the size of the original data set. Then the statistic of interest is computed from the resample from the first step. We repeat this routine many times to get a more precise estimate of the Bootstrap distribution of the statistic.\nThe 'exact' version for case resampling is similar, but we exhaustively enumerate every possible resample of the data set. This can be computationally expensive as there are a total of \n  \n    \n      \n        \n          \n            \n              (\n            \n            \n              \n                2\n                n\n                −\n                1\n              \n              n\n            \n            \n              )\n            \n          \n        \n        =\n        \n          \n            \n              (\n              2\n              n\n              −\n              1\n              )\n              !\n            \n            \n              n\n              !\n              (\n              n\n              −\n              1\n              )\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle {\\binom {2n-1}{n}}={\\frac {(2n-1)!}{n!(n-1)!}}}\n   different resamples, where n is the size of the data set. Thus for n = 5, 10, 20, 30 there are 126, 92378, 6.89 × 1010 and 5.91 × 1016  different resamples respectively.\n\nEstimating the distribution of sample mean\nConsider a coin-flipping experiment. We flip the coin and record whether it lands heads or tails. Let X = x1, x2, …, x10 be 10 observations from the experiment. xi = 1 if the i th flip lands heads, and 0 otherwise. By invoking the assumption that the average of the coin flips is normally distributed, we can use the t-statistic to estimate the distribution of the sample mean,\n\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n        \n        =\n        \n          \n            1\n            10\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        +\n        ⋯\n        +\n        \n          x\n          \n            10\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\bar {x}}={\\frac {1}{10}}(x_{1}+x_{2}+\\cdots +x_{10}).}\n  Such a normality assumption can be justified either as an approximation of the distribution of each individual coin flip or as an approximation of the distribution of the average of a large number of coin flips. The former is a poor approximation because the true distribution of the coin flips is Bernoulli instead of normal. The latter is a valid approximation in infinitely large samples due to the central limit theorem.\nHowever, if we are not ready to make such a justification, then we can use the bootstrap instead. Using case resampling, we can derive the distribution of \n  \n    \n      \n        \n          x\n          ¯\n        \n      \n    \n    {\\bar {x}}\n  . We first resample the data to obtain a bootstrap resample. An example of the first resample might look like this X1* = x2, x1, x10, x10, x3, x4, x6, x7, x1, x9. There are some duplicates since a bootstrap resample comes from sampling with replacement from the data. Also the number of data points in a bootstrap resample is equal to the number of data points in our original observations. Then we compute the mean of this resample and obtain the first bootstrap mean: μ1*. We repeat this process to obtain the second resample X2* and compute the second bootstrap mean μ2*. If we repeat this 100 times, then we have μ1*, μ2*, ..., μ100*.  This represents an empirical bootstrap distribution of sample mean. From this empirical distribution, one can derive a bootstrap confidence interval for the purpose of hypothesis testing.\n\nRegression\nIn regression problems, case resampling refers to the simple scheme of resampling individual cases – often rows of a data set. For regression problems, as long as the data set is fairly large, this simple scheme is often acceptable. However, the method is open to criticism.In regression problems, the explanatory variables are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information. As such, alternative bootstrap procedures should be considered.\n\nBayesian bootstrap\nBootstrapping can be interpreted in a Bayesian framework using a scheme that creates new data sets through reweighting the initial data. Given a set of \n  \n    N\n    N\n   data points, the weighting assigned to data point \n  \n    i\n    i\n   in a new data set \n  \n    \n      \n        \n          D\n        \n      \n      \n        J\n      \n    \n    {\\mathcal {D}}^{J}\n   is \n  \n    \n      \n        w\n        \n          i\n        \n        \n          J\n        \n      \n      =\n      \n        x\n        \n          i\n        \n        \n          J\n        \n      \n      −\n      \n        x\n        \n          i\n          −\n          1\n        \n        \n          J\n        \n      \n    \n    w_{i}^{J}=x_{i}^{J}-x_{i-1}^{J}\n  , where \n  \n    \n      \n        x\n      \n      \n        J\n      \n    \n    \\mathbf {x} ^{J}\n   is a low-to-high ordered list of \n  \n    \n      N\n      −\n      1\n    \n    N-1\n   uniformly distributed random numbers on \n  \n    \n      [\n      0\n      ,\n      1\n      ]\n    \n    [0,1]\n  , preceded by 0 and succeeded by 1. The distributions of a parameter inferred from considering many such data sets \n  \n    \n      \n        \n          D\n        \n      \n      \n        J\n      \n    \n    {\\mathcal {D}}^{J}\n   are then interpretable as posterior distributions on that parameter.\n\nSmooth bootstrap\nUnder this scheme, a small amount of (usually normally distributed) zero-centered random noise is added onto each resampled observation. This is equivalent to sampling from a kernel density estimate of the data. Assume K to be a symmetric kernel density function with unit variance. The standard kernel estimator \n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\hat {f\\,}}_{h}(x)}\n   of \n  \n    \n      f\n      (\n      x\n      )\n    \n    f(x)\n   is\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              n\n              h\n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        K\n        \n          (\n          \n            \n              \n                x\n                −\n                \n                  X\n                  \n                    i\n                  \n                \n              \n              h\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\hat {f\\,}}_{h}(x)={1 \\over nh}\\sum _{i=1}^{n}K\\left({x-X_{i} \\over h}\\right),}\n   where \n  \n    h\n    h\n   is the smoothing parameter. And the corresponding distribution function estimator \n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\hat {F\\,}}_{h}(x)}\n   is\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            x\n          \n        \n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        t\n        )\n        \n        d\n        t\n        .\n      \n    \n    {\\displaystyle {\\hat {F\\,}}_{h}(x)=\\int _{-\\infty }^{x}{\\hat {f}}_{h}(t)\\,dt.}\n\nParametric bootstrap\nBased on the assumption that the original data set is a realization of a random sample from a distribution of a specific parametric type, in this case a parametric model is fitted by parameter θ,  often by maximum likelihood, and samples of random numbers are drawn from this fitted model. Usually the sample drawn has the same sample size as the original data. Then the estimate of original function F can be written as \n  \n    \n      \n        \n          \n            \n              F\n              ^\n            \n          \n        \n        =\n        \n          F\n          \n            \n              \n                θ\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {F}}=F_{\\hat {\\theta }}}\n  . This sampling process is repeated many times as for other bootstrap methods. Considering the centered sample mean in this case, the random sample original distribution function \n  \n    \n      F\n      \n        θ\n      \n    \n    F_{\\theta }\n   is replaced by a bootstrap random sample with function \n  \n    \n      \n        \n          F\n          \n            \n              \n                θ\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{\\hat {\\theta }}}\n  , and the probability distribution of \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  n\n                \n              \n              ¯\n            \n          \n        \n        −\n        \n          μ\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X_{n}}}-\\mu _{\\theta }}\n   is approximated by that of \n  \n    \n      \n        \n          \n            \n              \n                X\n                ¯\n              \n            \n          \n          \n            n\n          \n          \n            ∗\n          \n        \n        −\n        \n          μ\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}_{n}^{*}-\\mu ^{*}}\n  , where \n  \n    \n      \n        \n          μ\n          \n            ∗\n          \n        \n        =\n        \n          μ\n          \n            \n              \n                θ\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mu ^{*}=\\mu _{\\hat {\\theta }}}\n  , which is the expectation corresponding to \n  \n    \n      \n        \n          F\n          \n            \n              \n                θ\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{\\hat {\\theta }}}\n  . The use of a parametric model at the sampling stage of the bootstrap methodology leads to procedures which are different from those obtained by applying basic statistical theory to inference for the same model.\n\nResampling residuals\nAnother approach to bootstrapping in regression problems is to resample residuals. The method proceeds as follows.\n\nFit the model and retain the fitted values \n  \n    \n      \n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {y\\,}}_{i}}\n   and the residuals \n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        −\n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        ,\n        (\n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle {\\widehat {\\varepsilon \\,}}_{i}=y_{i}-{\\widehat {y\\,}}_{i},(i=1,\\dots ,n)}\n  .\nFor each pair, (xi, yi), in which xi is the (possibly multivariate) explanatory variable, add a randomly resampled residual, \n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                \n                ^\n              \n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\varepsilon \\,}}_{j}}\n  , to the fitted value \n  \n    \n      \n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {y\\,}}_{i}}\n  . In other words, create synthetic response variables \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            ∗\n          \n        \n        =\n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        +\n        \n          \n            \n              \n                \n                  ε\n                  \n                \n                ^\n              \n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}={\\widehat {y\\,}}_{i}+{\\widehat {\\varepsilon \\,}}_{j}}\n   where j is selected randomly from the list (1, ..., n) for every i.\nRefit the model using the fictitious response variables \n  \n    \n      y\n      \n        i\n      \n      \n        ∗\n      \n    \n    y_{i}^{*}\n  , and retain the quantities of interest (often the parameters, \n  \n    \n      \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\mu }}_{i}^{*}}\n  , estimated from the synthetic \n  \n    \n      y\n      \n        i\n      \n      \n        ∗\n      \n    \n    y_{i}^{*}\n  ).\nRepeat steps 2 and 3 a large number of times.This scheme has the advantage that it retains the information in the explanatory variables. However, a question arises as to which residuals to resample. Raw residuals are one option; another is studentized residuals (in linear regression). Although there are arguments in favor of using studentized residuals; in practice, it often makes little difference, and it is easy to compare the results of both schemes.\n\nGaussian process regression bootstrap\nWhen data are temporally correlated, straightforward bootstrapping destroys the inherent correlations. This method uses Gaussian process regression (GPR) to fit a probabilistic model from which replicates may then be drawn. GPR is a Bayesian non-linear regression method. A Gaussian process (GP) is a collection of random variables, any finite number of which have a joint Gaussian (normal) distribution. A GP is defined by a mean function and a covariance function, which specify the mean vectors and covariance matrices for each finite collection of the random variables.Regression model:\n\n  \n    \n      \n        y\n        (\n        x\n        )\n        =\n        f\n        (\n        x\n        )\n        +\n        ε\n        ,\n         \n         \n        ε\n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle y(x)=f(x)+\\varepsilon ,\\ \\ \\varepsilon \\sim {\\mathcal {N}}(0,\\sigma ^{2}),}\n   \n  \n    ε\n    \\varepsilon\n   is a noise term.Gaussian process prior:\nFor any finite collection of variables, x1, ..., xn, the function outputs \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        …\n        ,\n        f\n        (\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle f(x_{1}),\\ldots ,f(x_{n})}\n   are jointly distributed according to a multivariate Gaussian with mean \n  \n    \n      \n        m\n        =\n        [\n        m\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        …\n        ,\n        m\n        (\n        \n          x\n          \n            n\n          \n        \n        )\n        \n          ]\n          \n            ⊺\n          \n        \n      \n    \n    {\\displaystyle m=[m(x_{1}),\\ldots ,m(x_{n})]^{\\intercal }}\n   and covariance matrix \n  \n    \n      \n        (\n        K\n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (K)_{ij}=k(x_{i},x_{j}).}\n  \nAssume \n  \n    \n      \n        f\n        (\n        x\n        )\n        ∼\n        \n          \n            G\n            P\n          \n        \n        (\n        m\n        ,\n        k\n        )\n        .\n      \n    \n    {\\displaystyle f(x)\\sim {\\mathcal {GP}}(m,k).}\n   Then \n  \n    \n      \n        y\n        (\n        x\n        )\n        ∼\n        \n          \n            G\n            P\n          \n        \n        (\n        m\n        ,\n        l\n        )\n      \n    \n    {\\displaystyle y(x)\\sim {\\mathcal {GP}}(m,l)}\n  ,\nwhere \n  \n    \n      \n        l\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        +\n        \n          σ\n          \n            2\n          \n        \n        δ\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle l(x_{i},x_{j})=k(x_{i},x_{j})+\\sigma ^{2}\\delta (x_{i},x_{j})}\n  , and \n  \n    \n      \n        δ\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta (x_{i},x_{j})}\n   is the standard Kronecker delta function.Gaussian process posterior:\nAccording to GP prior, we can get\n\n  \n    \n      \n        [\n        y\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        …\n        ,\n        y\n        (\n        \n          x\n          \n            r\n          \n        \n        )\n        ]\n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          m\n          \n            0\n          \n        \n        ,\n        \n          K\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle [y(x_{1}),\\ldots ,y(x_{r})]\\sim {\\mathcal {N}}(m_{0},K_{0})}\n  ,where \n  \n    \n      \n        \n          m\n          \n            0\n          \n        \n        =\n        [\n        m\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        …\n        ,\n        m\n        (\n        \n          x\n          \n            r\n          \n        \n        )\n        \n          ]\n          \n            ⊺\n          \n        \n      \n    \n    {\\displaystyle m_{0}=[m(x_{1}),\\ldots ,m(x_{r})]^{\\intercal }}\n   and \n  \n    \n      \n        (\n        \n          K\n          \n            0\n          \n        \n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        +\n        \n          σ\n          \n            2\n          \n        \n        δ\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (K_{0})_{ij}=k(x_{i},x_{j})+\\sigma ^{2}\\delta (x_{i},x_{j}).}\n  \nLet x1*,...,xs* be another finite collection of variables, it's obvious that\n\n  \n    \n      \n        [\n        y\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        …\n        ,\n        y\n        (\n        \n          x\n          \n            r\n          \n        \n        )\n        ,\n        f\n        (\n        \n          x\n          \n            1\n          \n          \n            ∗\n          \n        \n        )\n        ,\n        …\n        ,\n        f\n        (\n        \n          x\n          \n            s\n          \n          \n            ∗\n          \n        \n        )\n        \n          ]\n          \n            ⊺\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              (\n            \n            \n              \n                m\n                \n                  0\n                \n              \n              \n                m\n                \n                  ∗\n                \n              \n            \n            \n              )\n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    K\n                    \n                      0\n                    \n                  \n                \n                \n                  \n                    K\n                    \n                      ∗\n                    \n                  \n                \n              \n              \n                \n                  \n                    K\n                    \n                      ∗\n                    \n                    \n                      ⊺\n                    \n                  \n                \n                \n                  \n                    K\n                    \n                      ∗\n                      ∗\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle [y(x_{1}),\\ldots ,y(x_{r}),f(x_{1}^{*}),\\ldots ,f(x_{s}^{*})]^{\\intercal }\\sim {\\mathcal {N}}({\\binom {m_{0}}{m_{*}}}{\\begin{pmatrix}K_{0}&K_{*}\\\\K_{*}^{\\intercal }&K_{**}\\end{pmatrix}})}\n  ,where \n  \n    \n      \n        \n          m\n          \n            ∗\n          \n        \n        =\n        [\n        m\n        (\n        \n          x\n          \n            1\n          \n          \n            ∗\n          \n        \n        )\n        ,\n        …\n        ,\n        m\n        (\n        \n          x\n          \n            s\n          \n          \n            ∗\n          \n        \n        )\n        \n          ]\n          \n            ⊺\n          \n        \n      \n    \n    {\\displaystyle m_{*}=[m(x_{1}^{*}),\\ldots ,m(x_{s}^{*})]^{\\intercal }}\n  , \n  \n    \n      \n        (\n        \n          K\n          \n            ∗\n            ∗\n          \n        \n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n          \n            ∗\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n          \n            ∗\n          \n        \n        )\n      \n    \n    {\\displaystyle (K_{**})_{ij}=k(x_{i}^{*},x_{j}^{*})}\n  , \n  \n    \n      \n        (\n        \n          K\n          \n            ∗\n          \n        \n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n          \n            ∗\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (K_{*})_{ij}=k(x_{i},x_{j}^{*}).}\n  \nAccording to the equations above, the outputs y are also jointly distributed according to a multivariate Gaussian. Thus,\n\n  \n    \n      \n        [\n        f\n        (\n        \n          x\n          \n            1\n          \n          \n            ∗\n          \n        \n        )\n        ,\n        …\n        ,\n        f\n        (\n        \n          x\n          \n            s\n          \n          \n            ∗\n          \n        \n        )\n        \n          ]\n          \n            ⊺\n          \n        \n        ∣\n        (\n        [\n        y\n        (\n        x\n        )\n        \n          ]\n          \n            ⊺\n          \n        \n        =\n        y\n        )\n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          m\n          \n            post\n          \n        \n        ,\n        \n          K\n          \n            post\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle [f(x_{1}^{*}),\\ldots ,f(x_{s}^{*})]^{\\intercal }\\mid ([y(x)]^{\\intercal }=y)\\sim {\\mathcal {N}}(m_{\\text{post}},K_{\\text{post}}),}\n  where \n  \n    \n      \n        y\n        =\n        [\n        \n          y\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          y\n          \n            r\n          \n        \n        \n          ]\n          \n            ⊺\n          \n        \n      \n    \n    {\\displaystyle y=[y_{1},...,y_{r}]^{\\intercal }}\n  , \n  \n    \n      \n        \n          m\n          \n            post\n          \n        \n        =\n        \n          m\n          \n            ∗\n          \n        \n        +\n        \n          K\n          \n            ∗\n          \n          \n            ⊺\n          \n        \n        (\n        \n          K\n          \n            O\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        \n          I\n          \n            r\n          \n        \n        \n          )\n          \n            −\n            1\n          \n        \n        (\n        y\n        −\n        \n          m\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle m_{\\text{post}}=m_{*}+K_{*}^{\\intercal }(K_{O}+\\sigma ^{2}I_{r})^{-1}(y-m_{0})}\n  , \n  \n    \n      \n        \n          K\n          \n            post\n          \n        \n        =\n        \n          K\n          \n            ∗\n            ∗\n          \n        \n        −\n        \n          K\n          \n            ∗\n          \n          \n            ⊺\n          \n        \n        (\n        \n          K\n          \n            O\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        \n          I\n          \n            r\n          \n        \n        \n          )\n          \n            −\n            1\n          \n        \n        \n          K\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{post}}=K_{**}-K_{*}^{\\intercal }(K_{O}+\\sigma ^{2}I_{r})^{-1}K_{*}}\n  , and \n  \n    \n      I\n      \n        r\n      \n    \n    I_{r}\n   is \n  \n    \n      r\n      ×\n      r\n    \n    r\\times r\n   identity matrix.\n\nWild bootstrap\nThe wild bootstrap, proposed originally by Wu (1986), is suited when the model exhibits heteroskedasticity. The idea is, as the residual bootstrap, to leave the regressors at their sample value, but to resample the response variable based on the residuals values. That is, for each replicate, one computes a new \n  \n    y\n    y\n   based on\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            ∗\n          \n        \n        =\n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        +\n        \n          \n            \n              \n                \n                  ε\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}={\\widehat {y\\,}}_{i}+{\\widehat {\\varepsilon \\,}}_{i}v_{i}}\n  so the residuals are randomly multiplied by a random variable \n  \n    \n      v\n      \n        i\n      \n    \n    v_{i}\n   with mean 0 and variance 1. For most distributions of \n  \n    \n      v\n      \n        i\n      \n    \n    v_{i}\n   (but not Mammen's), this method assumes that the 'true' residual distribution is symmetric and can offer advantages over simple residual sampling for smaller sample sizes. Different forms are used for the random variable \n  \n    \n      v\n      \n        i\n      \n    \n    v_{i}\n  , such as\n\nThe standard normal distributionA distribution suggested by Mammen (1993).\n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  −\n                  (\n                  \n                    \n                      5\n                    \n                  \n                  −\n                  1\n                  )\n                  \n                    /\n                  \n                  2\n                \n                \n                  \n                    with probability \n                  \n                  (\n                  \n                    \n                      5\n                    \n                  \n                  +\n                  1\n                  )\n                  \n                    /\n                  \n                  (\n                  2\n                  \n                    \n                      5\n                    \n                  \n                  )\n                  ,\n                \n              \n              \n                \n                  (\n                  \n                    \n                      5\n                    \n                  \n                  +\n                  1\n                  )\n                  \n                    /\n                  \n                  2\n                \n                \n                  \n                    with probability \n                  \n                  (\n                  \n                    \n                      5\n                    \n                  \n                  −\n                  1\n                  )\n                  \n                    /\n                  \n                  (\n                  2\n                  \n                    \n                      5\n                    \n                  \n                  )\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle v_{i}={\\begin{cases}-({\\sqrt {5}}-1)/2&{\\text{with probability }}({\\sqrt {5}}+1)/(2{\\sqrt {5}}),\\\\({\\sqrt {5}}+1)/2&{\\text{with probability }}({\\sqrt {5}}-1)/(2{\\sqrt {5}})\\end{cases}}}\n  Approximately, Mammen's distribution is:\n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  −\n                  0.6180\n                  \n                  \n                    (with a 0 in the units' place)\n                  \n                \n                \n                  \n                    with probability \n                  \n                  0.7236\n                  ,\n                \n              \n              \n                \n                  +\n                  1.6180\n                  \n                  \n                    (with a 1 in the units' place)\n                  \n                \n                \n                  \n                    with probability \n                  \n                  0.2764.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle v_{i}={\\begin{cases}-0.6180\\quad {\\text{(with a 0 in the units' place)}}&{\\text{with probability }}0.7236,\\\\+1.6180\\quad {\\text{(with a 1 in the units' place)}}&{\\text{with probability }}0.2764.\\end{cases}}}\n  Or the simpler distribution, linked to the Rademacher distribution:\n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  −\n                  1\n                \n                \n                  \n                    with probability \n                  \n                  1\n                  \n                    /\n                  \n                  2\n                  ,\n                \n              \n              \n                \n                  +\n                  1\n                \n                \n                  \n                    with probability \n                  \n                  1\n                  \n                    /\n                  \n                  2.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle v_{i}={\\begin{cases}-1&{\\text{with probability }}1/2,\\\\+1&{\\text{with probability }}1/2.\\end{cases}}}\n\nBlock bootstrap\nThe block bootstrap is used when the data, or the errors in a model, are correlated. In this case, a simple case or residual resampling will fail, as it is not able to replicate the correlation in the data. The block bootstrap tries to replicate the correlation by resampling inside blocks of data (see Blocking (statistics)). The block bootstrap has been used mainly with data correlated in time (i.e. time series) but can also be used with data correlated in space, or among groups (so-called cluster data).\n\nTime series: Simple block bootstrap\nIn the (simple) block bootstrap, the variable of interest is split into non-overlapping blocks.\n\nTime series: Moving block bootstrap\nIn the moving block bootstrap, introduced by Künsch (1989), data is split into n − b + 1 overlapping blocks of length b: Observation 1 to b will be block 1, observation 2 to b + 1 will be block 2, etc. Then from these n − b + 1 blocks, n/b blocks will be drawn at random with replacement. Then aligning these n/b blocks in the order they were picked, will give the bootstrap observations.\nThis bootstrap works with dependent data, however, the bootstrapped observations will not be stationary anymore by construction. But, it was shown that varying randomly the block length can avoid this problem. This method is known as the stationary bootstrap. Other related modifications of the moving block bootstrap are the Markovian bootstrap and a stationary bootstrap method that matches subsequent blocks based on standard deviation matching.\n\nTime series: Maximum entropy bootstrap\nVinod (2006), presents a method that bootstraps time series data using  maximum entropy principles satisfying the Ergodic theorem with mean-preserving and mass-preserving constraints. There is an R package, meboot, that utilizes the method, which  has applications in econometrics and computer science.\n\nCluster data: block bootstrap\nCluster data describes data where many observations per unit are observed. This could be observing many firms in many states or observing students in many classes. In such cases, the correlation structure is simplified, and one does usually make the assumption that data is correlated within a group/cluster, but independent between groups/clusters. The structure of the block bootstrap is easily obtained (where the block just corresponds to the group), and usually only the groups are resampled, while the observations within the groups are left unchanged. Cameron et al. (2008) discusses this for clustered errors in linear regression.\n\nMethods for improving computational efficiency\nThe bootstrap is a powerful technique although may require substantial computing resources in both time and memory. Some techniques have been developed to reduce this burden. They can generally be combined with many of the different types of Bootstrap schemes and various choices of statistics.\n\nPoisson bootstrap\nThe ordinary bootstrap requires the random selection of n elements from a list, which is equivalent to drawing from a binomial distribution. This may require a large number of passes over the data and is challenging to run these computations in parallel. For large values of n, the Poisson bootstrap is an efficient method of generating bootstrapped data sets. When generating a single bootstrap sample, instead of randomly drawing from the sample data with replacement, each data point is assigned a random weight distributed according to the Poisson distribution with \n  \n    \n      λ\n      =\n      1\n    \n    \\lambda =1\n  . For large sample data, this will approximate random sampling with replacement. This is due to the following approximation:\n\n  \n    \n      \n        \n          lim\n          \n            n\n            →\n            ∞\n          \n        \n        Binomial\n        ⁡\n        (\n        n\n        ,\n        1\n        \n          /\n        \n        n\n        )\n        =\n        Poisson\n        ⁡\n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\lim _{n\\to \\infty }\\operatorname {Binomial} (n,1/n)=\\operatorname {Poisson} (1)}\n  This method also lends itself well to streaming data and growing data sets, since the total number of samples does not need to be known in advance of beginning to take bootstrap samples.\nFor large enough n, the results are relatively similar to the original bootstrap estimations.A way to improve on the poisson bootstrap, termed \"sequential bootstrap\", is by taking the first samples so that the proportion of unique values is ≈0.632 of the original sample size n. This provides a distribution with main empirical characteristics being within a distance of \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3\n            \n              /\n            \n            4\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{3/4})}\n  . Empirical investigation has shown this method can yield good results. This is related to the reduced bootstrap method.\n\nBag of Little Bootstraps\nFor massive data sets, it is often computationally prohibitive to hold all the sample data in memory and resample from the sample data. The Bag of Little Bootstraps (BLB) provides a method of pre-aggregating data before bootstrapping to reduce computational constraints. This works by partitioning the data set into \n  \n    b\n    b\n   equal-sized buckets and aggregating the data within each bucket. This pre-aggregated data set becomes the new sample data over which to draw samples with replacement. This method is similar to the Block Bootstrap, but the motivations and definitions of the blocks are very different. Under certain assumptions, the sample distribution should approximate the full bootstrapped scenario. One constraint is the number of buckets \n  \n    \n      \n        b\n        =\n        \n          n\n          \n            γ\n          \n        \n      \n    \n    {\\displaystyle b=n^{\\gamma }}\n  where \n  \n    \n      \n        γ\n        ∈\n        [\n        0.5\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\gamma \\in [0.5,1]}\n   and the authors recommend usage of \n  \n    \n      \n        b\n        =\n        \n          n\n          \n            0.7\n          \n        \n      \n    \n    {\\displaystyle b=n^{0.7}}\n   as a general solution.\n\nChoice of statistic\nThe bootstrap distribution of a point estimator of a population parameter has been used to produce a bootstrapped confidence interval for the parameter's true value if the parameter can be written as a function of the population's distribution.\nPopulation parameters are estimated with many point estimators. Popular families of point-estimators include mean-unbiased minimum-variance estimators, median-unbiased estimators, Bayesian estimators (for example, the posterior distribution's mode, median, mean), and maximum-likelihood estimators.\nA Bayesian point estimator and a maximum-likelihood estimator have good performance when the sample size is infinite, according to asymptotic theory. For practical problems with finite samples, other estimators may be preferable. Asymptotic theory suggests techniques that often improve the performance of bootstrapped estimators; the bootstrapping of a maximum-likelihood estimator may often be improved using transformations related to pivotal quantities.\n\nDeriving confidence intervals from the bootstrap distribution\nThe bootstrap distribution of a parameter-estimator has been used to calculate confidence intervals for its population-parameter.\n\nBias, asymmetry, and confidence intervals\nBias: The bootstrap distribution and the sample may disagree systematically, in which case  bias may occur.\nIf the bootstrap distribution of an estimator is symmetric, then percentile confidence-interval are often used; such intervals are appropriate especially for median-unbiased estimators of minimum risk (with respect to an absolute loss function).  Bias in the bootstrap distribution will lead to bias in the confidence interval.\nOtherwise, if the bootstrap distribution is non-symmetric, then percentile confidence intervals are often inappropriate.\n\nMethods for bootstrap confidence intervals\nThere are several methods for constructing confidence intervals from the bootstrap distribution of a real parameter:\n\nBasic bootstrap, also known as the Reverse Percentile Interval. The basic bootstrap is a simple scheme to construct the confidence interval: one simply takes the empirical quantiles from the bootstrap distribution of the parameter (see Davison and Hinkley 1997, equ. 5.6 p. 194):\n  \n    \n      \n        (\n        2\n        \n          \n            \n              \n                θ\n                \n              \n              ^\n            \n          \n        \n        −\n        \n          θ\n          \n            (\n            1\n            −\n            α\n            \n              /\n            \n            2\n            )\n          \n          \n            ∗\n          \n        \n        ,\n        2\n        \n          \n            \n              \n                θ\n                \n              \n              ^\n            \n          \n        \n        −\n        \n          θ\n          \n            (\n            α\n            \n              /\n            \n            2\n            )\n          \n          \n            ∗\n          \n        \n        )\n      \n    \n    {\\displaystyle (2{\\widehat {\\theta \\,}}-\\theta _{(1-\\alpha /2)}^{*},2{\\widehat {\\theta \\,}}-\\theta _{(\\alpha /2)}^{*})}\n   where \n  \n    \n      θ\n      \n        (\n        1\n        −\n        α\n        \n          /\n        \n        2\n        )\n      \n      \n        ∗\n      \n    \n    \\theta _{(1-\\alpha /2)}^{*}\n   denotes the \n  \n    \n      1\n      −\n      α\n      \n        /\n      \n      2\n    \n    1-\\alpha /2\n   percentile of the bootstrapped coefficients \n  \n    \n      θ\n      \n        ∗\n      \n    \n    \\theta ^{*}\n  .Percentile bootstrap. The percentile bootstrap proceeds in a similar way to the basic bootstrap, using percentiles of the bootstrap distribution, but with a different formula (note the inversion of the left and right quantiles):\n  \n    \n      \n        (\n        \n          θ\n          \n            (\n            α\n            \n              /\n            \n            2\n            )\n          \n          \n            ∗\n          \n        \n        ,\n        \n          θ\n          \n            (\n            1\n            −\n            α\n            \n              /\n            \n            2\n            )\n          \n          \n            ∗\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\theta _{(\\alpha /2)}^{*},\\theta _{(1-\\alpha /2)}^{*})}\n   where \n  \n    \n      θ\n      \n        (\n        1\n        −\n        α\n        \n          /\n        \n        2\n        )\n      \n      \n        ∗\n      \n    \n    \\theta _{(1-\\alpha /2)}^{*}\n   denotes the \n  \n    \n      1\n      −\n      α\n      \n        /\n      \n      2\n    \n    1-\\alpha /2\n   percentile of the bootstrapped coefficients \n  \n    \n      θ\n      \n        ∗\n      \n    \n    \\theta ^{*}\n  .\nSee Davison and Hinkley (1997, equ. 5.18 p. 203) and Efron and Tibshirani (1993, equ 13.5 p. 171).\nThis method can be applied to any statistic. It will work well in cases where the bootstrap distribution is symmetrical and centered on the observed statistic and where the sample statistic is median-unbiased and has maximum concentration (or minimum risk with respect to an absolute value loss function).  When working with small sample sizes (i.e., less than 50), the basic / reversed percentile and percentile confidence intervals for (for example) the variance statistic will be too narrow. So that with a sample of 20 points, 90% confidence interval will include the true variance only 78% of the time.  The basic / reverse percentile confidence intervals are easier to justify mathematically but they are less accurate in general than percentile confidence intervals, and some authors discourage their use.Studentized bootstrap. The studentized bootstrap, also called bootstrap-t, is computed analogously to the standard confidence interval, but replaces the quantiles from the normal or student approximation by the quantiles from the bootstrap distribution of the Student's t-test (see Davison and Hinkley 1997, equ. 5.7 p. 194 and Efron and Tibshirani 1993 equ 12.22, p. 160):\n  \n    \n      \n        (\n        \n          \n            \n              \n                θ\n                \n              \n              ^\n            \n          \n        \n        −\n        \n          t\n          \n            (\n            1\n            −\n            α\n            \n              /\n            \n            2\n            )\n          \n          \n            ∗\n          \n        \n        ⋅\n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            θ\n          \n        \n        ,\n        \n          \n            \n              \n                θ\n                \n              \n              ^\n            \n          \n        \n        −\n        \n          t\n          \n            (\n            α\n            \n              /\n            \n            2\n            )\n          \n          \n            ∗\n          \n        \n        ⋅\n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            θ\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\widehat {\\theta \\,}}-t_{(1-\\alpha /2)}^{*}\\cdot {\\widehat {\\text{se}}}_{\\theta },{\\widehat {\\theta \\,}}-t_{(\\alpha /2)}^{*}\\cdot {\\widehat {\\text{se}}}_{\\theta })}\n   where \n  \n    \n      t\n      \n        (\n        1\n        −\n        α\n        \n          /\n        \n        2\n        )\n      \n      \n        ∗\n      \n    \n    t_{(1-\\alpha /2)}^{*}\n   denotes the \n  \n    \n      1\n      −\n      α\n      \n        /\n      \n      2\n    \n    1-\\alpha /2\n   percentile of the bootstrapped Student's t-test \n  \n    \n      \n        \n          t\n          \n            ∗\n          \n        \n        =\n        (\n        \n          \n            \n              \n                \n                  θ\n                  \n                \n                ^\n              \n            \n          \n          \n            ∗\n          \n        \n        −\n        \n          \n            \n              \n                θ\n                \n              \n              ^\n            \n          \n        \n        )\n        \n          /\n        \n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            \n              \n                \n                  \n                    \n                      θ\n                      \n                    \n                    ^\n                  \n                \n              \n              \n                ∗\n              \n            \n          \n        \n      \n    \n    {\\displaystyle t^{*}=({\\widehat {\\theta \\,}}^{*}-{\\widehat {\\theta \\,}})/{\\widehat {\\text{se}}}_{{\\widehat {\\theta \\,}}^{*}}}\n  , and \n  \n    \n      \n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\text{se}}}_{\\theta }}\n   is the estimated standard error of the coefficient in the original model.The studentized test enjoys optimal properties as the statistic that is bootstrapped is pivotal (i.e. it does not depend on nuisance parameters as the t-test follows asymptotically a N(0,1) distribution), unlike the percentile bootstrap.Bias-corrected bootstrap – adjusts for bias in the bootstrap distribution.\nAccelerated bootstrap – The bias-corrected and accelerated (BCa) bootstrap, by Efron (1987), adjusts for both bias and skewness in the bootstrap distribution. This approach is accurate in a wide variety of settings, has reasonable computation requirements, and produces reasonably narrow intervals.\n\nBootstrap hypothesis testing\nEfron and Tibshirani suggest the following algorithm for comparing the means of two independent samples:\nLet \n  \n    \n      \n        x\n        \n          1\n        \n      \n      ,\n      …\n      ,\n      \n        x\n        \n          n\n        \n      \n    \n    x_{1},\\ldots ,x_{n}\n   be a random sample from distribution F with sample mean \n  \n    \n      \n        \n          x\n          ¯\n        \n      \n    \n    {\\bar {x}}\n   and sample variance \n  \n    \n      σ\n      \n        x\n      \n      \n        2\n      \n    \n    \\sigma _{x}^{2}\n  . Let \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle y_{1},\\ldots ,y_{m}}\n   be another, independent random sample from distribution G with mean \n  \n    \n      \n        \n          y\n          ¯\n        \n      \n    \n    {\\bar {y}}\n   and variance \n  \n    \n      σ\n      \n        y\n      \n      \n        2\n      \n    \n    \\sigma _{y}^{2}\n  \n\nCalculate the test statistic \n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n              \n              −\n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n            \n            \n              \n                σ\n                \n                  x\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              n\n              +\n              \n                σ\n                \n                  y\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {x}}-{\\bar {y}}}{\\sqrt {\\sigma _{x}^{2}/n+\\sigma _{y}^{2}/m}}}}\n  \nCreate two new data sets whose values are \n  \n    \n      \n        \n          x\n          \n            i\n          \n          ′\n        \n        =\n        \n          x\n          \n            i\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n        \n        +\n        \n          \n            \n              z\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i}'=x_{i}-{\\bar {x}}+{\\bar {z}}}\n   and \n  \n    \n      \n        \n          y\n          \n            i\n          \n          ′\n        \n        =\n        \n          y\n          \n            i\n          \n        \n        −\n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        +\n        \n          \n            \n              z\n              ¯\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle y_{i}'=y_{i}-{\\bar {y}}+{\\bar {z}},}\n   where \n  \n    \n      \n        \n          z\n          ¯\n        \n      \n    \n    {\\bar {z}}\n   is the mean of the combined sample.\nDraw a random sample (\n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle x_{i}^{*}}\n  ) of size \n  \n    n\n    n\n   with replacement from \n  \n    \n      \n        \n          x\n          \n            i\n          \n          ′\n        \n      \n    \n    {\\displaystyle x_{i}'}\n   and another random sample (\n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}}\n  ) of size \n  \n    m\n    m\n   with replacement from \n  \n    \n      \n        \n          y\n          \n            i\n          \n          ′\n        \n      \n    \n    {\\displaystyle y_{i}'}\n  .\nCalculate the test statistic \n  \n    \n      \n        \n          t\n          \n            ∗\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    \n                      x\n                      \n                        ∗\n                      \n                    \n                    ¯\n                  \n                \n              \n              −\n              \n                \n                  \n                    \n                      y\n                      \n                        ∗\n                      \n                    \n                    ¯\n                  \n                \n              \n            \n            \n              \n                σ\n                \n                  x\n                \n                \n                  ∗\n                  2\n                \n              \n              \n                /\n              \n              n\n              +\n              \n                σ\n                \n                  y\n                \n                \n                  ∗\n                  2\n                \n              \n              \n                /\n              \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle t^{*}={\\frac {{\\bar {x^{*}}}-{\\bar {y^{*}}}}{\\sqrt {\\sigma _{x}^{*2}/n+\\sigma _{y}^{*2}/m}}}}\n  \nRepeat 3 and 4 \n  \n    B\n    B\n   times (e.g. \n  \n    \n      \n        B\n        =\n        1000\n      \n    \n    {\\displaystyle B=1000}\n  ) to collect \n  \n    B\n    B\n   values of the test statistic.\nEstimate the p-value as \n  \n    \n      \n        p\n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  B\n                \n              \n              I\n              {\n              \n                t\n                \n                  i\n                \n                \n                  ∗\n                \n              \n              ≥\n              t\n              }\n            \n            B\n          \n        \n      \n    \n    {\\displaystyle p={\\frac {\\sum _{i=1}^{B}I\\{t_{i}^{*}\\geq t\\}}{B}}}\n   where \n  \n    \n      \n        I\n        (\n        \n          condition\n        \n        )\n        =\n        1\n      \n    \n    {\\displaystyle I({\\text{condition}})=1}\n   when condition is true and 0 otherwise.\n\nExample applications\nSmoothed bootstrap\nIn 1878, Simon Newcomb took observations on the speed of light.\nThe data set contains two outliers, which greatly influence the sample mean. (The sample mean need not be a consistent estimator for any population mean, because no mean needs to exist for a heavy-tailed distribution.) A well-defined and  robust statistic for the central tendency is the sample median, which is consistent and median-unbiased for the population median.\nThe bootstrap distribution for Newcomb's data appears below. We can reduce the discreteness of the bootstrap distribution by adding a small amount of random noise to each bootstrap sample. A conventional choice is to add noise with a standard deviation of \n  \n    \n      \n        σ\n        \n          /\n        \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\sigma /{\\sqrt {n}}}\n   for a sample size n; this noise is often drawn from a Student-t distribution with n-1 degrees of freedom. This results in an approximately-unbiased estimator for the variance of the sample mean. This means that samples taken from the bootstrap distribution will have a variance which is, on average, equal to the variance of the total population.\nHistograms of the bootstrap distribution and the smooth bootstrap distribution appear below. The bootstrap distribution of the sample-median has only a small number of values. The smoothed bootstrap distribution has a richer support. However, note that whether the smoothed or standard bootstrap procedure is favorable is case-by-case and is shown to depend on both the underlying distribution function and on the quantity being estimated.\nIn this example, the bootstrapped 95% (percentile) confidence-interval for the population median is (26, 28.5), which is close to the interval for  (25.98, 28.46) for the smoothed bootstrap.\n\nRelation to other approaches to inference\nRelationship to other resampling methods\nThe bootstrap is distinguished from:\n\nthe jackknife procedure, used to estimate biases of sample statistics and to estimate variances, and\ncross-validation, in which the parameters (e.g., regression weights, factor loadings) that are estimated in one subsample are applied to another subsample.For more details see resampling.\nBootstrap aggregating (bagging) is a meta-algorithm based on averaging model predictions obtained from models trained on multiple bootstrap samples.\n\nU-statistics\nIn situations where an obvious statistic can be devised to measure a required characteristic using only a small number, r, of data items, a corresponding statistic based on the entire sample can be formulated. Given an r-sample statistic, one can create an n-sample statistic by something similar to bootstrapping (taking the average of the statistic over all subsamples of size r). This procedure is known to have certain good properties and the result is a U-statistic. The sample mean and sample variance are of this form, for r = 1 and r = 2.\n\nSee also\nAccuracy and precision\nBootstrap aggregating\nBootstrapping\nEmpirical likelihood\nImputation (statistics)\nReliability (statistics)\nReproducibility\nResampling\n\nReferences\nFurther reading\nDiaconis, P.; Efron, B. (May 1983). \"Computer-intensive methods in statistics\" (PDF). Scientific American. 248 (5): 116–130. Bibcode:1983SciAm.248e.116D. doi:10.1038/scientificamerican0583-116. Archived from the original (PDF) on 2016-03-13. Retrieved 2016-01-19. popular-science\nEfron, B. (1981). \"Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods\". Biometrika. 68 (3): 589–599. doi:10.1093/biomet/68.3.589.\nHesterberg, T. C.; D. S. Moore; S. Monaghan; A. Clipson & R. Epstein (2005). \"Bootstrap methods and permutation tests\" (PDF).  In David S. Moore & George McCabe (eds.). Introduction to the Practice of Statistics. software. Archived from the original (PDF) on 2006-02-15. Retrieved 2007-03-23.\nEfron, Bradley (1979). \"Bootstrap methods: Another look at the jackknife\". The Annals of Statistics. 7: 1–26. doi:10.1214/aos/1176344552.\nEfron, Bradley (1981). \"Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods\". Biometrika. 68 (3): 589–599. doi:10.2307/2335441. JSTOR 2335441.\nEfron, Bradley (1982). The jackknife, the bootstrap, and other resampling plans, In Society of Industrial and Applied Mathematics CBMS-NSF Monographs, 38.\nDiaconis, P.; Efron, Bradley (1983), \"Computer-intensive methods in statistics,\" Scientific American, May, 116–130.\nEfron, Bradley; Tibshirani, Robert J. (1993). An introduction to the bootstrap, New York: Chapman & Hall, software.\nDavison, A. C. and Hinkley, D. V. (1997): Bootstrap Methods and their Application, software.\nMooney, C Z & Duval, R D (1993). Bootstrapping. A Nonparametric Approach to Statistical Inference.  Sage University Paper series on Quantitative Applications in the Social Sciences, 07-095. Newbury Park, CA: Sage.\nSimon, J. L. (1997): Resampling: The New Statistics.\nWright, D.B., London, K., Field, A.P. Using Bootstrap Estimation and the Plug-in Principle for Clinical Psychology Data. 2011 Textrum Ltd. Online: https://www.researchgate.net/publication/236647074_Using_Bootstrap_Estimation_and_the_Plug-in_Principle_for_Clinical_Psychology_Data. Retrieved on 25/04/2016.\nAn Introduction to the Bootstrap. Monographs on Statistics and applied probability 57. Chapman&Hall/CHC. 1998. Online https://books.google.com/books?id=gLlpIUxRntoC&q=plug+in+principle.&pg=PA35 Retrieved on 25 04 2016.\nGail Gong (1986) Cross-Validation, the Jackknife, and the Bootstrap: Excess Error Estimation in Forward Logistic Regression, Journal of the American Statistical Association, 81:393, 108-113, DOI: 10.1080/01621459.1986.10478245\n\nExternal links\nBootstrap sampling tutorial using MS Excel\nBootstrap example to simulate stock prices using MS Excel\nbootstrapping tutorial \nWhat is the bootstrap?\n\nSoftware\nStatistics101: Resampling, Bootstrap, Monte Carlo Simulation program. Free program written in Java to run on any operating system.",
    "Brain": "A brain is an organ that serves as the center of the nervous system in all vertebrate and most invertebrate animals. It is located in the head, usually close to the sensory organs for senses such as vision. It is the most complex organ in a vertebrate's body. In a human, the cerebral cortex contains approximately 14–16 billion neurons, and the estimated number of neurons in the cerebellum is 55–70 billion. Each neuron is connected by synapses to several thousand other neurons. These neurons typically communicate with one another by means of long fibers called axons, which carry trains of signal pulses called action potentials to distant parts of the brain or body targeting specific recipient cells.\nPhysiologically, brains exert centralized control over a body's other organs. They act on the rest of the body both by generating patterns of muscle activity and by driving the secretion of chemicals called hormones. This centralized control allows rapid and coordinated responses to changes in the environment. Some basic types of responsiveness such as reflexes can be mediated by the spinal cord or peripheral ganglia, but sophisticated purposeful control of behavior based on complex sensory input requires the information integrating capabilities of a centralized brain.\nThe operations of individual brain cells are now understood in considerable detail but the way they cooperate in ensembles of millions is yet to be solved. Recent models in modern neuroscience treat the brain as a biological computer, very different in mechanism from a digital computer, but similar in the sense that it acquires information from the surrounding world, stores it, and processes it in a variety of ways.\nThis article compares the properties of brains across the entire range of animal species, with the greatest attention to vertebrates. It deals with the human brain insofar as it shares the properties of other brains. The ways in which the human brain differs from other brains are covered in the human brain article. Several topics that might be covered here are instead covered there because much more can be said about them in a human context. The most important that are covered in the human brain article are brain disease and the effects of brain damage.\n\nAnatomy\nThe shape and size of the brain varies greatly between species, and identifying common features is often difficult. Nevertheless, there are a number of principles of brain architecture that apply across a wide range of species. Some aspects of brain structure are common to almost the entire range of animal species; others distinguish \"advanced\" brains from more primitive ones, or distinguish vertebrates from invertebrates.The simplest way to gain information about brain anatomy is by visual inspection, but many more sophisticated techniques have been developed. Brain tissue in its natural state is too soft to work with, but it can be hardened by immersion in alcohol or other fixatives, and then sliced apart for examination of the interior. Visually, the interior of the brain consists of areas of so-called grey matter, with a dark color, separated by areas of white matter, with a lighter color. Further information can be gained by staining slices of brain tissue with a variety of chemicals that bring out areas where specific types of molecules are present in high concentrations. It is also possible to examine the microstructure of brain tissue using a microscope, and to trace the pattern of connections from one brain area to another.\n\nCellular structure\nThe brains of all species are composed primarily of two broad classes of cells: neurons and glial cells. Glial cells (also known as glia or neuroglia) come in several types, and perform a number of critical functions, including structural support, metabolic support, insulation, and guidance of development. Neurons, however, are usually considered the most important cells in the brain.\nThe property that makes neurons unique is their ability to send signals to specific target cells over long distances. They send these signals by means of an axon, which is a thin protoplasmic fiber that extends from the cell body and projects, usually with numerous branches, to other areas, sometimes nearby, sometimes in distant parts of the brain or body. The length of an axon can be extraordinary: for example, if a pyramidal cell (an excitatory neuron) of the cerebral cortex were magnified so that its cell body became the size of a human body, its axon, equally magnified, would become a cable a few centimeters in diameter, extending more than a kilometer. These axons transmit signals in the form of electrochemical pulses called action potentials, which last less than a thousandth of a second and travel along the axon at speeds of 1–100 meters per second. Some neurons emit action potentials constantly, at rates of 10–100 per second, usually in irregular patterns; other neurons are quiet most of the time, but occasionally emit a burst of action potentials.Axons transmit signals to other neurons by means of specialized junctions called synapses. A single axon may make as many as several thousand synaptic connections with other cells. When an action potential, traveling along an axon, arrives at a synapse, it causes a chemical called a neurotransmitter to be released. The neurotransmitter binds to receptor molecules in the membrane of the target cell.Synapses are the key functional elements of the brain. The essential function of the brain is cell-to-cell communication, and synapses are the points at which communication occurs. The human brain has been estimated to contain approximately 100 trillion synapses; even the brain of a fruit fly contains several million. The functions of these synapses are very diverse: some are excitatory (exciting the target cell); others are inhibitory; others work by activating second messenger systems that change the internal chemistry of their target cells in complex ways. A large number of synapses are dynamically modifiable; that is, they are capable of changing strength in a way that is controlled by the patterns of signals that pass through them. It is widely believed that activity-dependent modification of synapses is the brain's primary mechanism for learning and memory.Most of the space in the brain is taken up by axons, which are often bundled together in what are called nerve fiber tracts. A myelinated axon is wrapped in a fatty insulating sheath of myelin, which serves to greatly increase the speed of signal propagation. (There are also unmyelinated axons). Myelin is white, making parts of the brain filled exclusively with nerve fibers appear as light-colored white matter, in contrast to the darker-colored grey matter that marks areas with high densities of neuron cell bodies.\n\nEvolution\nGeneric bilaterian nervous system\nExcept for a few primitive organisms such as sponges (which have no nervous system) and cnidarians (which have a nervous system consisting of a diffuse nerve net), all living multicellular animals are bilaterians, meaning animals with a bilaterally symmetric body shape (that is, left and right sides that are approximate mirror images of each other). All bilaterians are thought to have descended from a common ancestor that appeared late in the Cryogenian period, 700–650 million years ago, and it has been hypothesized that this common ancestor had the shape of a simple tubeworm with a segmented body. At a schematic level, that basic worm-shape continues to be reflected in the body and nervous system architecture of all modern bilaterians, including vertebrates. The fundamental bilateral body form is a tube with a hollow gut cavity running from the mouth to the anus, and a nerve cord with an enlargement (a ganglion) for each body segment, with an especially large ganglion at the front, called the brain. The brain is small and simple in some species, such as nematode worms; in other species, including vertebrates, it is the most complex organ in the body. Some types of worms, such as leeches, also have an enlarged ganglion at the back end of the nerve cord, known as a \"tail brain\".There are a few types of existing bilaterians that lack a recognizable brain, including echinoderms and tunicates. It has not been definitively established whether the existence of these brainless species indicates that the earliest bilaterians lacked a brain, or whether their ancestors evolved in a way that led to the disappearance of a previously existing brain structure.\n\nInvertebrates\nThis category includes tardigrades, arthropods, molluscs, and numerous types of worms. The diversity of invertebrate body plans is matched by an equal diversity in brain structures.Two groups of invertebrates have notably complex brains: arthropods (insects, crustaceans, arachnids, and others), and cephalopods (octopuses, squids, and similar molluscs). The brains of arthropods and cephalopods arise from twin parallel nerve cords that extend through the body of the animal. Arthropods have a central brain, the supraesophageal ganglion, with three divisions and large optical lobes behind each eye for visual processing. Cephalopods such as the octopus and squid have the largest brains of any invertebrates.There are several invertebrate species whose brains have been studied intensively because they have properties that make them convenient for experimental work:\n\nFruit flies (Drosophila), because of the large array of techniques available for studying their genetics, have been a natural subject for studying the role of genes in brain development. In spite of the large evolutionary distance between insects and mammals, many aspects of Drosophila neurogenetics have been shown to be relevant to humans. The first biological clock genes, for example, were identified by examining Drosophila mutants that showed disrupted daily activity cycles. A search in the genomes of vertebrates revealed a set of analogous genes, which were found to play similar roles in the mouse biological clock—and therefore almost certainly in the human biological clock as well. Studies done on Drosophila, also show that most neuropil regions of the brain are continuously reorganized throughout life in response to specific living conditions.\nThe nematode worm Caenorhabditis elegans, like Drosophila, has been studied largely because of its importance in genetics. In the early 1970s, Sydney Brenner chose it as a model organism for studying the way that genes control development. One of the advantages of working with this worm is that the body plan is very stereotyped: the nervous system of the hermaphrodite contains exactly 302 neurons, always in the same places, making identical synaptic connections in every worm. Brenner's team sliced worms into thousands of ultrathin sections and photographed each one under an electron microscope, then visually matched fibers from section to section, to map out every neuron and synapse in the entire body. The complete neuronal wiring diagram of C.elegans – its connectome was achieved. Nothing approaching this level of detail is available for any other organism, and the information gained has enabled a multitude of studies that would otherwise have not been possible.\nThe sea slug Aplysia californica was chosen by Nobel Prize-winning neurophysiologist Eric Kandel as a model for studying the cellular basis of learning and memory, because of the simplicity and accessibility of its nervous system, and it has been examined in hundreds of experiments.\n\nVertebrates\nThe first vertebrates appeared over 500 million years ago (Mya), during the Cambrian period, and may have resembled the modern hagfish in form. Jawed fish appeared by 445 Mya, amphibians by 350 Mya, reptiles by 310 Mya and mammals by 200 Mya (approximately). Each species has an equally long evolutionary history, but the brains of modern hagfishes, lampreys, sharks, amphibians, reptiles, and mammals show a gradient of size and complexity that roughly follows the evolutionary sequence. All of these brains contain the same set of basic anatomical components, but many are rudimentary in the hagfish, whereas in mammals the foremost part (the telencephalon) is greatly elaborated and expanded.Brains are most commonly compared in terms of their size. The relationship between brain size, body size and other variables has been studied across a wide range of vertebrate species. As a rule, brain size increases with body size, but not in a simple linear proportion. In general, smaller animals tend to have larger brains, measured as a fraction of body size. For mammals, the relationship between brain volume and body mass essentially follows a power law with an exponent of about 0.75. This formula describes the central tendency, but every family of mammals departs from it to some degree, in a way that reflects in part the complexity of their behavior. For example, primates have brains 5 to 10 times larger than the formula predicts. Predators tend to have larger brains than their prey, relative to body size.\nAll vertebrate brains share a common underlying form, which appears most clearly during early stages of embryonic development. In its earliest form, the brain appears as three swellings at the front end of the neural tube; these swellings eventually become the forebrain, midbrain, and hindbrain (the prosencephalon, mesencephalon, and rhombencephalon, respectively). At the earliest stages of brain development, the three areas are roughly equal in size. In many classes of vertebrates, such as fish and amphibians, the three parts remain similar in size in the adult, but in mammals the forebrain becomes much larger than the other parts, and the midbrain becomes very small.The brains of vertebrates are made of very soft tissue. Living brain tissue is pinkish on the outside and mostly white on the inside, with subtle variations in color. Vertebrate brains are surrounded by a system of connective tissue membranes called meninges that separate the skull from the brain. Blood vessels enter the central nervous system through holes in the meningeal layers. The cells in the blood vessel walls are joined tightly to one another, forming the blood–brain barrier, which blocks the passage of many toxins and pathogens (though at the same time blocking antibodies and some drugs, thereby presenting special challenges in treatment of diseases of the brain).Neuroanatomists usually divide the vertebrate brain into six main regions: the telencephalon (cerebral hemispheres), diencephalon (thalamus and hypothalamus), mesencephalon (midbrain), cerebellum, pons, and medulla oblongata. Each of these areas has a complex internal structure. Some parts, such as the cerebral cortex and the cerebellar cortex, consist of layers that are folded or convoluted to fit within the available space. Other parts, such as the thalamus and hypothalamus, consist of clusters of many small nuclei. Thousands of distinguishable areas can be identified within the vertebrate brain based on fine distinctions of neural structure, chemistry, and connectivity.\nAlthough the same basic components are present in all vertebrate brains, some branches of vertebrate evolution have led to substantial distortions of brain geometry, especially in the forebrain area. The brain of a shark shows the basic components in a straightforward way, but in teleost fishes (the great majority of existing fish species), the forebrain has become \"everted\", like a sock turned inside out. In birds, there are also major changes in forebrain structure. These distortions can make it difficult to match brain components from one species with those of another species.Here is a list of some of the most important vertebrate brain components, along with a brief description of their functions as currently understood:\n\nThe medulla, along with the spinal cord, contains many small nuclei involved in a wide variety of sensory and involuntary motor functions such as vomiting, heart rate and digestive processes.\nThe pons lies in the brainstem directly above the medulla. Among other things, it contains nuclei that control often voluntary but simple acts such as sleep, respiration, swallowing, bladder function, equilibrium, eye movement, facial expressions, and posture.\nThe hypothalamus is a small region at the base of the forebrain, whose complexity and importance belies its size. It is composed of numerous small nuclei, each with distinct connections and neurochemistry. The hypothalamus is engaged in additional involuntary or partially voluntary acts such as sleep and wake cycles, eating and drinking, and the release of some hormones.\nThe thalamus is a collection of nuclei with diverse functions: some are involved in relaying information to and from the cerebral hemispheres, while others are involved in motivation. The subthalamic area (zona incerta) seems to contain action-generating systems for several types of \"consummatory\" behaviors such as eating, drinking, defecation, and copulation.\nThe cerebellum modulates the outputs of other brain systems, whether motor-related or thought related, to make them certain and precise. Removal of the cerebellum does not prevent an animal from doing anything in particular, but it makes actions hesitant and clumsy. This precision is not built-in but learned by trial and error. The muscle coordination learned while riding a bicycle is an example of a type of neural plasticity that may take place largely within the cerebellum. 10% of the brain's total volume consists of the cerebellum and 50% of all neurons are held within its structure.\nThe optic tectum allows actions to be directed toward points in space, most commonly in response to visual input. In mammals, it is usually referred to as the superior colliculus, and its best-studied function is to direct eye movements. It also directs reaching movements and other object-directed actions. It receives strong visual inputs, but also inputs from other senses that are useful in directing actions, such as auditory input in owls and input from the thermosensitive pit organs in snakes. In some primitive fishes, such as lampreys, this region is the largest part of the brain. The superior colliculus is part of the midbrain.\nThe pallium is a layer of grey matter that lies on the surface of the forebrain and is the most complex and most recent evolutionary development of the brain as an organ. In reptiles and mammals, it is called the cerebral cortex. Multiple functions involve the pallium, including smell and spatial memory. In mammals, where it becomes so large as to dominate the brain, it takes over functions from many other brain areas. In many mammals, the cerebral cortex consists of folded bulges called gyri that create deep furrows or fissures called sulci. The folds increase the surface area of the cortex and therefore increase the amount of gray matter and the amount of information that can be stored and processed.\nThe hippocampus, strictly speaking, is found only in mammals. However, the area it derives from, the medial pallium, has counterparts in all vertebrates. There is evidence that this part of the brain is involved in complex events such as spatial memory and navigation in fishes, birds, reptiles, and mammals.\nThe basal ganglia are a group of interconnected structures in the forebrain. The primary function of the basal ganglia appears to be action selection: they send inhibitory signals to all parts of the brain that can generate motor behaviors, and in the right circumstances can release the inhibition, so that the action-generating systems are able to execute their actions. Reward and punishment exert their most important neural effects by altering connections within the basal ganglia.\nThe olfactory bulb is a special structure that processes olfactory sensory signals and sends its output to the olfactory part of the pallium. It is a major brain component in many vertebrates, but is greatly reduced in humans and other primates (whose senses are dominated by information acquired by sight rather than smell).\n\nReptiles\nBirds\nMammals\nThe most obvious difference between the brains of mammals and other vertebrates is in terms of size. On average, a mammal has a brain roughly twice as large as that of a bird of the same body size, and ten times as large as that of a reptile of the same body size.Size, however, is not the only difference: there are also substantial differences in shape. The hindbrain and midbrain of mammals are generally similar to those of other vertebrates, but dramatic differences appear in the forebrain, which is greatly enlarged and also altered in structure. The cerebral cortex is the part of the brain that most strongly distinguishes mammals. In non-mammalian vertebrates, the surface of the cerebrum is lined with a comparatively simple three-layered structure called the pallium. In mammals, the pallium evolves into a complex six-layered structure called neocortex or isocortex. Several areas at the edge of the neocortex, including the hippocampus and amygdala, are also much more extensively developed in mammals than in other vertebrates.The elaboration of the cerebral cortex carries with it changes to other brain areas. The superior colliculus, which plays a major role in visual control of behavior in most vertebrates, shrinks to a small size in mammals, and many of its functions are taken over by visual areas of the cerebral cortex. The cerebellum of mammals contains a large portion (the neocerebellum) dedicated to supporting the cerebral cortex, which has no counterpart in other vertebrates.\n\nPrimates\nThe brains of humans and other primates contain the same structures as the brains of other mammals, but are generally larger in proportion to body size. The encephalization quotient (EQ) is used to compare brain sizes across species. It takes into account the nonlinearity of the brain-to-body relationship. Humans have an average EQ in the 7-to-8 range, while most other primates have an EQ in the 2-to-3 range. Dolphins have values higher than those of primates other than humans, but nearly all other mammals have EQ values that are substantially lower.\nMost of the enlargement of the primate brain comes from a massive expansion of the cerebral cortex, especially the prefrontal cortex and the parts of the cortex involved in vision. The visual processing network of primates includes at least 30 distinguishable brain areas, with a complex web of interconnections. It has been estimated that visual processing areas occupy more than half of the total surface of the primate neocortex. The prefrontal cortex carries out functions that include planning, working memory, motivation, attention, and executive control. It takes up a much larger proportion of the brain for primates than for other species, and an especially large fraction of the human brain.\n\nDevelopment\nThe brain develops in an intricately orchestrated sequence of stages. It changes in shape from a simple swelling at the front of the nerve cord in the earliest embryonic stages, to a complex array of areas and connections. Neurons are created in special zones that contain stem cells, and then migrate through the tissue to reach their ultimate locations. Once neurons have positioned themselves, their axons sprout and navigate through the brain, branching and extending as they go, until the tips reach their targets and form synaptic connections. In a number of parts of the nervous system, neurons and synapses are produced in excessive numbers during the early stages, and then the unneeded ones are pruned away.For vertebrates, the early stages of neural development are similar across all species. As the embryo transforms from a round blob of cells into a wormlike structure, a narrow strip of ectoderm running along the midline of the back is induced to become the neural plate, the precursor of the nervous system. The neural plate folds inward to form the neural groove, and then the lips that line the groove merge to enclose the neural tube, a hollow cord of cells with a fluid-filled ventricle at the center. At the front end, the ventricles and cord swell to form three vesicles that are the precursors of the prosencephalon (forebrain), mesencephalon (midbrain), and rhombencephalon (hindbrain). At the next stage, the forebrain splits into two vesicles called the telencephalon (which will contain the cerebral cortex, basal ganglia, and related structures) and the diencephalon (which will contain the thalamus and hypothalamus). At about the same time, the hindbrain splits into the metencephalon (which will contain the cerebellum and pons) and the myelencephalon (which will contain the medulla oblongata). Each of these areas contains proliferative zones where neurons and glial cells are generated; the resulting cells then migrate, sometimes for long distances, to their final positions.Once a neuron is in place, it extends dendrites and an axon into the area around it. Axons, because they commonly extend a great distance from the cell body and need to reach specific targets, grow in a particularly complex way. The tip of a growing axon consists of a blob of protoplasm called a growth cone, studded with chemical receptors. These receptors sense the local environment, causing the growth cone to be attracted or repelled by various cellular elements, and thus to be pulled in a particular direction at each point along its path. The result of this pathfinding process is that the growth cone navigates through the brain until it reaches its destination area, where other chemical cues cause it to begin generating synapses. Considering the entire brain, thousands of genes create products that influence axonal pathfinding.The synaptic network that finally emerges is only partly determined by genes, though. In many parts of the brain, axons initially \"overgrow\", and then are \"pruned\" by mechanisms that depend on neural activity. In the projection from the eye to the midbrain, for example, the structure in the adult contains a very precise mapping, connecting each point on the surface of the retina to a corresponding point in a midbrain layer. In the first stages of development, each axon from the retina is guided to the right general vicinity in the midbrain by chemical cues, but then branches very profusely and makes initial contact with a wide swath of midbrain neurons. The retina, before birth, contains special mechanisms that cause it to generate waves of activity that originate spontaneously at a random point and then propagate slowly across the retinal layer. These waves are useful because they cause neighboring neurons to be active at the same time; that is, they produce a neural activity pattern that contains information about the spatial arrangement of the neurons. This information is exploited in the midbrain by a mechanism that causes synapses to weaken, and eventually vanish, if activity in an axon is not followed by activity of the target cell. The result of this sophisticated process is a gradual tuning and tightening of the map, leaving it finally in its precise adult form.Similar things happen in other brain areas: an initial synaptic matrix is generated as a result of genetically determined chemical guidance, but then gradually refined by activity-dependent mechanisms, partly driven by internal dynamics, partly by external sensory inputs. In some cases, as with the retina-midbrain system, activity patterns depend on mechanisms that operate only in the developing brain, and apparently exist solely to guide development.In humans and many other mammals, new neurons are created mainly before birth, and the infant brain contains substantially more neurons than the adult brain. There are, however, a few areas where new neurons continue to be generated throughout life. The two areas for which adult neurogenesis is well established are the olfactory bulb, which is involved in the sense of smell, and the dentate gyrus of the hippocampus, where there is evidence that the new neurons play a role in storing newly acquired memories. With these exceptions, however, the set of neurons that is present in early childhood is the set that is present for life. Glial cells are different: as with most types of cells in the body, they are generated throughout the lifespan.There has long been debate about whether the qualities of mind, personality, and intelligence can be attributed to heredity or to upbringing—this is the nature and nurture controversy. Although many details remain to be settled, neuroscience research has clearly shown that both factors are important. Genes determine the general form of the brain, and genes determine how the brain reacts to experience. Experience, however, is required to refine the matrix of synaptic connections, which in its developed form contains far more information than the genome does. In some respects, all that matters is the presence or absence of experience during critical periods of development. In other respects, the quantity and quality of experience are important; for example, there is substantial evidence that animals raised in enriched environments have thicker cerebral cortices, indicating a higher density of synaptic connections, than animals whose levels of stimulation are restricted.\n\nPhysiology\nThe functions of the brain depend on the ability of neurons to transmit electrochemical signals to other cells, and their ability to respond appropriately to electrochemical signals received from other cells. The electrical properties of neurons are controlled by a wide variety of biochemical and metabolic processes, most notably the interactions between neurotransmitters and receptors that take place at synapses.\n\nNeurotransmitters and receptors\nNeurotransmitters are chemicals that are released at synapses when the local membrane is depolarised and Ca2+ enters into the cell, typically when an action potential arrives at the synapse – neurotransmitters attach themselves to receptor molecules on the membrane of the synapse's target cell (or cells), and thereby alter the electrical or chemical properties of the receptor molecules.\nWith few exceptions, each neuron in the brain releases the same chemical neurotransmitter, or combination of neurotransmitters, at all the synaptic connections it makes with other neurons; this rule is known as Dale's principle. Thus, a neuron can be characterized by the neurotransmitters that it releases. The great majority of psychoactive drugs exert their effects by altering specific neurotransmitter systems. This applies to drugs such as cannabinoids, nicotine, heroin, cocaine, alcohol, fluoxetine, chlorpromazine, and many others.The two neurotransmitters that are most widely found in the vertebrate brain are glutamate, which almost always exerts excitatory effects on target neurons, and gamma-aminobutyric acid (GABA), which is almost always inhibitory. Neurons using these transmitters can be found in nearly every part of the brain. Because of their ubiquity, drugs that act on glutamate or GABA tend to have broad and powerful effects. Some general anesthetics act by reducing the effects of glutamate; most tranquilizers exert their sedative effects by enhancing the effects of GABA.There are dozens of other chemical neurotransmitters that are used in more limited areas of the brain, often areas dedicated to a particular function. Serotonin, for example—the primary target of many antidepressant drugs and many dietary aids—comes exclusively from a small brainstem area called the raphe nuclei. Norepinephrine, which is involved in arousal, comes exclusively from a nearby small area called the locus coeruleus. Other neurotransmitters such as acetylcholine and dopamine have multiple sources in the brain but are not as ubiquitously distributed as glutamate and GABA.\n\nElectrical activity\nAs a side effect of the electrochemical processes used by neurons for signaling, brain tissue generates electric fields when it is active. When large numbers of neurons show synchronized activity, the electric fields that they generate can be large enough to detect outside the skull, using electroencephalography (EEG) or magnetoencephalography (MEG). EEG recordings, along with recordings made from electrodes implanted inside the brains of animals such as rats, show that the brain of a living animal is constantly active, even during sleep. Each part of the brain shows a mixture of rhythmic and nonrhythmic activity, which may vary according to behavioral state. In mammals, the cerebral cortex tends to show large slow delta waves during sleep, faster alpha waves when the animal is awake but inattentive, and chaotic-looking irregular activity when the animal is actively engaged in a task, called beta and gamma waves. During an epileptic seizure, the brain's inhibitory control mechanisms fail to function and electrical activity rises to pathological levels, producing EEG traces that show large wave and spike patterns not seen in a healthy brain. Relating these population-level patterns to the computational functions of individual neurons is a major focus of current research in neurophysiology.\n\nMetabolism\nAll vertebrates have a blood–brain barrier that allows metabolism inside the brain to operate differently from metabolism in other parts of the body. The neurovascular unit regulates cerebral blood flow so that activated neurons can be supplied with energy. Glial cells play a major role in brain metabolism by controlling the chemical composition of the fluid that surrounds neurons, including levels of ions and nutrients.Brain tissue consumes a large amount of energy in proportion to its volume, so large brains place severe metabolic demands on animals. The need to limit body weight in order, for example, to fly, has apparently led to selection for a reduction of brain size in some species, such as bats. Most of the brain's energy consumption goes into sustaining the electric charge (membrane potential) of neurons. Most vertebrate species devote between 2% and 8% of basal metabolism to the brain. In primates, however, the percentage is much higher—in humans it rises to 20–25%. The energy consumption of the brain does not vary greatly over time, but active regions of the cerebral cortex consume somewhat more energy than inactive regions; this forms the basis for the functional brain imaging methods of PET, fMRI, and NIRS. The brain typically gets most of its energy from oxygen-dependent metabolism of glucose (i.e., blood sugar), but ketones provide a major alternative source, together with contributions from medium chain fatty acids (caprylic and heptanoic acids), lactate, acetate, and possibly amino acids.\n\nFunction\nInformation from the sense organs is collected in the brain. There it is used to determine what actions the organism is to take. The brain processes the raw data to extract information about the structure of the environment. Next it combines the processed information with information about the current needs of the animal and with memory of past circumstances. Finally, on the basis of the results, it generates motor response patterns. These signal-processing tasks require intricate interplay between a variety of functional subsystems.The function of the brain is to provide coherent control over the actions of an animal. A centralized brain allows groups of muscles to be co-activated in complex patterns; it also allows stimuli impinging on one part of the body to evoke responses in other parts, and it can prevent different parts of the body from acting at cross-purposes to each other.\n\nPerception\nThe human brain is provided with information about light, sound, the chemical composition of the atmosphere, temperature, the position of the body in space (proprioception), the chemical composition of the bloodstream, and more. In other animals additional senses are present, such as the infrared heat-sense of snakes, the magnetic field sense of some birds, or the electric field sense mainly seen in aquatic animals.\nEach sensory system begins with specialized receptor cells, such as photoreceptor cells in the retina of the eye, or vibration-sensitive hair cells in the cochlea of the ear. The axons of sensory receptor cells travel into the spinal cord or brain, where they transmit their signals to a first-order sensory nucleus dedicated to one specific sensory modality. This primary sensory nucleus sends information to higher-order sensory areas that are dedicated to the same modality. Eventually, via a way-station in the thalamus, the signals are sent to the cerebral cortex, where they are processed to extract the relevant features, and integrated with signals coming from other sensory systems.\n\nMotor control\nMotor systems are areas of the brain that are involved in initiating body movements, that is, in activating muscles. Except for the muscles that control the eye, which are driven by nuclei in the midbrain, all the voluntary muscles in the body are directly innervated by motor neurons in the spinal cord and hindbrain. Spinal motor neurons are controlled both by neural circuits intrinsic to the spinal cord, and by inputs that descend from the brain. The intrinsic spinal circuits implement many reflex responses, and contain pattern generators for rhythmic movements such as walking or swimming. The descending connections from the brain allow for more sophisticated control.The brain contains several motor areas that project directly to the spinal cord. At the lowest level are motor areas in the medulla and pons, which control stereotyped movements such as walking, breathing, or swallowing. At a higher level are areas in the midbrain, such as the red nucleus, which is responsible for coordinating movements of the arms and legs. At a higher level yet is the primary motor cortex, a strip of tissue located at the posterior edge of the frontal lobe. The primary motor cortex sends projections to the subcortical motor areas, but also sends a massive projection directly to the spinal cord, through the pyramidal tract. This direct corticospinal projection allows for precise voluntary control of the fine details of movements. Other motor-related brain areas exert secondary effects by projecting to the primary motor areas. Among the most important secondary areas are the premotor cortex, supplementary motor area, basal ganglia, and cerebellum. In addition to all of the above, the brain and spinal cord contain extensive circuitry to control the autonomic nervous system which controls the movement of the smooth muscle of the body.\n\nSleep\nMany animals alternate between sleeping and waking in a daily cycle. Arousal and alertness are also modulated on a finer time scale by a network of brain areas. A key component of the sleep system is the suprachiasmatic nucleus (SCN), a tiny part of the hypothalamus located directly above the point at which the optic nerves from the two eyes cross. The SCN contains the body's central biological clock. Neurons there show activity levels that rise and fall with a period of about 24 hours, circadian rhythms: these activity fluctuations are driven by rhythmic changes in expression of a set of \"clock genes\". The SCN continues to keep time even if it is excised from the brain and placed in a dish of warm nutrient solution, but it ordinarily receives input from the optic nerves, through the retinohypothalamic tract (RHT), that allows daily light-dark cycles to calibrate the clock.The SCN projects to a set of areas in the hypothalamus, brainstem, and midbrain that are involved in implementing sleep-wake cycles. An important component of the system is the reticular formation, a group of neuron-clusters scattered diffusely through the core of the lower brain. Reticular neurons send signals to the thalamus, which in turn sends activity-level-controlling signals to every part of the cortex. Damage to the reticular formation can produce a permanent state of coma.Sleep involves great changes in brain activity. Until the 1950s it was generally believed that the brain essentially shuts off during sleep, but this is now known to be far from true; activity continues, but patterns become very different. There are two types of sleep: REM sleep (with dreaming) and NREM (non-REM, usually without dreaming) sleep, which repeat in slightly varying patterns throughout a sleep episode. Three broad types of distinct brain activity patterns can be measured: REM, light NREM and deep NREM. During deep NREM sleep, also called slow wave sleep, activity in the cortex takes the form of large synchronized waves, whereas in the waking state it is noisy and desynchronized. Levels of the neurotransmitters norepinephrine and serotonin drop during slow wave sleep, and fall almost to zero during REM sleep; levels of acetylcholine show the reverse pattern.\n\nHomeostasis\nFor any animal, survival requires maintaining a variety of parameters of bodily state within a limited range of variation: these include temperature, water content, salt concentration in the bloodstream, blood glucose levels, blood oxygen level, and others. The ability of an animal to regulate the internal environment of its body—the milieu intérieur, as the pioneering physiologist Claude Bernard called it—is known as homeostasis (Greek for \"standing still\"). Maintaining homeostasis is a crucial function of the brain. The basic principle that underlies homeostasis is negative feedback: any time a parameter diverges from its set-point, sensors generate an error signal that evokes a response that causes the parameter to shift back toward its optimum value. (This principle is widely used in engineering, for example in the control of temperature using a thermostat.)\nIn vertebrates, the part of the brain that plays the greatest role is the hypothalamus, a small region at the base of the forebrain whose size does not reflect its complexity or the importance of its function. The hypothalamus is a collection of small nuclei, most of which are involved in basic biological functions. Some of these functions relate to arousal or to social interactions such as sexuality, aggression, or maternal behaviors; but many of them relate to homeostasis. Several hypothalamic nuclei receive input from sensors located in the lining of blood vessels, conveying information about temperature, sodium level, glucose level, blood oxygen level, and other parameters. These hypothalamic nuclei send output signals to motor areas that can generate actions to rectify deficiencies. Some of the outputs also go to the pituitary gland, a tiny gland attached to the brain directly underneath the hypothalamus. The pituitary gland secretes hormones into the bloodstream, where they circulate throughout the body and induce changes in cellular activity.\n\nMotivation\nThe individual animals need to express survival-promoting behaviors, such as seeking food, water, shelter, and a mate. The motivational system in the brain monitors the current state of satisfaction of these goals, and activates behaviors to meet any needs that arise. The motivational system works largely by a reward–punishment mechanism. When a particular behavior is followed by favorable consequences, the reward mechanism in the brain is activated, which induces structural changes inside the brain that cause the same behavior to be repeated later, whenever a similar situation arises. Conversely, when a behavior is followed by unfavorable consequences, the brain's punishment mechanism is activated, inducing structural changes that cause the behavior to be suppressed when similar situations arise in the future.Most organisms studied to date use a reward–punishment mechanism: for instance, worms and insects can alter their behavior to seek food sources or to avoid dangers. In vertebrates, the reward-punishment system is implemented by a specific set of brain structures, at the heart of which lie the basal ganglia, a set of interconnected areas at the base of the forebrain. The basal ganglia are the central site at which decisions are made: the basal ganglia exert a sustained inhibitory control over most of the motor systems in the brain; when this inhibition is released, a motor system is permitted to execute the action it is programmed to carry out. Rewards and punishments function by altering the relationship between the inputs that the basal ganglia receive and the decision-signals that are emitted. The reward mechanism is better understood than the punishment mechanism, because its role in drug abuse has caused it to be studied very intensively. Research has shown that the neurotransmitter dopamine plays a central role: addictive drugs such as cocaine, amphetamine, and nicotine either cause dopamine levels to rise or cause the effects of dopamine inside the brain to be enhanced.\n\nLearning and memory\nAlmost all animals are capable of modifying their behavior as a result of experience—even the most primitive types of worms. Because behavior is driven by brain activity, changes in behavior must somehow correspond to changes inside the brain. Already in the late 19th century theorists like Santiago Ramón y Cajal argued that the most plausible explanation is that learning and memory are expressed as changes in the synaptic connections between neurons. Until 1970, however, experimental evidence to support the synaptic plasticity hypothesis was lacking. In 1971 Tim Bliss and Terje Lømo published a paper on a phenomenon now called long-term potentiation: the paper showed clear evidence of activity-induced synaptic changes that lasted for at least several days. Since then technical advances have made these sorts of experiments much easier to carry out, and thousands of studies have been made that have clarified the mechanism of synaptic change, and uncovered other types of activity-driven synaptic change in a variety of brain areas, including the cerebral cortex, hippocampus, basal ganglia, and cerebellum. Brain-derived neurotrophic factor (BDNF) and physical activity appear to play a beneficial role in the process.Neuroscientists currently distinguish several types of learning and memory that are implemented by the brain in distinct ways:\n\nWorking memory is the ability of the brain to maintain a temporary representation of information about the task that an animal is currently engaged in. This sort of dynamic memory is thought to be mediated by the formation of cell assemblies—groups of activated neurons that maintain their activity by constantly stimulating one another.\nEpisodic memory is the ability to remember the details of specific events. This sort of memory can last for a lifetime. Much evidence implicates the hippocampus in playing a crucial role: people with severe damage to the hippocampus sometimes show amnesia, that is, inability to form new long-lasting episodic memories.\nSemantic memory is the ability to learn facts and relationships. This sort of memory is probably stored largely in the cerebral cortex, mediated by changes in connections between cells that represent specific types of information.\nInstrumental learning is the ability for rewards and punishments to modify behavior. It is implemented by a network of brain areas centered on the basal ganglia.\nMotor learning is the ability to refine patterns of body movement by practicing, or more generally by repetition. A number of brain areas are involved, including the premotor cortex, basal ganglia, and especially the cerebellum, which functions as a large memory bank for microadjustments of the parameters of movement.\n\nResearch\nThe field of neuroscience encompasses all approaches that seek to understand the brain and the rest of the nervous system. Psychology seeks to understand mind and behavior, and neurology is the medical discipline that diagnoses and treats diseases of the nervous system. The brain is also the most important organ studied in psychiatry, the branch of medicine that works to study, prevent, and treat mental disorders. Cognitive science seeks to unify neuroscience and psychology with other fields that concern themselves with the brain, such as computer science (artificial intelligence and similar fields) and philosophy.The oldest method of studying the brain is anatomical, and until the middle of the 20th century, much of the progress in neuroscience came from the development of better cell stains and better microscopes. Neuroanatomists study the large-scale structure of the brain as well as the microscopic structure of neurons and their components, especially synapses. Among other tools, they employ a plethora of stains that reveal neural structure, chemistry, and connectivity. In recent years, the development of immunostaining techniques has allowed investigation of neurons that express specific sets of genes. Also, functional neuroanatomy uses medical imaging techniques to correlate variations in human brain structure with differences in cognition or behavior.Neurophysiologists study the chemical, pharmacological, and electrical properties of the brain: their primary tools are drugs and recording devices. Thousands of experimentally developed drugs affect the nervous system, some in highly specific ways. Recordings of brain activity can be made using electrodes, either glued to the scalp as in EEG studies, or implanted inside the brains of animals for extracellular recordings, which can detect action potentials generated by individual neurons. Because the brain does not contain pain receptors, it is possible using these techniques to record brain activity from animals that are awake and behaving without causing distress. The same techniques have occasionally been used to study brain activity in human patients with intractable epilepsy, in cases where there was a medical necessity to implant electrodes to localize the brain area responsible for epileptic seizures. Functional imaging techniques such as fMRI are also used to study brain activity; these techniques have mainly been used with human subjects, because they require a conscious subject to remain motionless for long periods of time, but they have the great advantage of being noninvasive.\nAnother approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as biologically realistic neural networks. On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.Computational neurogenetic modeling is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes.\nRecent years have seen increasing applications of genetic and genomic techniques to the study of the brain  and a focus on the roles of neurotrophic factors and physical activity in neuroplasticity. The most common subjects are mice, because of the availability of technical tools. It is now possible with relative ease to \"knock out\" or mutate a wide variety of genes, and then examine the effects on brain function. More sophisticated approaches are also being used: for example, using Cre-Lox recombination it is possible to activate or deactivate genes in specific parts of the brain, at specific times.\n\nHistory\nThe oldest brain to have been discovered was in Armenia in the Areni-1 cave complex. The brain, estimated to be over 5,000 years old, was found in the skull of a 12 to 14-year-old girl. Although the brains were shriveled, they were well preserved due to the climate found inside the cave.Early philosophers were divided as to whether the seat of the soul lies in the brain or heart. Aristotle favored the heart, and thought that the function of the brain was merely to cool the blood. Democritus, the inventor of the atomic theory of matter, argued for a three-part soul, with intellect in the head, emotion in the heart, and lust near the liver. The unknown author of On the Sacred Disease, a medical treatise in the Hippocratic Corpus, came down unequivocally in favor of the brain, writing:\n\nMen ought to know that from nothing else but the brain come joys, delights, laughter and sports, and sorrows, griefs, despondency, and lamentations. ... And by the same organ we become mad and delirious, and fears and terrors assail us, some by night, and some by day, and dreams and untimely wanderings, and cares that are not suitable, and ignorance of present circumstances, desuetude, and unskillfulness. All these things we endure from the brain, when it is not healthy...\nOn the Sacred Disease, attributed to Hippocrates\nThe Roman physician Galen also argued for the importance of the brain, and theorized in some depth about how it might work. Galen traced out the anatomical relationships among brain, nerves, and muscles, demonstrating that all muscles in the body are connected to the brain through a branching network of nerves. He postulated that nerves activate muscles mechanically by carrying a mysterious substance he called pneumata psychikon, usually translated as \"animal spirits\". Galen's ideas were widely known during the Middle Ages, but not much further progress came until the Renaissance, when detailed anatomical study resumed, combined with the theoretical speculations of René Descartes and those who followed him. Descartes, like Galen, thought of the nervous system in hydraulic terms. He believed that the highest cognitive functions are carried out by a non-physical res cogitans, but that the majority of behaviors of humans, and all behaviors of animals, could be explained mechanistically.The first real progress toward a modern understanding of nervous function, though, came from the investigations of Luigi Galvani (1737–1798), who discovered that a shock of static electricity applied to an exposed nerve of a dead frog could cause its leg to contract. Since that time, each major advance in understanding has followed more or less directly from the development of a new technique of investigation. Until the early years of the 20th century, the most important advances were derived from new methods for staining cells. Particularly critical was the invention of the Golgi stain, which (when correctly used) stains only a small fraction of neurons, but stains them in their entirety, including cell body, dendrites, and axon. Without such a stain, brain tissue under a microscope appears as an impenetrable tangle of protoplasmic fibers, in which it is impossible to determine any structure. In the hands of Camillo Golgi, and especially of the Spanish neuroanatomist Santiago Ramón y Cajal, the new stain revealed hundreds of distinct types of neurons, each with its own unique dendritic structure and pattern of connectivity.\nIn the first half of the 20th century, advances in electronics enabled investigation of the electrical properties of nerve cells, culminating in work by Alan Hodgkin, Andrew Huxley, and others on the biophysics of the action potential, and the work of Bernard Katz and others on the electrochemistry of the synapse. These studies complemented the anatomical picture with a conception of the brain as a dynamic entity. Reflecting the new understanding, in 1942 Charles Sherrington visualized the workings of the brain waking from sleep:\n\nThe great topmost sheet of the mass, that where hardly a light had twinkled or moved, becomes now a sparkling field of rhythmic flashing points with trains of traveling sparks hurrying hither and thither. The brain is waking and with it the mind is returning. It is as if the Milky Way entered upon some cosmic dance. Swiftly the head mass becomes an enchanted loom where millions of flashing shuttles weave a dissolving pattern, always a meaningful pattern though never an abiding one; a shifting harmony of subpatterns.\n—Sherrington, 1942, Man on his Nature\nThe invention of electronic computers in the 1940s, along with the development of mathematical information theory, led to a realization that brains can potentially be understood as information processing systems. This concept formed the basis of the field of cybernetics, and eventually gave rise to the field now known as computational neuroscience. The earliest attempts at cybernetics were somewhat crude in that they treated the brain as essentially a digital computer in disguise, as for example in John von Neumann's 1958 book, The Computer and the Brain. Over the years, though, accumulating information about the electrical responses of brain cells recorded from behaving animals has steadily moved theoretical concepts in the direction of increasing realism.One of the most influential early contributions was a 1959 paper titled What the frog's eye tells the frog's brain: the paper examined the visual responses of neurons in the retina and optic tectum of frogs, and came to the conclusion that some neurons in the tectum of the frog are wired to combine elementary responses in a way that makes them function as \"bug perceivers\". A few years later David Hubel and Torsten Wiesel discovered cells in the primary visual cortex of monkeys that become active when sharp edges move across specific points in the field of view—a discovery for which they won a Nobel Prize. Follow-up studies in higher-order visual areas found cells that detect binocular disparity, color, movement, and aspects of shape, with areas located at increasing distances from the primary visual cortex showing increasingly complex responses. Other investigations of brain areas unrelated to vision have revealed cells with a wide variety of response correlates, some related to memory, some to abstract types of cognition such as space.Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively—current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.Furthermore, even single neurons appear to be complex and capable of performing computations. So, brain models that do not reflect this are too abstract to be representative of brain operation; models that do try to capture this are very computationally expensive and arguably intractable with present computational resources. However, the Human Brain Project is trying to build a realistic, detailed computational model of the entire human brain. The wisdom of this approach has been publicly contested, with high-profile scientists on both sides of the argument.\nIn the second half of the 20th century, developments in chemistry, electron microscopy, genetics, computer science, functional brain imaging, and other fields progressively opened new windows into brain structure and function. In the United States, the 1990s were officially designated as the \"Decade of the Brain\" to commemorate advances made in brain research, and to promote funding for such research.In the 21st century, these trends have continued, and several new approaches have come into prominence, including multielectrode recording, which allows the activity of many brain cells to be recorded all at the same time; genetic engineering, which allows molecular components of the brain to be altered experimentally; genomics, which allows variations in brain structure to be correlated with variations in DNA properties and neuroimaging.\n\nSociety and culture\nAs food\nAnimal brains are used as food in numerous cuisines.\n\nIn rituals\nSome archaeological evidence suggests that the mourning rituals of European Neanderthals also involved the consumption of the brain.The Fore people of Papua New Guinea are known to eat human brains. In funerary rituals, those close to the dead would eat the brain of the deceased to create a sense of immortality. A prion disease called kuru has been traced to this.\n\nSee also\nReferences\nExternal links\n\nThe Brain from Top to Bottom, at McGill University\nThe Brain, BBC Radio 4 discussion with Vivian Nutton, Jonathan Sawday & Marina Wallace (In Our Time, May 8, 2008)\nOur Quest to Understand the Brain – with Matthew Cobb Royal Institution lecture. Archived at Ghostarchive.",
    "Brain–computer interface": "A brain–computer interface (BCI), sometimes called a brain–machine interface (BMI) or smartbrain, is a direct communication pathway between the brain's electrical activity and an external device, most commonly a computer or robotic limb. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions. They are often conceptualized as a human–machine interface that skips the intermediary component of the physical movement of body parts, although they also raise the possibility of the erasure of the discreteness of brain and machine. Implementations of BCIs range from non-invasive (EEG, MEG, EOG, MRI) and partially invasive (ECoG and endovascular) to invasive (microelectrode array), based on how close electrodes get to brain tissue.Research on BCIs began in the 1970s by Jacques Vidal at the University of California, Los Angeles (UCLA) under a grant from the National Science Foundation, followed by a contract from DARPA. Vidal's 1973 paper marks the first appearance of the expression brain–computer interface in scientific literature.\nDue to the cortical plasticity of the brain, signals from implanted prostheses can, after adaptation, be handled by the brain like natural sensor or effector channels. Following years of animal experimentation, the first neuroprosthetic devices implanted in humans appeared in the mid-1990s.\nRecently, studies in human-computer interaction via the application of machine learning to statistical temporal features extracted from the frontal lobe (EEG brainwave) data has had high levels of success in classifying mental states (Relaxed, Neutral, Concentrating), mental emotional states (Negative, Neutral, Positive), and thalamocortical dysrhythmia.\n\nHistory\nThe history of brain–computer interfaces (BCIs) starts with Hans Berger's discovery of the electrical activity of the human brain and the development of electroencephalography (EEG). In 1924 Berger was the first to record human brain activity by means of EEG. Berger was able to identify oscillatory activity, such as Berger's wave or the alpha wave (8–13 Hz), by analyzing EEG traces.\nBerger's first recording device was very rudimentary and was a harpsichord. He inserted silver wires under the scalps of his patients. These were later replaced by silver foils attached to the patient's head by rubber bandages. Berger connected these sensors to a Lippmann capillary electrometer, with disappointing results. However, more sophisticated measuring devices, such as the Siemens double-coil recording galvanometer, which displayed electric voltages as small as one ten thousandth of a volt, led to success.\nBerger analyzed the interrelation of alternations in his EEG wave diagrams with brain diseases. EEGs permitted completely new possibilities for the research of human brain activities.\nAlthough the term had not yet been coined, one of the earliest examples of a working brain-machine interface was the piece Music for Solo Performer (1965) by the American composer Alvin Lucier. The piece makes use of EEG and analog signal processing hardware (filters, amplifiers, and a mixing board) to stimulate acoustic percussion instruments. To perform the piece one must produce alpha waves and thereby \"play\" the various percussion instruments via loudspeakers which are placed near or directly on the instruments themselves.UCLA Professor Jacques Vidal coined the term \"BCI\" and produced the first peer-reviewed publications on this topic. Vidal is widely recognized as the inventor of BCIs in the BCI community, as reflected in numerous peer-reviewed articles reviewing and discussing the field (e.g.,). A review pointed out that Vidal's 1973 paper stated the \"BCI challenge\" of controlling external objects using EEG signals, and especially use of Contingent Negative Variation (CNV) potential as a challenge for BCI control. The 1977 experiment Vidal described was the first application of BCI after his 1973 BCI challenge. It was a noninvasive EEG (actually Visual Evoked Potentials (VEP)) control of a cursor-like graphical object on a computer screen. The demonstration was movement in a maze.After his early contributions, Vidal was not active in BCI research, nor BCI events such as conferences, for many years. In 2011, however, he gave a lecture in Graz, Austria, supported by the Future BNCI project, presenting the first BCI, which earned a standing ovation. Vidal was joined by his wife, Laryce Vidal, who previously worked with him at UCLA on his first BCI project.\nIn 1988, a report was given on noninvasive EEG control of a physical object, a robot. The experiment described was EEG control of multiple start-stop-restart of the robot movement, along an arbitrary trajectory defined by a line drawn on a floor. The line-following behavior was the default robot behavior, utilizing autonomous intelligence and autonomous source of energy. This 1988 report written by Stevo Bozinovski, Mihail Sestakov, and Liljana Bozinovska was the first one about a robot control using EEG.In 1990, a report was given on a closed loop, bidirectional adaptive BCI controlling computer buzzer by an anticipatory brain potential, the Contingent Negative Variation (CNV) potential. The experiment described how an expectation state of the brain, manifested by CNV, controls in a feedback loop the S2 buzzer in the S1-S2-CNV paradigm. The obtained cognitive wave representing the expectation learning in the brain is named Electroexpectogram (EXG). The CNV brain potential was part of the BCI challenge presented by Vidal in his 1973 paper.\nStudies in 2010s suggested the potential ability of neural stimulation to restore functional connectively and associated behaviors through modulation of molecular mechanisms of synaptic efficacy. This opened the door for the concept that BCI technologies may be able to restore function in addition to enabling functionality.\nSince 2013, DARPA has funded BCI technology through the BRAIN initiative, which has supported work out of the University of Pittsburgh Medical Center, Paradromics, Brown, and Synchron, among others.\n\nBCIs versus neuroprosthetics\nNeuroprosthetics is an area of neuroscience concerned with neural prostheses, that is, using artificial devices to replace the function of impaired nervous systems and brain-related problems, or of sensory organs or organs itself (bladder, diaphragm, etc.). As of December 2010, cochlear implants had been implanted as neuroprosthetic device in approximately 220,000 people worldwide. There are also several neuroprosthetic devices that aim to restore vision, including retinal implants. The first neuroprosthetic device, however, was the pacemaker.\nThe terms are sometimes used interchangeably. Neuroprosthetics and BCIs seek to achieve the same aims, such as restoring sight, hearing, movement, ability to communicate, and even cognitive function. Both use similar experimental methods and surgical techniques.\n\nAnimal BCI research\nSeveral laboratories have managed to record signals from monkey and rat cerebral cortices to operate BCIs to produce movement. Monkeys have navigated computer cursors on screen and commanded robotic arms to perform simple tasks simply by thinking about the task and seeing the visual feedback, but without any motor output. In May 2008 photographs that showed a monkey at the University of Pittsburgh Medical Center operating a robotic arm by thinking were published in a number of well-known science journals and magazines. Sheep too have been used to evaluate BCI technology including Synchron's Stentrode.\nIn 2020, Elon Musk's Neuralink was successfully implanted in a pig, announced in a widely viewed webcast. In 2021, Elon Musk announced that he had successfully enabled a monkey to play video games using Neuralink's device.\n\nEarly work\nIn 1969 the operant conditioning studies of Fetz and colleagues,\nat the Regional Primate Research Center and Department of Physiology and Biophysics, University of Washington School of Medicine in Seattle, showed for the first time that monkeys could learn to control the deflection of a biofeedback meter arm with neural activity. Similar work in the 1970s established that monkeys could quickly learn to voluntarily control the firing rates of individual and multiple neurons in the primary motor cortex if they were rewarded for generating appropriate patterns of neural activity.Studies that developed algorithms to reconstruct movements from motor cortex neurons, which control movement, date back to the 1970s. In the 1980s, Apostolos Georgopoulos at Johns Hopkins University found a mathematical relationship between the electrical responses of single motor cortex neurons in rhesus macaque monkeys and the direction in which they moved their arms (based on a cosine function). He also found that dispersed groups of neurons, in different areas of the monkey's brains, collectively controlled motor commands, but was able to record the firings of neurons in only one area at a time, because of the technical limitations imposed by his equipment.There has been rapid development in BCIs since the mid-1990s. Several groups have been able to capture complex brain motor cortex signals by recording from neural ensembles (groups of neurons) and using these to control external devices.\n\nProminent research successes\nKennedy and Yang Dan\nPhillip Kennedy (who later founded Neural Signals in 1987) and colleagues built the first intracortical brain–computer interface by implanting neurotrophic-cone electrodes into monkeys.\n In 1999, researchers led by Yang Dan at the University of California, Berkeley decoded neuronal firings to reproduce images seen by cats. The team used an array of electrodes embedded in the thalamus (which integrates all of the brain's sensory input) of sharp-eyed cats. Researchers targeted 177 brain cells in the thalamus lateral geniculate nucleus area, which decodes signals from the retina. The cats were shown eight short movies, and their neuron firings were recorded. Using mathematical filters, the researchers decoded the signals to generate movies of what the cats saw and were able to reconstruct recognizable scenes and moving objects. Similar results in humans have since been achieved by researchers in Japan (see below).\n\nNicolelis\nMiguel Nicolelis, a professor at Duke University, in Durham, North Carolina, has been a prominent proponent of using multiple electrodes spread over a greater area of the brain to obtain neuronal signals to drive a BCI.\nAfter conducting initial studies in rats during the 1990s, Nicolelis and his colleagues developed BCIs that decoded brain activity in owl monkeys and used the devices to reproduce monkey movements in robotic arms. Monkeys have advanced reaching and grasping abilities and good hand manipulation skills, making them ideal test subjects for this kind of work.\nBy 2000, the group succeeded in building a BCI that reproduced owl monkey movements while the monkey operated a joystick or reached for food. The BCI operated in real time and could also control a separate robot remotely over Internet Protocol. But the monkeys could not see the arm moving and did not receive any feedback, a so-called open-loop BCI.\n\nLater experiments by Nicolelis using rhesus monkeys succeeded in closing the feedback loop and reproduced monkey reaching and grasping movements in a robot arm. With their deeply cleft and furrowed brains, rhesus monkeys are considered to be better models for human neurophysiology than owl monkeys. The monkeys were trained to reach and grasp objects on a computer screen by manipulating a joystick while corresponding movements by a robot arm were hidden. The monkeys were later shown the robot directly and learned to control it by viewing its movements. The BCI used velocity predictions to control reaching movements and simultaneously predicted handgripping force. In 2011 O'Doherty and colleagues showed a BCI with sensory feedback with rhesus monkeys. The monkey was brain controlling the position of an avatar arm while receiving sensory feedback through direct intracortical stimulation (ICMS) in the arm representation area of the sensory cortex.\n\nDonoghue, Schwartz and Andersen\nOther laboratories which have developed BCIs and algorithms that decode neuron signals include the Carney Institute for Brain Science at Brown University and the labs of Andrew Schwartz at the University of Pittsburgh and Richard Andersen at Caltech. These researchers have been able to produce working BCIs, even using recorded signals from far fewer neurons than did Nicolelis (15–30 neurons versus 50–200 neurons).\nJohn Donoghue's lab at the Carney Institute reported training rhesus monkeys to use a BCI to track visual targets on a computer screen (closed-loop BCI) with or without assistance of a joystick. Schwartz's group created a BCI for three-dimensional tracking in virtual reality and also reproduced BCI control in a robotic arm. The same group also created headlines when they demonstrated that a monkey could feed itself pieces of fruit and marshmallows using a robotic arm controlled by the animal's own brain signals.Andersen's group used recordings of premovement activity from the posterior parietal cortex in their BCI, including signals created when experimental animals anticipated receiving a reward.\n\nOther research\nIn addition to predicting kinematic and kinetic parameters of limb movements, BCIs that predict electromyographic or electrical activity of the muscles of primates are being developed. Such BCIs could be used to restore mobility in paralyzed limbs by electrically stimulating muscles.\nMiguel Nicolelis and colleagues demonstrated that the activity of large neural ensembles can predict arm position. This work made possible creation of BCIs that read arm movement intentions and translate them into movements of artificial actuators. Carmena and colleagues programmed the neural coding in a BCI that allowed a monkey to control reaching and grasping movements by a robotic arm. Lebedev and colleagues argued that brain networks reorganize to create a new representation of the robotic appendage in addition to the representation of the animal's own limbs.\nIn 2019, researchers from UCSF published a study where they demonstrated a BCI that had the potential to help patients with speech impairment caused by neurological disorders. Their BCI used high-density electrocorticography to tap neural activity from a patient's brain and used deep learning methods to synthesize speech. In 2021, researchers from the same group published a study showing the potential of a BCI to decode words and sentences in an anarthric patient who had been unable to speak for over 15 years.The biggest impediment to BCI technology at present is the lack of a sensor modality that provides safe, accurate and robust access to brain signals. It is conceivable or even likely, however, that such a sensor will be developed within the next twenty years. The use of such a sensor should greatly expand the range of communication functions that can be provided using a BCI.\nDevelopment and implementation of a BCI system is complex and time-consuming. In response to this problem, Gerwin Schalk has been developing a general-purpose system for BCI research, called BCI2000. BCI2000 has been in development since 2000 in a project led by the Brain–Computer Interface R&D Program at the Wadsworth Center of the New York State Department of Health in Albany, New York, United States.\nA new 'wireless' approach uses light-gated ion channels such as Channelrhodopsin to control the activity of genetically defined subsets of neurons in vivo. In the context of a simple learning task, illumination of transfected cells in the somatosensory cortex influenced the decision-making process of freely moving mice.The use of BMIs has also led to a deeper understanding of neural networks and the central nervous system. Research has shown that despite the inclination of neuroscientists to believe that neurons have the most effect when working together, single neurons can be conditioned through the use of BMIs to fire at a pattern that allows primates to control motor outputs. The use of BMIs has led to development of the single neuron insufficiency principle which states that even with a well tuned firing rate single neurons can only carry a narrow amount of information and therefore the highest level of accuracy is achieved by recording firings of the collective ensemble. Other principles discovered with the use of BMIs include the neuronal multitasking principle, the neuronal mass principle, the neural degeneracy principle, and the plasticity principle.BCIs are also proposed to be applied by users without disabilities. A user-centered categorization of BCI approaches by Thorsten O. Zander and Christian Kothe introduces the term passive BCI. Next to active and reactive BCI that are used for directed control, passive BCIs allow for assessing and interpreting changes in the user state during Human-Computer Interaction (HCI). In a secondary, implicit control loop the computer system adapts to its user improving its usability in general.\nBeyond BCI systems that decode neural activity to drive external effectors, BCI systems may be used to encode signals from the periphery. These sensory BCI devices enable real-time, behaviorally-relevant decisions based upon closed-loop neural stimulation.\n\nThe BCI Award\nThe Annual BCI Research Award is awarded in recognition of outstanding and innovative research in the field of Brain-Computer Interfaces. Each year, a renowned research laboratory is asked to judge the submitted projects. The jury consists of world-leading BCI experts recruited by the awarding laboratory. The jury selects twelve nominees, then chooses a first, second, and third-place winner, who receive awards of $3,000, $2,000, and $1,000, respectively.\n\nHuman BCI research\nInvasive BCIs\nInvasive BCI requires surgery to implant electrodes under scalp for communicating brain signals. The main advantage is to provide more accurate reading; however, its downside includes side effects from the surgery. After the surgery, scar tissues may form which can make brain signals weaker. In addition, according to the research of Abdulkader et al., (2015), the body may not accept the implanted electrodes and this can cause a medical condition.\n\nVision\nInvasive BCI research has targeted repairing damaged sight and providing new functionality for people with paralysis. Invasive BCIs are implanted directly into the grey matter of the brain during neurosurgery. Because they lie in the grey matter, invasive devices produce the highest quality signals of BCI devices but are prone to scar-tissue build-up, causing the signal to become weaker, or even non-existent, as the body reacts to a foreign object in the brain.In vision science, direct brain implants have been used to treat non-congenital (acquired) blindness. One of the first scientists to produce a working brain interface to restore sight was private researcher William Dobelle.\nDobelle's first prototype was implanted into \"Jerry\", a man blinded in adulthood, in 1978. A single-array BCI containing 68 electrodes was implanted onto Jerry's visual cortex and succeeded in producing phosphenes, the sensation of seeing light. The system included cameras mounted on glasses to send signals to the implant. Initially, the implant allowed Jerry to see shades of grey in a limited field of vision at a low frame-rate. This also required him to be hooked up to a mainframe computer, but shrinking electronics and faster computers made his artificial eye more portable and now enable him to perform simple tasks unassisted.\nIn 2002, Jens Naumann, also blinded in adulthood, became the first in a series of 16 paying patients to receive Dobelle's second generation implant, marking one of the earliest commercial uses of BCIs. The second generation device used a more sophisticated implant enabling better mapping of phosphenes into coherent vision. Phosphenes are spread out across the visual field in what researchers call \"the starry-night effect\". Immediately after his implant, Jens was able to use his imperfectly restored vision to drive an automobile slowly around the parking area of the research institute. Unfortunately, Dobelle died in 2004 before his processes and developments were documented. Subsequently, when Mr. Naumann and the other patients in the program began having problems with their vision, there was no relief and they eventually lost their \"sight\" again. Naumann wrote about his experience with Dobelle's work in Search for Paradise: A Patient's Account of the Artificial Vision Experiment and has returned to his farm in Southeast Ontario, Canada, to resume his normal activities.\n\nMovement\nBCIs focusing on motor neuroprosthetics aim to either restore movement in individuals with paralysis or provide devices to assist them, such as interfaces with computers or robot arms.\nResearchers at Emory University in Atlanta, led by Philip Kennedy and Roy Bakay, were first to install a brain implant in a human that produced signals of high enough quality to simulate movement. Their patient, Johnny Ray (1944–2002), developed 'locked-in syndrome' after having a brain-stem stroke in 1997. Ray's implant was installed in 1998 and he lived long enough to start working with the implant, eventually learning to control a computer cursor; he died in 2002 of a brain aneurysm.Tetraplegic Matt Nagle became the first person to control an artificial hand using a BCI in 2005 as part of the first nine-month human trial of Cyberkinetics's BrainGate chip-implant. Implanted in Nagle's right precentral gyrus (area of the motor cortex for arm movement), the 96-electrode BrainGate implant allowed Nagle to control a robotic arm by thinking about moving his hand as well as a computer cursor, lights and TV. One year later, professor Jonathan Wolpaw received the prize of the Altran Foundation for Innovation to develop a Brain Computer Interface with electrodes located on the surface of the skull, instead of directly in the brain.\nMore recently, research teams led by the BrainGate group at Brown University and a group led by University of Pittsburgh Medical Center, both in collaborations with the United States Department of Veterans Affairs, have demonstrated further success in direct control of robotic prosthetic limbs with many degrees of freedom using direct connections to arrays of neurons in the motor cortex of patients with tetraplegia.\n\nCommunication\nIn May 2021, a Stanford University team reported a successful proof-of-concept test that enabled a quadraplegic participant to input English sentences at about 86 characters per minute and 18 words per minute. The participant imagined moving his hand to write letters, and the system performed handwriting recognition on electrical signals detected in the motor cortex, utilizing hidden Markov models and recurrent neural networks for decoding.A report published in July 2021 reported a paralyzed patient was able to communicate 15 words per minute using a brain implant that analyzed motor neurons that previously controlled the vocal tract.In a recent review article, researchers raised an open question of whether human information transfer rates can surpass that of language with BCIs. Given that recent language research has demonstrated that human information transfer rates are relatively constant across many languages, there may exist a limit at the level of information processing in the brain. On the contrary, this \"upper limit\" of information transfer rate may be intrinsic to language itself, as a modality for information transfer.\n\nTechnical challenges\nThere exist a number of technical challenges to recording brain activity with invasive BCIs. Advances in CMOS technology are pushing and enabling integrated, invasive BCI designs with smaller size, lower power requirements, and higher signal acquisition capabilities. Invasive BCIs involve electrodes that penetrate brain tissue in an attempt to record action potential signals (also known as spikes) from individual, or small groups of, neurons near the electrode. The interface between a recording electrode and the electrolytic solution surrounding neurons has been modelled using the Hodgkin-Huxley model.\nElectronic limitations to invasive BCIs have been an active area of research in recent decades. While intracellular recordings of neurons reveal action potential voltages on the scale of hundreds of millivolts, chronic invasive BCIs rely on recording extracellular voltages which typically are three orders of magnitude smaller, existing at hundreds of microvolts. Further adding to the challenge of detecting signals on the scale of microvolts is the fact that the electrode-tissue interface has a high capacitance at small voltages. Due to the nature of these small signals, for BCI systems that incorporate functionality onto an integrated circuit, each electrode requires its own amplifier and ADC, which convert analog extracellular voltages into digital signals. Because a typical neuron action potential lasts for one millisecond, BCIs measuring spikes must have sampling rates ranging from 300 Hz to 5 kHz. Yet another concern is that invasive BCIs must be low-power, so as to dissipate less heat to surrounding tissue; at the most basic level more power is traditionally needed to optimize signal-to-noise ratio. Optimal battery design is an active area of research in BCIs.Challenges existing in the area of material science are central to the design of invasive BCIs. Variations in signal quality over time have been commonly observed with implantable microelectrodes. Optimal material and mechanical characteristics for long term signal stability in invasive BCIs has been an active area of research. It has been proposed that the formation of glial scarring, secondary to damage at the electrode-tissue interface, is likely responsible for electrode failure and reduced recording performance. Research has suggested that blood-brain barrier leakage, either at the time of insertion or over time, may be responsible for the inflammatory and glial reaction to chronic microelectrodes implanted in the brain. As a result, flexible and tissue-like designs have been researched and developed to minimize foreign-body reaction by means of matching the Young's modulus of the electrode closer to that of brain tissue.\n\nPartially invasive BCIs\nPartially invasive BCI devices are implanted inside the skull but rest outside the brain rather than within the grey matter. They produce better resolution signals than non-invasive BCIs where the bone tissue of the cranium deflects and deforms signals and have a lower risk of forming scar-tissue in the brain than fully invasive BCIs. There has been preclinical demonstration of intracortical BCIs from the stroke perilesional cortex.\n\nEndovascular\nA systematic review published in 2020 detailed multiple studies, both clinical and non-clinical, dating back decades investigating the feasibility of endovascular BCIs.In recent years, the biggest advance in partially invasive BCIs has emerged in the area of interventional neurology. In 2010, researchers affiliated with University of Melbourne had begun developing a BCI that could be inserted via the vascular system. The Australian neurologist Thomas Oxley (Mount Sinai Hospital) conceived the idea for this BCI, called Stentrode, which has received funding from DARPA. Preclinical studies evaluated the technology in sheep.\nThe Stentrode, a monolithic stent electrode array, is designed to be delivered via an intravenous catheter under image-guidance to the superior sagittal sinus, in the region which lies adjacent to motor cortex. This proximity to motor cortex underlies the Stentrode's ability to measure neural activity. The procedure is most similar to how venous sinus stents are placed for the treatment of idiopathic intracranial hypertension. The Stentrode communicates neural activity to a battery-less telemetry unit implanted in the chest, which communicates wirelessly with an external telemetry unit capable of power and data transfer. While an endovascular BCI benefits from avoiding craniotomy for insertion, risks such as clotting and venous thrombosis are possible.\nFirst-in-human trials with the Stentrode are underway. In November 2020, two participants with amyotrophic lateral sclerosis were able to wirelessly control an operating system to text, email, shop, and bank using direct thought through the Stentrode brain-computer interface, marking the first time a brain-computer interface was implanted via the patient's blood vessels, eliminating the need for open brain surgery. In January 2023, researchers reported no serious adverse events during the first year for all four patients who could use it to operate computers.\n\nECoG\nElectrocorticography (ECoG) measures the electrical activity of the brain taken from beneath the skull in a similar way to non-invasive electroencephalography, but the electrodes are embedded in a thin plastic pad that is placed above the cortex, beneath the dura mater. ECoG technologies were first trialled in humans in 2004 by Eric Leuthardt and Daniel Moran from Washington University in St. Louis. In a later trial, the researchers enabled a teenage boy to play Space Invaders using his ECoG implant. This research indicates that control is rapid, requires minimal training, and may be an ideal tradeoff with regards to signal fidelity and level of invasiveness.Signals can be either subdural or epidural, but are not taken from within the brain parenchyma itself. It has not been studied extensively until recently due to the limited access of subjects. Currently, the only manner to acquire the signal for study is through the use of patients requiring invasive monitoring for localization and resection of an epileptogenic focus.\nECoG is a very promising intermediate BCI modality because it has higher spatial resolution, better signal-to-noise ratio, wider frequency range, and less training requirements than scalp-recorded EEG, and at the same time has lower technical difficulty, lower clinical risk, and may have superior long-term stability than intracortical single-neuron recording. This feature profile and recent evidence of the high level of control with minimal training requirements shows potential for real world application for people with motor disabilities. Light reactive imaging BCI devices are still in the realm of theory.\nRecent work published by Edward Chang and Joseph Makin from UCSF revealed that ECoG signals could be used to decode speech from epilepsy patients implanted with high-density ECoG arrays over the peri-Sylvian cortices. Their study achieved word error rates of 3% (a marked improvement from prior publications) utilizing an encoder-decoder neural network, which translated ECoG data into one of fifty sentences composed of 250 unique words.\n\nNon-invasive BCIs\nThere have also been experiments in humans using non-invasive neuroimaging technologies as interfaces. The substantial majority of published BCI work involves noninvasive EEG-based BCIs. Noninvasive EEG-based technologies and interfaces have been used for a much broader variety of applications. Although EEG-based interfaces are easy to wear and do not require surgery, they have relatively poor spatial resolution and cannot effectively use higher-frequency signals because the skull dampens signals, dispersing and blurring the electromagnetic waves created by the neurons. EEG-based interfaces also require some time and effort prior to each usage session, whereas non-EEG-based ones, as well as invasive ones require no prior-usage training. Overall, the best BCI for each user depends on numerous factors.\n\nNon-EEG-based human–computer interface\nElectrooculography (EOG)\nIn 1989, a report was given on control of a mobile robot by eye movement using electrooculography (EOG) signals. A mobile robot was driven from a start to a goal point using five EOG commands, interpreted as forward, backward, left, right, and stop. The EOG as a challenge of controlling external objects was presented by Vidal in his 1973 paper.\n\nPupil-size oscillation\nA 2016 article described an entirely new communication device and non-EEG-based human-computer interface, which requires no visual fixation, or ability to move the eyes at all. The interface is based on covert interest; directing one's attention to a chosen letter on a virtual keyboard, without the need to move one's eyes to look directly at the letter. Each letter has its own (background) circle which micro-oscillates in brightness differently from all of the other letters. The letter selection is based on best fit between unintentional pupil-size oscillation and the background circle's brightness oscillation pattern. Accuracy is additionally improved by the user's mental rehearsing of the words 'bright' and 'dark' in synchrony with the brightness transitions of the letter's circle.\n\nFunctional near-infrared spectroscopy\nIn 2014 and 2017, a BCI using functional near-infrared spectroscopy for \"locked-in\" patients with amyotrophic lateral sclerosis (ALS) was able to restore some basic ability of the patients to communicate with other people.\n\nElectroencephalography (EEG)-based brain-computer interfaces\nAfter the BCI challenge was stated by Vidal in 1973, the initial reports on non-invasive approach included control of a cursor in 2D using VEP (Vidal 1977), control of a buzzer using CNV (Bozinovska et al. 1988, 1990), control of a physical object, a robot, using a brain rhythm (alpha) (Bozinovski et al. 1988), control of a text written on a screen using P300 (Farwell and Donchin, 1988).In the early days of BCI research, another substantial barrier to using electroencephalography (EEG) as a brain–computer interface was the extensive training required before users can work the technology. For example, in experiments beginning in the mid-1990s, Niels Birbaumer at the University of Tübingen in Germany trained severely paralysed people to self-regulate the slow cortical potentials in their EEG to such an extent that these signals could be used as a binary signal to control a computer cursor. (Birbaumer had earlier trained epileptics to prevent impending fits by controlling this low voltage wave.) The experiment saw ten patients trained to move a computer cursor by controlling their brainwaves. The process was slow, requiring more than an hour for patients to write 100 characters with the cursor, while training often took many months. However, the slow cortical potential approach to BCIs has not been used in several years, since other approaches require little or no training, are faster and more accurate, and work for a greater proportion of users.\nAnother research parameter is the type of oscillatory activity that is measured. Gert Pfurtscheller founded the BCI Lab 1991 and fed his research results on motor imagery in the first online BCI based on oscillatory features and classifiers. Together with Birbaumer and Jonathan Wolpaw at New York State University they focused on developing technology that would allow users to choose the brain signals they found easiest to operate a BCI, including mu and beta rhythms.\nA further parameter is the method of feedback used and this is shown in studies of P300 signals. Patterns of P300 waves are generated involuntarily (stimulus-feedback) when people see something they recognize and may allow BCIs to decode categories of thoughts without training patients first. By contrast, the biofeedback methods described above require learning to control brainwaves so the resulting brain activity can be detected.\nIn 2005 it was reported research on EEG emulation of digital control circuits for BCI, with example of a CNV flip-flop. In 2009 it was reported noninvasive EEG control of a robotic arm using a CNV flip-flop. In 2011 it was reported control of two robotic arms solving Tower of Hanoi task with three disks using a CNV flip-flop. In 2015 it was described EEG-emulation of a Schmitt trigger, flip-flop, demultiplexer, and modem.While an EEG based brain-computer interface has been pursued extensively by a number of research labs, recent advancements made by Bin He and his team at the University of Minnesota suggest the potential of an EEG based brain-computer interface to accomplish tasks close to invasive brain-computer interface. Using advanced functional neuroimaging including BOLD functional MRI and EEG source imaging, Bin He and co-workers identified the co-variation and co-localization of electrophysiological and hemodynamic signals induced by motor imagination.\nRefined by a neuroimaging approach and by a training protocol, Bin He and co-workers demonstrated the ability of a non-invasive EEG based brain-computer interface to control the flight of a virtual helicopter in 3-dimensional space, based upon motor imagination. In June 2013 it was announced that Bin He had developed the technique to enable a remote-control helicopter to be guided through an obstacle course.In addition to a brain-computer interface based on brain waves, as recorded from scalp EEG electrodes, Bin He and co-workers explored a virtual EEG signal-based brain-computer interface by first solving the EEG inverse problem and then used the resulting virtual EEG for brain-computer interface tasks. Well-controlled studies suggested the merits of such a source analysis based brain-computer interface.A 2014 study found that severely motor-impaired patients could communicate faster and more reliably with non-invasive EEG BCI, than with any muscle-based communication channel.A 2016 study found that the Emotiv EPOC device may be more suitable for control tasks using the attention/meditation level or eye blinking than the Neurosky MindWave device.A 2019 study found that the application of evolutionary algorithms could improve EEG mental state classification with a non-invasive Muse device, enabling high quality classification of data acquired by a cheap consumer-grade EEG sensing device.In a 2021 systematic review of randomized controlled trials using BCI for upper-limb rehabilitation after stroke, EEG-based BCI was found to have significant efficacy in improving upper-limb motor function compared to control therapies. More specifically, BCI studies that utilized band power features, motor imagery, and functional electrical stimulation in their design were found to be more efficacious than alternatives. Another 2021 systematic review focused on robotic-assisted EEG-based BCI for hand rehabilitation after stroke. Improvement in motor assessment scores was observed in three of eleven studies included in the systematic review.\n\nDry active electrode arrays\nIn the early 1990s Babak Taheri, at University of California, Davis demonstrated the first single and also multichannel dry active electrode arrays using micro-machining. The single channel dry EEG electrode construction and results were published in 1994. The arrayed electrode was also demonstrated to perform well compared to silver/silver chloride electrodes. The device consisted of four sites of sensors with integrated electronics to reduce noise by impedance matching. The advantages of such electrodes are: (1) no electrolyte used, (2) no skin preparation, (3) significantly reduced sensor size, and (4) compatibility with EEG monitoring systems. The active electrode array is an integrated system made of an array of capacitive sensors with local integrated circuitry housed in a package with batteries to power the circuitry. This level of integration was required to achieve the functional performance obtained by the electrode.\nThe electrode was tested on an electrical test bench and on human subjects in four modalities of EEG activity, namely: (1) spontaneous EEG, (2) sensory event-related potentials, (3) brain stem potentials, and (4) cognitive event-related potentials. The performance of the dry electrode compared favorably with that of the standard wet electrodes in terms of skin preparation, no gel requirements (dry), and higher signal-to-noise ratio.In 1999 researchers at Case Western Reserve University, in Cleveland, Ohio, led by Hunter Peckham, used 64-electrode EEG skullcap to return limited hand movements to quadriplegic Jim Jatich. As Jatich concentrated on simple but opposite concepts like up and down, his beta-rhythm EEG output was analysed using software to identify patterns in the noise. A basic pattern was identified and used to control a switch: Above average activity was set to on, below average off. As well as enabling Jatich to control a computer cursor the signals were also used to drive the nerve controllers embedded in his hands, restoring some movement.\n\nSSVEP mobile EEG BCIs\nIn 2009, the NCTU Brain-Computer-Interface-headband was reported. The researchers who developed this BCI-headband also engineered silicon-based microelectro-mechanical system (MEMS) dry electrodes designed for application in non-hairy sites of the body. These electrodes were secured to the DAQ board in the headband with snap-on electrode holders. The signal processing module measured alpha activity and the Bluetooth enabled phone assessed the patients' alertness and capacity for cognitive performance. When the subject became drowsy, the phone sent arousing feedback to the operator to rouse them. This research was supported by the National Science Council, Taiwan, R.O.C., NSC, National Chiao-Tung University, Taiwan's Ministry of Education, and the U.S. Army Research Laboratory.In 2011, researchers reported a cellular based BCI with the capability of taking EEG data and converting it into a command to cause the phone to ring. This research was supported in part by Abraxis Bioscience LLP, the U.S. Army Research Laboratory, and the Army Research Office. The developed technology was a wearable system composed of a four channel bio-signal acquisition/amplification module, a wireless transmission module, and a Bluetooth enabled cell phone.  The electrodes were placed so that they pick up steady state visual evoked potentials (SSVEPs). SSVEPs are electrical responses to flickering visual stimuli with repetition rates over 6 Hz that are best found in the parietal and occipital scalp regions of the visual cortex. It was reported that with this BCI setup, all study participants were able to initiate the phone call with minimal practice in natural environments.The scientists claim that their studies using a single channel fast Fourier transform (FFT) and multiple channel system canonical correlation analysis (CCA) algorithm support the capacity of mobile BCIs. The CCA algorithm has been applied in other experiments investigating BCIs with claimed high performance in accuracy as well as speed. While the cellular based BCI technology was developed to initiate a phone call from SSVEPs, the researchers said that it can be translated for other applications, such as picking up sensorimotor mu/beta rhythms to function as a motor-imagery based BCI.In 2013, comparative tests were performed on android cell phone, tablet, and computer based BCIs, analyzing the power spectrum density of resultant EEG SSVEPs. The stated goals of this study, which involved scientists supported in part by the U.S. Army Research Laboratory, were to \"increase the practicability, portability, and ubiquity of an SSVEP-based BCI, for daily use\". Citation It was reported that the stimulation frequency on all mediums was accurate, although the cell phone's signal demonstrated some instability. The amplitudes of the SSVEPs for the laptop and tablet were also reported to be larger than those of the cell phone. These two qualitative characterizations were suggested as indicators of the feasibility of using a mobile stimulus BCI.\n\nLimitations\nIn 2011, researchers stated that continued work should address ease of use, performance robustness, reducing hardware and software costs.One of the difficulties with EEG readings is the large susceptibility to motion artifacts. In most of the previously described research projects, the participants were asked to sit still, reducing head and eye movements as much as possible, and measurements were taken in a laboratory setting. However, since the emphasized application of these initiatives had been in creating a mobile device for daily use, the technology had to be tested in motion.\nIn 2013, researchers tested mobile EEG-based BCI technology, measuring SSVEPs from participants as they walked on a treadmill at varying speeds. This research was supported by the Office of Naval Research, Army Research Office, and the U.S. Army Research Laboratory. Stated results were that as speed increased the SSVEP detectability using CCA decreased. As independent component analysis (ICA) had been shown to be efficient in separating EEG signals from noise, the scientists applied ICA to CCA extracted EEG data. They stated that the CCA data with and without ICA processing were similar. Thus, they concluded that CCA independently demonstrated a robustness to motion artifacts that indicates it may be a beneficial algorithm to apply to BCIs used in real world conditions.\nOne of the major problems in EEG-based BCI applications is the low spatial resolution. Several solutions have been suggested to address this issue since 2019, which include: EEG source connectivity based on graph theory, EEG pattern recognition based on Topomap, EEG-fMRI fusion, and so on.\n\nProsthesis and environment control\nNon-invasive BCIs have also been applied to enable brain-control of prosthetic upper and lower extremity devices in people with paralysis. For example, Gert Pfurtscheller of Graz University of Technology and colleagues demonstrated a BCI-controlled functional electrical stimulation system to restore upper extremity movements in a person with tetraplegia due to spinal cord injury. Between 2012 and 2013, researchers at the University of California, Irvine demonstrated for the first time that it is possible to use BCI technology to restore brain-controlled walking after spinal cord injury. In their spinal cord injury research study, a person with paraplegia was able to operate a BCI-robotic gait orthosis to regain basic brain-controlled ambulation.\nIn 2009 Alex Blainey, an independent researcher based in the UK, successfully used the Emotiv EPOC to control a 5 axis robot arm. He then went on to make several demonstration mind controlled wheelchairs and home automation that could be operated by people with limited or no motor control such as those with paraplegia and cerebral palsy.\nResearch into military use of BCIs funded by DARPA has been ongoing since the 1970s. The current focus of research is user-to-user communication through analysis of neural signals.\n\nDIY and open source BCI\nIn 2001, The OpenEEG Project was initiated by a group of DIY neuroscientists and engineers. The ModularEEG was the primary device created by the OpenEEG community; it was a 6-channel signal capture board that cost between $200 and $400 to make at home. The OpenEEG Project marked a significant moment in the emergence of DIY brain-computer interfacing.\nIn 2010, the Frontier Nerds of NYU's ITP program published a thorough tutorial titled How To Hack Toy EEGs. The tutorial, which stirred the minds of many budding DIY BCI enthusiasts, demonstrated how to create a single channel at-home EEG with an Arduino and a Mattel Mindflex at a very reasonable price. This tutorial amplified the DIY BCI movement.\n\nMEG and MRI\nMagnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) have both been used successfully as non-invasive BCIs. In a widely reported experiment, fMRI allowed two users being scanned to play Pong in real-time by altering their haemodynamic response or brain blood flow through biofeedback techniques.fMRI measurements of haemodynamic responses in real time have also been used to control robot arms with a seven-second delay between thought and movement.In 2008 research developed in the Advanced Telecommunications Research (ATR) Computational Neuroscience Laboratories in Kyoto, Japan, allowed the scientists to reconstruct images directly from the brain and display them on a computer in black and white at a resolution of 10x10 pixels. The article announcing these achievements was the cover story of the journal Neuron of 10 December 2008.In 2011 researchers from UC Berkeley published a study reporting second-by-second reconstruction of videos watched by the study's subjects, from fMRI data. This was achieved by creating a statistical model relating visual patterns in videos shown to the subjects, to the brain activity caused by watching the videos. This model was then used to look up the 100 one-second video segments, in a database of 18 million seconds of random YouTube videos, whose visual patterns most closely matched the brain activity recorded when subjects watched a new video. These 100 one-second video extracts were then combined into a mashed-up image that resembled the video being watched.\n\nBCI control strategies in neurogaming\nMotor imagery\nMotor imagery involves the imagination of the movement of various body parts resulting in sensorimotor cortex activation, which modulates sensorimotor oscillations in the EEG. This can be detected by the BCI to infer a user's intent. Motor imagery typically requires a number of sessions of training before acceptable control of the BCI is acquired. These training sessions may take a number of hours over several days before users can consistently employ the technique with acceptable levels of precision. Regardless of the duration of the training session, users are unable to master the control scheme. This results in very slow pace of the gameplay. Advanced machine learning methods were recently developed to compute a subject-specific model for detecting the performance of motor imagery. The top performing algorithm from BCI Competition IV dataset 2 for motor imagery is the Filter Bank Common Spatial Pattern, developed by Ang et al. from A*STAR, Singapore.\n\nBio/neurofeedback for passive BCI designs\nBiofeedback is used to monitor a subject's mental relaxation. In some cases, biofeedback does not monitor electroencephalography (EEG), but instead bodily parameters such as electromyography (EMG), galvanic skin resistance (GSR), and heart rate variability (HRV). Many biofeedback systems are used to treat certain disorders such as attention deficit hyperactivity disorder (ADHD), sleep problems in children, teeth grinding, and chronic pain. EEG biofeedback systems typically monitor four different bands (theta: 4–7 Hz, alpha:8–12 Hz, SMR: 12–15 Hz, beta: 15–18 Hz) and challenge the subject to control them. Passive BCI involves using BCI to enrich human–machine interaction with implicit information on the actual user's state, for example, simulations to detect when users intend to push brakes during an emergency car stopping procedure. Game developers using passive BCIs need to acknowledge that through repetition of game levels the user's cognitive state will change or adapt. Within the first play\nof a level, the user will react to things differently from during the second play: for example, the user will be less surprised at an event in the game if they are expecting it.\n\nVisual evoked potential (VEP)\nA VEP is an electrical potential recorded after a subject is presented with a type of visual stimuli. There are several types of VEPs.\nSteady-state visually evoked potentials (SSVEPs) use potentials generated by exciting the retina, using visual stimuli modulated at certain frequencies. SSVEP's stimuli are often formed from alternating checkerboard patterns and at times simply use flashing images. The frequency of the phase reversal of the stimulus used can be clearly distinguished in the spectrum of an EEG; this makes detection of SSVEP stimuli relatively easy. SSVEP has proved to be successful within many BCI systems. This is due to several factors, the signal elicited is measurable in as large a population as the transient VEP and blink movement and electrocardiographic artefacts do not affect the frequencies monitored. In addition, the SSVEP signal is exceptionally robust; the topographic organization of the primary visual cortex is such that a broader area obtains afferents from the central or fovial region of the visual field. SSVEP does have several problems however. As SSVEPs use flashing stimuli to infer a user's intent, the user must gaze at one of the flashing or iterating symbols in order to interact with the system. It is, therefore, likely that the symbols could become irritating and uncomfortable to use during longer play sessions, which can often last more than an hour which may not be an ideal gameplay.\nAnother type of VEP used with applications is the P300 potential. The P300 event-related potential is a positive peak in the EEG that occurs at roughly 300 ms after the appearance of a target stimulus (a stimulus for which the user is waiting or seeking) or oddball stimuli. The P300 amplitude decreases as the target stimuli and the ignored stimuli grow more similar.The P300 is thought to be related to a higher level attention process or an orienting response using P300 as a control scheme has the advantage of the participant only having to attend limited training sessions. The first application to use the P300 model was the P300 matrix. Within this system, a subject would choose a letter from a grid of 6 by 6 letters and numbers. The rows and columns of the grid flashed sequentially and every time the selected \"choice letter\" was illuminated the user's P300 was (potentially) elicited. However, the communication process, at approximately 17 characters per minute, was quite slow. The P300 is a BCI that offers a discrete selection rather than a continuous control mechanism. The advantage of P300 use within games is that the player does not have to teach himself/herself how to use a completely new control system and so only has to undertake short training instances, to learn the gameplay mechanics and basic use of the BCI paradigm.\n\nSynthetic telepathy/silent communication\nIn a $6.3 million US Army initiative to invent devices for telepathic communication, Gerwin Schalk, underwritten in a $2.2 million grant, found the use of ECoG signals can discriminate the vowels and consonants embedded in spoken and imagined words, shedding light on the distinct mechanisms associated with production of vowels and consonants, and could provide the basis for brain-based communication using imagined speech.In 2002 Kevin Warwick had an array of 100 electrodes fired into his nervous system in order to link his nervous system into the Internet to investigate enhancement possibilities. With this in place Warwick successfully carried out a series of experiments. With electrodes also implanted into his wife's nervous system, they conducted the first direct electronic communication experiment between the nervous systems of two humans.Another group of researchers was able to achieve conscious brain-to-brain communication between two people separated by a distance using non-invasive technology that was in contact with the scalp of the participants. The words were encoded by binary streams using the sequences of 0's and 1's by the imaginary motor input of the person \"emitting\" the information. As the result of this experiment, pseudo-random bits of the information carried encoded words \"hola\" (\"hi\" in Spanish) and \"ciao\" (\"goodbye\" in Italian) and were transmitted mind-to-mind between humans separated by a distance, with blocked motor and sensory systems, which has low to no probability of this happening by chance.In the 1960s a researcher was successful after some training in using EEG to create Morse code using their brain alpha waves. Research funded by the US army is being conducted with the goal of allowing users to compose a message in their head, then transfer that message with just the power of thought to a particular individual. On 27 February 2013 the group with Miguel Nicolelis at Duke University and IINN-ELS successfully connected the brains of two rats with electronic interfaces that allowed them to directly share information, in the first-ever direct brain-to-brain interface.\n\nCell-culture BCIs\nResearchers have built devices to interface with neural cells and entire neural networks in cultures outside animals. As well as furthering research on animal implantable devices, experiments on cultured neural tissue have focused on building problem-solving networks, constructing basic computers and manipulating robotic devices. Research into techniques for stimulating and recording from individual neurons grown on semiconductor chips is sometimes referred to as neuroelectronics or neurochips.\nDevelopment of the first working neurochip was claimed by a Caltech team led by Jerome Pine and Michael Maher in 1997. The Caltech chip had room for 16 neurons.\nIn 2003 a team led by Theodore Berger, at the University of Southern California, started work on a neurochip designed to function as an artificial or prosthetic hippocampus. The neurochip was designed to function in rat brains and was intended as a prototype for the eventual development of higher-brain prosthesis. The hippocampus was chosen because it is thought to be the most ordered and structured part of the brain and is the most studied area. Its function is to encode experiences for storage as long-term memories elsewhere in the brain.In 2004 Thomas DeMarse at the University of Florida used a culture of 25,000 neurons taken from a rat's brain to fly a F-22 fighter jet aircraft simulator. After collection, the cortical neurons were cultured in a petri dish and rapidly began to reconnect themselves to form a living neural network. The cells were arranged over a grid of 60 electrodes and used to control the pitch and yaw functions of the simulator. The study's focus was on understanding how the human brain performs and learns computational tasks at a cellular level.\n\nCollaborative BCIs\nThe idea of combining/integrating brain signals from multiple individuals was introduced at Humanity+ @Caltech, in December 2010, by a Caltech researcher at JPL, Adrian Stoica; Stoica referred to the concept as multi-brain aggregation. A provisional patent application was filed on January 19, 2011, with the non-provisional patent following one year later. In May 2011, Yijun Wang and Tzyy-Ping Jung published, \"A Collaborative Brain-Computer Interface for Improving Human Performance\", and in January 2012 Miguel Eckstein published, \"Neural decoding of collective wisdom with multi-brain computing\". Stoica's first paper on the topic appeared in 2012, after the publication of his patent application. Given the timing of the publications between the patent and papers, Stoica, Wang & Jung, and Eckstein independently pioneered the concept, and are all considered as founders of the field. Later, Stoica would collaborate with University of Essex researchers, Riccardo Poli and Caterina Cinel. The work was continued by Poli and Cinel, and their students: Ana Matran-Fernandez, Davide Valeriani, and Saugat Bhattacharyya.\n\nEthical considerations\nSources:\n\nUser-centric issues\nLong-term effects to the user remain largely unknown\nObtaining informed consent from people who have difficulty communicating\nThe consequences of BCI technology for the quality of life of patients and their families\nHealth-related side-effects (e.g. neurofeedback of sensorimotor rhythm training is reported to affect sleep quality)\nTherapeutic applications and their potential misuse\nSafety risks\nNon-convertibility of some of the changes made to the brain\nLack of access to maintenance, repair and spare parts in case of company bankruptcy\n\nLegal and social\nIssues of accountability and responsibility: claims that the influence of BCIs overrides free will and control over sensory-motor actions, claims that cognitive intention was inaccurately translated due to a BCI malfunction.\nPersonality changes involved caused by deep-brain stimulation.\nConcerns regarding the state of becoming a \"cyborg\" - having parts of the body that are living and parts that are mechanical.\nQuestions personality: what does it mean to be a human?\nBlurring of the division between human and machine and inability to distinguish between human vs. machine-controlled actions\nUse of the technology in advanced interrogation techniques by governmental authorities\nSelective enhancement and social stratification.\nQuestions of research ethics regarding animal experimentation\nQuestions of research ethics that arise when progressing from animal experimentation to application in human subjects\nMoral questions\nMind reading and privacy\nTracking and \"tagging system\"\nMind control\nMovement control\nEmotion controlIn their current form, most BCIs are far removed from the ethical issues considered above. They are actually similar to corrective therapies in function. Clausen stated in 2009 that \"BCIs pose ethical challenges, but these are conceptually similar to those that bioethicists have addressed for other realms of therapy\". Moreover, he suggests that bioethics is well-prepared to deal with the issues that arise with BCI technologies. Haselager and colleagues pointed out that expectations of BCI efficacy and value play a great role in ethical analysis and the way BCI scientists should approach media. Furthermore, standard protocols can be implemented to ensure ethically sound informed-consent procedures with locked-in patients.\nThe case of BCIs today has parallels in medicine, as will its evolution. Similar to how pharmaceutical science began as a balance for impairments and is now used to increase focus and reduce need for sleep, BCIs will likely transform gradually from therapies to enhancements. Efforts are made inside the BCI community to create consensus on ethical guidelines for BCI research, development and dissemination. As innovation continues, ensuring equitable access to BCIs will be crucial, failing which generational inequalities can arise which can adversely affect the right to human flourishing.The ethical considerations of BCIs are essential to the development of future implanted devices. End-users, ethicists, researchers, funding agencies, physicians, corporations, and all others involved in BCI use should consider the anticipated, and unanticipated, changes that BCIs will have on human autonomy, identity, privacy, and more.\n\nLow-cost BCI-based interfaces\nRecently a number of companies have scaled back medical grade EEG technology to create inexpensive BCIs for research as well as entertainment purposes. For example, toys such as the NeuroSky and Mattel MindFlex have seen some commercial success.\n\nIn 2006 Sony patented a neural interface system allowing radio waves to affect signals in the neural cortex.\nIn 2007 NeuroSky released the first affordable consumer based EEG along with the game NeuroBoy. This was also the first large scale EEG device to use dry sensor technology.\nIn 2008 OCZ Technology developed a device for use in video games relying primarily on electromyography.\nIn 2008 Final Fantasy developer Square Enix announced that it was partnering with NeuroSky to create a game, Judecca.\nIn 2009 Mattel partnered with NeuroSky to release the Mindflex, a game that used an EEG to steer a ball through an obstacle course. It is by far the best selling consumer based EEG to date.\nIn 2009 Uncle Milton Industries partnered with NeuroSky to release the Star Wars Force Trainer, a game designed to create the illusion of possessing the Force.\nIn 2009 Emotiv released the EPOC, a 14 channel EEG device that can read 4 mental states, 13 conscious states, facial expressions, and head movements. The EPOC is the first commercial BCI to use dry sensor technology, which can be dampened with a saline solution for a better connection.\nIn November 2011 Time magazine selected \"necomimi\" produced by Neurowear as one of the best inventions of the year. The company announced that it expected to launch a consumer version of the garment, consisting of catlike ears controlled by a brain-wave reader produced by NeuroSky, in spring 2012.\nIn February 2014 They Shall Walk (a nonprofit organization fixed on constructing exoskeletons, dubbed LIFESUITs, for paraplegics and quadriplegics) began a partnership with James W. Shakarji on the development of a wireless BCI.\nIn 2016, a group of hobbyists developed an open-source BCI board that sends neural signals to the audio jack of a smartphone, dropping the cost of entry-level BCI to £20. Basic diagnostic software is available for Android devices, as well as a text entry app for Unity.\nIn 2020, NextMind released a dev kit including an EEG headset with dry electrodes at $399. The device can be played with some demo applications or developers can create their own use cases using the provided Software Development Kit.\n\nFuture directions\nA consortium consisting of 12 European partners has completed a roadmap to support the European Commission in their funding decisions for the new framework program Horizon 2020. The project, which was funded by the European Commission, started in November 2013 and published a roadmap in April 2015. A 2015 publication led by Clemens Brunner describes some of the analyses and achievements of this project, as well as the emerging Brain-Computer Interface Society. For example, this article reviewed work within this project that further defined BCIs and applications, explored recent trends, discussed ethical issues, and evaluated different directions for new BCIs.\nOther recent publications too have explored future BCI directions for new groups of disabled users (e.g.,)\n\nDisorders of consciousness (DOC)\nSome people have a disorder of consciousness (DOC). This state is defined to include people in a coma and those in a vegetative state (VS) or minimally conscious state (MCS). New BCI research seeks to help people with DOC in different ways. A key initial goal is to identify patients who can perform basic cognitive tasks, which would of course lead to a change in their diagnosis. That is, some people who are diagnosed with DOC may in fact be able to process information and make important life decisions (such as whether to seek therapy, where to live, and their views on end-of-life decisions regarding them). Some who are diagnosed with DOC die as a result of end-of-life decisions, which may be made by family members who sincerely feel this is in the patient's best interests. Given the new prospect of allowing these patients to provide their views on this decision, there would seem to be a strong ethical pressure to develop this research direction to guarantee that DOC patients are given an opportunity to decide whether they want to live.These and other articles describe new challenges and solutions to use BCI technology to help persons with DOC. One major challenge is that these patients cannot use BCIs based on vision. Hence, new tools rely on auditory and/or vibrotactile stimuli. Patients may wear headphones and/or vibrotactile stimulators placed on the wrists, neck, leg, and/or other locations. Another challenge is that patients may fade in and out of consciousness and can only communicate at certain times. This may indeed be a cause of mistaken diagnosis. Some patients may only be able to respond to physicians' requests for a few hours per day (which might not be predictable ahead of time) and thus may have been unresponsive during diagnosis. Therefore, new methods rely on tools that are easy to use in field settings, even without expert help, so family members and other people without any medical or technical background can still use them. This reduces the cost, time, need for expertise, and other burdens with DOC assessment. Automated tools can ask simple questions that patients can easily answer, such as \"Is your father named George?\" or \"Were you born in the USA?\" Automated instructions inform patients that they may convey yes or no by (for example) focusing their attention on stimuli on the right vs. left wrist. This focused attention produces reliable changes in EEG patterns that can help determine whether the patient is able to communicate. The results could be presented to physicians and therapists, which could lead to a revised diagnosis and therapy. In addition, these patients could then be provided with BCI-based communication tools that could help them convey basic needs, adjust bed position and HVAC (heating, ventilation, and air conditioning), and otherwise empower them to make major life decisions and communicate.\n\nMotor recovery\nPeople may lose some of their ability to move due to many causes, such as stroke or injury. Research in recent years has demonstrated the utility of EEG-based BCI systems in aiding motor recovery and neurorehabilitation in patients who have had a stroke. Several groups have explored systems and methods for motor recovery that include BCIs. In this approach, a BCI measures motor activity while the patient imagines or attempts movements as directed by a therapist. The BCI may provide two benefits: (1) if the BCI indicates that a patient is not imagining a movement correctly (non-compliance), then the BCI could inform the patient and therapist; and (2) rewarding feedback such as functional stimulation or the movement of a virtual avatar also depends on the patient's correct movement imagery.\nSo far, BCIs for motor recovery have relied on the EEG to measure the patient's motor imagery. However, studies have also used fMRI to study different changes in the brain as persons undergo BCI-based stroke rehab training. Imaging studies combined with EEG-based BCI systems hold promise for investigating neuroplasticity during motor recovery post-stroke. Future systems might include the fMRI and other measures for real-time control, such as functional near-infrared, probably in tandem with EEGs. Non-invasive brain stimulation has also been explored in combination with BCIs for motor recovery. In 2016, scientists out of the University of Melbourne published preclinical proof-of-concept data related to a potential brain-computer interface technology platform being developed for patients with paralysis to facilitate control of external devices such as robotic limbs, computers and exoskeletons by translating brain activity. Clinical trials are currently underway.\n\nFunctional brain mapping\nEach year, about 400,000 people undergo brain mapping during neurosurgery. This procedure is often required for people with tumors or epilepsy that do not respond to medication. During this procedure, electrodes are placed on the brain to precisely identify the locations of structures and functional areas. Patients may be awake during neurosurgery and asked to perform certain tasks, such as moving fingers or repeating words. This is necessary so that surgeons can remove only the desired tissue while sparing other regions, such as critical movement or language regions. Removing too much brain tissue can cause permanent damage, while removing too little tissue can leave the underlying condition untreated and require additional neurosurgery. Thus, there is a strong need to improve both methods and systems to map the brain as effectively as possible.\nIn several recent publications, BCI research experts and medical doctors have collaborated to explore new ways to use BCI technology to improve neurosurgical mapping. This work focuses largely on high gamma activity, which is difficult to detect with non-invasive means. Results have led to improved methods for identifying key areas for movement, language, and other functions. A recent article addressed advances in functional brain mapping and summarizes a workshop.\n\nFlexible devices\nFlexible electronics are polymers or other flexible materials (e.g. silk, pentacene, PDMS, Parylene, polyimide) that are printed with circuitry; the flexible nature of the organic background materials allowing the electronics created to bend, and the fabrication techniques used to create these devices resembles those used to create integrated circuits and microelectromechanical systems (MEMS). Flexible electronics were first developed in the 1960s and 1970s, but research interest increased in the mid-2000s.Flexible neural interfaces have been extensively tested in recent years in an effort to minimize brain tissue trauma related to mechanical mismatch between electrode and tissue. Minimizing tissue trauma could, in theory, extend the lifespan of BCIs relying on flexible electrode-tissue interfaces.\n\nNeural dust\nNeural dust is a term used to refer to millimeter-sized devices operated as wirelessly powered nerve sensors that were proposed in a 2011 paper from the University of California, Berkeley Wireless Research Center, which described both the challenges and outstanding benefits of creating a long lasting wireless BCI. In one proposed model of the neural dust sensor, the transistor model allowed for a method of separating between local field potentials and action potential \"spikes\", which would allow for a greatly diversified wealth of data acquirable from the recordings.\n\nSee also\nNotes\nReferences\nFurther reading\nBrouse, Andrew. \"A Young Person's Guide to Brainwave Music: Forty years of audio from the human EEG\". eContact! 14.2 – Biotechnological Performance Practice / Pratiques de performance biotechnologique (July 2012). Montréal: CEC.\nGupta, Cota Navin and Ramaswamy Palanappian. \"Using High-Frequency Electroencephalogram in Visual and Auditory-Based Brain-Computer Interface Designs\". eContact! 14.2 – Biotechnological Performance Practice / Pratiques de performance biotechnologique (July 2012). Montréal: CEC.\nOuzounian, Gascia. \"The Biomuse Trio in Conversation: An Interview with R. Benjamin Knapp and Eric Lyon\". eContact! 14.2 – Biotechnological Performance Practice / Pratiques de performance biotechnologique (July 2012). Montréal: CEC.\n\nExternal links\n\nThe Unlock Project",
    "CURE algorithm": "CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.\n\nDrawbacks of traditional algorithms\nThe popular K-means clustering algorithm minimizes the sum of squared errors criterion:\n\n  \n    \n      \n        E\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            k\n          \n        \n        \n          ∑\n          \n            p\n            ∈\n            \n              C\n              \n                i\n              \n            \n          \n        \n        (\n        p\n        −\n        \n          m\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle E=\\sum _{i=1}^{k}\\sum _{p\\in C_{i}}(p-m_{i})^{2},}\n  Given large differences in sizes or geometries of different clusters, the square error method could split the large clusters to minimize the square error, which is not always correct. Also, with hierarchic clustering algorithms these problems exist as none of the distance measures between clusters (\n  \n    \n      \n        \n          d\n          \n            m\n            i\n            n\n          \n        \n        ,\n        \n          d\n          \n            m\n            e\n            a\n            n\n          \n        \n      \n    \n    {\\displaystyle d_{min},d_{mean}}\n  ) tend to work with different cluster shapes.  Also the running time is high when n is large.\nThe problem with the BIRCH algorithm is that once the clusters are generated after step 3, it uses centroids of the clusters and assigns each data point to the cluster with the closest centroid. Using only the centroid to redistribute the data has problems when clusters lack uniform sizes and shapes.\n\nCURE clustering algorithm\nTo avoid the problems with non-uniform sized or shaped clusters, CURE employs a hierarchical clustering algorithm that adopts a middle ground between the centroid based and all point extremes. In CURE, a constant number c of well scattered points of a cluster are chosen and they are shrunk towards the centroid of the cluster by a fraction α. The scattered points after shrinking are used as representatives of the cluster. The clusters with the closest pair of representatives are the clusters that are merged at each step of CURE's hierarchical clustering algorithm. This enables CURE to correctly identify the clusters and makes it less sensitive to outliers.\nRunning time is O(n2 log n), making it rather expensive, and space complexity is O(n).\nThe algorithm cannot be directly applied to large databases because of the high runtime complexity. Enhancements address this requirement.\n\nRandom sampling :  random sampling supports large data sets. Generally the random sample fits in main memory. The random sampling involves a trade off between accuracy and efficiency.\nPartitioning : The basic idea is to partition the sample space into p partitions. Each partition contains n/p elements. The first pass partially clusters each partition until the final number of clusters reduces to n/pq for some constant q ≥ 1. A second clustering pass on n/q partially clusters partitions. For the second pass only the representative points are stored since the merge procedure only requires representative points of previous clusters before computing the representative points for the merged cluster. Partitioning the input reduces the execution times.\nLabeling data on disk : Given only representative points for k clusters, the remaining data points are also assigned to the clusters. For this a fraction of randomly selected representative points for each of the k clusters is chosen and data point is assigned to the cluster containing the representative point closest to it.\n\nPseudocode\nCURE (no. of points,k)\nInput : A set of points S\nOutput : k clusters\n\nFor every cluster u (each input point), in u.mean and u.rep store the mean of the points in the cluster and a set of c representative points of the cluster (initially c = 1 since each cluster has one data point). Also u.closest stores the cluster closest to u.\nAll the input points are inserted into a k-d tree T\nTreat each input point as separate cluster, compute u.closest for each u and then insert each cluster into the heap Q. (clusters are arranged in increasing order of distances between u and u.closest).\nWhile size (Q) > k\nRemove the top element of Q (say u) and merge it with its closest cluster u.closest (say v) and compute the new representative points for the merged cluster w.\nRemove u and v from T and Q.\nFor all the clusters x in Q, update x.closest and relocate x\ninsert w into Q\nrepeat\n\nAvailability\npyclustering open source library includes a Python and C++ implementation of CURE algorithm.\n\nSee also\nk-means clustering\nBFR algorithm\n\nReferences\nGuha, Sudipto; Rastogi, Rajeev; Shim, Kyuseok (1998). \"CURE: An Efficient Clustering Algorithm for Large Databases\" (PDF). Information Systems. 26 (1): 35–58. doi:10.1016/S0306-4379(01)00008-4.\nKogan, Jacob; Nicholas, Charles K.; Teboulle, M. (2006). Grouping multidimensional data: recent advances in clustering. Springer. ISBN 978-3-540-28348-5.\nTheodoridis, Sergios; Koutroumbas, Konstantinos (2006). Pattern recognition. Academic Press. pp. 572–574. ISBN 978-0-12-369531-4."
}