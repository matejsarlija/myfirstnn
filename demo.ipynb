{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial: 29sep2022  \n",
    "### revised: jul2023\n",
    "-----\n",
    "## NLP\n",
    "#### 0. ponavljanje\n",
    "#### 1. Vektorizacija\n",
    "#### 2. Obrada dataseta\n",
    "#### 3. Treniranje osnovnih modela\n",
    "#### 4. Spremanje, loading i deployanje modela putem API-ja\n",
    "#### 5. Conversational AI - Basic chatbot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "# import fasttext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Ponavljanje \n",
    "- Libraries: \n",
    "    - **NLTK** (word_tokenize, sent_tokenize, PorterStemmer, FreqDist, ...)\n",
    "    - **TextBlob** (.correct(), .words)\n",
    "    - https://www.nltk.org/book/ch02.html\n",
    "    - https://textblob.readthedocs.io/en/dev/\n",
    "    - https://www.analyticsvidhya.com/blog/2021/10/making-natural-language-processing-easy-with-textblob/ \n",
    "    - ...\n",
    "- NLP Terminologija: corpus, tokenizacija, stopwords, korjenovanje, POS tagging, lematizacija, …\n",
    "- Procesi: \n",
    "    - EDA (frekvencije riječi, gustoća rečenica, distribucija kroz dataset, izrada WordClouda, …), \n",
    "    - Feature Engineering (vektorizacija)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "#### 1.\n",
    "- Što je vektorizacija? \n",
    "    - Pretvaranje tokena, rečenica, dokumenata u vektorske reprezentacije, prigodne kao input algoritmima strojnog učenja \n",
    "\n",
    "- Bag-of-words\n",
    "- One-hot encoding\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Lipik is a town in western Slavonia, in the Požega-Slavonia County of northeastern Croatia. \n",
    "It is known for its spas, mineral water and Lipizzaner stables. \n",
    "Lipik was occupied by Ottoman forces along with several other cities in Slavonia until its liberation in 1691. \n",
    "In 1773, the warm waters of Lipik were described favorably by a Varaždin doctor. \n",
    "It continued to be used as a treatment spa for over a century, and in 1872, the first hotel was opened in the town. \n",
    "By 1920 the number of hotels grew to six. Spa treatment is still the major focus of economy for the town. \n",
    "In the late 19th and early 20th century, Lipik was part of the Požega County of the Kingdom of Croatia-Slavonia.\n",
    "'''\n",
    "\n",
    "\n",
    "corpus = text.split(' ')\n",
    "print(corpus)\n",
    "\n",
    "\n",
    "# bez interpunkcija\n",
    "import string\n",
    "corpus = text.translate(str.maketrans('', '', string.punctuation)).split(' ')\n",
    "print(corpus)\n",
    "\n",
    "\n",
    "VOCAB = len(corpus)\n",
    "print(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = text.split('.')\n",
    "print(sents)\n",
    "\n",
    "\n",
    "frekvencije = {}\n",
    "\n",
    "for sent in sents:\n",
    "    tokens = sent.split(' ')\n",
    "    for token in tokens:\n",
    "        token = token.strip().lower()\n",
    "        if token not in frekvencije.keys():\n",
    "            frekvencije[token] = 1\n",
    "        else:\n",
    "            frekvencije[token] += 1\n",
    "            \n",
    "frekvencije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vektorske reprezentacije riječi:\n",
    "\n",
    "vektori = []\n",
    "for sent in sents:\n",
    "    r_vector = []\n",
    "    tokens = [t.strip().lower() for t in sent.split(' ')]\n",
    "    for rijec in corpus:\n",
    "        if rijec in tokens:\n",
    "            r_vector.append(1)\n",
    "        else:\n",
    "            r_vector.append(0)\n",
    "    vektori.append(r_vector)\n",
    "    \n",
    "    \n",
    "print(len(vektori))\n",
    "### zašto 9?\n",
    "for i in vektori:\n",
    "    print(len(i))\n",
    "    ### zašto 122?\n",
    "\n",
    "print(vektori[0])\n",
    "\n",
    "\n",
    "\n",
    "vektori = np.asarray(vektori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency vektori, \n",
    "    - https://towardsdatascience.com/the-magic-behind-embedding-models-part-1-974d539f21fd\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- One-hot vektori \n",
    "    - One-hot vektori su pogodni za prikazivanje pojedinih tokena, kao i za prikaz cijelih rečenica ili dokumenata\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "- bez korištenja \"stopwordsa\"\n",
    "- **problemi?**\n",
    "    - \"sparse vektori\" - previše nula\n",
    "    - memorijski zahtjevni\n",
    "    - imaju problem s promijenjenim redom u rečenici\n",
    "    - ne računaju na semantičku distribuciju, tj uzorkovanost teksta na temelju njegovog značenja\n",
    "-------------\n",
    "- TF-IDF\n",
    "    - na logaritamskoj skali računa koje riječi se pojavljuju rjeđe, a koje češće te \"izjednačuje\" njihove vrijednosti tako da rjeđima daje veću \"težinu\"\n",
    "\n",
    "- https://machinelearningmastery.com/gentle-introduction-bag-words-model/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(sents)\n",
    "\n",
    "tokenizer.index_word\n",
    "\n",
    "tokenizer.texts_to_matrix(sents, mode=\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### bolje(novije) opcije - Word2vec algoritmi\n",
    "- https://arxiv.org/pdf/1301.3781.pdf \n",
    "- **CBOW**\n",
    "- **Skip-gram**\n",
    "\n",
    "![cbow&skip-gram](data/1_HmmFCZpKk3i4EvMYZ855tg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- najpoznatiji algoritmi za treniranje \"word embeddinga\", tj vektorskih značajki\n",
    "    - neuronske mreže od dva layera\n",
    "    - input je korpus \n",
    "    - output je set vektora\n",
    "    \n",
    "- skip-gram koristi riječ kako bi predvidio \"target\" context.\n",
    "    - vektori na izlazu iz skip-grama jako dobro hvataju semantičke relacije između riječi\n",
    "    - ![cbow&skip-gram](data/1_jpnKO5X0Ii8PVdQYFO2z1Q.png)\n",
    "\n",
    "- CBOW koristi kontekst kako bi predvidio \"target\" riječ\n",
    "    - svi primjeri s target riječima su feedani u mrežu\n",
    "    - prosjek vrijednosti u hidden layeru će biti vektor koji predstavlja našu target riječ\n",
    "---------------\n",
    "- Primjer: \"Lipik je divan grad\"\n",
    "        - U skip-gramu bi input bio divan, a output: 'Lipik', 'je', 'grad'\n",
    "        - kod CBOW-a bi input bio 'Lipik', 'je', 'grad', a on s određenom vjerojatnosti vraća target riječi, među kojima je \"divan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
    "- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/  \n",
    "- https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92 (+ T-SNE)\n",
    "- https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8 - kako od nule napraviti svoje embeddinge pomoću 2-slojne Neuronske mreže\n",
    "\n",
    "### Libraryji koji se koriste za izradu Word2vec reprezentacija teksta:\n",
    "- gensim (https://radimrehurek.com/gensim/)\n",
    "- fasttext (https://fasttext.cc/)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. kreiramo korpus tekstova/rečenica/tokena\n",
    "\n",
    "gensim_corpus = [[\n",
    "    token for token in sent.strip().lower().split() if len(sent) > 0\n",
    "    ] for sent in text.split('.')]\n",
    "import pprint\n",
    "pprint.pprint(gensim_corpus)\n",
    "\n",
    "\n",
    "# sentences = MyCorpus()\n",
    "# model = gensim.models.Word2Vec(sentences=gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. kreiramo rječnik - on sadrži sve moguće riječi koje će naš pipeline znati\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. s obzirom da imamo 75 unique tokena u rječniku, svaki token će dobiti unique id od 0 do 74\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instanciramo i treniramo model kojem su input liste rečenice koje sadrže liste tokena (matrice)\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=gensim_corpus,\n",
    "    vector_size=50,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    sg=1\n",
    ")\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-parameters\n",
    "\n",
    "\n",
    "lipik_vektor = model.wv['lipik']\n",
    "print(lipik_vektor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('lipik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('lipik'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Što ako tražimo riječ koja nije bila prisutna u modelu ??\n",
    "\n",
    "print(model.wv.most_similar('village'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fasttext je razvijen 2016. u Facebooku kao ekstenzija Word2Vec algoritama \n",
    "    - razbija sve tokene na \"character n-grame\" (podriječi) \n",
    "    - to mu omogućuje da bolje reprezentira riječi koje nije sreo u trening datasetu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText # (=character n-gram algoritam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spremanje modela\n",
    "\n",
    "# model.wv.save_word2vec_format('model.bin')\n",
    "# model.wv.save_word2vec_format('model.txt', binary=False)\n",
    "\n",
    "\n",
    "### druge vrste modela:\n",
    "# 1. LDA\n",
    "    # - https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#:~:text=In%20natural%20language%20processing%2C%20the,of%20the%20data%20are%20similar.\n",
    "    # - https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "    # - https://radimrehurek.com/gensim/auto_examples/tutorials/run_ensemblelda.html\n",
    "# 2. Doc2Vec\n",
    "    # https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#introducing-paragraph-vector\n",
    "        # nije isto što i prosjek svih word2vec vektora!!\n",
    "# 3. Latent Semantic Analysis\n",
    "    # - https://en.wikipedia.org/wiki/Latent_semantic_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### primjer vizualizacije ###\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    close_words = model.wv.most_similar(word)\n",
    "    print(type(close_words))\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "\n",
    "display_closestwords_tsnescatterplot(model, 'lipik', 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "- **Transfer learning**\n",
    "    - ekstrakcija \"znanja\" iz nekog drugog izvora i primjena na naš problem/dataset\n",
    "        - znanje = značajke\n",
    "        - znanje = data\n",
    "        - znanje = metadata u ML-u\n",
    "    - omogućuje primjenu istreniranih modela na drugačiji, sličan ili isti problem (ovisno o zadatku kojeg rješavamo)\n",
    "    - https://ruder.io/state-of-transfer-learning-in-nlp/ \n",
    "\n",
    "    - najpoznatiji Word2Vec modeli: \n",
    "        - https://nlp.stanford.edu/projects/glove/ \n",
    "        - GoogleNews-vectors-negative300.bin (https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz / https://figshare.com/articles/dataset/GoogleNews-vectors-negative300_bin/6007688)\n",
    "    \n",
    "    - današnji SOTA nlp modeli:\n",
    "        - transformeri (Bert (bertić), GPT, XLNet)\n",
    "        - https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# python -m spacy download en_core_web_lg # md # sm\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "### AKO NE PROĐE:\n",
    "## aktivirati environment i upisati naredbu:\n",
    "    # \"python -m spacy download en_core_web_lg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uzimamo tekst o lipiku s početka\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spacy omogućuje niz lingvističkih podataka po tokenu, rečenici ili cijelim dokumentima\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VEKTORI \n",
    "vectors = []\n",
    "for sent in doc.sents:\n",
    "    s = []\n",
    "    for token in sent:\n",
    "        # print(token.vector.shape)\n",
    "        if token.is_oov:\n",
    "            print(token.text)\n",
    "            token.vector == np.ones((300,), dtype=np.float32)\n",
    "        vector = token.vector\n",
    "        s.append(vector)\n",
    "    vectors.append(s)\n",
    "\n",
    "# print(len(vectors[0][5]))\n",
    "# print(vectors[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pretrained NER oznake\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity \n",
    "\n",
    "- Koliko su dvije riječi međusobno slične/različite? \n",
    "- Koje su sve moguće metode računanja sličnosti među riječima? \n",
    "\n",
    "-----------------\n",
    "- Koji bi bili načini računanja sličnosti između dva dokumenta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sciencedirect.com/topics/computer-science/cosine-similarity#:~:text=Cosine%20similarity%20measures%20the%20similarity,in%20roughly%20the%20same%20direction.&text=Thus%2C%20each%20document%20is%20an,called%20a%20term%2Dfrequency%20vector. \n",
    "# https://www.sciencedirect.com/topics/computer-science/euclidean-distance \n",
    "\n",
    "from scipy import spatial\n",
    "def get_cosine_similarity(*word_pairs, nlp_pipeline):\n",
    "    # izvor: https://towardsdatascience.com/a-little-spacy-food-for-thought-easy-to-use-nlp-framework-97cbcc81f977\n",
    "    cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "    for pair in word_pairs:\n",
    "        print(\"{} AND {}: \".format(pair[0], pair[1]), cosine_similarity(nlp_pipeline.vocab[pair[0]].vector, nlp_pipeline.vocab[pair[1]].vector))\n",
    "    # print(\"apple vs banana: \", cosine_similarity(nlp.vocab[\"apple\"].vector, nlp.vocab[\"banana\"].vector))\n",
    "    # print(\"car vs banana: \", cosine_similarity(nlp.vocab[\"car\"].vector, nlp.vocab[\"banana\"].vector))\n",
    "    # print(\"car vs bus: \", cosine_similarity(nlp.vocab[\"car\"].vector, nlp.vocab[\"bus\"].vector))\n",
    "    # print(\"tomatos vs banana: \", cosine_similarity(nlp.vocab[\"tomatos\"].vector, nlp.vocab[\"banana\"].vector))\n",
    "    # print(\"tomatos vs cucumber: \", cosine_similarity(nlp.vocab[\"tomatos\"].vector, nlp.vocab[\"cucumber\"].vector))\n",
    "\n",
    "\n",
    "get_cosine_similarity(\n",
    "        (\"apple\", \"banana\"),\n",
    "        (\"Slavonija\", \"Croatia\"),\n",
    "        (\"spa\", \"water\"),\n",
    "        (\"spa\", \"mineral\"),\n",
    "        (\"water\", \"mineral\"),\n",
    "        (\"Lipik\", \"Pakrac\"),\n",
    "        (\"Lipik\", \"Croatia\"),\n",
    "        nlp_pipeline = nlp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "man = nlp.vocab[\"man\"].vector\n",
    "woman = nlp.vocab[\"woman\"].vector\n",
    "queen = nlp.vocab[\"queen\"].vector\n",
    "king = nlp.vocab[\"king\"].vector\n",
    "calculated_king = man - woman + queen\n",
    "print(\"similarity between our calculated king vector and real king vector:\", cosine_similarity(calculated_king, king))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spacy nema \".most_similar() ili .similarity() metodu koja iz skupa podataka izvlači udaljenost točaka po Euklidu ili kosinusu\"\n",
    "\n",
    "\n",
    "### ne radi - need to fix def :) \n",
    "def get_most_similar(nlp_pipeline, word, limit=7):\n",
    "    word = nlp_pipeline.vocab[str(word)]\n",
    "    queries = [\n",
    "        w for w in word.vocab \n",
    "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "    ]\n",
    "    print(queries)\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:limit+1] if w.lower_ != word.lower_]\n",
    "\n",
    "# get_most_similar(nlp, \"Lipik\")\n",
    "# get_most_similar(nlp, \"Croatia\")\n",
    "# get_most_similar(nlp, \"Slavonija\")\n",
    "# get_most_similar(nlp, \"Lipizzaner\")\n",
    "get_most_similar(nlp, \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "Word2Vec_embeddings = os.path.join(os.getcwd(), 'models', 'GoogleNews-vectors-negative300.bin')\n",
    "w2v_model = KeyedVectors.load_word2vec_format(Word2Vec_embeddings, binary=True)\n",
    "\n",
    "embeddings = []\n",
    "for t in tokens:\n",
    "    try:\n",
    "        # https://radimrehurek.com/gensim_3.8.3/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "        embeddings.append(w2v_model.get_vector(t)) \n",
    "    except KeyError:\n",
    "        # embeddings.append(get_placeholder_vector(300))\n",
    "        continue\n",
    "\n",
    "\n",
    "# print(cosine_similarity(calculated_king, king))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using GloVe (stanford university)\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "filename = 'glove.6B.50d.txt'\n",
    "\n",
    "GloVe_embeddings = os.path.join(os.getcwd(), 'models', 'glove.6B', filename)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GloVe_embeddings, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        # print(coefs.shape)\n",
    "        vector_size = coefs.shape[0]\n",
    "        embeddings_index[word.strip().lower()] = coefs\n",
    "    print(\"Loaded %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "tokens = text.split(' ')\n",
    "embeddings = []\n",
    "\n",
    "for token in tokens:\n",
    "    t = token.strip().lower()\n",
    "    try:\n",
    "        embeddings.append(embeddings_index[t])\n",
    "    except KeyError:\n",
    "        # embeddings.append(get_placeholder_vector(vector_size))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASKS (if the semantic similarity and unsupervised approach to text analysis has not been clear): \n",
    "##### Textual (dis)similarity \n",
    "1. Given a new random article, to which article from an existing dataset is it most similar to?\n",
    "2. Performing a cluster analysis, divide a dataset into n clusters (use your ds skills and elbow method to decide which one is the best) -> define semantic classes of those articles\n",
    "3. Given a new random article, what class does this article belong to ? \n",
    "\n",
    "##### How-To:\n",
    "1. Get 1000 wikipedia articles (choose a topic about an AI field, or 3 AI fields)\n",
    "- use web scraping, no existing Kaggle or wikipedia dataset :) \n",
    "\n",
    "2. Model training\n",
    "    1. Don't train a model, use TF-IDF, CBOW, Jaccard similarity and Levenshtein distance (yes)\n",
    "    2. Train an unsupervised model on all the data and start using cosine similarity between both words and documents\n",
    "    3. Perform topic modelling with gensim --> use this data to both classify texts and perform similarity\n",
    "    4. Use TextRank or Rake to perform \"keyword extraction\" --> Does this work better on your dataset ?\n",
    "- see https://www.analyticsvidhya.com/blog/2022/01/four-of-the-easiest-and-most-effective-methods-of-keyword-extraction-from-a-single-text-using-python/ for kw extraction info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f5aae51581d997e20bdb25cb45c530a62d731a46244b83aceebe968b2766960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
